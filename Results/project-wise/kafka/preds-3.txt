Nit: insert `<p>` tag to actually get the new paragraph rendered. Nit: `Topology -> `{@link Topology}` It's not really clear what "deterministic" means. We should elaborate more.
`incompatible runtimes and unexpected results` -> `incompatible runtime code and unexpected results or errors.`
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
nit: {@link KGroupedTable}
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
Btw, we should take the chance and make `version` and `commitId` final. Something like: ```java private static final String version; private static final String commitId; static { Properties props = new Properties(); try (InputStream resourceStream = AppInfoParser.class.getResourceAsStream("/kafka/kafka-version.properties")) { props.load(resourceStream); } catch (Exception e) { log.warn("Error while loading kafka-version.properties :" + e.getMessage()); } version = props.getProperty("version", "unknown").trim(); commitId = props.getProperty("commitId", "unknown").trim(); } ```
looks like this is not passed to Metrics object. we can use reporter instance at below line.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
nit: move to line above.
If not, we should move the exception capturing logic inside the dbAccessor as well.
nit: 'else' can be dropped
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
none from what I can see, but I'm not sure it's worth holding up the PR for it.
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
This class is public API, so we cannot remove `setTimestamp` but can only deprecate it. We also need to update the KIP to mention the deprecation and the newly added methods.
I think we can.
Hmm. I feel the `final` would be worth capitalizing the var name.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
I was going to leave this, but since there's one other change to be made, I think you can just do: ```java recordsLag.add(this.metrics.metricName(name + "-max", ``` And similarly for `avg`. That avoids having to compute `toString` on the topic partition again.
We can use the `replace` overload that takes a `char`.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
I was using this test to print the topology and it shows two sub topologies while it should be one (seems the reason is that you use the same `StreamsBuilder` as in `setup()` method. Also, the naming of the operators seems to be incorrect. Also wondering if `KGroupedStream#cogroup()` needs on overload that takes a `Named` parameter? Maybe not, but the specified `Named` from `aggregate()` would need to be used for other processors, too. Atm there is this weird `COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test` ``` Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 (topics: [topic]) --> none Sub-topology: 1 Source: KSTREAM-SOURCE-0000000001 (topics: [one]) --> COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> test <-- KSTREAM-SOURCE-0000000001 Processor: test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> KTABLE-TOSTREAM-0000000005 <-- COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: KTABLE-TOSTREAM-0000000005 (stores: []) --> KSTREAM-SINK-0000000006 <-- test Sink: KSTREAM-SINK-0000000006 (topic: output) <-- KTABLE-TOSTREAM-0000000005 ```
nit: why not `k2` ? Should we use `A`, `B`, `C`, `D` for the values to make it easier to understand the expected result? It's unclear which A is which below.
`WithOverlappingKeys` (or `WithSharedKeys`)
We should also mention somewhere that we do not support concurrent transactions.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
```suggestion * Options for {@link Admin#electLeaders(ElectionType, Set, ElectLeadersOptions)}. ```
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
Just want to check my understanding. The user may attempt to commit offsets while the broker has a JoinGroup in purgatory. In this case, we would send the older generation which would be doomed to fail with `ILLEGAL_GENERATION` once the join completes. In this case, should we still reset the generation as we do below? I am wondering if it is useful to remember the generation that an offset commit was sent with (perhaps inside `OffsetCommitCompletion`) so that we only reset if necessary.
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
Detail: just to be sure, I would initialize it to `this.generation`, to make sure the generation always increments.
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
nit: should be removed (similar below)
Should require non-null for `fetchPosition`
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
Can you please elaborate why we no longer read the header during construction? It seems to me that `checkHC` could be a constructor parameter and then we could keep it as a private and final variable and less changes would be required. But maybe I am missing something. Note that public and mutable variables are generally avoided in Java.
Both `GZipInputStream` and `SnappyInputStream` read the header in the constructor, so it would make sense to me to remain consistent in that respect.
All these tests could also be parameterized - would be a lot less code, but would likely need reflection to look up the constructor.
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
We're returning the collection directly here which violates the synchronization. Actually there's some inconsistency between `groupSubscription` and `subscription`. The latter is effectively immutable in the sense that we do not update the set once it is created. Perhaps we can do the same for `groupSubscription` which would fix this problem.
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
Add to the end, "as long as they still match the subscribed pattern"
> Just clarifying: After the group has formed, both leader and follower can still trigger a rebalance: leader will trigger a rebalance if the existing topics number of partitions has changed (including the topic is deleted); follower will trigger a rebalance if the subscription has changed (both due to a metadata refresh with regex pattern or user called subscribe again). Is that right? Yes, right. > And if we change the consumer coordinator to allow passing regex to the leader, I think joinedSubscription can be removed completely and only leader need to trigger rebalances unless users call subscribe again on any of the consumer member, is that right? The leader will still have to deal with the potential for a metadata update during a rebalance, so I'm not sure we can remove `joinedSubscription`. At least we won't need this funky logic to try to change `joinedSubscription` after the rebalance though.
I'm not sure this works. The purpose of the `joinedSubscription` field is to remember the exact list of topics that were used when joining the group. If a metadata update arrives after the rebalance has begun, then we can notice the fact that the joined subscription does not match the current subscription and we can trigger another rebalance. With this change, we will no longer be able to detect this case, which means that consumption from a topic matching the subscribed regex will be delayed (perhaps indefinitely). It seems the behavior we want is to only add to `joinedSubscription` those topics which were added to the assignment by the leader.
Strictly speaking, this shouldn't be necessary as `SCHEMA_TYPE_CLASSES` should have a `Schema` instance for all `Schema.Type` literals. And with `SchemaBuilder` a connector or converter cannot create a schema instance with a null `Schema.Type`. However, it is possible to construct a `ConnectSchema` instance with a null `Type` reference (like what `FakeSchema` essentially does in the existing test), which of course without this change would result in this method returning a null list. So +1 for this line change since it simplifies the error handling in the calling code.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
I miss @shikhar!
Much better name :)
We lack unit test coverage for this case
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
- It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm. - And we know that KafkaStreams never writes headers into changelog topics. Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case. We can also do a version check as suggested by Guozhang.
I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content. Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
If I understand correct, a record read from the changelog topic should only be: 1) having no headers at all (old version) 2) having a singleton header with `v --> byte`. All other cases should indicate a bug in the code. So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value: 1) byte == 1: do as below. 2) otherwise: to not support forward compatibility, we can just throw unsupported.
not used: can be removed
nit: remove (was tested already)
nit: remove -- not used
We avoid using time-based operations in integrations since they usually leads to flaky tests. Consider using `TestUtils.waitForCondition()` with a timeout.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
nit: move .collect to new line
Instead of using `.format` and `+` to create the string, maybe use same way as `cmd` is constructed (using % to format, and multiline string without `+` but by ending with `\`)
Unless I'm wrong, we move to this directory and that's where we execute all the rest of the commands (such as the echos in output files below). Just want to make sure this is what we want (which looks like it is)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
Shouldn't you verify if topology 1 still produces output records at this point? When I read the test name I would expect that verification here.
Nice coverage with different num.partitions, thanks!
nit: insert space `String... expected`
For create / destroy maybe that's okay, but `process` is at the very critical path so we have to be careful not to incur any overhead.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
nit: add `final`
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
as above: we need to remove adminPrefix configs
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: blank line missing here
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
validateStoreOpen() can be outside of lock block.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
This statement is always false.
as above: we need to remove adminPrefix configs
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
I'd probably just `return error` here (I'm not a fan of `return`, but `break` isn't much better and I assume you wrote it like this to avoid a call to `getSuperclass()` in the very common case where we find the `Errors` on the first attempt). While I am nitpicking: space after `if` is missing.
It may be worth explaining what happens if both a subclass and superclass have a mapping (the subclass mapping is used).
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
nit: full-stop after the description.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
Could use `equalsIgnoreCase` directly.
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Ack, I get it now. Thanks for clarifying.
The other constructor calls the parameter `sampledStat`. We should be consistent.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
typo: `per reach record`
as above nit: double space `to Kafka`
nit: forward. EDIT: I realized it may be inherited from the overloaded function, we could fix both.
The `CachingKeyValueStore` doesn't throw an NPE here, rather `org.apache.kafka.common.utils.Bytes.LexicographicByteArrayComparator.compare` does. We probably should add `Objects.requireNonNull(...)` to `CachingKeyValueStore#put(..)` etc
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
You can remove this `assertThat` and the loop below as you've already proven this is true in the test above. So no need to assert it again.
Ok, it's your call. I think this might make the tests flaky, but I guess we can figure that out later.
How about also augment the built `Topology` with processor APIs to add more processors, and check that as long as we do not call addStores the resulted topology would still be stateless.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Maybe just check that `minikdc` is not None here
Does it make sense to set `self.kdc` in the constructor? And then having non-None self.kdc would be part of the logic in `has_sasl_kerberos`
If we're adding all of one list to another list, we should use `extend`, e.g. ```python sasl_mechanisms.extend(self.additional_sasl_mechanisms) ```
Instead of pulling the value out with a regex, what do you think of `streamsString.contains("appId")`. Although what you have works as well.
Is it really worth having this catch here? I think it's best to just let the exception propagate. Any method under test can throw an unknown exception after all.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
`joinThisName` is used in the store of `thisWindow` which is to be queried by the other stream, so my personal understanding is that: 1. for inner-join (`rightOuter` = false, `leftOuter` = false): both window-store has `JOINTHIS_NAME-store`. 2. for outer-join (`rightOuter` = true, `leftOuter` = true): both window-store has `OUTERTHIS_NAME-store`. 3. for left-join (`rightOuter` = false, `leftOuter` = true): the left window-store `THIS_NAME-store` and the right window-store `OUTERTHIS_NAME-store`, since we will join with `null` if the right window-store returns null (hence "outer"), but not vice-versa.
Sounds good. We can consider this resolved.
This should not have the `sink` suffix
Just curious, could we possibly call this function for the same node more than once? It seems yes as you are checking `!keyChangingOperationsToOptimizableRepartitionNodes.containsKey(node)` here, but I cannot tell from the code...
Generally speaking we should not rely on the caller to pass in parameters that are guaranteed to no pass the check. What I suggested (below) is to have a slightly modified recursion pattern which do not rely that the first caller would never satisfy the predicate.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
nit: ```newPosition``` can be created lazy.
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Another reason for having these classes in common (i.e. KAFKA-5265) is that they can potentially be used by the Authorizer interface when we move it to Java.
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
You might consider using `OptionalDouble`.
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
This is not necessary, since the for loop below would be a no-op.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
Shouldn't need this line, it's handled by the superclass's constructor.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Here it is better to use to distinct records, because if the code contains a bug that adds a record twice, you would not discover it.
nit: remove empty line
Add a check to verify, whether the iterator has no more elements.
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We can use `StringDeserializer.class.getName()`
nit: add `final`
nit: add `final`
nit: add `final`
By the way, I wonder if we should just say it should be idempotent? Seems redundant to mention KafkaProducer.
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
Note this correction
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
We seem to have lost the `info` message from the original.
Let's call this "StdoutMonitor" since that makes it more clear what it is doing. We may want to pass things other than status eventually. Also, the instance variable is called `stdoutMonitor`, which suggests that this is a better description.
How about just ``` log.error("{}: (stderr):{}", id, line); ```
Please add the exception at the end of the line so that we can get a stack trace, including any "cause" exceptions. Example: ``` log.error("{}: Failed to start the external process.", id, e); ```
No need to reorder imports.
No need to reorder imports.
Nit: let's avoid moving all of these imports around. Our convention is to place the `java` and static imports at the end, and moving them unnecessarily just complicates maintenance.
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
Could we rename this to something like "remainingPartitions"
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
nit: remove `this` (not required)
As mentioned on the KIP discussion, `BytesDeserializer` should not be included.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
Maybe we can just a better name for `path` since it makes this code look suspicious.
@hachikuji Yes, that sounds good to me. It should reduce heap usage when there are lots of requests requiring down-conversions while still avoiding blocking the network thread.
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
I like this parity check. :+1:
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
Oh you mean `UnsupportedForMessageFormatException`? That doesn't seem to be added.
It seems to be added on line 703.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
why removing this line? this test is used to make sure we can't initialize the `recorder` multiple times with different task id.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
cc @mjsax as well, LMK WDYT.
Since condition is just a comparison, you can put the comparison here directly
This can be moved to ConsumerRecords class
nit: when records2 is empty, you can return immediately.
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
As above: use `assertThrows` and verify error message
While existing test are written this way, we try to move off this pattern and not use the `expected` annotation. Instead, we should use `assertThrows` and also verify the exception error message.
nit: 4-space indention plus move `builder` down one line
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
@hachikuji Why don't we catch the exception here? If the task is being stopped, a wakeupexption is kind of noise in log especially for the application which needs to monitor error log.
Is this log line needed? Seems like we get all this info in `onCommitCompleted`.
I think we're missing a `{}`. Same in `doCommitAsync`.
If we throw InvalidTopicException directly, be change public API (possible exception are part of the API) and thus, this would require a KIP.
Just throwing on the first is probably fine. Alternatively, if you want to list them all, I'd suggest iterating through them and collecting them into a collection rather than using suppressed exceptions.
Yeah, it's a good question. `IllegalArgumentException` doesn't feel quite right. Another option would be `IllegalStateException`. Also, we should probably mention the configuration property in the exception message.
need a check for null on `obj` here as well
A common pattern for classes like this without any state is to create a static instance. ```java public static final UnknownAddressSpec INSTANCE = new UnknownAddressSpec(); ```
nit: unneeded parenthesis
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
It looks like this is not used anywhere
We can use `List<Class<? extends Connector>` to avoid the warning
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
`windowSize` should be `Duration`
original was better
nit: "another thread wrote to ..."
original was better
I meant to have "Note that enabling idempotence requires this config..." before "Allowing retries...". And break the two parts with a paragraph.
Hello and thanks for reviewing! The callout about record ordering was originally requested within Confluent; the same is mentioned at this page: https://developer.confluent.io/tutorials/message-ordering/kafka.html. We think this would also benefit AK docs.
maybe `then the records in the second batch` is a bit better
Hmm, normally `IllegalArgumentException` indicates that the argument to a function is bogus, right? That's not really the case here-- the function argument was fine, but the topic wasn't set up correctly. This can probably just be a generic `RuntimeException`, since we don't have a need to make it something fancier.
Thanks for the follow-up.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
That's right, changed it locally.
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
nit: `{@link KeyQueryMetadata}`
nit: line too long. `final` not required -- a static method cannot be overwritten anyway
`function` -> `method` ? `{@code null}`
We normally use `assertThat()` in new and refactored code. Please also change the other occurrences. ```suggestion assertThat(hasStateTransition(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING), is(true)); ```
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
The order is not really that important here, either way works
I would prefer defaulting to range just for consistency. We have seen similar cases in the producer where the behavior of partitioner's hashing function changes a bit, causing offset manager migrated for mirror-makers and hence resetting offset and data duplicates.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
We avoid using time-based operations in integrations since they usually leads to flaky tests. Consider using `TestUtils.waitForCondition()` with a timeout.
nit: move .collect to new line
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
We are tracking the LEO in two places: 1. In `ReplicatedLog::endOffset`. This gets increase every time the log gets appended: https://github.com/apache/kafka/blob/28ee656081d5b7984c324c3ea3fc9c34614d17db/core/src/main/scala/kafka/log/Log.scala#L1302 2. The `LeaderState` also stores what is now the LEO. One suggestion is for `LeaderState` to instead store the "flush offsets". In `LeaderState` the follower's flush offset is the LEO but for the local replica the "flush offset" may not be the LEO. An example of the high-watermark increasing but the LEO not changing: 1. follower: LEO = 10 2. leader: LEO = 100, FlushOffset = 100, HW = 0 Follower successfully fetches for offset 10 => Leader: LEO = 100, FlushOffset = 100, HW = 10. Follower successfully fetches for offset 20 => Leader: LEO = 100, FlushOffset = 100, HW = 20. In this example if the leader already flushed to the LEO then there is no need to flush again when increasing the HW.
The invariant that the leader most satisfy is that the `highWatermark <= flushOffset`. The current implementation satisfies this by flushing after every append and implicitly defining `flushOffset == logEndOffset`. At a high-level, I think the goals is to allow `highWatermark <= flushOffset <= logEndOffset`. On the follower, things are a little different. On the follower the `flushOffset == logEndOffset` before a `Fetch` request can be sent. This is because the leader assumes that the fetch offset in the `Fetch` request is the offset that the follower has successfully replicated. The advantage of appending without flushing as soon as possible replication latency. The leader cannot replicate record batches to the followers and observers until they have been appended to the log. I am not exactly sure how exactly we want to implement this since I haven't looked at the details but I think you are correct that on the leader side of things we want to increase the `flushOffset` in the `Fetch` request handling code as the leader attempts to increase the high-watermark.
Got it. For future readers the function is `maybeCompleteShutdown`.
request "got" re-sent to the control
Should be larger
Typo: should be "or larger than the number of available brokers"
nit: `isolate ... form other client configs` -> ``` override ... for the main consumer client from the general consumer client configs. The override precedence is the following (from highest to lowest precedence): 1. main.consumer.[config-name] 2. consumer.[config-name] 3. [config-name] ``` Ditto below for other two.
Nit: it's also to distinguish from other client configs -- not just consumer configs.
recommended; ditto below.
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
Should we have a separate `UnsupportedVersionException` for multiple endpoints? For example, we could have multiple plaintext endpoints.
nit: we can use `map#compute` to replace getOrDefault + put.
nit: align parameters.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
We should update the Scala `TestUtils` to call this method.
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
typo: byteArrray -> byteArray
`< Callback >` this explicit type is not necessary.
nit: we could split this lone line by different key, value by new line to make it clear. ex: ``` String[] args = new String[] { "--topic", "Hello-Kafka", "--num-records", "5", .... }; ``` Same as below.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
`rebalancing()` should never throw an `InvalidStateStoreException` as it is just constructing the `CompositeReadOnlyKeyValueStore` wrapper. The underlying stores should not be accessed until `get`, `range`, or `all` are called. So, i think this is safe to leave it as it is
typo: Woth -> With
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
I think at the moment, we should never get here, so `IllegalStateException` is fine.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
You are right. Never mind.
Can we not do: ```java if (appendResult != null) return appendResult; else { ... } ```
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
We lack unit test coverage for this case
Not sure. PENDING_SHUTDOWN indicates a clean shutdown while this lets the thread fail.
The fallback should be taken from `StreamsConfig` and not be earliest all the time.
We should add `else` and throw `StreamsException` with cause using original `NoOffsetForPartitionException` (ie, `ex`). Furthermore, the error message should explain in detail what happened and how a user can fit it. Something like: ``` No valid committed offset found for input topic T (partition P) and no valid reset policy configured. You need to set configuration parameter "auto.offset.reset" or specify a topic specific reset policy via KStreamBuilder#stream(...) or KStreamBuilder#table(...). ``` The reason for throwing an exception is, that Streams has no change to start reading a topic/partition in a meaningful way. The user needs to fix this issue! This is also current behavior, because `NoOffsetForPartitionException` would be raise for `auto.offset.reset=none` and missing committed offsets, too.
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later? It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
Are we intentionally not logging the exception as the extra parameter? If the exception wraps a more useful exception, we won't see any information about the wrapped exception unless we can see the stack trace in the warning log message.
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
Nit: maybe `("Topic: " + topic)`
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Sounds right to me.
@eliaslevy Since this part is covered in other unit test case, we want to remove redundant coverage to leave the unit test as succinct as possible.
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
nit: preserve empty line after `checkAndClearProcessResult`
nit: it would improve readability to factor out some functions for some of the work here. Here we can have a separate function with a nice name for building the assignments
nit: `Short.MAX_VALUE` is 32767
I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
nit: simplify `InterruptedException, IOException` to `Exception`
nit: just simplify to `throws Exception`
This is unnecessary as junit always create a new test class for each test case.
super nit: no `.` at the end or start sentence with `[T]he` :)
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
This is not a feedback: as we are changing the main loop of StreamThread, we may need to carefully benchmark if this change along with the main loop changes will have unexpected performance penalty: with low traffic input stream, we are effectively sending sync commit requests more frequently. cc @mjsax If it does become a problem for performance, we could consider making the commit request async, and consider a commit only completed after the commit response is returned. Of course it means more complicated logic.
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
This part and the line 525-529 below can be extracted out of if condition.
We should not include this along with the unit tests since it's not a unit test.
Is this line intentional? Unit tests normally don't need to print out to console.
Could also be `final`
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
Nit: let's not add an unnecessary extra line.
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
records to it, and reading all records from it, such that
Would it make sense to validate here that idx not in self.nodes_clean_shutdown? (although we might want a set instead of a list). Seems like it should be an error to receive a valid message after receiving a "shutdown_complete" message
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
I'm not sure in this. So in case of two properties (k1=v1 and k2=v2) do you generate the `--consumer-property k1=v1 --consumer-property k2=v2` string? The repeated property didn't work when I tried it out and it only picked up the first one. I think you need to pass `--consumer-property "k1=v1,k2=v2"` as with some other commands. In case the "k1=v1,k2=v2" format is needed, there is a nice one-liner way to do it: ``` ','.join("%s=%r" % (key,val) for (key,val) in k.iteritems()) ```
This is two methods calls, thus it should not be a one liner (test would pass if `getStateStore()` would throw `UnsupportedOperationException`. Should be: ``` @Test public void shouldNotAllowInit() { final StateStore store = globalContext.getStateStore(GLOBAL_STORE_NAME); try { store.init(null, null); fail("Should have thrown UnsupportedOperationException."); } catch(final UnsupportedOperationException expected) { } ``` Similar below.
Calling `maybe_start_jmx_tool` after each line that gets read from the producer process doesn't seem quite right. I think we want the behavior to be: - start producer "asynchronously" - start jmx tool asynchronously - wait for producer to finish - process each line of producer output I think it would look something like: ``` cmd = "same_as_before &" # now the cmd is "async" node.account.ssh(cmd) wait_until(producer is alive) self.start_jmx_tool(node) wait_until(producer is finished) for line in node.account.ssh_capture("cat /mnt/producer-performance.log"): # same as the previous for loop ```
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
Yes, it makes sense to return a range for an ApiKey instead of a single version. I was just wondering if this method is redundant given NodeVersions.apiVersionRange(ApiKey api). Also, it feels a bit weird for a public facing class to reference Protocol, which is not a public facing one.
But `toString` by default returns `name()`. So, I don't understand why we are overriding it.
Answering here the question about name changes. This is probably the best example, it has been renamed a couple of times: `ConsumerCoordinatorRequest` -> `GroupCoordinatorRequest` -> `FindCoordinatorRequest`. Also, I'd like to rename `Produce` to `ProduceRecords` and `Fetch` to `FetchRecords` so that all protocol APIs are consistent.
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
nit: I don't spot any, but safer to avoid typos by just having constants for these
@ijuma Sorry, I don't know of a standard way of doing this,
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
Harsha has done this.
nit: `Older` -> `older`; `topic Id` -> `topic ID`; `TopicId` -> `topic ID`; `Epoch` -> `epoch`.
It might be worthwhile having a separate case which goes through the sequence described in the jira. Basically this: 1. Receive metadata response with topicID A. 2. Receive metadata response with UNKNOWN_TOPIC error. 3. Receive metadata response with topicID B.
nit: `foo` reads a bit weird here. I would just remove it. Similarly, `topic A` is weird because the topic is name `topic`.
This is fine for everything past 1.0.0, but if we do want to make things releasable using this script on trunk (similar to how people use the merge script probably), then we'd want to maintain support for the 4-component version and validate ones starting with 0 vs >= 1. I'm fine going either way (not maintaining that for simplicity and bug fixes on older branches will need to use the older release script, or just adding in a bit more logic here).
yeah, seems valid. we already chop off whatever we don't need in `docs_version`, I guess just leftover from the `0.` days...
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
Sounds good. Actually I've seen the same situation for our current caching layer flushing logic as well: e.g. `put(A, v)` -> `delete(A)` and both only hit the cache layer. When flushing we tried to read the old value and found its null bytes, so we know nothing was flushed for `A` and nothing written to downstream before so we can skip putting a tombstone to underlying store as well as downstream. For suppression buffer though, it is harder since you do not have an underlying store to fetch the old value, and of course reading the whole changelog to see if there's any updates on this key `A` costs you everything. But suppose we always have a persistent buffer, this may be an easier task.
an -> a
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
nit: this can be final
What do we want to achieve with this throttle? Do we just want to backoff for `THROTTLE_PERIOD_MS` whenever we can't find a connection you sent? I think we should simply use a `Thread.sleep` call. To be concrete, I recommend we instantiate a `org.apache.kafka.common.utils.SystemTime` class and use both its `sleep()` to sleep and `milliseconds()` to get the current time
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
Changed it locally.
not be => not be able to
Changed it locally.
nit: unneeded newline
nit: can we make this debug level? Otherwise it will make this test a little spammy.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
This isn't our fault. When we added the timestamped stores, we chose not to make SessionStores timestamped because the session bounds already have the end timestamp available, which is identical to the timestamp we would have stored in the value.
This is why the test was failing for you. The query is for a range of window start times, not record times. Since the window size is five minutes, the range `[now - 1 minute, now]` wasn't going to contain the actual window start time of `now - 5 minutes`. In other words, just a simple oversight :/ Sorry for the trouble.
Since we're specifying the key, we expect only to get back windows with the value for that key. The aggregation we specified is to sum all values for the key, and it comes out to `2` because we only write one value for each key; namely, the value is the same number as the key.
I think we should test for the exception -- if we change the behavior intentionally, we should remove the `fail` as well as the `try-catch` instead of allowing both behavior to pass (it's an either or form my point of view). IMHO, tests should be on an as narrow code path as possible: if we change behavior and a test fails, we are forces to reflect on the change, what is good as it guards against undesired behavior changes...
Do we need this? The test doesn't actually use or test it so it seems irrelevant
nit: use `ConsumerConfig .SESSION_TIMEOUT_MS_CONFIG`
Is there similar behavior in the map parsing? I see a similar comma consume call being ignored. Consider the following test: ``` SchemaAndValue schemaAndValue = Values.parseString("{foo:bar baz:quux}"); assertEquals(Type.STRING, schemaAndValue.schema().type()); assertEquals("{foo:bar baz:quux}", schemaAndValue.value()); ```
I was not aware of the restriction on JSON object keys, and that seems like a fine standard to follow. I can't imagine it being too useful.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
nit: would be nice to be consistent on the pattern we use here
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
> That being said, I get that this is confusing. Do you think changing the check to `if (endTime == windows.TimeDifferenceMs() && !isLeftWindow(next))` would make it seem cleaner? Haha no, I don't think saying `if (!isLeftWindow(next)): then next = latestLeftTypeWindow` would be less confusing. If we call a variable `leftTypeWindow` then it should _always_ be a left type window. That said, I now see what you meant here and it's the same problem as above, with the same fix of replacing `latestLeftTypeWindow` with `previousRecordTimestamp`. In that case I think we can just remove this check entirely (ie, don't explicitly check if it's the combined window), and all we need to do is make sure `previousRecordTimestamp` is set correctly
I think with this replacement then we might be able to get out of doing any kind of special handling for the combined window outside of `processEarly`
Yeah sorry I didn't mean that we shouldn't have any conditionals here whatsoever, I just meant that we don't need the combined window check (or really anything other than what we need to accurately set `previousRecordTimestamp`)
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
req: This is unnecessary
`final` is for the var `activeTasksMetadata` (not for the method). We try apply a "use `final` whenever possible" policy. It's just some nit.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
Nit: use `{ }` for the loop body.
This adds an additional `get` when compared to the previous solution.
One extra line.
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
nit: unneeded newline
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
As an alternative, which might align better with Kafka in general, would be to set the timeout via `StreamsConfig`. This keeps the API clean. @enothereska argument that `close()` should not have any arguments is quite valid to keep APIs consistent within Kafka.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
The test case `BrokersToIsrsTest.testNoLeader` suggests that it is a possible case. It looks like the path through `ReplicationControlManager.handleNodeDeactivated` could result in a `PartitionChangeRecord` which has leaderId set to -1.
It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.
Hmm, why do we need to remove for -1 broker? It doesn't seem that brokersToIsrs tracks that.
Be careful about changing this -- it should probably be a LinkedHashMap or some similar order-preserving Map if we do this. Depending on the serialization format, they may be sensitive to field ordering so being able to preserve the ordering will be important. Even within a single process a converter could end up randomizing schema field orders and breaking equivalence if an order randomizing map was used. The alternative would be to add an extra Set to SchemaBuilder, which requires extra allocations but doesn't require jumping through hoops to preserve the behavior of the `fields()` method.
Possibly my favorite aspect of this PR is that you have reminded me that `SchemaBuilderException` is even a thing that exists...
Some fields might be safe to just shallow copy, but I think a number of these need to be deep copied to avoid accidentally modifying the original schema. I think `parameters and `defaultValue` at a minimum need to change. Schemas seem like they should be fine since they'd just be fully replaced, not modified, anyway.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
nit: avoid `this` if not required.
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
That makes sense, however you might be able not include the new field from the hash to prevent a chaotic assignment if you wanted
Nit: line too long
Thinking about this, I guess we could improve `StickyTaskAssinger`. If I am not off, load balancing on stream basis is not optimal -- but I am also not sure if the effort to improve it is worth it... If we extend this test to assign more tasks, let's say 12, client `p2` will get 7 tasks assigned and `p1` get 5 tasks assigned (while it would be better to assign 8 tasks to `p2` such that all 3 thread get 4 tasks each). The problem is, that the capacity factors are not considered: `p2` should get twice as many tasks assigned as `p1` -- but the algorithm says only "more" -- and this more is determined be the diff of the capacity (ie. in this case `p2` will get at most 2 more tasks assigned than `p1`. Or maybe my analysis is wrong (I did not run the code and step through it.)
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
This docstring could be improved. Perhaps we can just mention that we will may have a memberId before we are part of a group generation.
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
Thanks for bringing this up @dguy @enothereska , thinking about this more I feel it is Okay to state that users should not call any public APIs of the `KafkaStreams` object inside this callback, if they do then undefined behavior.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Should this be `num_lines=3` (cf. L116 and L126)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
and -> a
records to it, and reading all records from it, such that
and -> a
I suggest passing all of the variables we access in this constructor via`SustainedConnectionWorker.this.spec` to be switched to parameters we pass upon instantiation
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
No worries, let's keep the scope small for now. Just wanted to raise the question
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
This is the essential line of the test that verifies the change in #7097. Without the fix in #7097, the listener will get an update with *all 3 task IDs*, and to pass this line would then need to be changed to: ``` configUpdateListener.onTaskConfigUpdate(Arrays.asList(TASK_IDS.get(2), TASK_IDS.get(0), TASK_IDS.get(1))); ``` However, we don't want *all* task IDs to be updated. Instead only want only the task ID(s) for the tasks that were indeed updated. That's why this test case expects that the task config update only includes the one ID of the task that is actually updated (i.e., `TASK_IDS.get(2)`). So as is, this test method will fail unless the fix for #7097 is actually applied.
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
nit: use `{}` instead of string concat for `retries`
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
I'm not sure this is a good idea. If we're unlucky, the partition we're interested in may not be listed. Since this is an exceptional case anyway, I would suggest using the more verbose message.
There is an inconsistency with above code: `Map.Entry` vs `Entry` -- I guess we should use `Map.Entry` everywhere.
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
Seems this could be a function as well. For example: ``` java Map<TopicPartition, PartitionInfo> partitions(Map<String, InternalTopicMetadata>); ``` (I'm looking for small independent chunks of code that can be taken out of this function.)
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Seems you can replace this with this: ``` java topicPartitions.addAll(partitionsForTask.get(id)); ``` Same below.
Ouch! Sorry about that!
I think we're testing `testDir` Occupied here, not `AppDir`.
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
Nit: `.` full stop missing.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
Can you elaborate? Seems to be orthogonal to the timestamp fix.
Nit, suggested rewording ```java log.info("Fetch offset {} is out of range for partition {}. We only have log segments in the range of {} to {}. Resetting offset.", fetchOffset, tp, partition.logStartOffset, partition.lastStableOffset); ```
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
There's a space missing after `offset`, I'll fix it before pushing.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
I think we'd want to use `updateLastSeenEpochIfNewer`. The provided epoch just gives us a lower bound on an acceptable leader epoch.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
No worries, let's keep the scope small for now. Just wanted to raise the question
nit: add `final`
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
nit: missing empty line
I think using a string and the `NonEmptyString` validator would be a little clearer. That would allow us to skip any additional checks in `start`.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
style nit: normally we'd use braces around blocks unless they're a single line
nit: add a space before the `:`.
I don't have all the context, but isn't `3` pretty low? We don't do exponential back-offs, so the recommendation for no data loss is typically higher.
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Well, we wouldn't want to just remove that clause -- in the case of `SerializationException` you want to maintain the `SerializationException`, but if there's some other `RuntimeException` (which there can easily be for serializers that aren't aggressively catching exceptions and converting to `SerializationException`) then you still need to convert it to a basic `KafkaException`. I think you could do this: ``` try { // parse record } catch (SerializationException e) { throw new SerializationExceptionException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } catch (RuntimeException e) { throw new KafkaException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } ``` as long as we're confident there aren't any other `KafkaExceptions` we'd want to handle differently (i.e. any other more specific types of `KafkaException` where we'd want to preserve the same type instead of generalizing to `KafkaException`).
A quick look shows that other code has access to e.g. `topic` and doesn't include it in the exception message. Seems like having the fields there could help with better exception messages.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Thinking about this, I am wondering if we should just change the FSM to allow this transition and simplify the code here? \cc @guozhangwang
Sure. Why not. I just would avoid the term "invalid" because it might confuse users (they may thing something bad happens, but it's expected and not bad).
This might be miss leading for users who don't know the details. What about: ``` log.debug("Ignoring request to transit from PENDING_SHUTDOWN to {}", newState); ```
This is fine for everything past 1.0.0, but if we do want to make things releasable using this script on trunk (similar to how people use the merge script probably), then we'd want to maintain support for the 4-component version and validate ones starting with 0 vs >= 1. I'm fine going either way (not maintaining that for simplicity and bug fixes on older branches will need to use the older release script, or just adding in a bit more logic here).
yeah, seems valid. we already chop off whatever we don't need in `docs_version`, I guess just leftover from the `0.` days...
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
I really like this class.
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
Ditto on removing before/after
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
Ditto on removing these before/after methods.
Typo: should be "or larger than the number of available brokers"
We do not throw `InvalidTopicException` "if [the topic] is not found"
Should be larger
This adds an additional `get` when compared to the previous solution.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
Thanks for the explanation. Make sense.
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
req: This is unnecessary
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
Ah, yes, the magic is hardcoded here.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
may be use Objects.requireNonNull
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
It's a bit confusing, but it can: http://slf4j.org/faq.html#paramException (if I understood your point correctly).
Do we _know_ that it will resolve the problem? Maybe better: ``` Changing the location of state.dir may resolve the problem ```
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
Nit `.` at the end
nit: both lines missing . at end
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
We lack unit test coverage for this case
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
I don't think we need to mock `generation()` in this test.
It would be good to verify that the buffer contents are correct as well.
We usually avoid writing to standard out in test cases. A few more of these.
This is not a suggestion for change: while working on removing `KStreamBuilder` and `TopologyBuilder` I realized in some unit tests we may still need this class to access the internal topology builder. So probably we cannot remove it even after that, but we can discuss this later in the cleanup PR.
Better name as "setCurrentNodeInProcessorContext"? And then in java docs mention that it returns the processor context with current node set.
nit: add `final`
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
At least we try to not abuse Thread.sleep() ;-)
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
If we follow the same pattern as `ConsumerGroupOperationContext`, then this could be a static method which takes the response as a parameter.
nit: usually we drop the `get` prefix on getters.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
Ah, you're right.
nit: we could define this transition list in a variable to be reused.
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
I do not think we need to emulate the behavior of the java uncaught exception handler here. I do not see why a user should try to reset the uncaught exception handler. If we receive this requirement, we can still add it. I have the impression the "reset" makes this code unnecessarily complex.
"with a read-only key"
with a read only key
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
@guozhangwang @dguy we cannot guarantee that all the entries for one key will necessarily precede the entries for the next key. The following code still fails with this patch, and only returns `0001` and `0003`, since the key for `("a", "0005")` will come after the key for `("aa", "0004")` ``` final RocksDBWindowStoreSupplier<String, String> supplier = new RocksDBWindowStoreSupplier<>( "window", 0x7a00000000000000L, 2, true, Serdes.String(), Serdes.String(), 0x7a00000000000000L, true, Collections.<String, String>emptyMap(), false); windowStore = supplier.get(); windowStore.init(context, windowStore); windowStore.put("a", "0001", 0); windowStore.put("aa", "0002", 0); windowStore.put("a", "0003", 1); windowStore.put("aa", "0004", 1); windowStore.put("a", "0005", 0x7a00000000000000L - 1); final List expected = Utils.mkList("0001", "0003", "0005"); assertThat(toList(windowStore.fetch("a", 0, Long.MAX_VALUE)), equalTo(expected)); ```
Note that `WindowStore.fetch()` should return `WindowStoreIterator` where the key is `Long` indicating timestamp and `value` is the value.
Should we also check that ``` final long start = SessionKeySerde.extractStart(bytes.get()); final long end = SessionKeySerde.extractEnd(bytes.get()); return end >= from && start <= to; ``` Although the current impl of RocksDBWIndowStore naturally checked that for us, it does not guarantee all underlying store impls guarantee that.
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
nit: this is not introduced in this PR but, other places capitalize the first letter after log prefix.
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
A docstring for this method would be good :)
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
Seems like the indenting should be adjusted to the left, right? Applied to other changes in this file too.
The last commit added a bunch of indenting changes. We should revert them.
I guessed the npathcomplexity thing. :) My question is why we have a class instead of just a method.
Sounds good. I'm also ok with a more incremental change if it ends up being more complex than I suggested.
Coordinator changes are _usually_ associated with disconnects, but not necessarily. We have a `coordinatorDead()` function in `AbstractCoordinator` and I was thinking we could make a call to `client.failPendingSends(coordinator)` or something like that at the same time that we set the coordinator to null. I think that would make the behavior the same as the existing code. There may be in-flight sends to the coordinator at the time that we call `coordinatorDead()`, but that is true currently as well, and at least this would prevent any new requests from being sent.
Thanks for the explanation. A bit subtle as you had said. :)
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
I actually had a similar thought, but I am torn though. Using two variables required to keep them "in sync" was is not great. However, using `null` is less explicit... Thus overall I am fine either way as both seems to provide the overall same good/bad ratio.
Thanks. Understood. It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information. In `close()` before we actually "loose" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`). This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies.
nit: keep fields with the same access level together
nit: We could revert this change as it does not bring much and re-align like it was before.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
nit: We could revert this change as it does not bring much.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
remove try-fail-catch and rewrite to ``` final StreamsException expected = assertThrows(StreamsException.class, () -> collector.flush()); assertTrue(expected.getCause() instanceof TimeoutException); assertTrue(expected.getMessage().endsWith(topic1TimeoutHint)); ```
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
nit: style seems to be to not include braces when there is only one if or else statement
nit: missing `<p>` for new paragraph
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
@wicknicks would be useful to have some unit test for this class.
Is this the format required by the mbean stuff? It might be nice to return something more structured here.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
This exception can't be thrown by DeleteTopics.
Please include TopicDeletionDisabledException here.
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
Use `KafkaException` instead of `RuntimeException`
Nit: go with single parameter per line.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
That makes sense, I think keeping it as-is is better.
Think you might have forgotten to remove some debugging here
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
As above: use `assertThrows` and verify error message
While existing test are written this way, we try to move off this pattern and not use the `expected` annotation. Instead, we should use `assertThrows` and also verify the exception error message.
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
I guess in the end we will find these classes a better place (currently they are a bit scattered, e.g. `KeyValueStoreFacade` is in the materializer class).
Seems you got the same :) and ditto elsewhere.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
as above: we need to remove adminPrefix configs
typo: `consume` -> `restore`.
nit: break line
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)
do we need both the `expectedStarts` and `expectedRestarts`? It seems like the former should be just one more than the other.
Yeah it makes sense. Sorry for not getting back to you sooner
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
`than` -> `that`
nit: than -> that
the method name changed to `windowedTable` and `windowSize` parameter is missing
Blank line can be removed.
We probably have to keep the `size() == 0` behavior for compatibility.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
This is the same code as in `KTableFilter` -- we should refactor and share code.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
Not introduced in this patch: "is non" => "as non"
Let's rename `headers1` and `headers2` here too
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
Sorry, you are right! My bad!
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
prop: abortTransaction can also throw ProducerFenced.
I wonder if more of this code is generic and should be pushed somewhere else.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
response version should be 1.
response version should be 1.
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
It was removed from the other versions of `group` but not from here.
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Oh right duh I was thinking it was just a single bit but it's a byte. In that case we should have a test that verifies it goes from `0` to `1` to `2`, etc -- might be good to verify the behavior on overflow as well, if you call `subscriptionUserData` the max_value number of times
Yeah, it should be `0` the first time you call it, then `1` the second time, and then back to `0` again on the third call
nit: the mocked task manager would just call `getTaskOffsetSums(...)` so maybe we can just call `taskManager.getTaskOffsetSums()` here? Ditto elsewhere.
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Nit: space before `:`.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
\cc @lindong28 -- seem's you forgot to update this when dumping the version in `1.1` branch.
I think you should only do this when the version actually is published.
We already import the class on L26, so let's remove this import.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: would be nice to be consistent on the pattern we use here
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
Not sure if we need to call out EOS in particular? It's incorrect in any case.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
this will never be called if one of the assertions fails
Do we really want to do this? Might it be better to have a config for this? Or just run it with a fixed number of threads
Yeah, that works, too, and is more align with the current code.
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
`KeyValueStore` -> `TimestampedKeyValueStore`
nit: remove empty link
Use diamond (`<>`).
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
Great catch, thanks @showuon !
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
nit: should be `named` can't be null
nit: add `final`
Is it correct to set parent node name as `this.name + "GROUP_BY"`? Seems the parent node name is not set in this way. Ditto below.
Thanks for the explanation @dguy, very helpful to understand where caching and sequence numbers come into play. It might be worthwhile to put this in a JIRA somewhere. I do think it would be a useful optimization to have eventually, as fetches have some setup / teardown overhead.
@xvrl there is no `get` on `WindowStore`. We could add one and it would work in scenarios where we don't have duplicates, i.e., the key for a WindowStore is (recordkey, timestamp, sequenceNumber) - if the store doesn't have duplicates the sequence number is always 0. If the store does have duplicates then we don't know what the sequence number is. Without a KIP to add a `get()` to `WindowStore`, the only thing we could do is add a bit of a hack to see if the inner most store is a `RocksDBSegmentedBytesStore` and then we could call `get(..)` on that. If it isn't, then we'd still need to call `fetch`. For the DSL this would work as the only time we have duplicates in the `WindowStore` is for joins and we disable caching for those so it skips this code path. However, for the PAPI, we would need to always disable caching if duplicates are set. Which we probably should do anyway as it won't work as is.
I thought the timestamp would uniquely define the segment in which that key is stored.
nit: if you want a new paragraph you need to add `<p>`
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: add `a {@link Named} config`
nit: remove empty link
nit add `a {@link Named} config`
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
Ah, sorry to say, one more thing slipped by me before. We should `verify(inner)` at the end of both of these tests. It should actually fail for `shouldNotPutIfSameValuesAndGreaterTimestamp` because we should _not_ call `inner.put` in this case. To fix that, we would just delete L202, where we set up the mock for `inner.put`. Then, the mock would be initialized to expect no calls, and the verification would fail if we did call it.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Note that Kafka only supports kerberos as the SASL mechanism.
Do we still need these 2 blocks? In `setup()` we already consumed all messages
`replicaing` -> `replicating`
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
yeah, other stuff from `RuntimeMXBean` was just a suggestion, obviously i wouldn't limit to that. classpath was the main one i saw that is useful. i don't think that it's too confusing since it is also alongside a bunch of other general system info. also, we still support the classpath approach, we just won't fix conflicts. we've definitely had cases on the mailing list that ended up being classpath issues that we probably could have spotted the likely root cause more quickly if we had had access to the classpath info.
Should we add some more stuff to round this out (and make use of all the support for reporting more than one value...), e.g. some other `RuntimeMXBean` info? For example, classpath info seems like it'd be useful (probably more so before `plugin.path`, but still probably handy from time to time).
I'd suggest moving this static method after the non-static methods.
Oh... Thanks for the reminder. @RivenSun2 , could you summit a small KIP for this change? Thanks.
Why not use the copy constructor? It could even be done inline: ``` java for (Map.Entry<String, Object> entry : new TreeMap<>(this.values)) ```
Is this punctuate function necessary? Since we do not `schedule` in the init functions I think this punctuation will never be triggered.
Actually a more general question is that for assign(), is checking subscription.isEmpty() sufficient or not. Today we allow subscribe(empty_list) whose semantics is different from unsubscribe(), but they will leave the same state in subscription map.
I think the semantics of subscribe(empty-list) should be similar to pause(all), but leave the consumer still registered as member of the group with coordinator; for unsubscribe() the consumer means "do not talk to coordinator anymore" and moving forward we may add a leave-group request (there is already a ticket I think). As for now let's keep the original approach to check the assignment upon each commit call; thoughts? @hachikuji @onurkaraman
nit: remove newlines
You actually do not need `this` here, right? The values are the default initialization values in Java. And `super` is also called in the default constructor. So actually, you could remove this constructor completely.
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
I think that's the optimal compromise. Keep `null` since it's a keyword and avoid starting a sentence with it. *Any* seems to work fine here.
Seems to fit in one line
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
We no longer need `DEFAULT_OUTPUT_TOPIC_NAME` if all caller will use `createTopic` now
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
yes, it seems to be not what this test is checking on. I think we can drop it here.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
`start` is not used.
Would it be worth having the rate limiter have 2 levels: the first level logs every error and the second logs every nth error? My concern is that simply cutting off logs entirely once we hit a certain # of messages can mask later messages. You don't want to mask those entirely, you just want to cut them off. I think any approach using timestamps is probably going to get too complicated. But I think still printing every 1000th message would be useful so you eventually see the problem.
There is a built-in for this `Function.identity()`
Why do we need to do this? `log` is private and is not used
Add the stream task id prefix here as well for both exception message and the warning log entry.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
There's already an expected exception, we can remove the `fail(...)` call here.
nit: `fetchOffset` instead of `fetchedOffset`? I think this is meant to describe the offset from the fetch request.
Yes, the sensors are created in `Sender/Fetcher` to avoid knowledge of the different names to the network layer. Recording is done in the network layer since `Sender/Fetcher` don't see all responses (now that any response may be throttled) and to use common logic.
nit: do we still need this function? Could we just reference the `metricLock` object directly? Since it is private my understanding is that it was not intended to be used outside this class.
I don't think this is necessary to add here. AFAICT `StreamsKafkaClient` is only used in `InternalTopicManager` and it can just be constructed there
typo: `consume` -> `restore`.
nit: add `a {@link Named} config`
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
`[because] the tool`
typo: `will does not`
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
ditto for the rest of the test
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
I don't have the full context on the history, but it would not be easy to change the API... I talked to Jason about it, and it seem we can just move forward with this PR as-is, and could do a KIP later that allows us to store metadata as `byte[]` type if we really need to change it. Atm, the metadata is just a few bytes and the overhead does not really matter IMHO.
We should return `RecordQueue.UNKNOWN` instead.
Nit: (simplify to) `"Unsupported offset metadata version found. Supported version {}. Found version {}."`
This message should say "Consumers earlier than 0.10.1.0..." since KIP-74 was part of 0.10.1.0.0.
Hmm, we can't use shallowEntry.offset() since not all messages will be retained. Perhaps we could just maintain maxOffset as we add messages to retainedEntries.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
```suggestion public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates, final Set<TaskId> allTasks, final Set<TaskId> statefulTasks, final AssignmentConfigs configs) { ```
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
would be nice to mark all params final whilst we are changing this
@becketqin Yes, my preference, as mentioned above, is to deal with that problem separately. We should not make behavioral changes without first raising the issue at least in a separate JIRA. The unintuitive thing about the proposed behavior to me is the fact that although the consumer's position remains at the offset of the failed record, the next returned record will be from the offset after that position. You can see this in the test case below: the consumer's position is at 1, but the returned record is at offset 2. This makes the behavior less deterministic. It would be nice to maintain the invariant that the next fetched record is always the first record at an offset greater than or equal to the current position.
Please update the test case as I suggested. Thinking about the current patch. If there is an exception parsing or validating one of the records, we will update `PartitionRecords.nextFetchOffset`, but we will not change the current position (i.e. what is returned by `consumer.position()`. That means in the next call to `poll()`, we will simply discard the rest of the records. So there is no behavior change here and my suggestion above simply makes the behavior explicit. You can confirm this by updating the test case.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Yeah good catch, see above
Very good catch. Thanks, @abbccdda .
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Use `File.separator` instead of `/`
Just some more nits. Can you add `final` wherever possible: `ClassLoader`, `String filename`, `BufferedReader`, `for(final String...)`, `Exception`
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
True, will we ever want to have this ability? But the change seems fine to me.
why do we make lines longer? harder to read now
What is the reason for having `assertDoesNotThrow` here and below? The test will fail if an exception is thrown, so seems like unnecessary noise.
That's a good point.
This catch is a bit weird to me. Could you create a true `CompletableFuture` carrying the exception `new TimeoutException()` instead of mocking object? For example: ```java CompletableFuture<RecordMetadata> future = new CompletableFuture<>(); if (success) future.complete(new RecordMetadata(new TopicPartition("tp", 0), 0, 0, 0, 0, 0)); else future.completeExceptionally(new TimeoutException()); ```
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
nit: missing `<p>` for new paragraph
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
FWIW - this is probably something that could be tested easily with good usage of a real Mocking framework (rather than hand coded stubs). We should probably start making use of something in streams as it would save a lot of effort and with correct usage lead to a better overall design.
Is this the reason that `threadProducer` is not `private`. I'm not 100% sure, but if it is maybe there is another way of testing this so that it can be `private`, i.e., if you are using the per task model you know that the `clientSupplier` should be called n times. However, with the thread model it should only be called once.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
Yeah if it exists elsewhere let's just leave it as is for now.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
nit: add `{@link Named}` here and elsewhere below
nit: missing `<p>` for new paragraph
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
Yep. Thanks, @bbejeck !
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
same for the store
Might be excessive to add `final` everywhere. I'd say if it's self-evident that things aren't mutated I could skip final.
We do not need to mention "partition" here since it is supposed to be abstracted from users, ditto below.
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.
nit: initialize `appId` with the prefix you want (eg `appID_`, or something more descriptive like `TaskMetadataTest_`) and then just append testId, ie ``` appId = appId + testId; ``` Just makes it easier to locate what the prefix is (same with `inputTopic` below)
```suggestion processed = new AtomicBoolean(true); ```
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
Nit: ```suggestion @Override public void close() { ```
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
I'm ok saving this for #7409.
There's also https://github.com/apache/kafka/pull/11128 to consider.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
see my question above about using mocks.
See my question above regarding using mocks.
nit: Please fix code style.
`Count is a {@link SampledStat} that maintains a simple count of what it has seen.` So with this stat, its value will be increased for the window period, then suddenly drops to zero, then start rising again. So it's hard to alert on such a metric, on the other hand `Rate(Count())` will record the average rate per time unit (here second), so users in practice can easily set a threshold for alerting, and even if they want "zero tolerance", setting the threshold to be 0 can still satisfy their needs.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
We could refactor out a helper function here.
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content. Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
If I understand correct, a record read from the changelog topic should only be: 1) having no headers at all (old version) 2) having a singleton header with `v --> byte`. All other cases should indicate a bug in the code. So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value: 1) byte == 1: do as below. 2) otherwise: to not support forward compatibility, we can just throw unsupported.
- It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm. - And we know that KafkaStreams never writes headers into changelog topics. Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case. We can also do a version check as suggested by Guozhang.
Your understanding is correct @mjsax .
Add the stream task id prefix here as well for both exception message and the warning log entry.
The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).
seems like it should at least be info, if not warn
It seems like we ought to just define `log` at the AbstractTask level and avoid having two almost identical `maybeInitTaskTimeoutOrThrow` method definitions.
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
Currently the code iterates over the active tasks and assigns all standby tasks for each active task. If the standby tasks cannot all be assigned, we might end up with all standby tasks assigned for some active task but none for others. What do you think about to assign one standby task for all active task and then assign the second standby task for all active task, and so on. In this way, it is more likely that at all active tasks have at least one standby task assigned. I am aware that the default standby assignor has the same drawback.
When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting.
nit: including the acked offsets to checkpoint as well.
nit: add `final` (we add `final` to all variables when possible -- applies multiple time -- please update all variables (inc. loop-variables)
remove var -- only used once.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
Thanks for the explanation. A bit subtle as you had said. :)
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
`advanceMs` is not the same as provided input parameter `advance` -- this would make the error message miss leading.
We should call out explicitly that this is setting the grace period to 0, which means that out of order records arriving after the window end will be dropped. Otherwise it's too easy to just use this method without thinking any further about the grace period and what it means/whether you want it
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Changed it locally.
not be => not be able to
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
nit: we can use `map#compute` to replace getOrDefault + put.
You can use `EnumMap`.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
There's an interesting edge case where `record.isValid()` could throw a `IndexOutOfBoundsException` if the buffer size is smaller than 4 (i.e. we fail when we try to extract the checksum from the buffer). Obviously, this means that the record size field was itself corrupt (since it should never be that small), but it can happen. I'll file a separate PR for that.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Ack, I get it now. Thanks for clarifying.
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
nit: might be better to name it windowedKTable
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
nit: Starting a message with lower case feels a little unusual.
nit: I know it was already like that, but since we are now passing the actual class object, you might want to refer to the class object as `klass` (I like the keystrokes on this one) or `clazz`, which are common naming conventions when using class objects. Then call the String field `className` or similar. Of course JsonProperty will continue to be called `class`. Up to you.
Typo: should be "or larger than the number of available brokers"
We do not throw `InvalidTopicException` "if [the topic] is not found"
Should be larger
Maybe use log parameters instead? ``` java log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), t); ```
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
Actually I think it works: http://www.slf4j.org/faq.html#paramException.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
nit: since we're not doing anything in the EAGER case, couldn't we simplify this: ```java if (protocol == COOPERATIVE) adjustAssignment(ownedPartitions, assignments) ``` Similarly in `onJoinPrepare`
Please use string interpolation. There are a few other places like that.
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
createTime -> creationTime
We are passing `now` everywhere else. Maybe we can just keep the argument name the same.
Still not used
Minor but I'm not sure if we'd prefer the builder's toString or the underlying request struct's toString as was formerly the case.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
This test case passes without the fix. It doesn't look like it even goes through the auto-commit path.
The name should mention the fact that this case covers auto-commit.
I referred to the value serializer.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
nit: ".. select the grouping key and the value to be aggregated".
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Nit: let's avoid unrelated line additions.
It actually would be helpful to include the exception's error message in this line, since the message alone might be bubbled up via the REST API. ```suggestion log.error("{} Error converting message value in topic '{}' partition {} at offset {} and timestamp {}: {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e); ```
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
It seems like we can migrate away from the deprecated method in this test.
This is definitely a nitpick, but can you put this one fewer lines? 2 lines should be enough for this. Same for the ones below.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: blank line missing here
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Is this used anywhere? I see we have changed client code to use the other C'tor.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
ditto to `KStreamImpl`
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
nit: add a space before the `:`.
style nit: normally we'd use braces around blocks unless they're a single line
Do we really need to print `super.toString`? Ditto above.
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
doesn't look a great name for its behavior. perhaps something like currentContext
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
This is not related to your PR at all. It seems that if `offsetRequestSpec` is `null` here, `future` will be `null` as well cause `futures` is initialised based on `topicPartitionOffsets`. If it turns out to be correct, it may be better to just log a warning here like we do in `createTopics()`.
nit: align parameters.
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
This one is still not using a tab.
Is this necessary? The leader epoch is -1 by default.
We could make this field access `public`
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
this line is still a bit long... You could try a static import for `singletonList`.
cosmetic: extra space at the start
this is creative :)
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
is old metadata missing expected after we start off? Might be useful to add a debug log or trace if this is not normal.
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly: ``` log.error("Could not initialize partition time. Committed metadata is corrupted.", e); ```
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: could make access private and get an accessor.
explain why `Integer`, `Long` is used instead of `int`, `long`
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: Indicate that this needs shallow iterations on the entries.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
nit: add `final`
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
ah, right. nah, that's fine. just when reviewing I had the thought that if we guaranteed non-`null`/non-empty in the constructor, this wouldn't be necessary. i realized that it was actually intentional, but easy to miss when reviewing here and not getting the same highlighting as an IDE
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
to me it seems like we can't possibly know what the constraints of all reporters would be and they don't provide an interface for validation, so it should be up to them to figure out how to substitute. but i've also asked some other folks to maybe chime in here who may have better context on how we've handled this elsewhere.
nit: empty line.
We can remove the code block line 82-85 above since it will be called here.
nit: align parameters.
I think this config property key seems a misfit, and probably reflects an earlier incantation of the design before KIP acceptance. It might be worth - in a separate PR - renaming this to something like `errors.tolerance` to better align with its purpose.
We'll need a separate AK issue, then.
nit: matches the old behavior is very relative
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
typo: `per reach record`
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
as above nit: double space `to Kafka`
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
line too long
This line is failing checkstyle. I think we need a space after the first semicolon.
Yeah, the logic seems right to me.
We want the exception to be thrown in either case right? If requestTimeoutMs is greater than either sessionTimeOutMs or fetchMaxWaitMs an error should be thrown? I think this a difference between the english meaning of "and" and the programatic meaning of "&&".
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
nit: this loop is a little unconventional. Maybe we could use `pollFirstEntry` instead of the iterator? Similarly in `setNumKip500BrokerNodes`.
You might consider using `OptionalDouble`.
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
As `KafkaStreams` implements the `AutoCloseable` interface now, `close()` should be called automatically when the `try {}` block is left -- that is the whole purpose of `AutoClosable` and try-with-resource construct -- it frees you up to call `close()` explicitly (so you cannot forget any longer).
I missed the fact that we moved the `waitForCondition` check _inside_ of the try-catch block... For this case, we need to call `close` explicitly of course, as we are still in the block and `close()` is not auto-called yet... Sorry for the confusion.
Could you please add some line breaks? This and some of the other verifications are too long.
nit: add `final`
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
I think it is probably worth adding as even if it is deprecated it is still supported
Yes, does not hurt to leave it. Just for sure.
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
Nit: can be `final`
Nit: Please use `{ }` (even for one line blocks)
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
nit: we can put kafkaStreams in a try block.
Nit: should the method be named `testOptionsDoesNotIncludeWadlOutput()` instead? The point of this PR is to prevent including WADL in the OPTIONS output, but the existing method name makes it seem like we're testing the content of the WADL output.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
As above: need to keep default value.
Might be overkill if this is the only use case, but we could also add a composite validator.
`Note when the windowed serde class is used, one needs...`
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
Nit: `.` full stop missing.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: add `final` (same line below)
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
Hi, may I ask why do you do `@link` instead of `@see` annotations? :)
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
nit: move to line above.
req: typo unknown Pid
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
Don't need `Vin` and `W extends Window` here
Don't need `Vin` here
@vamossagar12 Up to you I guess. I'm ok doing it all here since the changes seem pretty small.
I think the original KIP stated that the LeaderChange message would encode the set of voters that had voted for the leader. We thought this might be useful for debugging. Later on, we had a change of heart and decided it would just be the set of voters. Now I'm thinking it might be useful to have both. The log will always remember who the voters were at the time of the election and which voters had granted the leader's candidacy, which could be helpful in case of misconfigurations. For the set of voters which voted for the current leader, I think what we want is `CandidateState.grantingVoters`. However, by the time `onBecomeLeader` is fired, we have already dropped the `CandidateState`. One option is to carry `grantingVoters` over to `LeaderState`. We might also be able to pass it through `onBecomeLeader`. This will be easier if we get rid of the call to `onBecomeLeader` in `initialize()`. Following KAFKA-10527, it is not possible to initialize as a leader, so we could raise an exception instead.
Yeah, if you don't mind, it seems like a gap. Thanks!
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
For consistency: {@link KafkaStreams} instance
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
@spena just ping to make sure you get this on the follow-up PR.
nit: double space
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Cheating the compiler, woohoo!
Nit: could just throw the exception directly here; doesn't appear to be much benefit to putting that in a separate `setup` method.
This can be final.
We shouldn't return `null`, but instead return a "unknown query" result.
Dropped this unnecessary duplicate code, as we discussed.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
just `name` should be fine
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
Well, if you want to match the use of `DESTROYED`, `RUNNING` probably makes the most sense since that is the target state you want the connector/task to be in. But I'm not picky, either one works.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
We should limit this suppression to the method for which we really need it instead of the whole class
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Hmm, we're using a raw type here and a few other places. This is discouraged (type checking is disabled in these cases). If we don't want to propagate the generics when we use the superclass, we should probably drop them.
To make this really interesting, we would need to add some sequence number bookkeeping. Really its the sequence/epoch bookkeeping which makes the implementation so complex.
Similar to the offset commit path, it would be useful to validate here that each partition that was written to was first added to the transaction properly.
nit: it was correct before
Nit: let's avoid adding new lines in code otherwise unaffected in the PR.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
Can just return `name.startsWith(acl.resourceName())`
Not sure if it makes a big difference, but we could use EnumSet for these.
nit: need to update this since resource name is before pattern type now
nit: line too long
nit: line too long
Ditto on removing these before/after methods.
Maybe use log parameters instead? ``` java log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), t); ```
Actually I think it works: http://www.slf4j.org/faq.html#paramException.
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
Nit: this can be written more concisely by using `Arrays.asList`.
Nit: this brace should be on the previous line.
Nit: there should be a space before and after the colon.
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
This exception can't be thrown by DeleteTopics.
Please include TopicDeletionDisabledException here.
We do not throw `InvalidTopicException` "if [the topic] is not found"
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
I think we can move this logic into ValueOrOtherValue as another static constructor.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
(especially given that below you use the simple name)
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
The number has changed and 5 is no longer relevant.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
Sorry for being late on this. Populating static variables from a non-static method is generally not a good practice since it's generally not thread-safe. Given how JMH works, it's fine for those fields to be non-static right? Also, it seems more realistic since the data is isolated per run.
@original-brownbear I understand that part, but IMHO it's best if we stick to some basic guidelines for our tests/benchmarks. To reduce the GC impact how about we decrease the cache size to 5, and the number of records to insert 25 or so? Or just create an array inline with the keys and declare as a private variable? Either way, we should be able to do the work in the `setUp` method.
But `toString` by default returns `name()`. So, I don't understand why we are overriding it.
Fair enough :)
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
nit: `{@link KeyQueryMetadata}`
`it is` -> `they are` (we user is a person :))
This method does not return a `String`. Maybe ``` @return StoreQueryParams a new {@code StoreQueryParams} instance configured with the specified partition ```
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
Should this ever happen? If it does happen should we consider it a bug? Ditto for the other `hasNextCondition`.
When a stream is created from multiple topics, we do not have any ordering semantics across those topics, but within a single topic we still follow the within-partition ordering.
remove "(if any)"
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
It was a good idea to remove the `processOutstanding` field from `CloseMode`, but I think this would be a little clearer if we kept the `notifyDisconnect` field.
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
nit: Indicate that this needs shallow iterations on the entries.
nit: "can not" -> "cannot", same below
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Do we need to do this `close` and `open` here? We do it also on lines 283 & 286
If not, we should move the exception capturing logic inside the dbAccessor as well.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
I find it strange that this method closes the consumer it received.
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
This is the callback from the `StreamThread`s so it will be called from multiple threads, i believe. See the inner class `StreamStateListener`
The state that is being updated is private data of `KafkaStreams`. It should be responsible for synchronizing access to its data, not external classes.
Probably want to make this a `ConcurrentHashMap` or `synchronize` access to it. It can be modified etc from multiple threads
`return stream(null, null, keySerde, valSerde, topics);` Do the call directly instead of the cast.
topics -> topic. This may well be elsewhere in the java-doc, too
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
`hasFetchedRecords` avoids the cost of populating the map returned by `fetchRecords`. No locking is needed for that. So, if I understood the suggestion right, it would look something like: ```java if (fetcher.hasFetchedRecords && client.hasPendingWakeup()) client.poll(0, ...) ``` I guess by calling `poll(0, ...)`, we don't have to expose `maybeTriggerWakeup()`.
The current solution seems more direct than what I was suggesting, which is good. A bit unfortunate that we have to expose `maybeTriggerWakeup()`. I guess we could keep the synchronization around `pollNoWakeup` that you've added and still do the `hasFetchedRecord()` approach. Probably it would look like this: ``` if (fetcher.hasFetchedRecords()) { client.poll(0); return fetcher.fetchedRecords(); } ``` The downside is that it `hasFetchedRecords()` may not be trivial to implement since it would require doing the work that `fetchedRecords()` is doing, but without updating the position. Given that, exposing `maybeTriggerWakeup` doesn't seem too bad.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Consider naming the topic "topic2" since there are only two topics in the test
as above: flip arguments
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
nit: Normally for getters we have the convention of dropping the `get` from the method name.
Checkstyle failure: ``` Name 'JITTER_MAX' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName] ```
Similar logic exists in `ClusterConnectionStates.updateReconnectBackoff`. Maybe we could extract it to a utilities class.
Making timeouts configurable could be a good idea, but it's better done in a general way in its own PR.
It seems a bit ad-hoc to have this environmental variable for one test only.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
nit: use `{}` instead of string concat for `retries`
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
and -> a
and -> a
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
calc -> calculate
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
`a graph containing`, and correct space between `1. Build`
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
as above mentioned, the `listStore.all()` is not closed here.
I referred to the value serializer.
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
I think we can use a utility method provided by the `ConfigDef` class here: ```suggestion List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST); if (topics.contains(dlqTopic)) { ```
Should we log the topic name for this exception? For example, ```has a topic name (xxx) which```
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
Nit: var should be named `deserializeValue`
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
nit: toString not necessary
nit: `--topic` and `--partition` could be extracted as helper static functions
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
nit: also add java doc for type `T, O` here
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
These changes are going to break existing users. For example, I have connectors with a few settings prefixed with `consumer.`. I wonder if we could keep the old behaviour (even if partially broken) while adding the proper prefixes
It's unfortunate for admin we use `source.admin` as the prefix ... So we'd be left with a configuration like: ``` "source.cluster.bootstrap.servers": "localhost:9092", "source.cluster.security.protocol": "SASL_SSL", "source.cluster.producer.some-producer-setting": 123, "source.cluster.consumer.some-consumer-setting": 123, "source.admin.some-admin-setting": 123 ```
I don't think the format mentioned in https://github.com/apache/kafka/pull/9313#discussion_r498298987 would break compatibility.
Add a reference to KIP-511 here
Same here, we can cache the result of `Builder.getPartitions(data)` for re-use.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
I think the name of the function is better defined as `interleaveTasksByConsumers`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
nit: again this is not introduced in this PR, but let's use `AssignorError.NONE` here and elsewhere to be less vulnerable to enum changes.
> Just clarifying: After the group has formed, both leader and follower can still trigger a rebalance: leader will trigger a rebalance if the existing topics number of partitions has changed (including the topic is deleted); follower will trigger a rebalance if the subscription has changed (both due to a metadata refresh with regex pattern or user called subscribe again). Is that right? Yes, right. > And if we change the consumer coordinator to allow passing regex to the leader, I think joinedSubscription can be removed completely and only leader need to trigger rebalances unless users call subscribe again on any of the consumer member, is that right? The leader will still have to deal with the potential for a metadata update during a rebalance, so I'm not sure we can remove `joinedSubscription`. At least we won't need this funky logic to try to change `joinedSubscription` after the rebalance though.
I'm not sure this works. The purpose of the `joinedSubscription` field is to remember the exact list of topics that were used when joining the group. If a metadata update arrives after the rebalance has begun, then we can notice the fact that the joined subscription does not match the current subscription and we can trigger another rebalance. With this change, we will no longer be able to detect this case, which means that consumption from a topic matching the subscribed regex will be delayed (perhaps indefinitely). It seems the behavior we want is to only add to `joinedSubscription` those topics which were added to the assignment by the leader.
Add to the end, "as long as they still match the subscribed pattern"
`The default "all" setting` -> `The default setting "all"`
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
deliveryTimeoutMs should be mentioned
isFull is no longer used.
This is still not used
@rondagostino are we ok with merging this to trunk? Since this is not required for existing tests which either use ZK or PLAINTEXT brokers, not planning to backport to older versions.
@rajinisivaram Yes, merging to just trunk seems fine to me.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
I don't think it makes a big difference either way. The intent of the offset commit interval is just to make sure committed offsets don't fall too far behind. It does not need to be a strict schedule. It seemed more intuitive and simpler to me to reset the interval. In any case, we should get rid of this relative tracking of the next commit. If we use an absolute time, then we will not have problems like this in the future.
Is there any reason not to accept this suggestion? I will go ahead and push an update to this PR next week if there are no further responses so that we can get this fix into the next release.
Maybe this is a little simpler? ```java nextCommit = now + offsetCommitIntervalMs; ```
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
We should log an error that prints out what the two configs actually are
There's already an expected exception, we can remove the `fail(...)` call here.
It's good refactor to use `assertThrow`, but the failed message is missed. Please add it in `assertThrow`. Thanks.
As I understand it, handleResponse will always be called by AdminClientRunnable from the single 'network thread' (KafkaAdminClient.thread).
OK, let's keep the change to that one field for now.
I wonder if we can just get a Map with the default size. I don't expect this code path to be very hot
Does this test ever encounter this exception? I don't think we will be able to backport this test to < 2.6 because the method won't exist at all, much less generate the exception that is being caught here. If anything, this generates a less informative NPE later in `put`, and hides the actual root cause.
Seems to fit in one line
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
if you have an unsigned ~~8-bit~~ 16-bit data source
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
Should we still log it, perhaps as a warning? If I understand the background, this case is unexpected except with 0.10 brokers, so it seems like swallowing it could mask an important condition.
Thanks for the follow-up.
Ditto here for different exception types.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
I don't think this is necessary to add here. AFAICT `StreamsKafkaClient` is only used in `InternalTopicManager` and it can just be constructed there
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
if we keep ending up with this pattern, it might be clearer to create a `Listener` implementation that delegates to a list of listeners instead of chaining them manually this way
same question as other pr -- this is `sink-task-metrics` instead of `sink-tasks-metrics` in the KIP
this state is missing from the KIP, it should be added
Filed this one: https://issues.apache.org/jira/browse/KAFKA-12607.
nit: ".. in epoch {}"? Similarly for other logs.
You could return Optional[String] probably where defined string would be the rejection reason. This would mean renaming the methods slightly though.
Adding to `connectorProps` won't change the already instantiated `config`.
If a public API change like this is required, you will need to propose a small KIP. I'm unclear why it's required tho, and ideally we would not alter the existing API if possible. If a new method is required, I think "track" is too ambiguous and should not be used here.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
nit: `final` (also next line)
The order is not really that important here, either way works
We don't need this for files, right? Just for directories (because of `file.deleteOnExit`)
Cleaner to just check if `tasks.isEmpty` after the loop is over.
nit: `leaveReason = "consumer poll timeout has expired..` So that the whole log entry would read as `Member sending leaveGroup request to coordinator due to consumer poll timeout has expired ..`.
I was thinking that we can just pass in the statement in the above warn as the root cause into `maybeLeaveGroup`.
Maybe "consumer poll timeout" would be clearer than "heartbeat poll timeout"? The problem is the delay between calls to `Consumer.poll`.
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
nit `stays at 2` seems to be correct -- it's `equalTo(2)` below.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
Please increase timeout to 30 seconds.
add `fail()` in next line to make sure we hit the timeout and throw an `AssertionError`
nit: add `final`
`assertNull`s shouldn't be here but few lines bellow.
nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with `env.getCluster().nodeById(..)`.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
Should be final.
I think it would be more intuitive if we would reorder parameters to "topic, pattern, topic, pattern".
It seems you can move this line after line422.
Q: I might have missed the discussion. Why does an unknown offset result in `1` and not in `Long.MAX_VALUE`? Sorry if you have already answered this question elsewhere.
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Not sure how many mirror maker tests we'll end up having, but would it make sense to have a `MirrorMakerTest` utility like the `KafkaTest` one, or does that end up being too minimal to be worth it (looking back at the `KafkaTest` one now, it looks like it's now just a few lines of code...)
use `try-catch` instead of `expected` annotation -- not a single line test.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
store not used
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Existing issue, space should be after the colon.
nit: Starting a message with lower case feels a little unusual.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Should be final.
@mjsax I think I'm sold on your arguments, let's keep them as WARN then :)
Good point, thanks!
nit: additional new line
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`before or after`
`if (ignoreWhenShuttingDownOrNotRunning && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING))`
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
The fact that we're ignoring it suggests that it's not an `error`. For something that indicates something unexpected has occurred, but that we can handle it and continue correct execution, we should use `warn` level. Also, the type of exception will be captured in the stacktrace, so I would just say something more descriptive like, "Channel closed unexpectedly before lock release."
@mjsax is right. Just to clarify, state store / changelogs today only do header-agnostic serde, so the scope of this PR is only for sink nodes.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
an -> a
Nit: remove the `:` after "type", since this forms a readable sentence.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
Is it valid to have a blank string as the replacement value? If not, then the `replacement` config should have a validator that prevents using invalid values, and it probably would be good to succinctly describe the limitations in the doc string. And it may be better to only set `this.replacement` to a non-null string that should always be used. This would centralize the logic of determining whether it should be used in one place, and line 135 becomes a lot simpler and more efficient: ```suggestion if (replacement != null) { ```
Why remove this? Do we need to instantiate this class now? (I only see static members still).
That makes sense, however you might be able not include the new field from the hash to prevent a chaotic assignment if you wanted
If we expect no warmups, we can assert it here with: ```suggestion assertValidAssignment(0, allTaskIds, emptySet(), clientStates, new StringBuilder()); ```
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
nit: might be helpful adding a little helper since we do this a few times in here
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
Nit: too many blank lines.
It's intentional to avoid build warnings about importing deprecated classes.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Are there ever situations where users would want the old behavior (to have access to the `ProcessorContext` for the record that triggered the lookup, rather than the context for the record that's being looked up)? For example, if the topic name is relevant for the transformer and all records (including the current one that triggered the lookup and the one being processed) are from the same topic, then the old behavior gives access to the topic name but this new behavior doesn't.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
Also: should only call onPartitionsLost on owned partitions that no longer exist
typo: moreq -> more
Nice tidy up of this test class :-)
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
Ditto on removing these before/after methods.
Ok, it looks better now. Let's leave it this way, with two lines.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
nit: I would rather use the full name instead of using acronyms.
typo: CompleteableFuture -> CompletableFuture
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
Good point, thanks for clarifying.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
not used here (InvalidTopicException is used instead)
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
Should probably add this as the first bit in this method: ```suggestion String strValue = (String) value; if (value == null || strValue.trim().isEmpty()) { return; } ```
ah, i missed that it was in the Validator and not ConfigDef itself.
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Similar questions below.
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
nit: The sentence sounds slightly better if you remove `the`
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
You can use `EnumMap`.
You can use `EnumMap`, which is a lot more efficient.
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
nit: add `final`
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
nit: 4-space indention plus move `builder` down one line
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
We should limit this suppression to the method for which we really need it instead of the whole class
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
Let's capitalize the log statements for consistency. There are a few of these.
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
nit: remove empty line
ditto on removing before/after.
exception can be improved a bit - "failed to flush within X ms, successfully completed Y/Z batches". wuold help distinguish between slow connection and no connection.
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
@mjsax What you suggested sounds right to me.
the method `restorePartition` is no longer used and can be removed
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
That's right, changed it locally.
Better be `cooperative-sticky`? `cooperative` is too general I think.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
nit: add a space so it is "StreamsMetadata {...} topologyName=xyz"
nit: add `final`
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
nit: I think it's better to just print the e.message in a single line.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
calc -> calculate
we could use `computeIfAbsent`
nit: maybe it's more helpful to use the error directly here since no one knows the codes.
We can probably output the error instead of the error code (the field name should be changed appropriately.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: full-stop after the description.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
I am wondering why this is not an assertion. Would the broker ever be expected to return only a subset of the partitions in a full fetch request? To be honest, I think it would be fine to skip these checks and just assume the broker gives us the right thing.
I am wondering if this can be lowered to `DEBUG` since it is handled internally.
Since topicPartition doesn't exist in next if we get here, there is no need to remove it.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
nit: need to update this since resource name is before pattern type now
Can just return `name.startsWith(acl.resourceName())`
We should not be using mockito internal classes.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
Might be good to add a debug log message right before this.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
This method name doesn't follow Kafka code conventions.
This exception is no longer possible since the constructor is taking `ObjectName`.
We can remove only if mbean.metrics.isEmpty. This line should be added in metricRemoval() method after unregister call.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
Discussed this with @junrao. The main challenge with this option is having the top level error field for every response. This would probably affect a lot of code: 1. We would need to handle this top level error code everywhere. 2. A bunch of protocols that currently have a top level error code would no longer have them, so a bunch of code would have to be updated as well. So, it doesn't seem appropriate to do this as part of this KIP.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
the message format => that message format
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
I find it strange that this method closes the consumer it received.
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
That is correct: ``` {@link org.apache.kafka.streams.kstream.KTable KTable} ``` will show on java docs as `org.apache.kafka.streams.kstream.KTable`, while the above will show as `KTable` whose ref links to `org.apache.kafka.streams.kstream.KTable` still.
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
Typo: "you can create [a] windowed ..."
Let's keep the existing `trace` and `error` log lines in the `else` block. My suggestion is to add a line at the debug or trace level in the `if` block so users can know if an error is ignored.
We can use `==` to compare enums.
The framework already retries this step if there is a retriable exception. I'm not sure if the operator is needed in the source connector (especially after resetting the processing context, which removes all useful context for the reporters).
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
There is probably a generic pattern that you could factor out here. Basically you are providing a translation from `Map<K, V1>` to `Map<K, V2>` in both of these cases. So you could construct a generic `MapView` or something like that with a `Function<V1, V2>` and save some of the duplication.
nit: as in `position` above, `this` is not required
nit: as in `position` below, `this` is not required
nit: as in `position` above, `this` is not required
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
We should test that delete twice in a row fails with `IllegalStateException`
This does work, but for a reason that is a bit obscure. When using an `ImplicitLinkedHashMultiCollection`, `remove` will remove the element b such that a == b, if it exists. This is necessary since if it just took the first element where `a.equals(b)`, it might be a different one than expected. It might be clearer to directly call `removeElementAtSlot`, since we already know the slot number.
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
Nit: Please use `{ }` (even for one line blocks)
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
I think we ditch the before/after methods as I previously recommended.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
I used `equalToObject()` because it makes the intent more explicit.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Fair enough, let's just leave it as is then. Thanks for the explanation.
This shouldn't throw an exception. The existing logic where we fail the task is what we want to do here.
Cleaner to just check if `tasks.isEmpty` after the loop is over.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
Ideally, we would exit the loop after a certain amount of time.
Have you verified that this change is forward compatible as well? Older versions of the sticky assignor need to be able to work with the updated version. The client definitely has access to the group's generation. The question is whether and how to expose it to the assigner.
`hasFetchedRecords` avoids the cost of populating the map returned by `fetchRecords`. No locking is needed for that. So, if I understood the suggestion right, it would look something like: ```java if (fetcher.hasFetchedRecords && client.hasPendingWakeup()) client.poll(0, ...) ``` I guess by calling `poll(0, ...)`, we don't have to expose `maybeTriggerWakeup()`.
The current solution seems more direct than what I was suggesting, which is good. A bit unfortunate that we have to expose `maybeTriggerWakeup()`. I guess we could keep the synchronization around `pollNoWakeup` that you've added and still do the `hasFetchedRecord()` approach. Probably it would look like this: ``` if (fetcher.hasFetchedRecords()) { client.poll(0); return fetcher.fetchedRecords(); } ``` The downside is that it `hasFetchedRecords()` may not be trivial to implement since it would require doing the work that `fetchedRecords()` is doing, but without updating the position. Given that, exposing `maybeTriggerWakeup` doesn't seem too bad.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
For transition to `NOT_RUNNING`: the instance will only shutdown if the user uncaught exception handler decides to shutdown the whole instance, by calling `close()`, in this case it will still go through the `PENDING_SHUTDOWN` transition first? For `REBALANCE -> REBALANCE`, this is related to the thread-level `partition revoked -> partition revoked`, which I'm still wondering if we can avoid. Let's sync a bit on that.
Did we mean to swap `REBALANCING` and `RUNNING` around? If people were depending on the `ordinal` then this will break them
Again, could you describe: 1) Running -> Running 2) Partition Revoked -> Partition Revoked 3) Partition Revoked -> Dead 4) Assigning Partitions -> Dead
Doing in `@Before` is fine. But we don't need to call `new` each time. `this.props` will be a new empty `Properties` instance anyway -- we don't need the second object.
ditto on the properties and the driver.
I think we ditch the before/after methods as I previously recommended.
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
Ack. By bad.
I know this is a bit opinionated, but ... I think we should make an effort to make all locals `final` where possible. It is just a few extra keystrokes (that intellij can do for you!), and it helps to eliminate a class of bugs.
key -> topic
ok, I get that though I think that's just tech debt. for any test related files, we really shouldn't be using anything other than `PERSISTENT_ROOT` so that we can at least attempt to ensure each test/service gets a clean workspace
If this is allowed to shutdown gracefully now, should the subsequent `join()` on the worker thread have a timeout? Otherwise it could hang indefinitely if the worker thread doesn't exit properly. I think some of these unlimited timeouts have carried over from some initial test code I wrote originally, but they really should have timeouts.
I'm wondering if we should log this exception in case `thread_dump` raises an unexpected error. We don't want to lose the original error.
`KeyValueStore` -> `TimestampedKeyValueStore`
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
Why is this needed? This is worse than the previous approach as it opens, closes and reopens the file.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
That has its own complications because if my provider is only providing TrustManagerFacotry.PKIX then it can't provider other services+algorithms that might be expected in calls like SSLContext.getInstance(String protocol, String provider). SSLContext.getInstance might look for TLSv1.1 or TLSv1.2 etc which my provider doesn't really have and I don't have a way to fallback anymore once I go route of specifying "provider" in getInstance() calls. In short - once we have a Provider providing a Standard service+algorithm we may have to implement other services+algorithm also otherwise it may not work (like I mentioned for SSLContext)
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
So let us say - we already have a single provider for JSSE but for Kafka we need one more to override with some of the common services+algorithm (example only override TrustManagerFactory.PKIX). We are at the mercy of the initialization sequence of calls, isn't it? Here the different providers could be owned by different teams and when we are in bigger infra setup it may be difficult to overcome this technical limitation. Only for Kafka applications the init sequence is different vs rest of the infra in the company. Basically, we are making an assumption which may not hold true and then we will be really stuck. What I am suggesting is - If we are calling out that limitation with these changes it is okay but otherwise it will result in a bug.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
remove unnecessary newline
Ah I see, all good then.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
We need to check again after we grab the lock, otherwise the store might get closed after we check but before we grab the lock. Once we get the lock, we're guaranteed that this block is serialized wrt close(). But we can still check beforehand to avoid grabbing the lock if it is closed.
It would be sufficient. I assumed that we were checking before the lock to avoid synchronization overhead, so I left that in place.
We can extract a helper, but maybe we can do _that_ in a follow-up at least, since the concurrency controls needs to be standardized across all the caching stores anyway.
Unless i'm misunderstanding something, I think these test names are inverted. ```suggestion public void shouldParseUnquotedEmbeddedMapKeysAsStrings() { ```
It would be good to have some assertions.
nit: don't need the type params on the next three lines
nit: add `final`
nit: `final` I know it wasn't before, but let's stick to making params as final
I'd probably extract this to an inner class. I just find it a bit unwieldy having an anonymous class of this size. I find it a bit distracting. But i'm not overly bothered either way. Just a suggestion
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
In order to reproduce this issue, we need to reset the generation via `maybeLeaveGroup` before the `onJoinComplete(gen.generationId, gen.memberId, gen.protocol, memberAssignment);` is triggered, but after the join-group response handler is exercised to set the generation id. I think this can still be doable with a single thread, to execute in the following ordering: 1. we only prepare the join-group response in the mock network client, but not the sync-group response, and we also make the MockTime to be able to advance time automatically, then by calling `joinGroupIfNeeded` with a small timeout instead of Long.MAX_VALUE, it should be able to finish the join-group round trip, trigger the handling logic to set the generation id, and then send the sync-group request, but then time out on https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L400 waiting for the sync-group response and return false. 2. Then we call `maybeLeaveGroup` within the same thread. 3. Then we prepare the sync-group response in the mock network client, and call `joinGroupIfNeeded` again, this time the response would be received, and the rest of the logic https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L406-L415 would be executed.
Not clear why we want to use a separate thread to call `joinGroupIfNeeded`? In unit test we would try to avoid any unnecessary multi-threading as it can easily cause flaky tests.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: Indicate that this needs shallow iterations on the entries.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
I'm fine as well, will make a reference to 10055 of this PR
Might be better to add a proper POJO maybe `StreamsMetadata` or something that wraps the `streamTime` Long plus `ProcessorMetadata` instead of using `KeyValue` ? We might add new fields later on what is easier to do for a new POJO.
format: no need for curly braces
format: ``` if (exception != null) throw exception; ```
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
nit: as above, use `@link`
```suggestion * Example objects needing to be closed include {@code org.rocksdb.Filter} and {@code org.rocksdb.Cache}. ```
nit: insert `<p>`
@jeffchao traditionally Kafka used key,value pairs in properties and pass it everywhere and each implementation takes look at this config and pulls their interested key,value pairs. Example, authorizer interface https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/security/auth/Authorizer.scala#L35 . The pluggable class when it gets instantiated a configure method will be called and all the key,value in server.properties will be passed and it will pick whats relevant to the class. We can do the same here instead of asking users append key,values into the a config which is hard to configure and hard to get it right.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
`long,long` is used for `WindowStore` while `Instance,Duration` (or `Instance,Instance` if we correct it) is use for `ReadOnlyWindowStore` that return the same iterator.
nit: remove empty link
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
req: I think we want to introduce some `acceptableLag` config within which a task is considered caught-up, otherwise this is way too strict. ie the condition should be `lag <= acceptableLag`
req: drop the `!caughtUpClients.isEmpty()` check here, if it's in the map it should have at least 1 caught-up client
```suggestion * @return map from tasks with caught-up clients to the list of client candidates ``` or something similar to make it clear the map only contains tasks with caught-up clients
Why the qualification that says "that generates stateful processors"? AFAIU nothing prevents the user from supplying a state-less transformer (supplier), even though yes, you can also provide the names of state stores when calling this method.
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
I think we ditch the before/after methods as I previously recommended.
ditto on removing before/after.
Ditto on the properties and the driver.
`schemaType` is null means that the value type is not supported by the Connect's Data API. May be we should throw an exception with a message indicating this.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
nit: extra space.
Regardless of where the `lookupCoordinator` is triggered, we are only raising it from `ensureCoordinatorReady`, so I am not sure I follow the point about raising from other contexts. Note there doesn't appear to be any logic preventing multiple listeners from getting attached to the future. I think it would be better to always attach the listener when the future is created.
the method `restorePartition` is no longer used and can be removed
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
@mjsax What you suggested sounds right to me.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
Maybe we could use a different value here.
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
@tadsul We could perhaps convert this to a immutable map and store `in `originals` like we do for `values`.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
How about "runs an external command for the worker."
I don't think limiting the pool to 2 threads is really what you want, since you have 3 runnables that you are creating later on. Check out some of the other newXYZThreadPool functions. There is at least one that doesn't limit the number of threads.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
`KStream` => `{@link KStream}`
same... `@link` > `@close` for this case.
I would use the full qualified name of `Filter` and `Cache`. If anybody wants to look them up it is easier. Additionally, I would enclose it into `{@code ...}`.
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
I actually had a similar thought, but I am torn though. Using two variables required to keep them "in sync" was is not great. However, using `null` is less explicit... Thus overall I am fine either way as both seems to provide the overall same good/bad ratio.
Thanks. Understood. It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information. In `close()` before we actually "loose" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`). This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies.
nit: keep fields with the same access level together
Nit: can be `final`
Nit: both parameters can be `final`
Nit: can be `final`
Great catch, thanks @showuon !
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Just a small nit: this is how my brain works -- otherwise the condition is "reverse" and I need to flip it in my head
Maybe this looks better? ```suggestion // we're at the end of the input. if (queryResult.getResult().value() == batch1NumMessages - 1) return; ```
So we basically do 10 re-tries? Is this intended? Or should be just sleep for a hard-coded "backup time"
Why the qualification that says "that generates stateful processors"? AFAIU nothing prevents the user from supplying a state-less transformer (supplier), even though yes, you can also provide the names of state stores when calling this method.
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
I haven't seen this struct being used.
`...build a graph to calculate partition number of repartition topic, and numOfRepartitions of underlying TopicsInfo is used for memoization.`
nit: we could require non-negative value for `numOfRepartitions`
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Not sure if this will actually be cleaner or end up more complicated, but you may be able to reuse some of the `StickyTaskAssignor` code here which does similar things
maybe also add a `lastAssignedTask(List<TaskId>)` helper to clean up `source.get(source.size() - 1)` used here and below
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
Thanks for the explanation: checking per-commit is indeed easier. Moving forward we can even make them two separate PRs for other reviewers to easily review.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
`Integer.toString` is a slightly more concise way of doing this.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
@becketqin Yes, my preference, as mentioned above, is to deal with that problem separately. We should not make behavioral changes without first raising the issue at least in a separate JIRA. The unintuitive thing about the proposed behavior to me is the fact that although the consumer's position remains at the offset of the failed record, the next returned record will be from the offset after that position. You can see this in the test case below: the consumer's position is at 1, but the returned record is at offset 2. This makes the behavior less deterministic. It would be nice to maintain the invariant that the next fetched record is always the first record at an offset greater than or equal to the current position.
Please update the test case as I suggested. Thinking about the current patch. If there is an exception parsing or validating one of the records, we will update `PartitionRecords.nextFetchOffset`, but we will not change the current position (i.e. what is returned by `consumer.position()`. That means in the next call to `poll()`, we will simply discard the rest of the records. So there is no behavior change here and my suggestion above simply makes the behavior explicit. You can confirm this by updating the test case.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
`KeyValueStore` -> `TimestampedKeyValueStore`
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
an -> a
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
IIUC, this changes the behavior of the `WorkerConnector` created below. Prior to this PR, the `WorkerConnector` was always created with the `Worker.offsetBackingStore`, even for sink connectors. However, with this PR, the `WorkerConnector` will be instantiated with a null `offsetReader` parameter, which will cause a NPE in `WorkerConnector#doShutdown()` and `WorkerConnector#cancel()` since `WorkerConnector` does not check for a null parameter there.
Suggestion: ```suggestion // Set up the offset backing store for this connector instance ```
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
Can/Should these be immutable values? Maybe you can use the singleton `Collections.emptyList()` to avoid making memory allocations.
@adyach I have noticed that this "hidden" logic is what already happens today for the `ConsoleConsumer` tool for example (in the related `ConsumerConfig` class) so I think that we can live with that. FYI as part of the refactoring for #3453 (for having it more testable), I decided to use that PR for introducing the `CommandOptions` class even getting your good ideas here. In this case when the #3453 will be closed we should have a first version of some of the common components we need for tools refactoring.
@adyach having the parsing here could be a good idea but exiting from the application maybe not. This should be the base class for other command options classes but changing the application flow could be misleading here (i.e. Exit.exit(1)).
This definitely doesn't cover the full space of errors that are possible here -- `asSubclass` could throw a `ClassCastException`, `newInstance` could also throw `SecurityException`. I think the `catch` was broad because this ensures that except for extreme cases like other `Throwables` or `Errors` we get everything converted to `KafkaExceptions`.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
nit: would be nice to be consistent on the pattern we use here
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
Better be `cooperative-sticky`? `cooperative` is too general I think.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
We can use `ApiResult.completed()`
Yes, we can open a JIRA to do it later.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
nit: we do this same thing in the other `#resize` for thread count changes, can you factor it out into a helper method? Then I think we can narrow the scope and make only that helper synchronized (should double check that though)
Nit: we typically just say `partition` in these cases. Same for the other `log.debug`.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
maybe "Restoration completed for partitions:"
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
This is an interesting question. One low-fi solution would be to think about using `equals()` (I think to pull this off, we'd need to introduce a requirement that serde/serializer/deserializers implement equals in a way that would be semantically sound for us. This would not be a back-ward compatible change. On the other hand, since callers actually subclass `Serde<T>` with a fixed type like `Serde<String>`, it actually should be available at runtime. I don't remember the hoops you have to jump through to get it right now, but I'll revisit it tomorrow.
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: remove unnecessary `this`.
I was debating the same thing. Won't `NetworkClient` keep the node under the `CONNECTING` state though? It seems like either approach involves a change in the contract that could affect users who are not expecting it. It's an internal class though, so we just need to make sure that the affected Kafka code is updated (if necessary). It would be nice to include a test for this so that we can verify that things truly work under this scenario.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Ouch! Sorry about that!
We don't want to remove the `stateDir` entirely, because then `stateDir.exists` will return false and we won't actually call `listFiles` which is what this test is trying to test. Can we just rename to `"state-renamed" + TestUtils.randomString(5)`? Or, clean up the previous contents at the beginning of the test? That said, `cleanUp` should be called after every test, so there shouldn't be any left over state...right? cc/ @guozhangwang
That's true, but I suspect that this leftover folder is not from this test suite but likely from others? I think the right fix here should be to check which test (highly doubt it is one of the `StateDirectoryTest` because of `cleanup` ) has this leftover and fix that one instead.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
Any reason you change to import all classes under `java.util`? I think we should import what we used in this class only.
I know we're violating this a few places (due to the initial code import), but I think we want to avoid converting to `*` imports.
`log` not used
Thanks for cleaning up the code duplication.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
I don't think it can be. It needs to be a TimelineHashMap to work and needs to receive the snapshot registry in the constructor.
nit: we can use `map#compute` to replace getOrDefault + put.
We could make this field access `public`
Ah ok fair enough -- thanks!
Seems to fit in one line
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
method name changes
`windowSize` should be `Duration`
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
Is this really worth it? It seems like a `forMagic` without a transactional id gives you most of the benefit.
Same as above: need to check `clientResponse.hasResponse()`
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Since shouldBeSinkNode.get(0) may not be SinkNode, consider renaming the variable
I think one call of storeToChangelogTopic.keySet().removeAll() outside the loop should be equivalent to what you have now.
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
This was a separate bug, I guess? Might be worth mentioning in the PR description.
This doesn't seem to be used.
This doesn't seem to be used.
Thanks for the explanation @dguy, very helpful to understand where caching and sequence numbers come into play. It might be worthwhile to put this in a JIRA somewhere. I do think it would be a useful optimization to have eventually, as fetches have some setup / teardown overhead.
Here we need to do: `final Agg oldValue == newValue == null || sendOldValues ? fetchPervious(..) : null;` This is because `SessionWindows` have a dynamic time range, the the start is always fixed. So we need to send deletes for the previous smaller window when a window is merged, i.e, a simple count: a@0 -> SessionKey(key=a start=0, end=0), 1 a@5 -> SessionKey(key=a start=0, end=0), null (delete this as it is merged) SessionKey(key=a start=0, end=5), 2 (this is the new merged session)
@xvrl there is no `get` on `WindowStore`. We could add one and it would work in scenarios where we don't have duplicates, i.e., the key for a WindowStore is (recordkey, timestamp, sequenceNumber) - if the store doesn't have duplicates the sequence number is always 0. If the store does have duplicates then we don't know what the sequence number is. Without a KIP to add a `get()` to `WindowStore`, the only thing we could do is add a bit of a hack to see if the inner most store is a `RocksDBSegmentedBytesStore` and then we could call `get(..)` on that. If it isn't, then we'd still need to call `fetch`. For the DSL this would work as the only time we have duplicates in the `WindowStore` is for joins and we disable caching for those so it skips this code path. However, for the PAPI, we would need to always disable caching if duplicates are set. Which we probably should do anyway as it won't work as is.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
We should also create a client connection with one of the newly disabled protocols like TLSv1.1 and verify that the client connection fails.
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
`deteremined` => `determined`
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
I think a better test scenario is to move the logic in `close()` call, i.e. when the stream thread is being shutdown, and topology is closing, we call `processorNode.close()` in which we wait for a while and then tries to access the global store. It mimics the case where in closing the store cache is flushed and hence tries to access the global store again.
Nit: might be worth adding a simple assertion on the result just to make sure.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
Something does not work as expected in this algorithm. According to this doc, the assignor should fall back to distributing tasks on least-loaded clients. However, the following test case fails: ``` @Test public void shouldDistributeTasksOnLeastLoadedClientsWhenThereAreNoEnoughUniqueTagDimensions() { final Map<UUID, ClientState> clientStates = mkMap( mkEntry(UUID_1, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_1), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_0)), mkEntry(UUID_2, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_2)), TASK_0_1)), mkEntry(UUID_3, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_2)), mkEntry(UUID_4, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_1_0)) ); final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates); final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, CLUSTER_TAG, ZONE_TAG); new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs); assertEquals(1, clientStates.get(UUID_1).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_2).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_3).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_4).standbyTaskCount()); } ``` The standby task for active task 0_0 can be put on client UUID_2 and the standby task for active task 0_1 can be put on client UUID_1 without breaking rack awareness constraints. Standby tasks for active tasks 0_2 and 1_0 cannot be put on any client without breaking rack awareness, so they should be distributed on least-loaded clients. However, that does apparently not happen, because client UUID_3 and UUID_4 are not assigned any standby.
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
Q: Why do you use a mock here? In the ticket you said you just want to replace `MockStreamsMetrics` with `StreamsMetricsImpl`. req: If there is no specific reason, I would propose to either create a common mock that can be used everywhere as I proposed or to consistently replace `MockStreamsMetrics` with `StreamsMetricsImpl`.
If it is no more an integration test, this should be removed.
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
store not used
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
nit: add `final`
nit: add `final`
nit: add `final`
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
Sounds like a good idea
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
I had a look at this and your are right. It seems that keeping `TopicPartition` is better and difficult to change. In this case, have you considered pushing the conversion to the `Builder` by providing an overload of `setTargetTimes` which accepts a `Map<TopicPartition, ListOffsetPartition>`? That could make the code in the `Fetcher` a bit cleaner.
nit: "another thread wrote to ..."
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
@rajinisivaram, hmm, I'd rather us specify the details or link to a config that specifies them. With security, people often struggle so the more information we can provide, the better.
methods => `mechanisms`
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
This part will have some conflicts with @mjsax 's PR, just a note.
You mean `now`? :) If yes please feel free to resolve the ticket when you merge this.
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
Nit: add `final`
Nit: use `{ }` for the loop body.
cosmetic: extra space at the start
guaranteed -> guarantees
remove this line -- not required.
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
Yeah, it's not pretty. However, reducing the concurrency of `BufferPool` in the common path is not desireable. We definitely need to handle OOMs correctly, but they are relatively rare and it's OK if that path is slower.
Good point. I think it is still worth keeping the optimization, although typically the the producer will only allocate poolable batch size, so the actual memory allocation should not happen very often.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
ditto on removing before/after.
Personally I think we should call out the specific method that was removed so we don't cause undue panic. It was only a single method, after all.
> It seems to me a lot of work for little result to research all methods removed without deprecation period. Exactly -- we shouldn't push this to every user on every upgrade. It's annoying, so, we should just do it exactly once, and only before considering an upgrade.
I would be more concrete: ``` RocksDB version will be bumped to version 6+ via KAFKA-8897 in a future release. If you use `org.rocksdbOptions#WhatEverMethodWasRemoved`, you will need to rewrite your code after KAFKA-8897 is resolved because those methods were removed from `org.rocksdbOptions` class without a deprecation period. ``` Or something like this.
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Good point, thanks for clarifying.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
The issue is that local cache may not contain this topic metadata yet or not up-to-date, and that's why we may want to send an `MetadataRequest` in these two function calls.
API changes will call for a KIP I'm afraid. We'll also want to ensure that we preserve the old method for compatibility.
I think the root cause of the hanging is that, the `list offset` response returns empty indicating the broker did not know about this topic, and the client will hence retry forever. So what I suggest is that just making the metadata refresh once in `position()`. As for the flaky unit tests @baluchicken , we can use waitForConditions in those test rather than block waiting until it succeeds.
It seems you can move this line after line422.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: maybe iterate over `entrySet()` instead.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
We should not print the stacktrace imho, but verify the exception message using `assertThat`.
typo: CompleteableFuture -> CompletableFuture
nit: add `final`
I think this parameterization is a pretty good idea, and I can add it to my delete topics PR. But if we are going to change the public API, we should update the KIP and potentially update the mailing list with the changes.
We need to keep the Admin API backwards compatible. An application that was written using the 2.7.0 should not break if it is compiled with a 2.8.0 clients jar. You can always add an internal class with shared code to avoid duplication, but the public API itself needs to remain compatible.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
as per previous `assertThat(..., instanceOf(...))` would be better
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
I'm ok saving this for #7409.
I think we'd want to override `parseResponse` for `API_VERSIONS` only.
In the parsing logic, we still convert to struct first before calling `AbstractRequest.parseRequest`. I think we could bypass the `Struct` conversion by changing `AbstractRequest.parseRequest` to take the `ByteBuffer` instead of the `Struct`. ```java public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, ByteBuffer buffer) { ``` Then in the fetch case, we could just call this method.
We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
I don't see bucketing
I personally was on the side of always using task stream time everywhere but more people feel that we should use processor stream time :P Anyways, all I'm trying to say is that we need to make an educated decision here, and if we concluded that either 1) we rely on task time here, but still use processor time on other expiration logic, or 2) we rely on processor time on all logic, or 3) we rely on task time on all logic, we have a good rationale for whichever we choose.
Do we need to maintain it manually? Could we use `context.streamTime()` instead? Note that `context.streamTime()` might be slightly different because we advance it for every input record. Thus, if there is a filter before the join, the join might not get all records and thus it's locally observed stream-time could differ from the task stream-time. It's a smaller semantic impact/difference and it's unclear to me, if we should prefer processor-local stream-time or task stream-time? \cc @guozhangwang @vvcephei
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
nit: would be nice to be consistent on the pattern we use here
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
Please add the exception at the end of the line so that we can get a stack trace, including any "cause" exceptions. Example: ``` log.error("{}: Failed to start the external process.", id, e); ```
Please include the problem in the errMsg which is used to complete `doneFuture`. For example: ``` errMsg = "Failed to start the external process: " + e.getMessage(); ```
Maybe something like "No command specified" would be more descriptive
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
`replicaing` -> `replicating`
nit: The mention of join group comes out of nowhere
Also add `@params topics`
nit: remove `which is`
i.e., add `fail` after this line
Nice tidy up of this test class :-)
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Thanks for the explanation, makes sense.
`final` ? All of the fields should be `final` really
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
We shouldn't return `null`, but instead return a "unknown query" result.
Dropped this unnecessary duplicate code, as we discussed.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
We should include `startPosition` in the message.
I don't think we need the `null` checks.
Nit: "The file channel position..."
`KAFKA-13046: Improve the test coverage for stickyAssignor` is created. Let me handle it! :)
I'm thinking we can have a test for package scope `partitionsTransferringOwnership` in `AbstractStickyAssignorTest`. I found we didn't test it before. We can verify the doubly assigned partitions and other revoked partitions are put into `partitionsTransferringOwnership` correctly.
@ableegoldman , I reviewed it again, and found we forgot to sort the `unfilledMembersWithExactlyMinQuotaPartitions` list here, to have deterministic result.
nit: add `final` and line too long
Can you elaborate? What do you mean by > otherwise the state won't proceed
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
Should be final.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
Harsha has done this.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
nit: simpler or not? ```java Map<String, Uuid> newTopicIds = topicIds.entrySet().stream() .filter(entry -> shouldRetainTopic.test(entry.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); ```
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
I think this config property key seems a misfit, and probably reflects an earlier incantation of the design before KIP acceptance. It might be worth - in a separate PR - renaming this to something like `errors.tolerance` to better align with its purpose.
We'll need a separate AK issue, then.
nit: matches the old behavior is very relative
same here -- sounds like CachingKeyValue with TimestampStore
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
nit: parameter/line formatting
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Uggh, yeah, I forgot about this. We kind of inherit some annoying types from Kafka's config setup but I tried to ensure we're using String types where possible. It gets a bit hard to figure out what is valid where -- the JsonConverter actually gets passed a `Map<String, Object>` as that's what is returned by `AbstractConfig.originalsWithPrefix`, but in practice they are all `String` so type erasure allows this to work...
Sounds good! There's no rush, but I'll make sure we have your new PRs reviewed and merged quickly whenever they are ready, since you've worked so hard on this already. I'm sorry I wasn't able to make another pass on your original PR, but hopefully this won't be too much of a bother.
Ah, I see the confusion. The `#isTopologyOverride` method checks whether the config has been overridden for the specific topology, ie has been set in the Properties passed in to `StreamsBuilder#build` -- it's not looking at what we call the `globalAppConfigs` which are the actual application configs: ie those passed in to the `KafkaStreams` constructor. So basically there are two sets of configs. The value should be taken as the first of these to be set by the user, in the following order: 1) `statestore.cache.max.bytes` in `topologyOverrides` 2) `cache.max.bytes.buffering` in `topologyOverrides` 3)`statestore.cache.max.bytes` in `globalAppConfigs` 4) `cache.max.bytes.buffering` in `globalAppConfigs` Essentially, using `#getTotalCacheSize` on the `topologyOverrides` if either of them is set (which this PR is doing) and on the `globalAppConfigs` if they are not (which is the regression here). On that note -- we also need to move `##getTotalCacheSize` out of StreamsConfig, because it's a public class and wasn't listed as a public API in the KIP (nor should it be, imo). I recommend creating a new static utility class for things like this, eg `StreamsConfigUtils` in the `org.apache.kafka.streams.internals` package. There are some other methods that would belong there, for example the `StreamThread` methods `#processingMode` and `#eosEnabled` should be moved as well Hope that all makes sense -- and lmk if you don't think you'll have the time to put out a full patch, and I or another Streams dev can help out 
If there are race conditions, we need to fix them. Remember that Streams itself has to use AdminClient.
This is a one-node cluster, though, right? > /** > * Runs an in-memory, "embedded" Kafka cluster with 1 ZooKeeper instance and 1 Kafka broker. > */ > public class EmbeddedKafkaCluster extends ExternalResource { >
I'm a bit concerned using `listTopics` than using `JavaConverters.seqAsJavaListConverter(brokers[0].kafkaServer().zkClient().getAllTopicsInCluster()).asJava())` as only the latter can get the source-of-truth on ZK while the former may be subject to race conditions (e.g. if you create the topic and then call listTopics, it may not be included if the metadata was not propagated yet).
We should have a constant rather than using '262' directly
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
nit: should be removed (similar below)
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
nit: The sentence sounds slightly better if you remove `the`
Here I'd suggest doing the opposite: `poll(0)` since it is during the normal processing, not during restoration; so we can afford to not having some time in a few iterations. Instead, we want to proceed to the next iteration to call the normal-consumer.poll sooner to not be kicked out of the group.
nit: these three can be package private
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
Yeah I think that makes sense here
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
I'm not sure how `removeMembersFromConsumerGroup` would behave if you passed in `""` as the `group.instance.id`, do you know? If not then let's just be safe and check what `streamThread.getGroupInstanceID()` returns, and skip this call if there is no group.instance.id (ie if not static)
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
Does this actually buy us anything here? If `forceClose` is false, then doesn't that mean that there are no more in-flight requests? I think we still increment the in-flight request count even if the client doesn't expect a response.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
nit: remove empty line
nit: add `final` (2x)
nit: final on params here and methods below.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
nit: why double space? (similar below and further below)
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: preserve empty line after `checkAndClearProcessResult`
Another tab here that should be replaced.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
If we decide to not turn on bulk loading, then we should be able to not close and restart again after restoring is done.
yeah, the existing `IllegalStateException` is confusing and we should fix it.
nit: top of class
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
nit: single parameter per line
nit: move parameter to next line
nit: I'd make this final with no assignment, then assign it in both branches below.
nit: log.info("Suspended {}", state());
req: no longer used
Again, a bit more information would be more useful: ```suggestion // Then if we delete the connector, it and each of its tasks should be stopped by the framework // even though the producer is blocked because there is no topic ```
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
I'm wondering if we should make this `info` or `warn` level. It doesn't seem like it would be very verbose, and it might be nice to see by default because it will have secondary effects later on when we try to start a new transaction, but get blocked. But I also don't feel strongly about it, so I leave it to your discretion.
req: This is unnecessary
ok - same thing three times. Maybe extract it to a method `verifyTransactionInflight`
nit: extra space before `anyObject`
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
req: Please also verify `stateDirectory.unlock("0_2")`. Only verifying `lockedTaskDirectories()` seems too weak to me.
Would it make sense to do this check even before checking if the coordinator is known? Moreover, it seems that we could skip calling `doCommitOffsetsAsync` entirely by returning a completed future directly. What do you think? ``` if (offsets.isEmpty()) return RequestFuture.voidSuccess(); ```
I considered this but I think that it is clearer when kept separated.
I checked again, and I think it's OK. Thanks.
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
Same thing here as above: probably need to use `worker.getConnectorType(className)` here.
Same issue here wrt connector name vs type. We probably need some chain like `connectorType(connectorClass(map))`.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
IMO, this would be a bit cleaner with a separate executor with a single thread for the status updater.
Yeah might as well change it I think, it results in shorter code
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
nit: we can do without the curly braces here and above. However, these will soon be replaced by the actual impl
the key type of `topics` is `Uuid` and hence this check is weird. Maybe it should be replaced by `topicNameToId`
This could be more concise: ``` topicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment); ```
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
nit: add `final` and line too long
Can you elaborate? What do you mean by > otherwise the state won't proceed
this test doesn't seem to throw `InterruptedException` as well
I'm happy for this to be merged if @hachikuji is happy fwiw.
You mean if the offset is out of range? I'm not sure we have a good way to check this at the moment. It can't be done on the coordinator because we don't know what the valid offsets are for each topic partition, so that leaves the client where the check may end up stale anyway. By the way, there are a couple `commitSync` overloads that may need to be updated as well.
What is the offset commit is positive and invalid? cc @hachikuji
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We can use `StringDeserializer.class.getName()`
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
Same as above, e.g. `shouldNotThrowWithoutPendingShutdownInRunOnce`
nit: remove empty line
instead of `fail` we should use ``` assertThrows( ClassCastException.class, () -> producer.send(record); ); ``` For this case, we can remove the `catch` block below.
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
Could we rename this to something like "remainingPartitions"
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
```suggestion log.trace("Topic creation by the connector is disabled or the topic {} was previously created." + ```
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Should we include the brokers instead of removing them? Same below.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
Would it be easier to understand if this handled all of the unwrap exceptions after the IOException? And then we could call this method `processUnwrapExceptionAfterIOException`.
req: This is unnecessary
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Adding to `connectorProps` won't change the already instantiated `config`.
If a public API change like this is required, you will need to propose a small KIP. I'm unclear why it's required tho, and ideally we would not alter the existing API if possible. If a new method is required, I think "track" is too ambiguous and should not be used here.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
@zhuchen1018 : But the issue is that we already processed the response. Next time, when we come back to handleCompletedReceives() again, we will get a response intended for a request different from what's in the head of inFlightRequests.
@becketqin : Yes, if it's a bug, it's going to be hard to auto fixing it in the client code. Just propagating the exception to the caller is probably the best that we can do.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
Same issue here wrt connector name vs type. We probably need some chain like `connectorType(connectorClass(map))`.
Same thing here as above: probably need to use `worker.getConnectorType(className)` here.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
Why is the order of these methods different than in `ConnectorStatusListener`? Also, the `TaskStatusListener` methods always forward the method to the delegate _last_, whereas the methods of the `ConnectorStatusListener` use a mixture. Let's make them consistent.
This is going to be modified and accessed on potentially different threads, right? If so, we should add the `volatile` modifier here.
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
nit: as in `position` above, `this` is not required
Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations: ```suggestion if (reporters.size() == 1) { return reporters.get(0).report(this); } List<Future<RecordMetadata>> futures = new LinkedList<>(); for (ErrorReporter reporter: reporters) { Future<RecordMetadata> future = reporter.report(this, callback); if (!future.isDone()) { futures.add(future); } } if (futures.isEmpty()) { return CompletableFuture.completedFuture(null); } return new ErrantRecordFuture(futures); ``` And since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a `LinkedList` instead to avoid excessive allocation when adding the first element to the `ArrayList`.
```suggestion return futures.stream().allMatch(Future::isDone); ```
same question here and below about locking around a `volatile` variable. Is this the only reason to lock here? One would think so based on previous usage.
I don't think locking buys you anything w/r/t to that. The producer#send in the status backing store is asynchronous. So what would describe above can happen anyways, regardless of whether you lock this object. Of course, if it wasn't asynchronous things would be much worse. A bottleneck would be created by the lock, waiting for the `send` to finish in every locked block, so that's not an option. Wdyt? That's what I see at the high level without spending to much time on it, but see if you can check this assumption and we can return to this question.
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
It isn't about blocking vs non-blocking. It is about how a Kafka broker behaves to a non-authenticated request on SASL ports... we need to test it (since it is the requirement of this JIRA). We hope that SASL behaves exactly the same as PLAINTEXT, but we don't know that it does without a test.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
Good call. I'll update the code to throw an exception like in `Worker` when creating a source and sink task.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
Shouldn't the indentation be as follows? ``` log.debug( "Encountered assignment error during partition assignment: {}. Will skip the task initialization", streamThread.assignmentErrorCode ); ``` The first parameter is one or two characters too long, though.
as above. (don't make lines longer)
There are two different error codes: - `VERSION_PROBING`: for this case we continue and rejoin the group; also the assignment is empty - `INCOMPLETE_SOURCE_TOPIC_METADATA`: for this case we shut down, and this case is already handled above. Hence, I think we don't need to log anything for this case? (Note that we log the version probing in StreamThread later and the metadata error is logged above already)
Make all locals `final`
add `final` wherever possible
Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
Nit: space before `:`.
Nit: space missing before `:`. There are other cases like this in the file.
These iterators need to be closed or they'll leak resources (it's the same for IQv1 as well).
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Would it help to actually list the method that was used, in case somebody thought they were using basic? ```suggestion log.trace("Request credentials used {} authentication, but only {} supported; ignoring", BASIC, method); ```
nit: `addMetadata` -> `put`
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: top of class
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
Nit: "Assigning tasks to streams clients: ..."; also better to be `log.debug`.
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
I think we need to skip all the code in this else block if the partition is no longer assigned.
@enothereska Yes, I think that is a better solution. But I think @hachikuji was right that we don't need to cover both if/else branches. We just need to cover the `parseCompletedFetch`
@enothereska The trunk code does not need to access `subscription.position`, instead it uses `PartitionRecords.nextInlineOffsets` which should be the same as position because the position is updated to this value every time after a successful `fetchRecords()`. The big try/catch is to make sure the the exception from `fetchRecords` will also be caught and not result in loss of non-empty `fetched`.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Could you please add some line breaks? This and some of the other verifications are too long.
Nit. use `{ }` for all code blocks
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
nit: also add java doc for type `T, O` here
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
nit: in spite of the getter convention, I still prefer setters be prefixed with `set`
Technically we may still have a connection. Do we need to close the client first before we decrement these? (ditto for others)
I suggest only catching `KafkaException` and only close the client then. About other exceptions, we can probably try retrying or fail the thread, I don't have a strong preference. I don't think we should catch Throwable though, as that catches errors like `OutOfMemoryError`
I think that's fine, I don't think there's a way to recover (nor if it makes sense) from an OutOfMemoryError - https://stackoverflow.com/a/352842
I feel we could actually simplify the test by calling `streamsProducer.kafkaProducer()` every time for the check for the internal producer, instead of keeping a reference here, as the `eosAlphaMockProducer` and `eosAlphaMockProducer` look quite similar.
Not necessarily, we could pass in both processing mode and the expected output as parameters, if the test workflow looks essentially the same.
Sounds more like `common tests`
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
I don't mind diff noise if it makes things better btw (even slightly)
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
Add a reference to KIP-511 here
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
Though now I look at the message for `UINT16` I see it would be consistent with that. Still I think because there are two types involved here, the Java type and the network type, including both is clearest.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
even clearer: "Represents a signed integer"
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
As above: need to keep default value.
For backward compatibility, we need to keep the old default value. Btw: we don't do any ordering here yet -- just above. so no need to reorder anything here.
Might be overkill if this is the only use case, but we could also add a composite validator.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Thanks for the clarification @hachikuji
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
It would be good to elaborate on why we need to do this as it's not obvious by just reading the code.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
nit: Could we move `new DeleteTopicsRequestData.DeleteTopicState().setName(topic1)` to a new line in order to align it with the following `new DeleteTopicsRequestData`? There are few other cases like this.
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
Missing newline character.
That is right, thanks @omkreddy .
Thanks @omkreddy. I should have expanded the diff to see the docs.
nit: it was correct before
Could you please already open the follow-up PR with scaffolding and link it here? I think otherwise we risk to forget about it.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
Nitpick: We don't need to explicitly check for `other == null` -- `instanceof` (which is required for the casting logic anyways) will return false if its first argument is null.
The `KeyValue` class allows null values for `key` and `value` (at least I didn't see input validations such as throwing IAE in its constructor when either key or value are null). So we must guard against nulls / NPEs here.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
nit: add `final`
It's unusual to hold a reference to an abstract class like this. I believe the intent is to be able to transparently handle either `KeyValueSegments` or (I'm guessing) `KeyValueTimestampSegments`. The full expression would be to have a `Segments` interface implemented by `AbstractSegments`, which is then extended by your two implementations. Then this line would reference `Segments<S>`. It's fine to collapse this into just the abstract class (although questionable in the presence of package-protected fields). But to maintain transparency, I'd name the abstract class `Segments` instead of `AbstractSegments`. That way, to an outsider class (like this one), you're still just interacting with the interface (i.e., the public interface of the class), rather than specifically an abstract class. Adhering to this pattern leaves the door open in the future to extract `Segments` into a full interface without having to change any outside code (which is what I meant by maintain transparency).
How much effort would it be to have a test case for this? We have a few LeaveGroup tests in `ConsumerCoordinatorTest`.
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
nit: use `{}` instead of string concat for `retries`
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Do we need to pass in all admin client configs? Actually, we only need retries. Also, I am not sure if we would set the correct default here. If nothing is specified, we would use `StreamsConfig` retry as default, but we actually should set `AdminClientConfig` default -- atm, both might be the same so it doesn't matter too much. Just is doesn't seems to be "correct" (in a very strong sense).
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
I think nicer to just `return requests.first()` here
The `connectorStatus(connector)` call might fail with a `NotFoundException` if the connector was removed after the `configState.connector()` method is called but before the status for the removed connector is asked for. Although this shouldn't happen within the process (since requests are handled sequentially by a single thread), it may be possible that this herder is not the leader, that the leader performed the change, and that this herder's config state backing store read that change after the `configState.connector()` method was called but before the `connectorStatus(connector)` method is called for that connector. Should be easily handled with a try-catch, and if the connector with the specified name is not found then simply continue to the next connector name. Something like: ```suggestion try { out.put(connector, connectorStatus(connector)); } catch (NotFoundException e) { // do nothing with connectors that were just removed } ``` Note that if a connector is *added* with similar timing, the new connector name will not be returned from `configState.connectors()` and the new connector will not be included in the results. I think that's fine, considering the result of this method call would still be consistent with the state at the time the `configState.connectors()` call is made. Call it again, and you'd see the new connector.
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
nit: I think it's better to just print the e.message in a single line.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
just `name` should be fine
nit: remove var `newJoined` (also not used for left-hand-side code)
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
My bad. My suggestion inserted a typo. At least I saw it before I start the build. ```suggestion return new HashMap<>(connectorConfigCallback.get(herderRequestTimeoutMs, TimeUnit.MILLISECONDS)); ```
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Strictly speaking, this shouldn't be necessary as `SCHEMA_TYPE_CLASSES` should have a `Schema` instance for all `Schema.Type` literals. And with `SchemaBuilder` a connector or converter cannot create a schema instance with a null `Schema.Type`. However, it is possible to construct a `ConnectSchema` instance with a null `Type` reference (like what `FakeSchema` essentially does in the existing test), which of course without this change would result in this method returning a null list. So +1 for this line change since it simplifies the error handling in the calling code.
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
nit: typo in description
line is too long
Nit: add `{}` to block
Good find! > However, gradle does not allow the usage of Base64 since it "could not be found" according to the compiler anyways. How did you try to use it? Everything from the standard library should be available... I would prefer to use Base64 if we can. If not possible, we can still fall back to using String, but I would really like to avoid it if we can.
@guozhangwang @bbejeck @vvcephei Do you think it's worth to add a version number for the binary format of the committed offsets (I tend to think we should add a version number). I would also not encode the timestamps as `String` but as 8-byte binary long.
To be future prove, we should encode a version number as prefix in case we ever what to change this metadata. What about `<version>:<partitionTime>` with version number "1" ? Also, line is too long. Move both parameters to their own lines.
I'm assuming this is just extracting the inlined function and hence skipped and did not compare line by line :)
```suggestion log.debug("The offsets have been reset by another client or the group has been deleted, no need to retry further."); ```
That's right. Thanks for the explanation.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
Let's rename `headers1` and `headers2` here too
this test doesn't seem to throw `InterruptedException` as well
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
as above. `requireNotNull` not necessary any longer
Good point, I think we should add `this.nano += TimeUnit.NANOSECONDS.convert(autoTickMs, TimeUnit.MILLISECONDS)` in `nanoseconds()` as well.
My reasoning is that the tests that expect the clock to auto-tick would be affected if the code under test changed from `milliseconds` to `nanoseconds`. Am I missing something? And is there a reason to only auto-tick `milliseconds`? `FastClock` is an interesting idea, it seems to depend less on how often `milliseconds` is invoked by the code under test. It seems more realistic too (in a sense, it's like reducing all timeout values by a multiplier).
This seems to overlap with #4095 -- should not be part of this PR IMHO.
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: fits in one line
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
nit: these three can be package private
empty line needed
you don't need this. Junit gives you a new instance of the test class for every test method
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
We can use `TestUtils.assertFutureThrows()` here too
nit: Indentation of those lines seems to be off here.
nit: We could use `TestUtils.assertFutureThrows` here.
We can remove `requireNonNull` here, because `getter.keySerde()` would already throw a `ConfigException` if the default serde is null.
as above. `requireNotNull` not necessary any longer
as above. `requireNotNull` not necessary any longer
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
Yeah, I don't think it's worth doing it for broker properties at the moment.
Hmm, this doesn't seem great.
typo: byteArrray -> byteArray
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
nit: we have assertions like this in many test cases. With a more direct api to update quorum state, we can move these assertions into that api.
I don't think we need this `null` check either.
I don't think we need the `null` checks.
Nit: "The file channel position..."
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
yes, it seems to be not what this test is checking on. I think we can drop it here.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: add `final`
Minor but I'm not sure if we'd prefer the builder's toString or the underlying request struct's toString as was formerly the case.
We don't need to log this on every request. Perhaps in ApiVersions.update(), we can log in debug level of any request in nodeApiVersions that's older than the version the client has. This way, this is only logged every time a client connects to the broker.
Yes, this looks good to me.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
need a check for null on `obj` here as well
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
ditto on removing before/after.
nit: remove empty line
Ditto on removing these before/after methods.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
I guess another way would be to let this configuration be a delta which is added to the the current time. That way we wouldn't have a lot of messages with the same timestamp, which might be a little uncommon in practice.
Actually, I'm not sure we necessarily even _need_ to call on the `FallbackPriorTaskAssignor`, we just need to schedule the followup and remove the affected tasks from the assignment
I was tempted to say we should just return an empty assignment, which would prompt everyone to rejoin again immediately, but I think the FallbackPriorTaskAssignor is a preferable alternative. IIUC, we should be able to rely on the precondition that any previously assigned tasks we correctly initialized _before_ they were assigned initially, right? So we know they are all safe to keep working (if possible) while we wait a suitable backoff period before trying to create these topics again. I could see the idea to instead just remove any tasks we couldn't initialize instead of calling the FallbackPriorTaskAssignor, but if I'm reading this code right, we might just have failed to verify that the topics exist, not only fail to create topics we know didn't exist. So, we might actually remove tasks that were previously assigned if we do this. It's not clear which strategy is better, since it would depend on the exact nature of the failure, but maybe at a very high level, it's better to continue processing existing work and delay starting new work than potentially to start new work but delay processing existing work. Or we could try for the "best of both worlds", where we assign the union of all previously assigned tasks and any new tasks we _were_ able to set up. Finally, even if we re-assign previously assigned tasks, I'm not sure if we actually need/want to use the FallbackPriorTaskAssignor in particular. There doesn't seem to be anything wrong with just computing a new assignment for a subset of the tasks while we also schedule a re-attempt to set up the rest of the tasks after a back-off period.
For 441 we added a `nextScheduledRebalance` field to the assignment in order to signal when a followup rebalance is needed. Can we leverage that here as well so we don't have to go through the whole ordeal of `onPartitionsLost`? Check out the call to `fetchEndOffsets`in `StreamsPartitionAssignor#populateClientStatesMap` where we schedule a followup rebalance on the leader if the `listOffsets` request fails. I think we can reuse the same logic/code path and keep track of a general flag like `adminClientRequestSuccessful` so the assignor can still finish the assignment
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
nit: add `final` -> `for (final Map.Entry...`
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
typo: CompleteableFuture -> CompletableFuture
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Also discussed offline with Becket, but this test can probably be simplified significantly
Technically, this is `numDrainedRecords`.
Yes, and more generally, my understanding is that we also need to consider if the consumer may return a "super-list" of the expected values, for example (still assume the above expected `{A, 1}, {A, 2}`): * `{A, 1}, {A, 2}`: this should be correct. * `{A, 1}, {A, 1.5}, {A, 2}`: this should be correct. * `{A, 1}, {A, 2}, {A, 2.5}`: this should be wrong, since `{A, 2}` should be the last for the key. * `{A, 2}, {A, 1}`: this should be wrong. * `{A, 2}, {A, 2.5}, {A, 1}`: this should be wrong.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Ok, it looks better now. Let's leave it this way, with two lines.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
One thing I would suggest we do is to create an intermediate struct to store all the parameters, in case later we need to add more fields for sensor creation.
nit: for method _calls_, we usually format like this: ``` addAmountRateAndTotalMetricsToSensor( sensor, ...); Breaking the first parameter already reduced line length and seems preferable.
`Moving average duration` may be a bit confusing to readers, maybe just `Average duration of ..`.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
One extra line.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
nit: align parameters.
We can remove the code block line 82-85 above since it will be called here.
nit: empty line.
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
nit: Use braces & separate lines
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Nit: remove `this`
It'd be more powerful to do an assertion on the complete set of returned plugins, since that will only require one test run to discover all differences between the expected plugins and the actual ones: ```suggestion Set<Class<?>> excludes = Stream.of(ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES) .flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of(SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS) .flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(true)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ``` (This assumes we split out `CONNECTOR_EXCLUDES`, but the same general strategy should apply even if we don't).
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
It seems like we're duplicating some of the logic contained in `Plugins` into this class by tracking class alias names and pre-computing plugin type based on them. Did you consider a `Herder` method that only accepted the name of the plugin, and took on the responsibility of deducing the plugin type itself? ```java List<ConfigKeyInfo> connectorPluginConfig(String pluginName); ``` In `AbstractHerder`, we could do something like this: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { Object plugin = Plugins.newPlugin(pluginName); PluginType pluginType = PluginType.from(plugin.class); List<ConfigKeyInfo> results = new ArrayList<>(); ConfigDef configDefs; switch (pluginType) { case SINK: case SOURCE: configDefs = ((Connector) plugin).config(); break; case CONVERTER: configDefs = ((Converter) plugin).config(); break; // ... Rest of switch statement follows same pattern, and rest of the method remains unchanged } ``` And in `Plugins` we could do this: ```java public Object newPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends Object> klass = pluginClass(delegatingLoader, classOrAlias, Object.class); return newPlugin(klass); } ``` Or alternatively, we could introduce a common interface for plugins that expose a `ConfigDef`: ```java interface DefinedConfigPlugin { ConfigDef config(); } ``` Which could really simplify some of the `AbstractHerder` logic: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { DefinedConfigPlugin plugin = Plugins.newDefinedConfigPlugin(pluginName); ConfigDef configDefs = plugin.config(); // No switch statement on plugin type necessary // ... Rest of the method remains unchanged } ``` And the change to `Plugins` would be lightweight as well: ```java public DefinedConfigPlugin newDefinedConfigPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends DefinedConfigPlugin> klass = pluginClass(delegatingLoader, classOrAlias, DefinedConfigPlugin.class); return newPlugin(klass); } ``` Worth noting that if we want to differentiate to users between "this plugin is not on the worker" and "we don't expose config information for this type of plugin", we'd have to make a few further tweaks.
The message doesn't seem to match the condition above.
Same as above - use StringBuilder and rename method.
Use `StringBuilder`? Possibly also change the name of the method since it is not a getter.
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Nit: dowrade -> downgrade.
nit: add `{ }` to block
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
Same as above mentioned, the validation didn't get handled in new API.
We can save this for a follow-up, but it doesn't seem too difficult to allow the consumer to seek to the offsets that were successfully fetched. I think we just need to change the type to something like `Map<TopicPartition, Either<Errors, OffsetAndMetadata>>`.
This condition should probably be like a few lines below: ``` java if (subscriptions.getSubscribedPattern().matcher(topic).matches() && !(excludeInternalTopics && TopicConstants.INTERNAL_TOPICS.contains(topic))) ``` So we would want to extract that condition to a helper method perhaps.
Need to fix indentation below.
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
this is creative :)
We should log an error that prints out what the two configs actually are
We can reuse `streams`.
same as above with try/catch
Why is this changing? `infos` are not used below.
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
I really like this class.
Relatedly, I think there might be some sort of checks in unit tests in maybe the producer or consumer that validate metrics are unregistered, might be able to use a similar approach here.
`error` is unused
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
What if that never happens? It's better to always have a timeout in tests.
Seems like we should do a single `close` with some timeout. cc @cmccabe
Stream instance "one" -> "two"
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
the method name changed to `windowedTable` and `windowSize` parameter is missing
Any reason why this block of code isn't in the `if(tasksByTopicGroup.get(topicGroupId) != null)` block? After the `for(..)`? If it is not null then it is going to have some tasks, right? So numPartitions will always be > -1
`a graph containing`, and correct space between `1. Build`
we could use `computeIfAbsent`
This part and the line 525-529 below can be extracted out of if condition.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
Also, instead of adding an extra operator node, I'd suggest we just do the checking within the operators themselves to reduce virtual function call overheads, for example see `KStreamKTableLeftJoinProcessor.process`.
an -> a
Thanks for verifying @vvcephei!
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
Well, we wouldn't want to just remove that clause -- in the case of `SerializationException` you want to maintain the `SerializationException`, but if there's some other `RuntimeException` (which there can easily be for serializers that aren't aggressively catching exceptions and converting to `SerializationException`) then you still need to convert it to a basic `KafkaException`. I think you could do this: ``` try { // parse record } catch (SerializationException e) { throw new SerializationExceptionException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } catch (RuntimeException e) { throw new KafkaException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } ``` as long as we're confident there aren't any other `KafkaExceptions` we'd want to handle differently (i.e. any other more specific types of `KafkaException` where we'd want to preserve the same type instead of generalizing to `KafkaException`).
A quick look shows that other code has access to e.g. `topic` and doesn't include it in the exception message. Seems like having the fields there could help with better exception messages.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
Yes, does not hurt to leave it. Just for sure.
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
Input parameter `partitionTimes` should always contain the correct partition time, hence, we can just get it: ``` final long partitionTime = partitionTimes.get(partition); ```
`replicaing` -> `replicating`
I think the assertion on 219 would pass even if the 1st mocked interaction never happened. Do we need something to tighten up the expected behaviour? Maybe something like: ```java verify(kafkaBasedLog, times(2)).send(any(), any(), any()); ```
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
Hmm. I feel the `final` would be worth capitalizing the var name.
I think we can.
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
Why do we need this? Seems to be a wrapper for `NetworkClient`, but does not add too much value IMHO.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
nit: Please fix code style.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Shouldn't need this line, it's handled by the superclass's constructor.
an -> a
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
I wonder if the id check is sufficient. If a broker was reprovisioned with a new IP address, for example, we'd probably want to update this collection with the new Node.
nit: Empty line could be removed.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
@alexjg The code is actually looping in this line, before the `waitForCondition` call is actually triggered, I think that is why you are seeing the indefinite hanging.
typo: we want to test the **case** that poll() returns no records.
as above `final` and one parameter per line
It's worth including the broker here as it was originally intended.
Does it still make sense to have the if/else here? we log debug anyway and the exception should contain enough information to figure out the type of error.
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
is a topic node
`with TopicNode and TopicsInfo` looks weird to phrase like this when we are defining `TopicNode`, maybe something like `with topic information associated with each node`.
do a link: `method setRepartitionTopicMetadataNumberOfPartitions` -> `{@link #setRepartitionTopicMetadataNumberOfPartitions}`
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This is not necessary, since the for loop below would be a no-op.
Is this ok to do for both reasons this would be called? Do we actually want to request a commit when in the case of the connector throwing a RetriableException? That'll result in the connector being forced to flush all its data.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
For a nice example where caps make sense see right below, where two sentences are included.
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Nit: reword to avoid "log" being ambiguous as verb or noun: "Writes errors and their context to application logs."
+1 to this
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
@dguy @hachikuji if it sounds good to you I can go ahead and make this change while merging.
Yeah, it seems to me like we should remove it.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
We don't use null entries in JSON, because it gets too confusing. You should check against empty string here.
There's a pattern for all of the Trogdor JSON code where we don't use null anywhere. The problem with null is it gets annoying to check each collection for empty vs. null, each string for empty vs. null, etc. etc. null is also handled kind of inconsistently in Jackson. Sometimes Jackson will serialize a field that is null as `"foo": null` whereas sometimes it will just omit the field. (I think that `"foo": null` is actually not conforming JSON, by the way...) There are probably ways to configure all this, but null doesn't really provide any value 99% of the time, so it's simpler to just treat empty as null.
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
`return stream(null, null, keySerde, valSerde, topics);` Do the call directly instead of the cast.
nit: can we break this line? (similar below)
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
I think, it is better to keep the default initial capacity of an `ArrayList`. Otherwise, the first time a stream thread is added, we immediately run into a memory allocation. Since we do not know how many stream thread we might expect, let's use the default. We could also consider using a `LinkedList` since we never access by index in production code.
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
Please simplify to ```suggestion Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count())); ```
You are right. NVM.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
nit: add a space after "multiple". i.e. `despite being claimed by multiple[ ]`
nit: to be consistent, we can just add `consumer` to `membersWithOldGeneration` and then let them to be cleared at the end.
NVM, I realized it should never happen.
Hmmm... Seems to be in issue... The actual final return type is `KTable<Window<K>, V>` and thus is window-type agnostic. So we already have such a "container". -- However, `windowedBy(SlidingWindow)` returns a `TimeWindowedKStream`... Return types are not easy to change... And I don't think we can just switch from `TimeWindow` to `SlidingWindow` as concrete type either for the sliding window case... Maybe we are stuck and cannot fix the bug without a breaking change? For this case, we would indeed need to carry on with the KIP (but we could only do it in 4.0...), but I am wondering if it's worth fixing given the impact? Also: we have a few issues with the current DSL that we cannot fix easily (eg KIP-300). Thus, a long term solution could be, to leave the current API as-is, and built a new DSL in parallel (we did this in the past when we introduced `StreamsBuilder`). This way, we can change the API in any way, but it would be a long-term solution only. It might also help with regard to the new PAPI that uses `Record` instead of `<K,V>` type, and that is not easily adopted for `transform()` (and siblings). We could change the whole DSL to `Record` (ie, `KStream<Record<K,V>` -- or course we don't need `Record` in the generic type -- it's just for illustrative purpose). It would also cover the "add headers" KIP, fix KIP-300, we could introduce a `PartitionedKStream` (cf current KIP-759 discussion) and a few other minor issue (like rename `KGroupedStream` to `GroupedKStream`) all at once... And we could cleanup the topology optimization step and operator naming rules (which are a big mess to understand which `Named` object overwrites others...) -- We can also get rid of the wrappers for `KeyValueStore` to `TimestampedKeyValueStore` and change the interface from `Materialized<XxxStore>` to `Materialized<TimestampXxxStore`) -- In the past it was never worth to start a new DSL, but it seem we collected enough individual cases to maybe justify this investment now? The only thing that we should consider is our investment into "versioned state stores / version KTables". If we build a new DSL it should be compatible to it -- if we cannot guarantee it, we might want to wait until we understand what API we need to versioned KTables in the DSL and make the cut afterwards? \cc @ableegoldman @guozhangwang @vvcephei @bbejeck @cadonna (also @inponomarev @jeqo @vcrfxia)
Are you saying the `CachingWindowStore` internally uses a `TimeWindow`? Or is the `TimeWindow` somewhere along the store supplier code path...? Either way, doesn't this mean there's still a hole in the API since you can't use a custom WindowStore for a sliding windowed aggregation with the windowSize set to 0? If the WindowStore is going to represent different kinds of constant-size windows, it should probably be agnostic to the specific type of constant-sized window.
But I wouldn't be afraid to just use a full if/else block, either. ```suggestion final WindowBytesStoreSupplier storeSupplier; if (inOrderIterator) { storeSupplier = new InOrderMemoryWindowStoreSupplier("InOrder", 50000L, 10L, false); } else { storeSupplier = Stores.inMemoryWindowStore("Reverse", ofMillis(50000), ofMillis(10), false); } ```
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
Are we intentionally not logging the exception as the extra parameter? If the exception wraps a more useful exception, we won't see any information about the wrapped exception unless we can see the stack trace in the warning log message.
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
The goal of the ticket is to actually remove this check.
we also want to remove this check
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
That sounds good to me 
If we swallow the exception here, and the test always throws an IO exception, we will never notice. I guess it would be better to use `fail()` with a message.
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
Ah, I see. Thanks for the explanation
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
im not *100%* sure, but i think you want this to also hit the controller. i hit the controller in my personal client, and @cmccabe is doing it here https://github.com/apache/kafka/pull/2472/files#diff-7378a806dbf302c1e7a9098c4780d2a8R283
I personally don't think it makes sense to wait for the metadata to propagate to all brokers. A broker may be partitioned away from the controller, but not the clients, for example. I would prefer to make the consumer and producer smarter when they get stale metadata from a random broker.
Sounds fine then (what else could you do if you don't even know who the controller is :)).
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
We probably shouldn't change this default to true -- we should override it in the specific test we need by calling `self.mark_for_collect(consumer, 'verifiable_consumer_stdout')` where `consumer` is the `VerifiableConsumer` instance. stdout for verifiable consumer can result in _huge_ log files, so collecting them by default will result in very large archived data for some tests.
I don't think this is a problem yet, but we should start thinking about these `Service` classes as at least semi-public interfaces. I know we rely on them in muckrake (although perhaps not this particular one yet) and I know others are starting to/planning to/want to be able to build tests on top of the pieces included in Kafka. While these definitely aren't the same as our client APIs, I think we should make an effort to provide some degree of compatibility.
@hachikuji Actually I asked for a name which made it clearer that this object contains numerical indices, and not actual node objects... :)
You could just do `selector.poll(100)` instead of `poll(0) + sleep(100)`. `poll()` returns when an operation is ready, so we are not waiting unnecessarily.
Stream instance "one" -> "two"
we need to shutdown the executor service even if there is a test failure (either in a finally block or the teardown).
nit: seems you can use `new ArrayList<>`
Maybe it's worth adding a code snippet which shows how to use the sticky assignment with the rebalance listener? It's a little different than with "range" and "roundrobin" from memory.
It might be better to use a Kafkaesque schema definition.
Should be final
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
```suggestion /** * Metadata of a stream thread. */ ```
i know it was motivated by findbugs, but this was probably a good refactoring anyway :) `RestServer.httpRequest` to *make* an http request has always been a bit awkward
We should read the metadata inside the while loop since it could change.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
The `CachingKeyValueStore` doesn't throw an NPE here, rather `org.apache.kafka.common.utils.Bytes.LexicographicByteArrayComparator.compare` does. We probably should add `Objects.requireNonNull(...)` to `CachingKeyValueStore#put(..)` etc
Can we change `subject` to `rockDdStore` -- it's a weird name IMHO.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
nit: add `{ }` -- we use curly braces for all blocks nit: remove double space after `==`
nit: add space `configEntry :` -- wondering why this is not detected by checkstyle...
Similarly, `adminClient.describeConfigs` does not read from ZK but from broker cache, and hence maybe subject to race conditions.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
I'd suggest keeping the format slightly closer to what we had before. Specifically: `Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted`
Hmm... I don't believe "cancelled" is a term we've used in public-facing surfaces in the past. For example, when a task takes too long to shut down now and we have to cancel it, we log the message that "Graceful stop... failed": https://github.com/apache/kafka/blob/5964401bf9aab611bd4a072941bd1c927e044258/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L866 Personally I think the additional code complexity is worth it; the original ticket mentions a case where these messages confuse users because they're generated for cancelled tasks, so I'd rather err on the side of making things as obvious as possible to them. It might be possible to keep things simple and eliminate branches by tweaking the message to make it clear that newer task instances won't be impacted by this failure, though. A possible downside to this is that it might be confusing if there are no newer instances that will be brought up on the worker (because the connector has been deleted, the number of tasks has been reduced, or the task has been reassigned to another worker). But with some careful wording we might be able to avoid misleading people into thinking that this message implies there's already another instance running.
Looks good to me!
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
We should not duplicate code, but instead extract an internal `private` helper method `putInternal` that can be called by `put` and `putAll`
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
original was better
nit: since we're not doing anything in the EAGER case, couldn't we simplify this: ```java if (protocol == COOPERATIVE) adjustAssignment(ownedPartitions, assignments) ``` Similarly in `onJoinPrepare`
original was better
maybe: `inputKeySerde` and `inputValSerde`
nit: ".. select the grouping key and the value to be aggregated".
records to it, and reading all records from it, such that
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
It may also be useful to have a test that checks that IBM Kerberos classes are available if `Java.isIBMJdk` is true and `com.sun` Kerberos classes are available if false. In particular, you could check the classes `com.ibm.security.krb5.internal.Config` and `sun.security.krb5.Config` which are loaded in `SaslChannelBuilder`.
@mimaison It is true that the test would check only one class depending on the JRE. But it checks that the relationship between `java.vendor` and Kerberos classes matches the expectation in the code (for that JRE). The other unit test is checking if String comparison works, which is fine as a unit test, but it doesn't really test the actual System property based on the JRE.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
Should we preserve the error message? Otherwise the user can't actually tell why this happened, they'll see the same error message for both stale config and forwarding errors. It's pretty important to expose enough info to tell what's going on because in cluster mode the user making the request may not have easy access to the worker logs.
nit: due _to_ no known leader URL ... or similar
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
This is public API meant to be used by users. I don't mind if our tests are a bit more verbose but we should aim to have succinct public APIs
Does this need to be public? Making it private forces use of `of` method, which I think is good.
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
Since this only returns tasks whose previous client was caught up, I think we can simplify the first half of `assignTasksWithCaughtUpClients` as just ``` tasksToPreviousClients.forEach((t, c) -> assignment.computeIfPresent(c, (k, v) -> { v.add(t); return v;})); unassignedTasksWithCaughtUpClients = new ArrayList<>(tasksToCaughtUpClients.keySet()); unassignedTasksWithCaughtUpClients.removeAll(tasksToPreviousClients.keySet()); ```
```suggestion * Assigns tasks for which one or more caught-up clients exist to one of the caught-up clients. ```
```suggestion final Map<TaskId, List<ID>> taskToCaughtUpClients = statefulTasksToRankedClients.entrySet().stream().collect(Collectors.toMap( Entry::getKey, t -> t.getValue().stream() .filter(c -> c.lag() == 0 || c.lag() == Task.LATEST_OFFSET) .map(ClientIdAndLag::clientId) .collect(Collectors.toList()))); ```
Nitpick: `maxWaitMs` would be a better match for Kafka's naming convention.
I would say something like: ``` java /** * Wait for condition to be met for at most {@code maxWaitMs} and throw assertion failure otherwise. * This should be used instead of {@code Thread.sleep} whenever possible as it allows a longer timeout to be used * without unnecessarily increasing test time (as the condition is checked frequently). The longer timeout is needed to * avoid transient failures due to slow or overloaded machines. */ ```
What is the reasoning for not throwing an exception if the condition is not met after the timeout? That would make the tests more concise.
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
s/assignments/oldAssignments, and name could be simplified as `toNewAssignment`
This name seems backwards.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
Not for this patch, but we should do a KIP to add support for batch topic creation.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Thanks for the catch!
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
Maybe we should simply pass the `ProducerRecord` in the constructor? We could then also just use the `ProducerRecord.toString` in the error so that we don't have similar issues in the future.
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Nit: new line is unnecessary, and there's a misspelling: ```suggestion log.error("Encountered an error while awaiting an errant record future's completion."); ```
prop: make the value a `SortedSet` so we can just insert clients as we build the map and use a custom comparator to automatically sort the clients based on lag
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
Worked fine when I tried it locally: ```java assertEquals(Collections.singleton(tp0), records.partitions()); ```
We can use `assertThrows` for this kind of pattern: ```java RecordDeserializationException rde = assertThrows(RecordDeserializationException.class, () -> consumer.poll(Duration.ZERO)); assertEquals(invalidRecordOffset, rde.offset()); assertEquals(tp0, rde.partition()); ```
With this approach, it seems we could drop this case? There is also the insertion of the null for the UNSUPPORTED_FOR_MESSAGE_FORMAT case in `handleListOffsetResponse` that we probably can drop.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
That sounds good to me 
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
Hmm, this doesn't need to block merging this, but we should think carefully about doing delay this way. The rest of Connect avoids trying to rely on Java's `interrupt` behavior because it's not really a reliable way to *actually* interrupt threads, and in a system where there are pluggable components that are allowed to block indefinitely, relying on functionality that most Java developers don't understand well probably isn't going to work all that well. It may not have actually gotten to a KIP, but there was at least some discussion on a JIRA somewhere about making connect perform interrupts in addition to the basic task `stop()` calls it already does, but it doesn't currently do this. For anything that can end up with pretty long sleep periods, we should try to make sure there's a good way of interrupting it and moving on (e.g. so rebalances wouldn't get delayed because there's a connector that's encountering errors). At a minimum, since we don't do interrupts currently, I think we wouldn't interrupt this code currently. The other approach we use elsewhere is to `wait` on a monitor so we can set a flag and interrupt with `notify` and have it bail out immediately.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
@rajinisivaram makes sense. I was thinking if we can break the JAAS config file and made them into client config properties. But just passing JAAS config file will make it easier and extensible.
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
methods => `mechanisms`
Also, 5ms seems a bit extreme. Maybe this could be 20ms or so and we could use the minimum of this and the configured retry backoff so that users can adjust it lower if they need to.
Yeah, I have no doubt the performance is better. It's just that it seems like a lot of excess traffic and is going to be amplified by the number of transactional producers. It may be fine in the common case if the write markers are pretty quick, but if there is any kind of delay, then I'd be concerned about the brokers being overwhelmed with these requests (though maybe it's not as bad with request throttling). I'd rather err on the safe side for now since users can manually adjust the backoff. For the 0.11.0.1 release, we can provide a better solution. Most users will probably hold off until then anyway.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
@edoardocomar `Prepared` doesn't convey much meaning in terms of an externally visible metric. I imagine you chose it rather than `authenticated` since you intended it to work for `PLAINTEXT`. But `PLAINTEXT` doesn't go through this if-block since `channel.ready()` returns `true`.
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
This syntax is a bit hard to follow with the conditional at the end. Can you rewrite it to something like: ```python self.jmx_tool = None if jmx_object_names is not None: self.jmx_tool = ... ``` (and also check the attributes as mentioned above)
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
looks good, thanks
I think we can simplify these functions. Something like this: ```java private static byte readByte(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 1) readMore(buffer, input, bytesRemaining); return buffer.get(); } private static long readVarLong(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarlong(buffer); } private static int readVarInt(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarint(buffer); } ``` I think we shouldn't need more than one call to `readMore`.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
We could port this function when it is actually needed.
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
nit: remove empty line
nit: remove empty line
Add a check to verify, whether the iterator has no more elements.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Can initialize to `new HashMap<>()` here as is done with `invalidExtensions` below.
Yes, we could add `ignoredExtensions` and include that in the log in the server.
@rajinisivaram @stanislavkozlovski LGTM with the possible exception of maybe adding support for retrieving/logging any ignored extensions? I'll defer to your preference on this.
I think it'd be useful to verify the behavior of casting all of the logical types to strings, just so that we verify the formats (e.g., timestamps, times, and dates should use the ISO 8601 representation) and have some regression tests for the future to help ensure we maintain backward compatibility.
It would be great to have a few more test cases to cover more scenarios: 1. cast to types other than `int32` and `int64` (e.g., `float64` and `float32`), including where we lose precision 2. verify we can cast these to strings 3. verify the cast fails in expected ways 4. casting null values should not fail
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
It is a shame we have to do it like this, but i don't see a viable alternative
We can drop the parenthesis here. Same below
Looks good. I like the additional checking that you're doing here.
Good catch, thanks for cleaning this up!
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
We should also verify the thrown cause
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
Could combine these into a single `testCycleCollection()` method if you put a `null` value in the list (e.g. `"A", null, "C"`) and for every one of the 4 positions (0-2 and cycling back to 0) you also check the `peek()` value. I think it would be clearer compared to what you have currently since the last 2 methods you have now are a bit haphazard.
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
nit: Could we move `new DeleteTopicsRequestData.DeleteTopicState().setName(topic1)` to a new line in order to align it with the following `new DeleteTopicsRequestData`? There are few other cases like this.
Nit: maybe `("Topic: " + topic)`
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
nit: double space
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Instead of pulling the value out with a regex, what do you think of `streamsString.contains("appId")`. Although what you have works as well.
Could just use `false`
could use `assertFalse`
Sounds fine to me.
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
How about this. First, we can augment the message above to something like this: ```scala log.trace("Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`", position, nextPosition, completedFetch.partition, partRecords.size()); ``` This gives us enough information in the logs to see which partitions caused the `poll()` to return and it tell us exactly where in the log the aborted transaction/control records exist. Second, maybe we can add back your previous log line, but make it a little more terse and put it at trace level: ```scala log.trace( "Returning empty records from `poll()` since the consumer's position has advanced " + "for at least one topic partition") ```
nit: could be useful to log the type of exception in the assertion message.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
typo: moreq -> more
If stop throws we won't count down the latch. No harm will result except there will be an erroneous log messages about exceeding the stop timeout.
We should name the thread so that thread dumps are a bit more informative. I _think_ these should be daemon threads because if we're prepared to basically ignore the non-return of `task.stop()` during runtime I don't see why we'd block jvm exit for them.
`toString` is not required.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Just let the Exception flow, that will automatically fail the test
The `enable.idempotence` will be true by default in 3.0, so we should perhaps have the "up to 5" thing first.
Hello and thanks for reviewing! The callout about record ordering was originally requested within Confluent; the same is mentioned at this page: https://developer.confluent.io/tutorials/message-ordering/kafka.html. We think this would also benefit AK docs.
typo: with message ordering preserved for any allowable **vlaue** --> **value**
@cnZach Looks like the intention was to print out the exception with stack trace? You would want to use: ``` public void warn(String msg, Throwable t); ```
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Is this test needed? It seems that loginContextName can never be null.
nit: add a size? There are a few cases in here where we could do this.
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
We should read the metadata inside the while loop since it could change.
You are right. Checking ```Error reading field 'field2'``` should be enough for this test case.
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
```suggestion // Only return a default value fallback instead of `null` when the field is actually required ```
SGTM. We can keep it as is then.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
rewrite test as above using `assertThrows()`.
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
As for all tests, we should verify `topologyDescription`
Do we need to wrap with the LinkedHashMap? Could we just do `Collections.unmodifiableMap(metrics.metrics());`
This is not required as contained in the check next line.
@rodesai I see your point here. However, the downside of not throwing is that we will also not notice the bad behavior in our tests like the soak tests. I personally prefer to improve tests instead of downgrading the reaction to bad behavior. Assume in future somebody makes a change that breaks the assumption of the non-shared metrics registry, we would find this bug immediately during development instead of during production. Another option that comes to my mind is to classify exceptions that originate from the metrics framework differently in the uncaught exception handler, but that would probably need some more work.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
Up to you I guess. No need to expand the scope even further for a tiny nit.
Yeah, that might be better.
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
AK convention is to not use `set` setters or `get` getters.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Let's move this helper into `ListOffsetRequest`
`new ArrayList<>` is suffice
As we don't have the list of TopicPartition available to filter the list of futures, we could actually directly complete the future within the loop instead of populating the HashSet. It avoids building the HashSet and having to traverse the futures.
We should use `aasertThrows` and verify the error message similar to the `TransformerSupplier` test
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
as above -- guess some more below
nit: move to line above.
Some of these lines have extra 4 space. Other lines have extra 8 spaces.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
We could detect if the processorTopology contains only `StaticTopicNameExtractors` and still throw in that case if the topic name isn't in the topology.
Ok, let's just keep it in our back pocket for now.
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
Why could we not fix this? The underlying issue seems to be the state transitions of `StreamThread` -- it allows to go from `CREATED -> RUNNING` -- if we change this, and we can only go to `CREATED -> PARTITION REVOKED` we should be able to tackle this issue? (Maybe a different PR to do this though.)
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Nit: fix line break
Hmm, if we performs an unclean leader election, the only replica in ISR should just be the new leader since the data in existing ISR is not guaranteed to match with the new leader.
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Maybe I am wrong, but shouldn't this be moved inside the `while` loop, so it is initialized every time a new assignment is selected? Because, now I think only the first consumer will be challenged against this previous partitions.
Detail: just to be sure, I would initialize it to `this.generation`, to make sure the generation always increments.
You can probably simplify this using `computeIfAbsent`.
Cool, I was kind of hoping you would put this in a separate integration test class
Why set this? zero is the default anyway
Could we define it as `@BeforeClass`? I think we do not need to create input/output topics for each test case.
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
the field "ERROR_CODE" is never used.
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
Let's use `Map` on the left side instead of `HashMap`
Minor: maybe move this to initialization? ``` java int newTaskCount = configs.size(); ```
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: line too long
nit: break line
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
I think we ditch the before/after methods as I previously recommended.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
Let's remove the brace changes please.
Kafka doesn't mandate braces in `if` statements.
Seems there's no need for this method to declare `ConcurrentLinkedQueue` as the return type. You can use a normal `Collection`, which would then allow you to use `Collections.emptyList()` instead of pointlessly creating an empty queue.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
I think this is the issue you reported in the Jira. The `RaftClient.Listener` should not use `RaftClient.leaderAndEpoch` to determine if it is the leader. It should instead use `RaftClient.Listener.handleLeaderChange`. For this state machine `ReplicatedCounter` we should look at the `claimedEpoch` variable. I am going to create an issue to remove this method. cc @hachikuji
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
Not really sure this has value if the test case expects the leader change correctly.
need to update `./gradlew uploadArchivesAll` at line no: 644
I meant `public_html` directory.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
We can use `==` to compare enums.
Let's keep the existing `trace` and `error` log lines in the `else` block. My suggestion is to add a line at the debug or trace level in the `if` block so users can know if an error is ignored.
you dont need the `String.format` here would need `%s`->`{}`
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
nit: do we still need this function? Could we just reference the `metricLock` object directly? Since it is private my understanding is that it was not intended to be used outside this class.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
Instead of doing this, I think we can have the following 3 methods in `SslFactory`. Thoughts? ```java public SSLEngine createSslEngine(Socket socket) { return createSslEngine(peerHost(socket), socket.getPort()); } /** * Prefer `createSslEngine(Socket)` if a `Socket` instance is available. If using this overload, * avoid reverse DNS resolution in the computation of `peerHost`. */ public SSLEngine createSslEngine(String peerHost, int peerPort) { if (sslEngineFactory == null) { throw new IllegalStateException("SslFactory has not been configured."); } if (mode == Mode.SERVER) { return sslEngineFactory.createServerSslEngine(peerHost, peerPort); } else { return sslEngineFactory.createClientSslEngine(peerHost, peerPort, endpointIdentification); } } /** * Returns host/IP address of remote host without reverse DNS lookup to be used as the host * for creating SSL engine. This is used as a hint for session reuse strategy and also for * hostname verification of server hostnames. * <p> * Scenarios: * <ul> * <li>Server-side * <ul> * <li>Server accepts connection from a client. Server knows only client IP * address. We want to avoid reverse DNS lookup of the client IP address since the server * does not verify or use client hostname. The IP address can be used directly.</li> * </ul> * </li> * <li>Client-side * <ul> * <li>Client connects to server using hostname. No lookup is necessary * and the hostname should be used to create the SSL engine. This hostname is validated * against the hostname in SubjectAltName (dns) or CommonName in the certificate if * hostname verification is enabled. Authentication fails if hostname does not match.</li> * <li>Client connects to server using IP address, but certificate contains only * SubjectAltName (dns). Use of reverse DNS lookup to determine hostname introduces * a security vulnerability since authentication would be reliant on a secure DNS. * Hence hostname verification should fail in this case.</li> * <li>Client connects to server using IP address and certificate contains * SubjectAltName (ipaddress). This could be used when Kafka is on a private network. * If reverse DNS lookup is used, authentication would succeed using IP address if lookup * fails and IP address is used, but authentication would fail if lookup succeeds and * dns name is used. For consistency and to avoid dependency on a potentially insecure * DNS, reverse DNS lookup should be avoided and the IP address specified by the client for * connection should be used to create the SSL engine.</li> * </ul></li> * </ul> */ private String peerHost(Socket socket) { return new InetSocketAddress(socket.getInetAddress(), 0).getHostString(); }
I think we need to skip all the code in this else block if the partition is no longer assigned.
@enothereska Yes, I think that is a better solution. But I think @hachikuji was right that we don't need to cover both if/else branches. We just need to cover the `parseCompletedFetch`
@enothereska The trunk code does not need to access `subscription.position`, instead it uses `PartitionRecords.nextInlineOffsets` which should be the same as position because the position is updated to this value every time after a successful `fetchRecords()`. The big try/catch is to make sure the the exception from `fetchRecords` will also be caught and not result in loss of non-empty `fetched`.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
The number has changed and 5 is no longer relevant.
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I think we should use the same name here; the metrics-scope would differentiate rocksdb from in-memory, which is sufficient.
nit: The sentence sounds slightly better if you remove `the`
`replicaing` -> `replicating`
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
nit: add `final` (we use `final` whenever possible)
nit: `maybeDeleteInternalTopics` and `maybeReset...`
could we turn this into a loop? i.e., sth-like? ``` List<KeyValue> records = Arrays.asList(...); for(KeyValue record : records) { mockTime.sleep(10); IntegrationTestUtils.... } ```
Do we lose anything if we use `Set` instead of `Collection`? Seems like set is the right semantics.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
This is also related to the following PR https://github.com/apache/kafka/pull/1015 that uses sentinels in a number of places. It would be nice to be consistent on -1 versus null.
`long` -> `Long` is a binary incompatible change.
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
Got it. For future readers the function is `maybeCompleteShutdown`.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
this is creative :)
consumerThread -> rebalancingConsumerThread
Use log object if this is to be kept
Since condition is just a comparison, you can put the comparison here directly
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Should we include the brokers instead of removing them? Same below.
same question around log level as above
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
Why adding a suffix? Is there any problem if the topic name is equal to the store name? This breaks ktable.
I am not sure we want to move all the tasks in a topology? Maybe we can do that by task or sub topology? maybe topology is best but I will need to think about it a bit
It would be much simpler but unfortunately its not as simple as we first thought. The producer has only one transaction, so the records of the good tasks are mixed in with the records of the failed task and there is no way to separate them. So we need to take the tasks that we know will fail and process all the other tasks without them. That way we continue making progress. We can take the failing tasks and backoff and retry again later.
It might be nice to keep the tasks/topologies that have failed in another list entirely. Then when reprocessing after an exception we can run all the good tasks first and commit them before running the failures. This will be important for EOS as we con't commit only part of a transaction. The larger part of that doesn't need to be done it this PR but keeping the groups separate would be nice in my mind
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
Nit: I think `0.0` is idiomatic in Java.
nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
I hope we can get rid of those conversion in the future :)
I would call this one `topics()` as you did already in the request.
nit: we can use `map#compute` to replace getOrDefault + put.
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
The parameters should be the other way round (expected is `conns` and actual is the metric value).
We cannot guarantee that this poll will see all completed connections, so it would be better to poll in a loop until the total connections returned from`selector.connected()` after the poll equals `conns`.
You could just do `selector.poll(100)` instead of `poll(0) + sleep(100)`. `poll()` returns when an operation is ready, so we are not waiting unnecessarily.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
Thanks for identifying this issue. Piggybacking on OP_WRITE may not be the best way to fix this though. I was thinking, if connected is true, we can probably just call socketChannel.finishConnect() and register the socket channel with OP_READ, which is what PlaintextTransportLayer.finishConnect() and SslTransportLayer.finishConnect() will do. Then, we don't need to change the logic in poll.
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
Should be more specific about the type of error being caught -- catching all exceptions should be reserved for very special cases, like protecting the top stack frame of a thread to avoid uncleanly exiting the thread. I suspect that here you specifically want to capture `CalledProcessError`, which indicates an issue running the command on the remote host and/or `ValueError`.
Does mirror maker support multiple consumer configs? A quick glance at the code suggests it only supports one.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
We can use the `replace` overload that takes a `char`.
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
I don't think limiting the pool to 2 threads is really what you want, since you have 3 runnables that you are creating later on. Check out some of the other newXYZThreadPool functions. There is at least one that doesn't limit the number of threads.
Should this be `num_lines=3` (cf. L116 and L126)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Like said earlier, I think we could just return `return new StreamsResetter().run(parameters, cleanUpConfig) == 0`
I looked at the test and at least one failed test namely `testReprocessingFromDateTimeAfterResetWithoutIntermediateUserTopic` does not have any internal topics to create, and hence none to delete. So it's not clear if bumping up the timeout would help here. I'd suggest we first augment the error messages to include the expected topics and the actual topics
I am always a little "concerned" about bumping timeouts if we don't understand why it actually fails. 30 seconds seems like quite some time. How long is deleting topics supposed to take? As far as I understand, we send a single request to delete all internal topics via AdminClient to the brokers. Is there a relationship between the expected completion time to delete all topic the the number of topics in the request? \cc @cmccabe
`validateStoreOpen()` can be outside of lock block.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
nit: if you want a new paragraph you need to add `<p>`
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
Ok. then LGTM
I noticed that `queryableName` is a different parameter than the one we actually build the parent processors with (`storeBuilder.name()`). It wouldn't surprise me if there's a subtle difference between them.
Yes, that sounds like the right thing to do. Thanks!
> this logic Seems only needed because we have the check in 309 (?) No, I don't think so. It should be for line 279: ```java // to handle the case that when there are still unassignedPartition left, but no more members to be assigned. if (unfilledMembersWithUnderMinQuotaPartitions.isEmpty() && unfilledMembersWithExactlyMinQuotaPartitions.isEmpty()) { throw new IllegalStateException("No more unfilled consumers to be assigned."); ``` In line 309, it is just an early error detect and log for it. Not related to `potentiallyUnfilledMembersAtMinQuota` (or now `unfilledMembersWithExactlyMinQuotaPartitions` members)
Honestly it took me quite a while to understand the fix :P After understanding that I think maybe it's better to rename these two collections more explicitly: 1) `unfilledMembers` -> `MembersWithLessThanMinQuotaPartitions`. 2) `potentiallyUnfilledMembersAtMinQuota` -> `MembersWithExactMinQuotaPartitions`. And also (since the maxQuota is always either == minQuota or minQuota + 1): 3) `expectedNumMembersAssignedOverMinQuota` -> `expectedNumMembersWithMaxQuota` 4) `numMembersAssignedOverMinQuota` -> `numMembersWithMaxQuota`
@ableegoldman , I reviewed it again, and found we forgot to sort the `unfilledMembersWithExactlyMinQuotaPartitions` list here, to have deterministic result.
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
We can use `List<Class<? extends Connector>` to avoid the warning
This is unused too
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
That's a good point.
This catch is a bit weird to me. Could you create a true `CompletableFuture` carrying the exception `new TimeoutException()` instead of mocking object? For example: ```java CompletableFuture<RecordMetadata> future = new CompletableFuture<>(); if (success) future.complete(new RecordMetadata(new TopicPartition("tp", 0), 0, 0, 0, 0, 0)); else future.completeExceptionally(new TimeoutException()); ```
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
@xiaodongdu, I was not suggesting getting rid of the field. It's fine to have a new field, but we should call the field `kafkaClusterId` rather than `clusterId` since the latter could be misinterpreted to mean the _Connect_ cluster ID.
I wonder if you also considered moving startup and shutdown into the `Worker`? The advantage is that we already have an executor there and then we'd get the parallel implementation for `StandaloneHerder` as well, though admittedly that may not matter as much.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Maybe doesn't matter, but seems a little more intuitive to check that the error is not NONE.
Given usage, this could probably be a Set.
nit: usually we drop the `get` prefix on getters.
@hachikuji is correct. We can't do blocking operations in the admin client service thread. We certainly can't do blocking operations that wait for the service thread itself. This will deadlock. I think it's a good idea to have a coordinator node provider, but we need to build out a little more infrastructure to make it possible. I have a change which should help with that, at https://github.com/apache/kafka/pull/4295
The ideal would be to use the `CoordinatorNodeProvider` here. There is not much benefit in having it if we just invoke it inline. The problem is that the `provide()` method is called by the send thread, so we cannot have it block on an operation which itself depends on the send thread. To make it work nicely in this way, we probably need an asynchronous `NodeProvider` API which effectively lets us chain the `DescribeGroup` request on to its completion. For example, maybe something like this could work: ```java interface AsyncNodeProvider { KafkaFuture<Node> provide(); } ``` cc @cmccabe (who may have some ideas as well)
nit: also add java doc for type `T, O` here
As above: need to keep default value.
Are we sure we want to make them required config for `KerberosLogin`? I noticed that we have actually defined default value for these configs in `SaslConfigs.addClientSaslSupport()`. These default values are used by producer and consumer. And if these configs should be explicitly provided by user, should we have a class that extends `AbstractConfig`, define these configs as required, and use this class in KerberosLogin to handle user-provided properties, in the same way that ConsumerConfig is used? This would allow us to throw exception in case of missing config using a unified mechanism.
I was also wondering how you ran into these NPEs. This is an internal class and the configs should never be null as there are default values.
super nit: not part of this PR proper, but maybe set `RuntimeException` returned from the `suspendTask()` call to variable then use it in the `assert` statement? Makes things a little easier to understand.
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
We should also verify the thrown cause
nit: toString not necessary
nit: `--topic` and `--partition` could be extracted as helper static functions
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
And nitpick: there should be a space after the comma.
super nit: taskCreationLock --> taskDirCreationLock
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
these overrides don't seem to add much.
never mind then. I'll leave this to AI.
ditto here, should be moved to RocksDBStore#close
configSetter.close() will clean up any resources constructed in configSetter.setConfig, and should only be called in RocksDBStore#close()
Just following on my other idea about collapsing into a single class here: maybe instead of naming it as keyValueWithTimestamp, we just name it as: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 to now. And the flag can just be indicating if it is 1) or 2) above; in the future if we need to do this again we can then have: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 - 2.5 (just made that up). 3) "3.0" -> version 3.0 - now. etc.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
In line 58 above, in `recordLatency()`, we need to mention that `If the passed sensor includes throughput metrics (e.g. it is create via the XX function), the throughput metrics will also be recorded as a single occurrence of this event.`
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
To keep this logic same as one in `Call::fail` method, lets set the new time as: > nextAllowedTryMs = now + retryBackoffMs
@hachikuji Yup, you're right -- I missed the fact that the overridden `needRejoin` takes into account the data in `subscriptions` and was only accounting for the base class's `rejoinNeeded` variable. Makes sense now.
Does this always work if you're the leader during a rejoin? The flag gets reset by `JoinGroupResponseHandler` but the leader will still need to complete the sync group phase. These two steps are combined, but using `ConsumerNetworkClient.poll(RequestFuture)` to wait for the entire operation to run can invoke the internal `ConsumerNetworkClient.poll(timeout)` multiple times, which in turn runs delayed tasks. So with bad luck with timing I think you may execute the delayed task between the two requests and this `needsRejoin()` check won't work.
Although the checkstyle rules currently do not enforce curly brackets in if/else blocks that contain a single statement, because this statement here spans multiple lines, I feel it'd be best to enclose it within `{ }`, even if that's optional.
nit: `,` (comma) doesn't seem required in the sentence.
This change is a good suggestion.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
We should improve this test by moving this line outside/before `try`
I think we need to insert a `fail` here to fix this test
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
visibility could be restricted to be package-level (i.e remove keyword `protected`)
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Would `isUnknown` be clearer? I find that boolean methods without any prefix feel a bit ambiguous when reading them.
nit: braces unneeded
Is the `myCommittedToken == null` check unnecessary here since it can never be the case when there are extensions? I think we make sure of this since we only call `identifyExtensions()` when there is a token.
nit: style seems to be to not include braces when there is only one if or else statement
An error is intended to shut down the worker. So we should call `doneFuture#complete` here.
It seems like the race conditions for `process` won't turn into any errors. It maybe is better to be explicit about the race conditions and have locks as to ease future maintainability of this task (especially by other contributors)
There is a race condition here. `ExternalCommandWorker#process` is used by the `ProcessMonitor` thread, and also here in a different thread. Similarly, `controlChannel` is accessed by multiple threads and could be be used before it is initialized. I think `process` and `controlChannel` need to be initialized and accessed under a lock. I would suggest something like: 1. take lock 2. check if process is null. if so then EXIT 3. create ControlCommand 4. release lock 5. call executor#awaitTermination for 1 minute 6. if the executor did not finish all tasks, then take the lock again, invoke process#destroy, release lock, call awaitTermination again
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: missing `<p>` for new paragraph
We can use the `replace` overload that takes a `char`.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Yeah I think that makes sense here
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
Shouldn't this be `timeoutMs - (time.milliseconds() - startTimeMs)`? Also, it's not too big of a deal, but the checks for `Long.MAX_VALUE` seem like overkill.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
I considered this but I think that it is clearer when kept separated.
Let's capitalize the log statements for consistency. There are a few of these.
Since we have this logic of checking what the type of `t` is in multiple places, we could consolidate this logic within `call.retry()`, since we are already passing the throwable instance. We could then also rename the method to be a bit more encompassing, like `attemptToRetry` or something.
Right, if retries are exhausted and it's a retriable exception, then it seems like it should be a TimeoutException.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Right, if retries are exhausted and it's a retriable exception, then it seems like it should be a TimeoutException.
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
You could also use `MemoryRecords.EMPTY` here (I think).
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
Thanks. I prefer to avoid using `Atomic*` classes when we don't need atomicity guarantees. I introduced a `LongRef` class in the broker for this reason. I think the right solution long-term is probably to introduce a class to complement `Time` that caches the last returned value so that one can choose when to refresh the value (for performance reasons). This was suggested some time ago in a different PR. For this PR, as per your suggestion, I think using a private static class with two public fields is probably the way forward (since Java doesn't have tuples).
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Ack. By bad.
apply `final` wherever possible (also within method)
Why do we need to augment this function here? I.e. by the time the stream task is created, we should have populated the `sourceByTopics` map with the pattern matched topics already, so I'm not sure if the additional computational logic is needed? cc @bbejeck .
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
Just FYI, for KIP-360 I'm doing this check for both idempotent and transactional, since it triggers an epoch bump instead of a producerId reset. I'll just pull this call out to a shared code path, the rest of this method shouldn't need to change.
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
We do not need to have a separate `KTableForeach`, since it does not generate any new KTable object and hence no need for `view` etc. Instead we can just reuse `KStreamForeach` inside `KTableImpl`.
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Still not used
`tp` is not used anymore.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
Aha! Thanks. Yeah, I'd be in favor of coding defensively here as well.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
When I suggested it, I thought we could do a bit better, maybe something like `(id: 5, www.example.com:9123)`, but maybe that's actually worse.
I was mostly trying to get rid of the word `Node` because it's a bit redundant when you look at the log messages.
I think the intent was to remove generation in the original PR.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage). I am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the `Fetch` and `BeginQuorum` protocol and not from the state machine (Kafka Controller) itself. It is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
Minor typo "will is"
nit: "name" => "joinThisName"
I see. Could we do something like this: first assign the partitions for all internal topics as the writing topology's number of tasks, i.e.: ``` // for all internal source topics, // first set the number of partitions to the maximum of the depending sub-topologies source topics for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) { Set<String> internalTopics = entry.getValue().interSourceTopics; for (String internalTopic : internalTopics) { Set<TaskId> tasks = internalSourceTopicToTaskIds.get(internalTopic); if (tasks == null) { int numPartitions = -1; for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> other : topicGroups.entrySet()) { Set<String> otherSinkTopics = other.getValue().sinkTopics; if (otherSinkTopics.contains(internalTopic)) { for (String topic : other.getValue().sourceTopics) { List<PartitionInfo> infos = metadata.partitionsForTopic(topic); if (infos != null && infos.size() > numPartitions) numPartitions = infos.size(); } } } internalSourceTopicToTaskIds.put(internalTopic, Collections.singleton(new TaskId(entry.getKey(), numPartitions))); } } } ``` And then update the Cluster metadata with `Cluster.withPartitions`, then in the `ensureCopartitioning` call, if after the first for-loop, `numPartitions` is still -1, it means all topics in this co-partition group are internal topics, and then in this case read from `metadata.partitionsForTopic` and took the maximum among all of them; and then later after calling `prepareTopic`, the metadata will be updated again with `metadata.withPartitions()`.
It seems like this test would be more useful just as an assertion of the default behavior.
```suggestion public void should createChangelogByDefault() { ```
```suggestion assertThat(CLUSTER.getAllTopicsInCluster(), contains(changeLog)); ``` And we can remove line 426, `final Properties config = CLUSTER.getLogConfig(changeLog);`, since it would be unused.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
nit: `,` (comma) doesn't seem required in the sentence.
Although the checkstyle rules currently do not enforce curly brackets in if/else blocks that contain a single statement, because this statement here spans multiple lines, I feel it'd be best to enclose it within `{ }`, even if that's optional.
This change is a good suggestion.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
nit: This block is repeated in many tests. I wonder if we could define an helper for it.
nit: We could also define a helper method for this one to avoid the code repetition.
nit: we could just build the expected result as a whole set and compare
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Nit: add `final`
nit: `maybeDeleteInternalTopics` and `maybeReset...`
This might be instable in Jenkins.
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Not sure what has changed here.
Nit: we should probably include a little more detail. Maybe something like: ```"Found invalid wrapper offset in compressed v1 message set, wrapper offset '" + wrapper offset + "' is less than last inner record offset '" +lastOffsetFromWrapper + "'and it is not zero."```
> Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right Yes. > I think we can skip the call if consumedOffsetsAndMetadataPerTask is empty. For non-eos, yes, because for non-eos `commitOffsetsOrTransaction()` would only commit offsets via the consumer (this can be skipped if empty). However, for eos (alpha and beta), we might have a pending transaction that we need to commit on the producer, too.
Yeah I think if the actual `consumer.commit` call failed, then we should not trigger postCommit for any one. As for `postCommit`, I think it should never fail (we swallow the IO exception happened, because for non-EOS it is just fine, for EOS we would bootstrap from scratch).
Sounds good, in that case the nested try-catch would be necessary.
A better way is to first call `this.mockTime.milliseconds()`, then `this.mockTime.sleep(1000)` then call the `milliseconds` again with the second batch.
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
Ah got it, my bad :)
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
Ack, I get it now. Thanks for clarifying.
nit: might be better to name it windowedKTable
Calling `maybe_start_jmx_tool` after each line that gets read from the producer process doesn't seem quite right. I think we want the behavior to be: - start producer "asynchronously" - start jmx tool asynchronously - wait for producer to finish - process each line of producer output I think it would look something like: ``` cmd = "same_as_before &" # now the cmd is "async" node.account.ssh(cmd) wait_until(producer is alive) self.start_jmx_tool(node) wait_until(producer is finished) for line in node.account.ssh_capture("cat /mnt/producer-performance.log"): # same as the previous for loop ```
Maybe this line and the one below could be moved into `start_jmx_tool`? If we go that far, we could do the locking in there as well.
A docstring for this method would be good :)
It does seem like we could pass the node id to `RequestSend` without much issue.
This is not required as contained in the check next line.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
Another tab here that should be replaced.
Also we can just pass in the `StringBuilder` as an argument rather than create a new one here
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
@spena just ping to make sure you get this on the follow-up PR.
nit: add `final`
nit: add `final`
nit: add `final`
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
Gotcha. Okay, two points: 1) It may be overkill, but you may want to consider separating the input (source, resource, etc.) files from the output (compiled class, JAR) files. It'd more closely mirror the build setup that's commonly used for Java projects and would probably make this code easier to modify in the future. 2) This bug initially surfaced with use of the `ServiceLoader` mechanism; it'd be great if we could have a test that verifies that the changes here fix how that works in isolated plugins.
Is it guaranteed that the contents of `errors` will only actually be compilation errors and not, e.g., warnings? Might be worth just using the return value of [call()](https://docs.oracle.com/javase/8/docs/api/javax/tools/JavaCompiler.CompilationTask.html#call--) to determine if compilation succeeded.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
Thanks! Will push this shortly.
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
Yes, we can open a JIRA to do it later.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
nit: We should use `groupId.idValue` here and in the others.
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
Sorry, you're right. The thing being tested should be second.
generally in assertEquals, the thing being tested comes second. If there is an error message, the first thing appears as the "expected value"
a standby task is never in
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nevermind, I see that's the pattern we follow everywhere else
`expectedTimestamp` parameter missing (insert before `expectedHeaders` to align with method name.
nit: missing comma `headers[,]`
`timestamp` missing (twice)
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Yeah I think that makes sense here
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
i.e., add `fail` after this line
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
Thanks for the explanation. A bit subtle as you had said. :)
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Use log object if this is to be kept
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
+1, we should just throw in this case.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
nit: log an error and include the `inputRecordTimestamp`
nit: `fall between 0 < inputRecordTimestamp` -> `fall between 0 <= inputRecordTimestamp`
nit: log an error and include the relevant info (eg `windowStart` and `inputRecordTimestamp` at least). Same for the IllegalStateException in `processEarly`
Also discussed offline with Becket, but this test can probably be simplified significantly
Still not used
`tp` is not used anymore.
nit: I think there's no need to reference the JIRA. The explanation seems clear enough without any additional context.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Why don't we need this check anymore? It's still done for `globalThread`.
add check for restore-consumer and admitclient
as above -- add check for two missing clients
I think, we should use three different values to make sure that the different prefixes overwrite the configs for the corresponding clients. Looking into the test below, they seem to be redundant with this one? We can also remove this test and keep the other three (that would avoid redundancy, too)
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
We could get away with a single `*`
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
Updated this when merging.
req: This is unnecessary
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
add `final` twice
Also, are we fine with the config logging being bumped up to `info` (which is what `logAll` does) vs `debug` (which is what it was here).
It's probably worth while to mention that this method starts the task for a source connector with older behavior (without exactly once support).
if we keep ending up with this pattern, it might be clearer to create a `Listener` implementation that delegates to a list of listeners instead of chaining them manually this way
Why is `false` (inexact decimals) the default for the no-arg constructor? If this is an attempt to maintain backward compatibility, we should consider whether this bug, when fixed, compatible. Seems like it would be, since having the deserializer use the trailing zeros would be fine/better than not using them.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Consider renaming to `safeToDropTombstones`: ```suggestion boolean safeToDropTombstones() { ```
can we change the type definition of these 2 to be `Admin`? Then we don't need the cast
nit: indentation is 2 tabs (total 8 spaces) and its best to match the similar method below.
nit: fix indention (same below in other constructor)
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
same here. let's make all method params as `final`
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Sensor names don't appear in JMX.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
Make all params `final`
And again with `final` if you don't mind
remove this line -- not required.
Nit: should the method be named `testOptionsDoesNotIncludeWadlOutput()` instead? The point of this PR is to prevent including WADL in the OPTIONS output, but the existing method name makes it seem like we're testing the content of the WADL output.
is there a reason for using protected here? seems like it could be package private or private
Could also use `Collections.singletonList`, which would also make this immutable
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
Seems this duplicates `L733`. Might be good to extract into a small helper method.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
Thinking a bit more. This is a bit tricky since we probably can't just continue here. For channels like SSL, we need to do the handshake after the socket is connected. Currently, the handshake will be triggered immediately after the connection is established through channel.prepare() and this has to be done in the same poll(). Otherwise, the selector may not be able to select the key again for initiating the SSL handshake. This applies to all those immediately connected keys not coming from the selector. So, to get around this issue. We can probably create a new KeyIterator that iterates both keys in this.nioSelector.selectedKeys() and those in connectableChannels. The iterator can return a <key, alreadyConnected> tuple. For keys coming from selectedKeys(), alreadyConnected will be false. For keys from connectableChannels, alreadyConnected will be true. Then, we just need to change line 291 to check if (key.isConnectable()) || alreadyConnected) and leave the rest of the code as it is. This way, keys in connectableChannels will be handled in the same as way as those from selectedKeys().
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
nit: no need newline of 104 below.
This method is not `synchronized`. So there could be a race condition here. I think it should be: ``` final NamedCache cache = getOrCreateCache(namespace); final LRUCacheEntry result = cache.putIfAbsent(Bytes.wrap(key), value); maybeEvict(namespace); return result; ```
This line is failing checkstyle.
We can use `computeIfAbsent(...)` to eliminate the prior newly-added line: ```suggestion allAddedPlugins.computeIfAbsent(pluginClassName, n -> new ArrayList<>()).add(plugin); ```
Mmmm, I'm not sure we should be making decisions here based on dynamic plugin loading for two reasons: 1. This change can be backported to older versions of Connect, which will never have that feature. 2. It's unclear exactly what the mechanism for dynamic plugin loading will be, and it's possible that a re-scan of all known plugins after loading has taken place (either the initial start load or a subsequent dynamic load at runtime) could still be beneficial Also, it's actually not that uncommon for 3+ copies of the same plugin to appear on the plugin path for a worker. For example, some connectors come packaged directly with converters; all it takes is at least two such connectors and a separately-installed copy of that converter to lead to that number of copies, without any error or misconfiguration on the part of the user.
Oh, gotcha--in that case, should we do a check somewhere else, since this will be triggered potentially multiple times for a single plugin? For example, if there are three copies of a connector, the warning will be logged twice right now, with different values for `inner.keySet()` each time. Also, it may also help to log exactly which one we're going to use either instead of or in addition to the complete set of discovered versions of the duplicated plugin.
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
Ah got it, my bad :)
same question around log level as above
same question around log level as above
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
This data construction seems better to be put in the `AlterConfigsResponse` constructor.
Yea, my suggestion would be to reuse the existing constructor as the construction of the `AlterConfigsResponseData` seems non trivial for a caller to do, compared with passing a map of errors.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
Let's keep this as a private method.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Add the `@Overrride` annotation to this method.
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
I think we might want to consider dropping some of these `log.debug`s to `log.trace`. Some of the logs in error conditions make sense at `debug`, but logging every fetch request and response at `debug` might make changing from `info` to `debug` a bit overwhelming.
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Not sure this is what people will generally mean by lag -- while the committed offset matters, normally if the consumer is in the process requesting the lag I think it'd mean the FetchRequest lag, i.e. how far behind *processing* the records is the consumer in comparison to what the broker indicates is the most recent offset. in other words, I think i'd update this at the end of each `put()` and change from `committedOffsets` to `processedOffsets`.
Hmm, good question. I may have actually been wrong about which values should be involved. I think @gwenshap and I had a long discussion about this awhile ago too and there are many ways you could define lag. I think the real problem here is that we may not be exposing enough information from the consumer to compute what I would really think of as lag -- FetchRequests include high watermark info so you know how many records are in the log but not yet returned to you, and the consumer creates metrics based on that. But we don't have access to that info. A connector that commits on every message would look like it has 0 lag, but it could be very far behind in the topic.
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
Where do you cleanup the childrenSensors object? Otherwise we will maintain a reference to the Sensor objects always.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
I see @mjsax has already a PR for that, great!
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
line/sentence formatting `{@code null}`.
nit: parameter/line formatting
nit: single parameter per line
Not for this patch, but we should do a KIP to add support for batch topic creation.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
req: Lag should be a long
Actually, do we even need this at all? It seems like we get everything we need from `statefulTasksToRankedClients` -- it should have all tasks (and clients), and either a) the previous client was caught-up, in which case it should be the first rank and we can determine it was the previous host from a lag of `Task.LATEST_OFFSET`, or b) the previous client was not caught-up, in which case we don't really care what the previous host was for that task
Hmm...I'm wondering if `Map<String, List<TaskId>> previousAssignment` is sufficient to pass in, won't we lose all the tasks that were assigned to a client that no longer exists for whatever reason? Maybe we should just pass in `Map<TaskId, String> tasksToPreviousClients` (aka `tasksToHostClients`) directly. We can build that map up during other steps in `assign`
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
`KeyValueStore` -> `TimestampedKeyValueStore`
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Let's be consistent to name it throwable
it might be a bit cleaner to invert this if statement to write it like this: ```java if (this.retryContext.tries() <= maxRetries) { log.debug("{} failed: {}. Beginning retry #{}", this, prettyPrintException(throwable), retryContext.getTries()); runnable.call(this, now); } else { failWithTimeout(now, throwable); } ```
It's probably more of an issue now, but I think we may have already not been entirely thread safe with the methods we can call when starting connectors/tasks. In particular, `reconfigureConnectorTasksWithRetry` can invoke `addRequest`, and `addRequest` expects to already be locked but `reconfigureConnectorTasksWithRetry` doesn't guarantee that. I'm guessing we've missed that issue until now because that happens rarely and I think the only thing that could conflict with it would be external HTTP requests. I think the only other piece that needs to be protected as fallout from this parallelization is the step where we call `configBackingStore.putTaskConfigs` in `reconfigureConnector`. The backing store is not thread safe (and neither is its underlying `KafkaBasedLog`).
They are using the same underlying enum, but `TaskStatus.State.DESTROYED` would probably be clearer here.
Aha, good point.
Nit `.` at the end
nit: both lines missing . at end
nit: single parameter per line
@junrao We are relying on fall-through for the second case, so it's not straightforward to make that change.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Thanks, looks good. Yes, it's O(1), but a lot less efficient than returning a local variable. Take a look. :) ```java public V get(Object key) { Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1) & h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } else if (eh < 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) return e.val; } } return null; } ```
You could remove this `if(..)` block as `putIfAbsent` below will cover it
I did not find any `toString` in `RocksDBStore` so I added one in `Segment` (which can then expose the id, which is not something that `RocksDBStore` knows about).
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Docstring doesn't match the class
nit: "which provide underlying producer and consumer clients for this {@code KafkaStream} instance".
`keySerde` -> `valueSerde`
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
Can this be a `byte`.
I was checking the code in the protobuf repository and it turns out that the optimised version of this code is pretty complex: https://github.com/google/protobuf/blob/master/java/core/src/main/java/com/google/protobuf/CodedInputStream.java#L2473 I think it makes sense to start with the simple implementation, but we should file a JIRA for considering the optimisations in their implementation.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
the message format => that message format
We could make this field access `public`
nit: we can use `map#compute` to replace getOrDefault + put.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Fine with me to keep the guard. Was just double checking.
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
As we expect a specific `groupId`, I would check the provided `groupIds`.
`mkProperties` could compactify this code, but it's not necessary; your call.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
ditto about the log level (also for the below uses of `debug`)
In this case I think we should include some error details here. In particular, the last seen error for each topic. I'm worried about cases where we try to create but the create times out but is eventually successful. We'd return an error back, but the user would have no way to know that setup failed because an internal topic already exists.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
We don't use LeaderNotAvailableException in listTopics
request "got" re-sent to the control
We don't use ReplicaNotAvailableException in listTopics
Use `File.separator` instead of `/`
Just some more nits. Can you add `final` wherever possible: `ClassLoader`, `String filename`, `BufferedReader`, `for(final String...)`, `Exception`
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
this overwrite of mm2config should go in the setup method, IMHO
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
prop: Could we pass into `getMovements()` the number of warm-up replicas and only compute as many movements as needed instead of computing all movements and then using just the first couple of movements.
Q: IIUC, we do not check if the movement is for free. That is, if the destination is a caught-up client. If it were we would not need to assign a warm-up replica and could consider one more movement. I am also fine with post-poning that to a follow-up PR.
This is a bit suspicious... If we're polling the queue, we should just loop until the queue is empty, not iterate over another another collection we happen to know has the same number of elements. More specifically, `poll` might return `null`, but `offer` throws an NPE if `client` is `null`.
nit: add `final`
I am just wondering: it seems that we don't have a check in place that `storeBuilder.name()` does not return `null` -- this would be bad and should not be allowed. Also, we should never put `null` as name, but generate a name. This check `isQueryable()` should be done in `GlobalKTableImpl#queryableStoreName()`.
Ack, I get it now. Thanks for clarifying.
Sure, but that led to the opposite problem, in which the enum was inconsistent with the state. In regard to position, I think we should handle this at transition time as mentioned below. If we ensure that position is not null in the fetching and validating states, then I don't see a problem changing `hasPosition` to check it directly.
Thanks for the explanation. A bit subtle as you had said. :)
Should we also check that ``` final long start = SessionKeySerde.extractStart(bytes.get()); final long end = SessionKeySerde.extractEnd(bytes.get()); return end >= from && start <= to; ``` Although the current impl of RocksDBWIndowStore naturally checked that for us, it does not guarantee all underlying store impls guarantee that.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
We lack unit test coverage for this case
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Thanks for the follow-up.
Ditto here for different exception types.
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
only one parameter should be `null` -- otherwise it's unclear what this test actually does
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
I asked you exactly that a few months ago :) You referenced some old PR but basically the takeaway was, a restoring task hasn't initialized anything but its state, therefore needs to close only the state manager
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
I think we need to insert a `fail` here to fix this test
req: This is unnecessary
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: it is naming a source node, not a processor node. -> `"source"`
This method is not `synchronized`. So there could be a race condition here. I think it should be: ``` final NamedCache cache = getOrCreateCache(namespace); final LRUCacheEntry result = cache.putIfAbsent(Bytes.wrap(key), value); maybeEvict(namespace); return result; ```
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
Overall LGTM. But can we format it differently? We should start a new line for each sentence. If we update the docs later, it make the diff simpler to read.
extra space after `*` needs to be removed
nit: needs a comma after the `{@link ...}`
nit: should we have a newline for each partition? Otherwise that ling maybe too long.
I wonder if we want to print the actual list of partitions here, which might be long. And do it twice. I see the same pattern is applied elsewhere. I understand the value of explicit listing.
I was thinking it was odd that this wasn't reusing `flushAndCommitOffsets` since they are basically the same. But I see the exception handling is different, so that makes sense. But then I noticed there's no call to `onCommitCompleted` if there's an exception during the flush, which we do for every other path during commits. _Then_ I realized that this is actually for the special case of closing, and that the callbacks that are invoked by `commitOffsets` in this special case are weird since the seqno will never match and it is probably always getting logged as an "error" at debug level. This isn't critical since it's just at debug level, but should we have `onCommitCompleted` check for the sentinel value and ignore the callback in that case? But also, I think the seqno handling is kind of unnecessary now. I think this is a holdover from possibly 2 separate things. First, we previously handled offset commit differently with different threads. Second, the semantics of offset commit in the new consumer were very unclear at the time this code was original developed (that was back when it wasn't even fully implemented and I was trying to sort out what semantics we wanted, so ended up being somewhat defensive in this code). I believe it is the case now that you cannot have callbacks for offset commit come back out of order (despite the fact that they are async) and that since we guard offset commits with a check on whether we are currently committing, we can only end up with multiple because we explicitly allow commits to expire to allow a newer one to be submitted. But will this ever actually help? Because of the way the protocol works, won't the offset commits just get queued up anyway? Would it make sense to adjust this so we have to wait for the commit to finish regardless, but that we might do something like pause processing if it takes too long? Basically, since we don't do it synchronously, we allow processing to continue since it's nice to not have to stall during the commit, but at some point if we couldn't commit, we shouldn't just cancel that commit, we should wait until it actually completes. (And, of course, we also need better handling if the commits repeatedly fail.)
In the ZK based code, we also take live brokers into consideration when selecting a new leader.
We need to choose at least a live replica.
Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
request "got" re-sent to the control
Should be larger
Typo: should be "or larger than the number of available brokers"
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
nit: add `final`
nit: add `final`
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
We can use `new ProducerRecord<>(topic, "value");` to simplify it a tiny bit
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
`orderInGroup` param is duplicated for key & value converter
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
Don't insist -- would prefer though (for this case) -- in general, this pattern can be useful -- just don't see it for this particular case.
This should not have the `sink` suffix
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Currently in this file the indentation style used is: ```java protected boolean maybeAddConfigErrors(ConfigInfos config, Callback<Created<ConnectorInfo>> callback) { ``` Still, once we move to single arguments per line it should be: ```java protected boolean maybeAddConfigErrors( ConfigInfos config, Callback<Created<ConnectorInfo>> callback ) { ``` I'd pick one of these. (First I confused `callback` for a local variable)
Method should not be `final`. Additionally the `final` keyword for method arguments and local variables is not required and does not improve readability of the code here. Indeed Java does not distinguish between readonly and read-write variables. But unless an anonymous class is declared (this requirement is removed after Java 8) or the variable is used further down in the code (improved readability) marking every single readonly variable as final does not make things better IMHO.
nit: config is an overloaded term in the code, you might prefer to name this argument configInfos for instance.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
placeholder may not be required for exception
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
Ah got it, my bad :)
We lack unit test coverage for this case
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
I don't think we need this `null` check either.
Nit: "The file channel position..."
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
We should mention somewhere that users should prefer this new assignor for newer clusters.
We should limit this suppression to the method for which we really need it instead of the whole class
this line can be merged. for example: ```java FindCoordinatorRequestData data = new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.rebalanceConfig.groupId); ```
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
We can do it separately if you like.
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
Thanks for doing this. I also noticed it was missing and fixed it in this PR: https://github.com/apache/kafka/pull/10085/files#diff-1da15c51e641ea46ea5c86201ab8f21cfee9e7c575102a39c7bae0d5ffd7de39R134-R137 Maybe reconcile the two changes and we can merge this one.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
nit: `e` -> `fatal`
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
nit: one parameter per line -> move `retryBackOffMs` to its own line
nit formatting. `cause` is indented correctly, it's weird that it align to the arguments above that are not parameters of `StreamsException` (like `cause`). Better: ``` throw new StreamsException( String.format( "Could not create topic %s, because brokers don't support configuration " + "replication.factor=-1. You can change the replication.factor config or " + "upgrade your brokers to version 2.4 or newer to avoid this error.", topicName ), cause ); ```
Should we still log it, perhaps as a warning? If I understand the background, this case is unexpected except with 0.10 brokers, so it seems like swallowing it could mask an important condition.
I wonder if we can just get a Map with the default size. I don't expect this code path to be very hot
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
+1 for consistency
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Since you've noticed that these don't depend on any instance state, do you think it makes sense to move them to a separate file (e.g. `ConnectorUtils`)? The `instantiate` function below would be another candidate.
I believe that including the name of the property in the error message is redundant as that information will be available already in the REST response. I also think we may want to be clearer about the error message here. Users can't supply null values, but developers (by specifying `null` as the default value for a property in a `ConfigDef`, for example) definitely can, and we may want to make it clear which variety we're prohibiting. What do you think about this? ```suggestion .map(configEntry -> new ConfigValueInfo(configEntry.getKey(), "The JSON literal `null` may not be used in connector configurations")) ```
```suggestion /** * Task ID of the task. * * @return task ID consisting of subtopology and partition ID */ ```
```suggestion /** * Source topic partitions of the task. * * @return source topic partitions */ ```
```suggestion * Metadata of a Kafka Streams client. ```
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
We usually avoid writing to standard out in test cases. A few more of these.
Please remove empty lines here and in the other test methods.
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
nit: 'else' can be dropped
We may as well use the more specific type `TimeoutException` given the name of the method.
In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
Nevermind, I was thinking of `position += n`, but `limit` is actually the result of that (bounded by records.size).
Thinking about it, this may be the root of the problem. We are removing from the head of the ArrayList, one at a time. For each removal, we cause all the elements to be shifted in the underlying array, which is very inefficient if n is not small.
Also, `n >= records.size()`? Seems we could clear `this.records` and use this fast path if they are equal. Would also remove need for checking `iterator.hasNext()` below.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
```suggestion * Disable the changelog for this suppression's internal buffer. ```
nit: might be better to set end to another timestamp.
`windowSize` should be `Duration`
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
`orderInGroup` param is duplicated for key & value converter
as above mentioned, the `listStore.all()` is not closed here.
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
nit: `final` params
Probably want to make this a `ConcurrentHashMap` or `synchronize` access to it. It can be modified etc from multiple threads
The state that is being updated is private data of `KafkaStreams`. It should be responsible for synchronizing access to its data, not external classes.
This is the callback from the `StreamThread`s so it will be called from multiple threads, i believe. See the inner class `StreamStateListener`
This is added in #986 as well. The later patch to go in will have to rebase, or we can extract out the common code as a separate PR. I am fine with either way. FYI @becketqin.
Is this constructor used at all? If not, I'd not include it one should generally provide a message explaining more.
while this is fine, perhaps we should follow the same pattern for getters in these 3 new classes.
placeholder may not be required for exception
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
The extra parameter seems a little more annoying than the effort it saves (which is really just checking a couple flags). If we really wanted to avoid the redundant checks, maybe a better way is to add a private `doAutoCommitAsync` or something like that, which simply sends the request assuming the coordinator is known and autocommit is enabled. Then the two `maybeAutoCommit` calls could delegate to `doAutoCommitAsync` after doing the checks themselves.
nit: we can use `map#compute` to replace getOrDefault + put.
nit: add a space before the `:`.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
That's right, changed it locally.
original was better
original was better
original was better
nit: empty line.
We can remove the code block line 82-85 above since it will be called here.
nit: align parameters.
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
Done now, thanks @ijuma
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Cleaner to just check if `tasks.isEmpty` after the loop is over.
`TaskManager#tasks` has to be accessed through the state change thread. It can't be accessed here by incoming requests since there is no lock or synchronization. Probably the easiest thing to do is to have a CreateTasks runnable which does what you want.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
The next 2 methods aren't used in this PR. I guess they are used in one of the others...
nit: should be `named` can't be null
This should be new `ValueMapperWithKey`
nit: add `final` (2x)
nit: remove empty line
nit: `child` -> `toChild`
Personally I think it makes sense to just disallow calling `ofTimeDifferenceAndGrace(...).grace(...)` entirely, this seems like abusing the API
Missing `.` after `since 3.0`
But grace and retention are two different things. In fact, I just had another conversation about this issue, and it seem we need to fix this by allowing people to specify a retention time IMHO. Not sure if we need to add a `Materialized` parameter or add `Joined#withRetention()` that we use to specify serdes etc.
`This conversation was marked as resolved by wcarlson5` :)
Removed this, as it describes `CogroupedKStream` what is not appropritate here.
Nit: we don't need this tag before the parameter list.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
There is code duplication between these 3 methods. Let's figure out a way to remove this duplicate code.
Let's check that `needsDrain` returns true after this point.
s/assignments/oldAssignments, and name could be simplified as `toNewAssignment`
This name seems backwards.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
This seems to change the behaviour...
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
This is added in #986 as well. The later patch to go in will have to rebase, or we can extract out the common code as a separate PR. I am fine with either way. FYI @becketqin.
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
I am wondering, if we might want to remove this method? Do use it only at one place and can easily pass the record timestamp there instead? Or is it useful for some special cases? Of course, this would require a KIP and separate PR -- it's might also not be high priority and just creating a JIRA for it might be fine. WDYT? \cc @guozhangwang @bbejeck
I created https://issues.apache.org/jira/browse/KAFKA-7245
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
Should we use this check as a condition to wait? Sleeping 10 secs feels pretty brittle
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
That sounds good to me 
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
We tend to use the steam api for such small transformations but I don't feel strong about this.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
This statement is always false.
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
Not introduced in this patch: "is non" => "as non"
As i said above, we should `requireNonNull(mapper, ...)`
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
Is`atexit` is the right approach here? The registered function will only run when the entire ducktape process finishes (i.e. _after_ all ~215 tests finish). Use of the `tempfile.mkdtemp` will still avoid path collisions from concurrent processes, so maybe it's good enough. Another possibility that at least results in immediate cleanup: put directory removal into `clean_node` (but check the containing directory is present before removing to avoid errors)
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
Shouldn't need this line, it's handled by the superclass's constructor.
nit: add `final`
nit: add `final`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
Could we rename this to something like "remainingPartitions"
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Oh good point, we definitely need the key. But I think separating them turned out well
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
If you want to check that records for non-pause partitions are cleared, we should have a separate test.
```suggestion if (!batch.records().isEmpty()) { return Optional.of(batch); } ```
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
Oh yeah, duh. Nevermind this 
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
these overrides don't seem to add much.
never mind then. I'll leave this to AI.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
This is the same code as in `KTableFilter` -- we should refactor and share code.
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
what's a requested topic partition? Also, above we mention just `partition`
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Consider naming the topic "topic2" since there are only two topics in the test
why do we put "unknownTopic" here? Should we subscribe to "topic1" and "topic3"? Or we can actually pass in an empty list? I guess this is copied from `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks` -- there we need to pass in `unknownTopic` as this topic does not exist in cluster metadata.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
Cheating the compiler, woohoo!
Nit: could just throw the exception directly here; doesn't appear to be much benefit to putting that in a separate `setup` method.
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: if you want a new paragraph you need to add `<p>`
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
seems like it should at least be info, if not warn
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
nit: you could just as well use a ternary operator here: ``` return (parts == null) ? Collections.<List<PartitionInfo>>emptyList() : parts; ```
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
@ijuma Sorry, I don't know of a standard way of doing this,
nit: maybe iterate over `entrySet()` instead.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
Similarly here, this state check could be internalized.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
This should say `AdminClient`, not `Consumer`.
Personally, yes, I prefer one call, but I leave it up to you.
Thanks! This should have been done quite some time ago.
Let's use try with resources here and the other test so that the file is closed after it's used.
Maybe we should add one case where `position > 0`.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Ack, I get it now. Thanks for clarifying.
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
nit: might be better to name it windowedKTable
This first argument (`1L`) can be missed, given that that's the only line in which we pass 2 arguments. I'd add one argument per line.
I would move this assertion out to the assertion on line 351. With this move you save one parameter and you have both assertion next to each other.
This is not required as contained in the check next line.
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
How about using `MessageDigest.isEqual` method here also: ``` MessageDigest.isEqual(new String(password).getBytes(StandardCharsets.UTF_8), expectedPassword.getBytes(StandardCharsets.UTF_8)` ``` cc @rajinisivaram
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Style nit for "note" and whitespace between `Bytes` and `byte[]`: > (note, state stores always have key/value types {@code <Bytes,byte[]>} should be > (note: state stores always have key/value types {@code <Bytes, byte[]>}
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
Nit: `.` missing at end of sentence
I understand that. My question is whether there is some thinking on which configs make sense to be set by users. Ideally, we'd do that instead of being reactive.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Might be overkill if this is the only use case, but we could also add a composite validator.
Oh you're totally right, sorry for letting my paranoia start spreading conspiracy theories here  Given all this I'd still claim that the FSM is in need to being cleaned up a bit (or a lot), but if you'd prefer to hold off on that until the add thread work then I'm all good here. Thanks for humoring me and explaining the state of things. I just wanted/want to make sure we don't overlook anything, since there's a lot going on. For example in the current code, if the global thread dies with the old handler still in use then we'll transition to ERROR. However the user still has to be responsible for closing the client themselves, and it will ultimately transition from ERROR to NOT_RUNNING. Whereas if we transition to ERROR as the result of a SHUTDOWN_APPLICATION error code, the user should NOT try to invoke close themselves, and the ERROR state will be terminal. That's pretty confusing eg for users who use a state listener and wait for the transition to ERROR to call close(). We should make sure that ERROR has the same semantics across the board by the end of all this work. Anyways I'm just thinking out loud here, to reiterate I'm perfectly happy to merge this as-is. But for reasons like the above, I think it's important to tackle the FSM in the next PR and make sure it all gets sorted out by the next AK release
Hm ok this might be a problem. Since this is thrown from another catch block and not from the try block, it won't be caught by the catch block below and will slip through the exception handler.
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: break line
nit: line too long
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
Let's avoid making whitespace changes like this, since they tend to make the change harder to read
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
Can you elaborate? Seems to be orthogonal to the timestamp fix.
I think the expectation is that these 2 would be atomic (i.e. would be bad if one thread executed 615, then another thread executed 615 again and got the same sequence number, before the first thread got a chance to execute 616). Also I think the expectation is that batches that are ordered one after another in the queue would get the sequence numbers in the same order (i.e. that batch that is later in the queue would get higher sequence number). Previously these expectations were protected by the queue lock so "poll", "get sequence", "update sequence" would execute as atomic block, with this change the operations could interleave.
Moving `close` outside of locked scope LGTM
No, not a blocker.
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
`hop` vs `advance` is subjective :) I am fine with `hop`, too; it's called a `HoppingWindow` after all. `advance` is just a term that is quite common in literature so I am somewhat used to it.
I would prefer `advanceBy(long advance)`
Remove double blank.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
recommended; ditto below.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok
That makes sense, I think keeping it as-is is better.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
I think we can just add these configs as part of the PR.
I think this could just be if(!SecurityProtocol.values().contains(securityProtocol))
That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
I think you missed this one.
nit: since this is indicating entry into a method, I think it can be trace level.
I wonder if it would be worth improving this log message slightly, to something like: > Timed out waiting to flush offsets to storage; will try again on next flush interval with new offsets Strictly speaking, it's unrelated to the changes made in this PR. But for users seeing this in the log it would be helpful to know that despite it being an error that should be looked into, the next flush interval will attempt to commit all (potentially-updated) offsets.
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
`timestamp` missing (three times)
nit: missing comma `headers[,]`
`timestamp` missing (twice)
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
NPE: ![image](https://user-images.githubusercontent.com/925755/55269396-d55f3480-5292-11e9-9c29-78c524d63c65.png) I'm not using a topic pattern, equality should still work.
Mentioned offline, we can probably move move the conversion to empty string into the `OffsetAndMetadata` constructor so that we always handle this consistently.
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
For standby tasks, I think cleanup the state directory is fine; adding it into the rebalance protocol needs bump up the metadata serialization version for upgrade and hence more complicated.
I think it is less of an issue to be a separate JIRA, but either is fine to me.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Nit: we should probably include a little more detail. Maybe something like: ```"Found invalid wrapper offset in compressed v1 message set, wrapper offset '" + wrapper offset + "' is less than last inner record offset '" +lastOffsetFromWrapper + "'and it is not zero."```
Should be `post 0.10.0 Java clients`, I guess.
I think this should be used in the define method below too.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
We could iterate through v1 to v5 here to test every case.
```suggestion public void shouldInstantiateAssignor() { ```
nit: add a size? There are a few cases in here where we could do this.
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
WDYT about this: ```suggestion long millisRemaining = Math.max(0, end - System.currentTimeMillis()); if (millisRemaining > 0) { Utils.sleep(millisRemaining) } ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
nit: move `Serde.String()` into next line (also, I would prefer to not have static import `Serde` but prefix. I was initially confuse why this compiles and why it's not `new String()`... (because the method name starts with capital `S`...)
nit: remove blank line
Can you elaborate? What do you mean by > otherwise the state won't proceed
nit: should be `named` can't be null
nit: add `final`
nit: missing `<p>` for new paragraph
@spena just ping to make sure you get this on the follow-up PR.
Side improvement: I think we should skip late record directly and also record it in `TaskMetrics.droppedRecordsSensorOrExpiredWindowRecordDropSensor`
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
Yes, I think it's worthwhile to check the result of `Connector.config()` just in case `Connector.validate()` is overridden and the default check there is no longer used.
I was referring to the call to `connector.validate` two lines above. That is where the other null check in this patch in `Connector` would be applied, unless the user has overridden `validate`.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
the drawback of renaming here is that the name `start_cmd` is used with `start`/`start_node` pretty standardly across the rest of this code base
Unless I'm wrong, we move to this directory and that's where we execute all the rest of the commands (such as the echos in output files below). Just want to make sure this is what we want (which looks like it is)
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
@vahidhashemian, yes, that's what I mean.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Nit: remove unnecessary `this`.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
What kinds of failures are we trying to mask here? Frequently retries also require delays between retries.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
I don't think it makes a big difference either way. The intent of the offset commit interval is just to make sure committed offsets don't fall too far behind. It does not need to be a strict schedule. It seemed more intuitive and simpler to me to reset the interval. In any case, we should get rid of this relative tracking of the next commit. If we use an absolute time, then we will not have problems like this in the future.
Maybe this is a little simpler? ```java nextCommit = now + offsetCommitIntervalMs; ```
Is there any reason not to accept this suggestion? I will go ahead and push an update to this PR next week if there are no further responses so that we can get this fix into the next release.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
```suggestion for (final Map.Entry<TaskId, SortedSet<ClientIdAndLag<ID>>> taskToRankedClient : statefulTasksToRankedClients.entrySet()) { ``` Just to resolve a warning about referencing the subclass instead of the interface.
TBH, I'm a little skeptical of using this style too much. Nothing against functional programming; it's just that, having done quite a bit of FP-heavy programming and maintenance for quite a few years, I've settled into an opinion that it's most efficient when employed in simple contexts. When you get into nested transformations like this, it becomes harder to come back to the code in three years with a completely blank slate and read it. Plus, it has a tendency to steer you away from efficient code and you can wind up doing multiple iterations over the same collection when one would have done. So, I tend to use the FP APIs to do stuff like turn a list of Tasks into a list of TaskId, and I'm happier to see regular loops and conditionals for stuff like this. This is very much a matter of preference, though, and I'm only expressing mine.
Fine with me -- it's just a "prop" after all. FWIW I generally find these functional-style methods harder to read, but for whatever reason in this case I was finding the original a bit hard to understand and thought this suggestion helped to "get to the point" faster. But of course it's always easier to read your own code than someone else's 
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Your understanding is correct @mjsax .
nit: avoid unnecessary `this.` prefix
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
typo: byteArrray -> byteArray
`< Callback >` this explicit type is not necessary.
nit: we could split this lone line by different key, value by new line to make it clear. ex: ``` String[] args = new String[] { "--topic", "Hello-Kafka", "--num-records", "5", .... }; ``` Same as below.
`replicaing` -> `replicating`
I think the assertion on 219 would pass even if the 1st mocked interaction never happened. Do we need something to tighten up the expected behaviour? Maybe something like: ```java verify(kafkaBasedLog, times(2)).send(any(), any(), any()); ```
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
ditto here and others below
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
rewrite test as above using `assertThrows()`.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
nit: add `final`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
nit: add `final`
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
We'll need to fix this in a follow-up so that followers send the right replicaId.
The current implementation is problematic in the sense that all cached data will be removed if users subscribe to the existing set of topics and `topicPartitionsToClear` is empty. Can we replace `clearBufferedDataForTopicPartitions(...)` with `updateCompletedFetches(Set<TopicPartition> assignedPartitions)`? updateCompletedFetches will keep only those data whose partition is still assigned. This will fix the problem above and also makes the logic in the caller simpler. Instead of having to determine the partitions that have been removed, the caller only needs to provide the latest assigned partitions as `subscriptions.assignedPartitions()`.
Do we need to also clear data from `nextInLineRecords`? Also, it seems that `assignedTopicPartitions` is never null, right? Maybe we can skip this check.
It is public API for a class that we are using internally. Thus we are able to enforce rule via code review that `assignedTopics` is never null. And in general it is preferred not to call a function with null parameter, since otherwise we will have a lot of null check in the code which will make the code unnecessarily verbose. So it seems simpler not to check whether it is null. Note that a few other public methods do not check whether the input parameter is null. For example `Fetcher.getTopicMetadata` does not check whether request is empty. So it is probably consistent not having to check it. And for public APIs that are exposed to user, e.g. `Consumer.assign(partitions)`, because we can not ensure via code review that `partitions` is not null, we currently throw `IllegalArgumentException` if the value is null, instead of allowing user to call the method with null value.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
`KStream` => `{@link KStream}`
Typo: "_of_ key-value pairs"
could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok
You need to pass in a `Pattern` but not a `String` here. This actually exposes a "bug" in the test-driver -- it should check if the topic name is valid -- and a pattern is not a valid topic name. Not sure if we should have a different PR for a fix or piggy back on this PR. \cc @guozhangwang @bbejeck @vvcephei
Let's piggy-back on this PR: it should not be a big fix.
nit: `completeRefresh` to be more concrete in what it completes? Complete can be understood to mean a total completion of the connection, whereas this completes a single refresh (something finer-grained and more often)
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
This is unused too
Why do we have SSL specific methods here? Could we move all the SSL bits into the SSL class? We have fields for the configurations. So we could set them accordingly (without or without SSL) in each concrete class. Then in the base class, we just use the fields to create the clusters without having to know if it's SSL or not.
nit: formatting -> should be in the line above.
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
We actually don't need to name the store. This could be `.count()` plus updating the name for the repartitioning and changelog topic.
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
may be use Objects.requireNonNull
We should test that delete twice in a row fails with `IllegalStateException`
The goal of the ticket is to actually remove this check.
we also want to remove this check
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
You could do `Assert.fail(...)` here rather than tracking it in a boolean etc
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
same question around log level as above
same question around log level as above
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
This is a change in semantics, right? Before, we would never expire a connection that has been processed during the `poll` because we use `currentTimeNanos`. After this change, if something in `poll` takes long enough, we could end up expiring a connection in `completedReceived` for example.
this problem also affect replication node fetch process? our server occur a error: [2019-04-05 23:59:46,084] WARN [ReplicaFetcherThread-1-1], Error in fetch kafka.server.ReplicaFetcherThread$FetchRequest@b22a64c (kafka.server.ReplicaFetcherThread) java.io.IOException: Connection to 1 was disconnected before the response was read but return to normal after a reboot kafka server.
nit: seems these could be final? Same in `ConsumerUserData`.
nit: we can use `map#compute` to replace getOrDefault + put.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Also, 5ms seems a bit extreme. Maybe this could be 20ms or so and we could use the minimum of this and the configured retry backoff so that users can adjust it lower if they need to.
Yeah, I have no doubt the performance is better. It's just that it seems like a lot of excess traffic and is going to be amplified by the number of transactional producers. It may be fine in the common case if the write markers are pretty quick, but if there is any kind of delay, then I'd be concerned about the brokers being overwhelmed with these requests (though maybe it's not as bad with request throttling). I'd rather err on the safe side for now since users can manually adjust the backoff. For the 0.11.0.1 release, we can provide a better solution. Most users will probably hold off until then anyway.
ditto (code style)
Same (should be threads_per_worker)
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
Maybe just check that `minikdc` is not None here
Huh. Bummer that you have to make it queriable in order to use the in-memory store. I never noticed that before.
Yes, this would be better. Not sure if it helps, but for reference, this is what we did in `org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinMaterializationIntegrationTest#getTopology`
Instead of specifying the whole thing for both cases, you could just create a ``` final WindowBytesStoreSupplier supplier = inOrderIterator ? new InOrderMemoryWindowStoreSupplier(...) : Stores.InMemoryWindowStore(...) ``` and then pass that into the `Materialized` without having to list the whole topology out twice.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Kafka doesn't mandate braces in `if` statements.
Let's remove the brace changes please.
Better be `cooperative-sticky`? `cooperative` is too general I think.
We should mention somewhere that users should prefer this new assignor for newer clusters.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
I wonder if we should move all of this into a new private method that takes the interceptor callback and intercepted record. The reason is that it's a bit easy to make a mistake and use `record` and `callback` instead of the intercepted ones (with the current approach).
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Q: I might have missed the discussion. Why does an unknown offset result in `1` and not in `Long.MAX_VALUE`? Sorry if you have already answered this question elsewhere.
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
This should call `configState.rawConnectorConfig(connector)`, which returns the _user-supplied_ configuration _with variables not resolved_. The `connectorConfig(connector)` call returns the configuration _with variables already replaced_, which means we might be leaking passwords and other secrets specified using variables.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
This should not have the `sink` suffix
Sounds good. We can consider this resolved.
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
There is code duplication between these 3 methods. Let's figure out a way to remove this duplicate code.
Let's check that `needsDrain` returns true after this point.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
I think we put args on the same line unless the list is too big
the method name changed to `windowedTable` and `windowSize` parameter is missing
and -> a
and -> a
Nit: "..and producer's {@link DefaultPartitioner}".
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
Thanks for the catch!
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
SGTM. We can keep it as is then.
remove var -- only used once.
nit: move `"topic2"` to next line and fix indention (should only indent by 4 spaced) -- this will reduce the line length and make code better readable.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
nit: I don't spot any, but safer to avoid typos by just having constants for these
super nit: extra blank line
Please include TopicDeletionDisabledException here.
We don't use LeaderNotAvailableException in listTopics
This was probably discussed in the KIP, but obviously the downside is that users won't get any warning or hint that they should transition. But I guess we don't get a substantial benefit from removing `AdminClient`, so maybe we'll just never do it.
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
Is this just to prevent it from processing anything until you're ready to proceed? It seems like you can/are doing that just by controlling when to produce input messages and doing so one at a time (if that's accurate, then WDYT about renaming `process` to `processed` and flipping the boolean so it more clearly serves the purpose of indicating whether a record has yet been processed)
No worries. I was only recommending to change the name if you stopped using it to gate the processing and just relied on producing messages to control when they could be processed, if you did want to do that. Either way is fine with me so you can just leave it as is
 fair enough
How about returning a Set instead of List? ``` return topics .stream() .filter(topic -> !topic.isEmpty()) .collect(Collectors.toSet()); ```
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
I think a better test scenario is to move the logic in `close()` call, i.e. when the stream thread is being shutdown, and topology is closing, we call `processorNode.close()` in which we wait for a while and then tries to access the global store. It mimics the case where in closing the store cache is flushed and hence tries to access the global store again.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
Ditto on the properties and the driver.
ditto on the properties and the driver.
Ditto on the properties and the driver.
Just a question: Why is this not `6L` ? (it should be `5L` after you applied the fix you want to do in a follow up PR).
nit: should we merge this into existing test? If not, rename to `shouldChooseNextRecordBasedOnHeadTimestampe`
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
The overhead is minimal. The benefit is clarity: if someone looks at the code it is immediately obvious that null is not allowed.
nit: should we have a newline for each partition? Otherwise that ling maybe too long.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
Is this test needed? It seems that loginContextName can never be null.
Harsha did this.
It seems that we need to set the login time during the initial login as well.
building a topology is done on the main thread when calling `StreamBuilder.build()` so I think it's safe to remove `synchronized`.
How can we be sure that `partNums` is not empty? If empty, `next()` would throw.
Since we are only going to verify number of partitions, I think we could just set value as integer
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
Why are we changing this? The constructor with a single argument takes a message and not the groupId
typo: drop "the" before "whether"
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
What you had is fine.
Sorry for not catching this earlier, shall we try to be consistent when it comes to `toString` output? Here's an example of a recent Java class: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClientResponse.java#L67 What do you think? Also, it would be good to include an example of the logging output.
I wonder if it is worth refactoring this to remove duplication, i.e, add a Functional interface, then implement it three times to just perform the op on `globalStateRestoreListener` then have a method sth like: ``` performOnGlobalListener(GlobalListenerAction action, String storeName, TopicPartition partition) { if (globalStateRestoreListener != null) { try{ action.perform(); } catch (final Exception e) { /// streams exception stuff } } } ``` Other methods delegate to this new method. The guard and error handling are encapsulated in one place. When we eventually get to java 8 we can just do a lambda call
There is similar code forming StreamsException. Consider refactoring to reduce code dup (can be done in another issue).
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
Seems to fit in one line
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
nit: insert space `String... expected`
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
nit: move .collect to new line
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
given that the previous messages say "Reading to ..." maybe it would make sense to say: ```suggestion log.trace("Read to end offset {} for {}", endOffset, topicPartition); ```
unboxing will happen in the comparison in the `if` branch anyways, so probably better to do it early declaring the type `long` here.
This check `records.size() < offset` seems a bit sketchy to me. Basically we are assuming that the input topic `data`'s starting offset is always 0, and there is no "holes" in the topic partitions so that the `offset` indicates the number of records we can read from the input topic. Maybe a more robust way to do that would be 1) read the input topics `data` and optionally `repartition` based on `withRepartitioning`, stop when the current record's offset is equal to or larger than the committed offset, and remember the number of records; 2) read the output topics (again optionally `repartition`) from the beginning to the end (use `seekTo`), and check that the number of records are the same as the number of records read from the input. Then we do not need to truncate, and also in verification we do not need to check list size again since they are already checked here.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
It's better to keep the parameters aligned (having same indentation)
spelling -> recrord -> record
This might be instable in Jenkins.
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
Compiler still gives some warning here, as you are erasing here the java generic type. Adding the `<>` infects the rest of your code with the `? super T` you have to add at the correct places.
Fixed this in my latest update of #4033.
For this to be useful to me in my work on KIP-183 I also need `addWaiter()` to be made `public` on `KafkaFuture`.
I wonder if it is worth refactoring this to remove duplication, i.e, add a Functional interface, then implement it three times to just perform the op on `globalStateRestoreListener` then have a method sth like: ``` performOnGlobalListener(GlobalListenerAction action, String storeName, TopicPartition partition) { if (globalStateRestoreListener != null) { try{ action.perform(); } catch (final Exception e) { /// streams exception stuff } } } ``` Other methods delegate to this new method. The guard and error handling are encapsulated in one place. When we eventually get to java 8 we can just do a lambda call
There is similar code forming StreamsException. Consider refactoring to reduce code dup (can be done in another issue).
nit: "another thread wrote to ..."
We probably have to keep the `size() == 0` behavior for compatibility.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
nit: parameter/line formatting
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
nit: move below the shortcut return below.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
I was looking at the `append` code and it seems a bit brittle. It assumes that: - The collection returned from `PartitionRecords.take` is safe to mutate even though the latter returns `Collections.emptyList()` if `records == null` - That `part.take` will always return at least one element This is fine today, but it may be worth making it a bit more robust to refactorings.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Shouldn't need this line, it's handled by the superclass's constructor.
Actually, it seems more than one figure needs updating. Makes me wonder if we should actually remove them altogether and let people read the code (which can't go stale) instead.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
Sounds legit. Thanks.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
style nit: normally we'd use braces around blocks unless they're a single line
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
nit: add a space before the `:`.
I guess in the end we will find these classes a better place (currently they are a bit scattered, e.g. `KeyValueStoreFacade` is in the materializer class).
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
I wonder if this could be generalised further? Probably a broader discussion, but `KStreamPeek` is really the same as `KStreamMapValues` (there may be others). They just don't share a common interface for the action. I personally feel this would be a better rationalization of classes etc than combining `KStreamForeach` and `KStreamPeek`
Also as stated in the JIRA, it is worth exploring whether it is cleaner to collapse the implementation of `KStreamForeach` and `KStreamPeek` into a single class, with an flag indicating whether to `context().forward(key, value)` after `action.apply`.
Part of the JIRA is to remove KeyValuePrinter class after it was replaced with `ForeachAction`
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
Nit: I know this was inherited from the existing code, but it would be nice to do something like `ByteArrayDeserializer.class.getName()` for serializers and deserializers.
```suggestion public void shouldInstantiateAssignor() { ```
```suggestion protected final List<Future<Void>> futures; ```
The first exception will be the "cause"
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
nit: final on params here and methods below.
`skipBytes` doesn't avoid decompression though, the `readBlock` call below decompresses the buffer: ```java class: KafkaLZ4BlockInputStream public long skip(long n) throws IOException { if (finished) { return 0; } if (available() == 0) { readBlock(); } if (finished) { return 0; } int skipped = (int) Math.min(n, available()); decompressedBuffer.position(decompressedBuffer.position() + skipped); return skipped; } ``` We do avoid GC pressure though.
I pasted the code from `skip` which is called from `skipBytes`. I know we don't call `readFully`.
nit: extra space between `structure` and comma.
Maybe we should say `a partition (which consists of log segments) can grow to...`
I think this should be used in the define method below too.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
Seems this duplicates `L733`. Might be good to extract into a small helper method.
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
Maybe we can just a better name for `path` since it makes this code look suspicious.
Let's keep this as a private method.
I see. I do not have a strong preference actually. But I remember we use singulars not plurals in many other classes and just wanted to be consistent. If it is actually the opposite case I'm happy to have them all to be plural.
I meant to have all "singulars" for consistency, i.e * Creates a * Starts the * Shuts down this * Does a clean up I'm OK with imperative style.
`.. of this instance`.
Hmm, why did we do this? I thought we'd have a try/catch block.
There are a couple of checkstyle failures in these two lines because of missing spaces
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Since we have three threads for this test, there can be multiple rebalances before the streams instance stabilize and start processing...
Couldn't we simply wait for the current state to become `RUNNING`? ```suggestion private void waitForRunning() throws Exception { waitForCondition( () -> kafkaStreams.state() == KafkaStreams.State.RUNNING, DEFAULT_DURATION.toMillis(), () -> String.format("Client did not transit to state %s in %d seconds", expected, DEFAULT_DURATION.toMillis() / 1000) ); } ```
Why do you remove `transform` ? We only add a new `flatTransform` but `transform` is not removed.
Unless @mjsax objects, I vote to just delete these lists. At this point, it almost looks like it's directing you to all the other methods in this interface, which seems redundant. I'm not sure I follow your last question. The list exists to direct readers to other relevant methods. I'm not sure why adding `flatTransform` renders `transform` irrelevant...
Yeah, I'm fine with defining and applying a coherent strategy. In lieu of that, I guess the default thing to do here would be to just add the new method to the list without removing any other items.
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
This is assuming that `totalTopics` is always a multiple of `MAX_BATCH_SIZE. Is that always true? Perhaps it is better not to make that assumption in any case.
@cmccabe Since `flush` blocks until all records are sent, wouldn't it be better to compute the delay time after flush completes? Ideally, an async flush would be better to avoid more delay than required, but that would need another thread. Alternative is not to flush at all, since only the records in the last incomplete batch would be delayed when `linger.ms > 0`.
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
Ok, I was thinking more along the lines that we have just one requestCommit instance while we would have multiple punctuator instances all using it. It looked like a bug, but it seems like it'll be fine.
Yeah, that's what I was thinking
Good catch! I would prefer to remove it then.
@rajinisivaram makes sense. I was thinking if we can break the JAAS config file and made them into client config properties. But just passing JAAS config file will make it easier and extensible.
by no means a blocker (especially given public interfaces relying on this same problem), but i would not be opposed to making internal APIs not rely on these booleans -- clear enums, with properly handled default cases (presumably throwing exceptions) would be better. we tend to let things like this leak into internal (and subsequently external) APIs, but I think it is probably always a mistake -- unclear from the caller standpoint and not extensible.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
this is the kind of assertion that could become flaky given incremental population of `connectors`.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
Also, it might be better to use `synchronized (AbstractCoordinator.this) { }` to mutate both `rejoinReason` and `rejoinNeeded` in order to ensure that they are consistent with each others.
Is ```JoinGroupResponseHandler``` a better place to log error? For example, the error ```UNKNOWN_MEMBER_ID``` is log twice. (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L605)
Your reasoning makes sense to me. From a first read, the PR looks pretty good. I will make a second pass on Monday to ensure that I cover all the cases.
This block can be moved outside of the `try-catch-block`
Why not do this unconditionally? If it's not `clear` we won't commit anyway. It's seems cleaner to avoid to many branches and it's not on the hot code path so the overhead of updating `partitionTime` is not relevant.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
Maybe we can create a JIRA for tracking, but it's not that important for now. Since the common parts that can be consolidated may be not much: in the actual topology building process we need to set the internal topic names, set copartition topics etc which are not needed for the topology description building at all. What I was originally thinking is the the topology building process may be extending from the topology description building process with its additional functionalities like I mentioned above, but I am all hand-wavy on the devil details now.
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
Also, same as above (and elsewhere), mixture of `final` and not `final` locals. They could all be `final` - i don't really care either way, but consistency would be good
nit: move parameter to next line
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
It's a little nice for future reference when we also say when it became deprecated, such as "since 2.6".
`use {@link #toStream()} followed by {@link KStream#to(String)} and {@link StreamsBuilder#table(String)} to read back as a {@code KTable}` ?? same below
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say `assertDescriptions`, that verifies a descriptions map contains the information about a single log dir/topic partition? Something like `assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture)`.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
Can you do something like: ```java static final int tableSizeFor(int cap) { int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1); return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } ```
Or https://github.com/google/guava/blob/master/guava/src/com/google/common/math/IntMath.java#L56-L72 It is safe to look as it is Apache License 2.0.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Right, sorry I misread that line.
we also want to remove this check
nit: "another thread wrote to ..."
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
The order is not really that important here, either way works
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
nit: this can be final
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
nit: `sooner` -> `as soon as possible`
Here I'd suggest doing the opposite: `poll(0)` since it is during the normal processing, not during restoration; so we can afford to not having some time in a few iterations. Instead, we want to proceed to the next iteration to call the normal-consumer.poll sooner to not be kicked out of the group.
`replicaing` -> `replicating`
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Perhaps something like "Represents a pattern that is used by ACLs to match zero or more Resources"
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Can `rebalancing()` throw a `InvalidStateStoreException` ? If yes, we need to split this an apply try-catch-fail pattern instead of using `@expected`
`rebalancing()` should never throw an `InvalidStateStoreException` as it is just constructing the `CompositeReadOnlyKeyValueStore` wrapper. The underlying stores should not be accessed until `get`, `range`, or `all` are called. So, i think this is safe to leave it as it is
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
nit: use "{}.x." vs. string concatenation
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
The verb following 'should' would be in active tense. consider naming this test: userSerdesShouldBeInitialized
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
I think we're testing `testDir` Occupied here, not `AppDir`.
`assertNull`s shouldn't be here but few lines bellow.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
You can use `EnumMap`.
You can use `EnumMap`, which is a lot more efficient.
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
nit: should we merge this into existing test? If not, rename to `shouldChooseNextRecordBasedOnHeadTimestampe`
Just a question: Why is this not `6L` ? (it should be `5L` after you applied the fix you want to do in a follow up PR).
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
The call to `closeGracefully` can result in some responses being returned from the brokers. Is it intentional that we do not invoke the completion handlers? I think this probably makes sense since anything awaiting the responses has probably shutdown by now, but wanted to check.
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
Maybe we can just a better name for `path` since it makes this code look suspicious.
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
may be use Objects.requireNonNull
Don't we have to do something with the futures returned by `send`? How do we know if the requests completed? Also, cc @hachikuji.
The consumer close code needs to block until the fetch sessions are closed. `KafkaConsumer` has `close()` and `close(long timeout, TimeUnit timeUnit)`. In the case of the latter one, we don't want to wait longer than the specified time.
More specifically, there's no point in having this if we close the network client before we even get to the point where the request has reached the network layer.
This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
```suggestion ConfigDef.Type.LIST, ```
```suggestion ConfigDef.Importance.HIGH, ```
Might be overkill if this is the only use case, but we could also add a composite validator.
nit: move first parameter to next line, too
either move `this` to next line, or fix indention.
nit: no need newline of 104 below.
> most recent max timestamp Huh? I think I know what you're trying to say here but it seems like two different phrases got a bit mixed up here
Oh good point, we definitely need the key. But I think separating them turned out well
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
Should have a comma after "for example"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
Yeah I think that makes sense here
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
This name seems backwards.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
Please remove empty lines here and in the other test methods.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
And again with `final` if you don't mind
Make all params `final`
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
This is a breaking change, right? Same for the other `create` method in this class.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
Maybe: Include in the log the Connect key, value, and other details of records that resulted in errors and failures.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Do we need to prefix these with `msg.`? Also, it might be nice to control the message format when message keys and values are to be included. I know that's more code, but it could be made to be similar to the other option.
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
This field can be final as well.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
nit: I don't spot any, but safer to avoid typos by just having constants for these
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
nit: remove the `<` and `>` here and next line. Just to keep it consistent with everywhere else we do this
nit: here and the next line remove the `<` and `>` to be consistent
as above. `requireNotNull` not necessary any longer
this line is still a bit long... You could try a static import for `singletonList`.
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
this is creative :)
as above -- add check for two missing clients
add check for restore-consumer and admitclient
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Do we need to do this `close` and `open` here? We do it also on lines 283 & 286
If not, we should move the exception capturing logic inside the dbAccessor as well.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
nit: add a size? There are a few cases in here where we could do this.
You might consider using `OptionalDouble`.
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: move this `if` statement below/above version check on line 62
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
Since log.error(.. ex) will print the stack trace already, may be we can save re-throwing the exception again. EDIT: if we want to stop the whole process by throwing the exception, we can then save log.error().
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: `child` -> `toChild`
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
would be nice if this was a method in the config
Also, topic can never be `null` if it's coming from a parsed config value that doesn't have `null` as its default value. (another way to think of that is that you can't pass a `null` value from properties)
Nit formatting: ```suggestion return WorkerErrantRecordReporter.createAndSetup(adminProps, producerProps, connConfig, keyConverter, valueConverter, headerConverter); ```
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
createMetadataTopic() is no longer used.
Fair enough, let's keep it inside StreamThread for now. In a longer term refactoring, maybe we could have an StreamsUtil class where such static functions / fields can be stuffed in.
`Not for this PR`: I think a better place for these static methods is StreamsConfig.
nit: put this in the `Tasks` class with the other metadata
req: This is unnecessary
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
I'm fine as well, will make a reference to 10055 of this PR
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
nit: missing space between logPrefix and 'found'
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Why replace `null` with `"null"` ? (similar for value)
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
Oh yeah, duh. Nevermind this 
No, what I was suggesting is to add synchronization to the methods inside `Heartbeat` itself. For example: ``` public synchronized long timeToNextHeartbeat(long now); ```
I think the original code was unintentional. We changed it in #8702 to the following: ```scala int joinGroupTimeoutMs = Math.max(client.defaultRequestTimeoutMs(), rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE); ```
The previous code handles overflow.
I would remove `will`: `This tool reset[s] offsets` same below
`[because] the tool`
Can you please format this differently. No line should be longer than 120 chars and please start a new line after each sentence (otherwise reviewing is quite cumbersome). Thx
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
I think it would be good to include a message giving context before we start listing unresolved issues.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
nit: add `final`
nit: add `final`
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
@ijuma Sorry, I don't know of a standard way of doing this,
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We don't use ReplicaNotAvailableException in listTopics
We don't use LeaderNotAvailableException in listTopics
We don't use UnknownTopicOrPartitionException in listTopics
I think it'd be useful to verify the behavior of casting all of the logical types to strings, just so that we verify the formats (e.g., timestamps, times, and dates should use the ISO 8601 representation) and have some regression tests for the future to help ensure we maintain backward compatibility.
It would be great to have a few more test cases to cover more scenarios: 1. cast to types other than `int32` and `int64` (e.g., `float64` and `float32`), including where we lose precision 2. verify we can cast these to strings 3. verify the cast fails in expected ways 4. casting null values should not fail
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
I know, but you can save it in a static final.
Other enums in Kafka tend to use id for this.
Another reason for having these classes in common (i.e. KAFKA-5265) is that they can potentially be used by the Authorizer interface when we move it to Java.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
@rhauch I don't believe that's the effect the code here has. In the method call: ```java assertEquals( connectorName != null, connectorName != null ? context.startsWith("[" + connectorName) : false ); ``` if `connectorName` is null, then both arguments are guaranteed to evaluate to `false`. I think your intent may have been something like this: ```java assertEquals( connectorName != null, context.startsWith("[" + connectorName) ); ``` which would probably be acceptable, but may also benefit from a `message` that clarifies the expected behavior, possible something like `"Context should begin with connector name if and only if connector is non-null"`
Line too long (also some lines above and further below)
fyi, the pr for non-double stats got merged, so we could switch this back to the original design / fill in the missing string metric
Yes, does not hurt to leave it. Just for sure.
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
Nit: space missing after `for`.
Use diamond (`<>`).
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
nit: move below the shortcut return below.
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
nit: I would rather use the full name instead of using acronyms.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
typo: CompleteableFuture -> CompletableFuture
Let's be consistent and just use string concatenation for both fields.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
Similarly here, this state check could be internalized.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
The intent was to remove the input partitions from the map any time we remove a task from `tasks`. It looks like your code maintains this (in a clearer and cleaner way).
As mentioned above, we can make this constructor default access.
nit: usually we write this like this: ```java this.groupInstanceId = requireNonNull(groupInstanceId, "group.instance.id can't be null"); ```
nit: extra line
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
nit: missing . at end
nit: missing . at end
maybe: `inputKeySerde` and `inputValSerde`
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
nit: I would rather use the full name instead of using acronyms.
typo: CompleteableFuture -> CompletableFuture
Nit: var should be named `deserializeValue`
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Just a note. This may need consideration together with #1707 where the metadata age may subject to change during producer startup.
Should this be `num_lines=3` (cf. L116 and L126)
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Cool, I was kind of hoping you would put this in a separate integration test class
Why set this? zero is the default anyway
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
Effectively what we are doing is sorting the assignments for each partition using the generation and picking the latest two for the current and previous assignments. Is that right? Could we simplify the logic by building a `SortedMap<Integer, String>` for each partition where the key is the generation and the value is the memberID? Then the current assignment would be the last entry and the previous assignment would be the one prior.
Small remark: By throwing here an exception you are giving up. But I am wondering if you should not try to fix it. I do not know which consistency guarantees a kafka consumer group exactly gives. But if it is possible to get into a split brain situation (network split), where temporarily a consumer group is split in two and gets two leaders. You get in a situation where the `SitckyAssignor` will never recover. If it was me, I would write a big fat error, and drop one of the two assignments. The same for line#322.
You can probably simplify this using `computeIfAbsent`.
This class is public API, so we cannot remove `setTimestamp` but can only deprecate it. We also need to update the KIP to mention the deprecation and the newly added methods.
nit. I think there is `.` missing `since 3.0[.] Use`
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
req: `subscription` -> `subscription info`
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
nit: remove newlines
nit: I'd suggest "Ignoring the fetched committed offset"
This was probably unintentionally reduced to debug level while fixing conflicts.
nit: it's a small thing, but the assertion failure message is more useful if we use the `Errors` type. ```java assertEquals(Errors.NONE, Errors.forCode(createTopicsResponseData.topics().find("foo").errorCode())); ```
Perhaps we can use `Uuid.randomUuid`? It's a little weird for all brokers to have the same incarnationId.
Is there a point to setting `min.insync.replicas` in this test? I am wondering why we don't just reuse the original create request.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
You could probably create the `StreamsConfig` in an `@Before`
nit: i don't mind if you use `final` or not in the methods, but it would be good to be consistent. Same in other tests
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
nit: as in `position` below, `this` is not required
nit: as in `position` above, `this` is not required
For a nice example where caps make sense see right below, where two sentences are included.
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
I now saw that in the consumer tests you use `Duration.ofSeconds(1).toMillis()` and `Duration.ofMillis(999).toNanos()`. This makes it already clearer. I think a variable with a meaningful name for the lower bound would make it even clearer.
I'm not sure this is a good idea. If we're unlucky, the partition we're interested in may not be listed. Since this is an exceptional case anyway, I would suggest using the more verbose message.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
There are some reason to move from varargs to Collections? It's breaking a lot of projects.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
No worries, let's keep the scope small for now. Just wanted to raise the question
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
I think this is the issue you reported in the Jira. The `RaftClient.Listener` should not use `RaftClient.leaderAndEpoch` to determine if it is the leader. It should instead use `RaftClient.Listener.handleLeaderChange`. For this state machine `ReplicatedCounter` we should look at the `claimedEpoch` variable. I am going to create an issue to remove this method. cc @hachikuji
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
Not really sure this has value if the test case expects the leader change correctly.
For some test cases we may want to use the same `MockTime` object on both server and client, for example in window store changelog truncation (cc @dguy ). So instead of creating the object internally we may want to pass it through parameters.
We should update the Scala `TestUtils` to call this method.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
I think it was my suggestion to add it. I was going off the pattern for tagged node metrics in Selector.
We can use the `replace` overload that takes a `char`.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Out of curiosity, why do we do this here? In the normal case, we only update the position of the duplicate buffer, it seems.
Ah, yes, the magic is hardcoded here.
However, the ```@RecordBuilderSource(haveInvalidCompress = true)``` is a inscrutable to me :(
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Yeah good catch, see above
Very good catch. Thanks, @abbccdda .
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
> most recent max timestamp Huh? I think I know what you're trying to say here but it seems like two different phrases got a bit mixed up here
Oh good point, we definitely need the key. But I think separating them turned out well
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
How about `return admin.endOffsets(assignment);`
When I suggested it, I thought we could do a bit better, maybe something like `(id: 5, www.example.com:9123)`, but maybe that's actually worse.
I was mostly trying to get rid of the word `Node` because it's a bit redundant when you look at the log messages.
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
nit: move to line above.
req: typo unknown Pid
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
nit: remove -- not used
not used: can be removed
nit: remove (was tested already)
Ditto here, we can use AssertionError
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
There's one extra thing to do. We should set `minTime = MAX` before opening the `store.all()` so it resets the minimum in case there are no records available in the iterator. This is an example I run: I have a few records in the shared state store (1,5,7). Then a new record arrives that expire all the 3 records. Record 50 for instance. For each record, the minTime will be set to 1, then 5, then 7. Now for every new record after 50 that is still part of the window, the condition at the beginning of this method `minTime > maxStreamTime - ...` will be false, thus opening the iterator again and again. If we reset the minTime to MAX, then the next time, the iterator will be opened, but no records will be available, so minTime will stay in MAX. And the future records that do not expire will not open the iterator because `minTime (MAX) >= maxObservedTime - ...`
Why adding both joinAfterMs and joinBeforeMs? The records expire when `window().start() + joinAfterMs + joinGraceMs` are lower than maxStreamTime. For instance, say we have a record in the shared state store with time = 1. Now a new record arrives with time = 17. ``` inputRecordTime = 17 maxObservedTime = 17 minTime = 1 window = 10 (beforeMs = 10, afterMs = 10) grace = 5 ``` Isn't the record 1 suppose to expire and be emitted because 1 + 10 (afterMs) + 5 (grace) = 16? which is lower than maxStreamTime? With the condition you have, the minTime registered is 1, so `(1 >= 17 - 10 - 10 - 5)` is true, and thus it returns and do not emit the record 1 until another 10 ms has passed.
I think we should use the same name here; the metrics-scope would differentiate rocksdb from in-memory, which is sufficient.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
Not for this patch, but we should do a KIP to add support for batch topic creation.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
Checked with Jun and this is fine.
Why do we need this? This is logged anyway.
`setNeedsCommit` -> `{@link #setNeedsCommit}`
`needCommit` -> `needsCommit`
`than` -> `that`
nit: than -> that
nit: add `a {@link Named} config`
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
this test doesn't seem to throw `InterruptedException` as well
@mjsax I think I'm sold on your arguments, let's keep them as WARN then :)
Good point, thanks!
nit: additional new line
How about setting initial size of `records`? `new ArrayList<>(ids.size())`
I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.
In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.
We should check `stream` parameter is not null here.
could move line 368 above this and then declare this as: `String [] parentNames = {this.name, streamImpl.name}`
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
Ideally, we'd always use brackets on control flow operators.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
Since this is basically just a pass-through method anyway, there isn't that much to test here -- you could simplify this test just to use a simple set of connectors you create yourself. There's a lot of code in this test when all you really want to see is that the set makes it back out of the call to ConnectorPluginsResource
It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in `RebalanceSourceConnectorsIntegrationTest` or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Ok. Thanks for clarifying.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
I guess we could pull this into the `partitionsAutoAssigned` block.
We have three `remainingTimeAtLeastZero` functions, in AbstractCoordinator, ConsumerCoordinator and KafkaConsumer. Is it intentional? If not we could leave just one to avoid unintentional code divergence in the future. cc @vvcephei
Thanks for the explanation. A bit subtle as you had said. :)
nit: should be `named` can't be null
nit: add `final`
nit: 4-space indention plus move `builder` down one line
Thanks for the explanation. A bit subtle as you had said. :)
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Would it be more intuitive to put this logic in AdminClientRunnable.makeMetadataCall.handleResponse()? This allows us to reorder calls in `callsToSend` only when the MetadataResponse has no error. And we don't have to check timestamp to know whether the metadata has been updated.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
nit: ".. select the grouping key and the value to be aggregated".
This looks unintentional.
nit: extra line
This only works if the parent dir of generationDir exists. Try using generationDir.mkdirs().
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
It is probably cleaner to have an explicit `EXPIRED` state.
nit: 'else' can be dropped
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
How about `return admin.endOffsets(assignment);`
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
can we move this down such that it is the last line? Again, just making sure that the store has been initialized before we put it into the map.
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
nit: Add `.` at the end of the sentence.
nit: I would move this one next to `consumeInBatches` as they are used together.
nit: As we don't reuse `batch`, we could directly pass `list.subList(batchStartIndex, batchEndIndex)` to `accept`.
Similarly `Consumed#toString` is not implemented either, we can remove this line.
Just for reference: fixed via https://github.com/apache/kafka/pull/5588
Good catch. I think you're right.
We can return `false` on the first mismatch no need to check the rest of the arrays.
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
with in => within
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
nit: extra spaces after the `->`
```suggestion public void shouldDropWindowsOutsideOfRetention() { ```
```suggestion final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore("aggregated", ofMillis(1200L), ofMillis(100L), false); ```
Actually a more general question is that for assign(), is checking subscription.isEmpty() sufficient or not. Today we allow subscribe(empty_list) whose semantics is different from unsubscribe(), but they will leave the same state in subscription map.
I think the semantics of subscribe(empty-list) should be similar to pause(all), but leave the consumer still registered as member of the group with coordinator; for unsubscribe() the consumer means "do not talk to coordinator anymore" and moving forward we may add a leave-group request (there is already a ticket I think). As for now let's keep the original approach to check the assignment upon each commit call; thoughts? @hachikuji @onurkaraman
nit: remove newlines
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Nit: too many blank lines.
It's intentional to avoid build warnings about importing deprecated classes.
Hi, may I ask why do you do `@link` instead of `@see` annotations? :)
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
The additional validation doesn't hurt, but it might be more natural to move this check into the `if` below since we don't actually cast unless the condition is true.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
To get rid of the test failure, you need to change this: ```suggestion final KafkaMetric metric = metrics.metric(new MetricName("prefix-scan-rate", STORE_LEVEL_GROUP, "", tags)); ``` Sorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.
This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.
nit: Please use 4 instead of 8 spaces indentation.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
the changes to the optimizer code LGTM
@ijuma I looked into this in more detail. We don't need a new type `R` -- instead, we should update to ``` public class KTableKTableJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K, Change<V1>, Change<V2>, Change<VR>> ``` and use `private final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal;` (ie, `VR` instead of `R`. (Note, the you always pass `<..., Change<X>, X>` in the current PR, what is redundant and can be avoided.)
as mentioned above: remove this overlaod
I see what you intended. Thanks for the response.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
I think people want to use a global store in DSL, too. And forcing people to call `build()` to add one, is not a good idea IMHO. (cf option (2)). Also, if you argue like this, we could remove `addStore`, too, because people can also add the store via `builder.builder().addStore()` -- however, in KIP-120 discussion, it was explicitly requested to add both methods to `StreamsBuilder` to avoid this pattern. Also note, people who are new and want to use a stateful process() will always ask: how can I add a store? There is not API on `StreamsBuilder`.
For option 2 seems a little awkward IMHO, I second what @mjsax says.
I think in DSL, users may still wants to access a global state store in `process/transform`, that was the motivation for adding this API. Personally I'd vote for option 2) above, to keep the APIs succinct without semi-duplicated calls. But seems you all prefer option 1) in terms of user convenience, so I'm fine with that as well.
same question as above about moving this above the call to `configure`
`HeaderConverter` and this method don't exist prior to AK 1.1.
yeah, that's just about internal code being readable and understandable, not critical to current issue.
Maybe use a semicolon instead: "task failure; 'all' changes..."
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
Java doc for lifecycleListener.
nit: seems unnecessary? similar for `State.FAILED` below.
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
Actually, WDYT about adding this class in the "add configs" PR and then rebasing this PR on top of that? Then I could do the same (since I need this class in my next PR as well)
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
This is an interesting question. One low-fi solution would be to think about using `equals()` (I think to pull this off, we'd need to introduce a requirement that serde/serializer/deserializers implement equals in a way that would be semantically sound for us. This would not be a back-ward compatible change. On the other hand, since callers actually subclass `Serde<T>` with a fixed type like `Serde<String>`, it actually should be available at runtime. I don't remember the hoops you have to jump through to get it right now, but I'll revisit it tomorrow.
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: remove unnecessary `this`.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
nit: add `{ }`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: use static imports to get rid of `Assert.`
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
nit: Could just to `new ArrayList<>();`
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
Yeah might as well change it I think, it results in shorter code
Changing old version encoding would break our upgrade path, ie, all existing `encodeVersionX()` methods cannot be modified.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
nit: should be removed (similar below)
nit: in kafka we usually don't use `get`/`set` prefixes
For a nice example where caps make sense see right below, where two sentences are included.
nit: as in `position` below, `this` is not required
I'd err on the side of not adding configs for now and only add them if we find a need, so this seems fine.
I wonder if you also considered moving startup and shutdown into the `Worker`? The advantage is that we already have an executor there and then we'd get the parallel implementation for `StandaloneHerder` as well, though admittedly that may not matter as much.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
Not sure if it makes sense to convert a `TimeoutException` into a `TaskMigrated` exception... Also note that we created https://issues.apache.org/jira/browse/KAFKA-7932, so it might be best to tackle all Streams related changes there? \cc @vvcephei
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
The fact that we're ignoring it suggests that it's not an `error`. For something that indicates something unexpected has occurred, but that we can handle it and continue correct execution, we should use `warn` level. Also, the type of exception will be captured in the stacktrace, so I would just say something more descriptive like, "Channel closed unexpectedly before lock release."
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L149 can reference to this field
I tweaked this a little before merging.
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
Don't need `Vin` here
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
nit: the 'else' can be omitted.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
Nit: `describeTopics(Collection<String> topicNames, ...)`
not used here (InvalidTopicException is used instead)
We should emphasize that this partitioner should be stateless as it can be shared across topics / sink nodes.
Nit: might be worth adding a simple assertion on the result just to make sure.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
nit: Empty line could be removed.
Yes, this seems fine then.
Shouldn't need this line, it's handled by the superclass's constructor.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
nit: fill in `@return` docs
nit: parameters on a separate line
either move `this` to next line, or fix indention.
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
exception can be improved a bit - "failed to flush within X ms, successfully completed Y/Z batches". wuold help distinguish between slow connection and no connection.
We should spell out that if the string is empty, it's because the remote end didn't send this information
Would `isUnknown` be clearer? I find that boolean methods without any prefix feel a bit ambiguous when reading them.
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
We lack unit test coverage for this case
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
remove "on a window basis"
"top-k records" -> "top-k values"
remove "on a window basis"
We can make this clearer if it helps. Maybe something like FullFetch? Not sure if there is a better option
These should be non-empty checks. Thanks for catching.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
I think we can move this logic into ValueOrOtherValue as another static constructor.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
It's a bit awkward to modify `onJoinPrepareAsyncCommitFuture` inside the `maybeAutoCommitOffsetsAsync` function since the function name itself indicate a general purpose, but specifically for join-prepare --- though I understand today it is indeed only used for that caller. How about letting the `maybeAutoCommitOffsetsAsync` to return the future instead of the boolean, and then let the caller a.k.a. the `onJoinPrepare` today to check if the future is completed or not.
I cannot understand this logic clearly.. my original thought is that, we do not reference the `onJoinPrepareAsyncCommitFuture` here at all, just create a future and return to the caller.
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
Do you mean it should NOT be included...
This message should say "Consumers earlier than 0.10.1.0..." since KIP-74 was part of 0.10.1.0.0.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
The intent was to remove the input partitions from the map any time we remove a task from `tasks`. It looks like your code maintains this (in a clearer and cleaner way).
Nice one on projecting the value!
It is wall clock indeed.
I'd suggest we put the time span as 4 days to be on the safer side, the 7 day log retention may kicks in a bit earlier.
Double.valueOf not required, auto-boxing can be used. Here and in the lines below.
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
nit: add `final`
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This could be more concise: ``` topicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment); ```
recommended; ditto below.
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
`instantiateConfigProviders` since this is potentially creating multiple providers
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
That's a good idea. Note: Kafka does not use this JUnit functionality yet (i.e. no use of ExternalResource, ClassRule, Rule as far as I can tell). @ijuma: Would it ok for us to introduce this? There's no additional dependency etc., it's just using a new JUnit feature that was introduced in 4.7 (we're on 4.12).
Seems that this class is a bit redundant, i.e, we could just construct an `AssignedTasks` with the `logContext` and `"standby task"`
nit: an object -> an object has associated partition offsets that can be ...
do we need this invalid config step here
```suggestion waitForCondition( this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each." ); ```
`final` and initialize in constructor instead? Doesn't seem to depend on the config at all.
This is an internal class -- no need to mark as deprecated.
nit: add `final`
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
Not sure if it makes a big difference, but we could use EnumSet for these.
Can just return `name.startsWith(acl.resourceName())`
nit: need to update this since resource name is before pattern type now
Nit: param alignment.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
nit: `java.util.` can be removed (note, we only specify the whole package for `Topology`, because otherwise we would need to add an `import` statement and get a warning about "unused imports" and a failing build.
It's intentional to avoid build warnings about importing deprecated classes.
recommended; ditto below.
Again, `ConfigProviders` doesn't make sense here. The `config.providers` property is listing the _names_ of the ConfigProviders, whereas the other `config.providers.<providerName>.class` (for example) specifies the name of the `ConfigProvider` implementation class for the named provider. IMO, we should use the term `ConfigProvider` to mean one of two things: 1. The name of the `ConfigProvider` interface 2. The instances of the `ConfigProvider` implementations that are instantiated by this class. Always using "instances" in these cases will help disambiguate the use of `ConfigProvider`. Also, it's probably worthwhile to begin this paragraph with: > The "{@code config.providers}" configuration property and all configuration properties that begin with the "{@code config.providers.}" prefix are reserved. The "{@code config.providers}" configuration property specifies the names of the config providers, and properties that begin with the "{@code config.providers.<providerName>.}" prefix correspond to the properties for that named provider. For example, the "{@code config.providers.<providerName>.class}" property specifies the name of the {@link ConfigProvider} implementation class that should be used for the provider.
We should specify whether the keys in this map have the `config.providers.` prefix, like they would in the `originals`.
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
Thanks for clarifying this.
nit: I would delete these two line
Let's tweak this API to Throwable: ```suggestion StreamsUncaughtExceptionHandler.StreamsUncaughtExceptionHandlerResponse handle(final Throwable exception); ``` Here's a good explanation of why: https://stackoverflow.com/questions/2274102/difference-between-using-throwable-and-exception-in-a-try-catch The benefit is that we could handle `Error`s as well as `Exception`s. However, this comes with the obligation that we should not continue to use the thread after an Error occurs. I think we can deal with this restriction reasonably.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
nit: we could use mkMap helper here as well.
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
You should use try with resources here too.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
This and the other deserialize method have a lot of duplicated code. We could have a method that takes an InputStream with the shared logic.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Ditto on removing these before/after methods.
Ditto on removing before/after
It's intentional to avoid build warnings about importing deprecated classes.
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
please add `final` for parameter and wherever else possible (i.e., also for vars within this method, and all other classes you add in this PR)
Nit: remove `this` (we try to avoid `this` wherever possible)
For transition to `NOT_RUNNING`: the instance will only shutdown if the user uncaught exception handler decides to shutdown the whole instance, by calling `close()`, in this case it will still go through the `PENDING_SHUTDOWN` transition first? For `REBALANCE -> REBALANCE`, this is related to the thread-level `partition revoked -> partition revoked`, which I'm still wondering if we can avoid. Let's sync a bit on that.
Did we mean to swap `REBALANCING` and `RUNNING` around? If people were depending on the `ordinal` then this will break them
Again, could you describe: 1) Running -> Running 2) Partition Revoked -> Partition Revoked 3) Partition Revoked -> Dead 4) Assigning Partitions -> Dead
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
As we mentioned in the ticket, the only test case that requires the underlying `metrics` object is `testMetrics`. For this purpose we should just rewrite this test function, to not reuse the class field `task` which is relying on the `MockStreamsMetrics`, but create another task that does pass in a real `StreamsMetricsImpl`.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
@ijuma Sorry, I don't know of a standard way of doing this,
Need to check if `group` is `null` in both `k1` and `k2`. Using this on, e.g., `DistributedConfig` from Kafka Connect doesn't currently work.
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
```suggestion public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler uncaughtExceptionHandler) { ``` We prefer to resist the urge to abbreviate, especially in the public-facing APIs.
Oh, sure. Now I know why you picked this name :)
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
The typo is still there.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
nit: I don't feel strong about the style here, but maybe we should consider align with other functions which has the first parameter on the same line with function name.
The previous approach was intended to avoid having this logic in many places.
Yeah, it's a bit error prone to have that logic in every constructor. We could move the `propsToMap` method to a utility class and use it on the consumer too.
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Make sense, and that's also what I've seen. Thanks for confirmation!
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
Again, `ConfigProviders` doesn't make sense here. The `config.providers` property is listing the _names_ of the ConfigProviders, whereas the other `config.providers.<providerName>.class` (for example) specifies the name of the `ConfigProvider` implementation class for the named provider. IMO, we should use the term `ConfigProvider` to mean one of two things: 1. The name of the `ConfigProvider` interface 2. The instances of the `ConfigProvider` implementations that are instantiated by this class. Always using "instances" in these cases will help disambiguate the use of `ConfigProvider`. Also, it's probably worthwhile to begin this paragraph with: > The "{@code config.providers}" configuration property and all configuration properties that begin with the "{@code config.providers.}" prefix are reserved. The "{@code config.providers}" configuration property specifies the names of the config providers, and properties that begin with the "{@code config.providers.<providerName>.}" prefix correspond to the properties for that named provider. For example, the "{@code config.providers.<providerName>.class}" property specifies the name of the {@link ConfigProvider} implementation class that should be used for the provider.
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
AK convention is to not use `set` setters or `get` getters.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
Testing against log message is error-prone and hard to maintain, I think just making sure the thrown exception type is expected should be sufficient.
If you decide to still log the condition, we could rehabilitate this test to check for the log.
this doesn't seem to be used
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
nit: use "{}.x." vs. string concatenation
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
as above `final` and one parameter per line
nit: add `final` to parameters / reformat one parameter per line
nit: remove (was tested already)
nit: empty line.
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
The logic here is a bit over-complicated to me, can we simplify to sth like standard sort-merge here? We have e.g. `AbstractMergedSortedCacheStoreIterator` for a similar pattern.
It seems to be added on line 703.
Oh you mean `UnsupportedForMessageFormatException`? That doesn't seem to be added.
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
We could port this function when it is actually needed.
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
May as well add `topicName` to the message so that the user knows which case they've hit? Same for the others.
Discussed offline. This can instead be the producer's retry count. If the retry count is zero, then we will have to allow for at least one metadata request past its max age. So the staleness threshold will be: `retryCount * (backoff + requestTimeout) + maxAge`
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Similar to above we should rename this.
> I ignore 1 out of every 2 punctuations Uh. That's kinda painful... I think we need to discuss this in more details, ie, what semantics we want to provide. \cc @bbejeck @dguy @guozhangwang @miguno
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
nit: why double space? (similar below and further below)
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
@hachikuji @tedyu @ijuma OK, I am going to revert and commit for now. We could improve on it later if we make updates to the code again.
I think the enum approach was better. I'd go with `DISCARD`, `GRACEFUL` and `NOTIFY_ONLY`.
Yeah, no rush.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
I see what you mean, and yea that is a fair point 
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
nit: `equals` => `equal`
We tend to use the steam api for such small transformations but I don't feel strong about this.
Nit: fix line break
`return` is not necessary
Updated this when merging.
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
nit: why double space? (similar below and further below)
validateStoreOpen() can be outside of lock block.
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
There is https://github.com/ben-manes/concurrentlinkedhashmap. Guava and Caffeine (Java 8 required) also have Cache implementations with the same underlying behaviour.
That's a good point.
Maybe say something similar to the PR about this method being commonly used to set up ssl clients even though not public API so we're temporarily keeping it for backwards compatibility as of 2.3 (that will give a sense of timing when people see this code a year or two from now).
Can we call this `toHtml` to go along with the generically named `toRst`? If we want to change the output in the future, we won't need to add a new API.
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
We should also create a client connection with one of the newly disabled protocols like TLSv1.1 and verify that the client connection fails.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
nit: remove `this` here and line below
Nit: ```suggestion * @return list of {@link ConfigValue} instances that describe each client configuration in the request and includes an {@link ConfigValue#errorMessages error} if the configuration is not allowed by the policy; never null ```
@rajinisivaram, hmm, I'd rather us specify the details or link to a config that specifies them. With security, people often struggle so the more information we can provide, the better.
methods => `mechanisms`
getters should not use get. i.e. use `networkDevice` here, etc.
we don't typically use "get" in our getters, right? so this should just be `latencyMs`
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
It's a fair point that `Cluster` is public and we should be careful about what we add there.
Do we lose anything if we use `Set` instead of `Collection`? Seems like set is the right semantics.
Do we really need this method? It seems like we could pass the relevant information in the `update` method. Not yet sure which way is better, but I'd like to consider the option.
add `final` twice
initialize the `KStream` here and make it `final`
This can be initialized here and be `final`
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)
as per previous `assertThat(..., instanceOf(...))` would be better
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
This was the checkstyle error that was failing your build.
More explanatory error, as discussed.
Dropped this unnecessary duplicate code, as we discussed.
We should test settings with the `source.cluster.` prefix too. Same in the test below
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
Don't we need to keep the existing methods for backward compatibility? We could perhaps deprecate them.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
nit: use `logContext
It's useful if the simulation test is deterministic. That way failures are easy to reproduce. Perhaps we can use a shared `Random` instance (between this class and the coordinator) with a defined seed.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
This should probably just return a boolean
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
I think you will need a space between `kafka_principals` and `self.extra_principals`
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
nit: move this `if` statement below/above version check on line 62
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
Could you try to factor out some setup code? For example, in each test you create and initialize the names of the topics, member IDs and the consumer IDs. You could initialize a bunch of topic names and IDs globally and reuse them in the tests. Another example is the creation of `partitionPerTopic`, which is really similar in all tests, only the number of partitions vary. Factoring out setup code, would make the tests more readable IMO.
I do not say, you have to use the setup method for `partitionPerTopic` in all tests. You could implement a method `setupPartitionsPerTopicWithTwoTopics(long numberOfPartitions1, long numberOfPartitions2)` or similar and use it in those tests that need two topics.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
We need to be able to remove just a few specific metrics without disrupting the others, while also making sure to actually close/cleanup during an actual shutdown
I think it should be safe to call remove here multiple times, the important thing is just to make sure it's actually *at least* once
After closer look, I don't think there's a conflict between 429 and how the metrics are currently closed 
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
Nit: remove `this` (we try to avoid `this` wherever possible)
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
`while` seems to be missing
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think `will go` should simply be `go`.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
This was not addressed yet. the whole sentence can be removed
There is no `CoGroupedStream#reduce()` -- we can remove this
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
Note this correction
nit: "User rebalance callback **threw** an error"
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
I don't think we really need this function any more... we can just submit to the executor from the other function.
Incase => In the case
Should we remove references to implementation from the interface? We can keep this doc in the implementation class.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
To keep this logic same as one in `Call::fail` method, lets set the new time as: > nextAllowedTryMs = now + retryBackoffMs
nit: I think it's better to just print the e.message in a single line.
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I had a look at this and your are right. It seems that keeping `TopicPartition` is better and difficult to change. In this case, have you considered pushing the conversion to the `Builder` by providing an overload of `setTargetTimes` which accepts a `Map<TopicPartition, ListOffsetPartition>`? That could make the code in the `Fetcher` a bit cleaner.
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
Sounds like a good idea
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
I think the risk of introducing `options()` is that some developers might accidentally use `values()`. The pattern used in `ConnectorType` is far better, as it overrides the `toString()` method. That doesn't handle the case-independence for parsing, though `ConverterType` is a better pattern to follow if that's required. Let's be consistent with the new enums, and have each follow one of those two patterns depending upon whether parsing case-independently is required.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Nit: move into the `if` block where it is used. (also add `final`)
nit: final int `activeTaskAssignmentLength`.
nit: add `final` (same line below)
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Fine with me to keep the guard. Was just double checking.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
nit: add a blank line before this one to make it easier to read
See above: we shouldn't rely on `previousRightWindow` here. Actually I don't think we need it at all? (assuming we move the check in it to the condition above where we use the combined window agg)
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
I don't think we need all this, if we delay adding the store to `writeToTopology()`
nit: avoid unnecessary `this.` prefix
nit: remove `this` here and line below
Should have thought of this before, but `CollectionUtils.groupDataByTopic` already does this.
Does this really need to be `Serializable`? Same for the other comparators above.
Wow, that's annoying. No harm making the Serializable I guess, but perhaps we can just add the `serialVersionUID` so that we don't need the suppressions.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
Nitpick: Is there a better term than "join-tuples"? Perhaps we should highlight a bit more that that we return tuples of values (but e.g. not of keys).
Nit: we sometimes use "that contains", and sometimes use "containing". Either should be fine, we can just make it consistent. Also: "joined records computed by the given {@link ValueJoiner}, one for each matched record-pairs with the same key and within the joining window intervals." Similar below.
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
Do we really want to always set like this? What's if a user want to provide a custom Long-Serde that just works fine? Maybe we should only overwrite if value serde is `null` ? It's just a thought -- not sure about it.
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
nit: move first parameter to next line, too
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
nit: remove empty line
ditto on removing before/after.
Ditto on removing these before/after methods.
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
We need to call `transactionManager#failIfNotReadyForSend` here, so that we don't try to append to the batch when we are not ready to send. Also, we should remove `failIfNotReadyForSend` from `TransactionManager#maybeAddPartitionToTransaction`
OK, makes sense. I was missing that context.
It would be nice to have information about what the "previous" partition was here in this log message. Also "retrying because of a new batch..." might sound nicer.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
Nit: keep first two parameters in their own lines (we either put all parameters in one line, or use one parameter per line -- hybrid formatting makes it harder to read the code).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
Nit: line too long
Thinking about this, I guess we could improve `StickyTaskAssinger`. If I am not off, load balancing on stream basis is not optimal -- but I am also not sure if the effort to improve it is worth it... If we extend this test to assign more tasks, let's say 12, client `p2` will get 7 tasks assigned and `p1` get 5 tasks assigned (while it would be better to assign 8 tasks to `p2` such that all 3 thread get 4 tasks each). The problem is, that the capacity factors are not considered: `p2` should get twice as many tasks assigned as `p1` -- but the algorithm says only "more" -- and this more is determined be the diff of the capacity (ie. in this case `p2` will get at most 2 more tasks assigned than `p1`. Or maybe my analysis is wrong (I did not run the code and step through it.)
Seems to be covered by `shouldAssignMultipleReplicasOfStandbyTask()` already
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
Line too long (also some lines above and further below)
This seems to be the same program as in the test above -- if yes, I think we should merge both tests into one.
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
Is this name correct? It seems like the buffer is never filled in this test.
It would be good to verify that the buffer contents are correct as well.
We should also mention somewhere that we do not support concurrent transactions.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
i don't think this is what you want here. `broker_ids` is evaluated once. the `wait_until` implies you want to wait until the `broker_ids` appear in the the log files for the broker. but that doesn't seem like the target you want.
Safer to check `is not None`
Does it make sense to set `self.kdc` in the constructor? And then having non-None self.kdc would be part of the logic in `has_sasl_kerberos`
Yes, this seems fine then.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
I wonder if more of this code is generic and should be pushed somewhere else.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
This was changed to`INVALID_PRODUCER_EPOCH` in the kip. Let's change it here too.
The exception can still be `ProducerFencedException`. Just the name of the error code should change.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
+1 -- also note that this sort of only makes sense when using the named topology feature, as otherwise you don't have multiple topologies and not really any reason to set any of these configs differently in the TopologyConfigs vs StreamsConfig passed in to the KafkaStreams constructor. That said, it will definitely happen, so I guess we should check for overrides whether the topology is named or not. Maybe you can use the `isTopologyOverride` method, and remove the check for `namedTopology ~= null`? ie, you can/should use `isTopologyOverride`
Sounds good! There's no rush, but I'll make sure we have your new PRs reviewed and merged quickly whenever they are ready, since you've worked so hard on this already. I'm sorry I wasn't able to make another pass on your original PR, but hopefully this won't be too much of a bother.
Ah, I see the confusion. The `#isTopologyOverride` method checks whether the config has been overridden for the specific topology, ie has been set in the Properties passed in to `StreamsBuilder#build` -- it's not looking at what we call the `globalAppConfigs` which are the actual application configs: ie those passed in to the `KafkaStreams` constructor. So basically there are two sets of configs. The value should be taken as the first of these to be set by the user, in the following order: 1) `statestore.cache.max.bytes` in `topologyOverrides` 2) `cache.max.bytes.buffering` in `topologyOverrides` 3)`statestore.cache.max.bytes` in `globalAppConfigs` 4) `cache.max.bytes.buffering` in `globalAppConfigs` Essentially, using `#getTotalCacheSize` on the `topologyOverrides` if either of them is set (which this PR is doing) and on the `globalAppConfigs` if they are not (which is the regression here). On that note -- we also need to move `##getTotalCacheSize` out of StreamsConfig, because it's a public class and wasn't listed as a public API in the KIP (nor should it be, imo). I recommend creating a new static utility class for things like this, eg `StreamsConfigUtils` in the `org.apache.kafka.streams.internals` package. There are some other methods that would belong there, for example the `StreamThread` methods `#processingMode` and `#eosEnabled` should be moved as well Hope that all makes sense -- and lmk if you don't think you'll have the time to put out a full patch, and I or another Streams dev can help out 
Should this be `num_lines=3` (cf. L116 and L126)
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
```suggestion * A byte array comparator based on lexicographic ordering. ```
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
nit: add `final`
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Map.Entry<String, String> to avoid the check below
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
`instantiateConfigProviders` since this is potentially creating multiple providers
nit: the previous location seemed a little better since it was listed next to `autoCommitIntervalMs`.
Currently our configs are still going to pass-in a single partitioner which is then be used as a singleton list, hence we use `getConfiguredInstance` here.
Is this just temporary until we add better support in the configs for multiple assignors? I'd imagine we need to think through the exact semantics, if ordering matters at all, etc. Is the plan to eventually just switch this to a comma-separated list of class names? One thing I found with Copycat was that the more things that needed to be configured via the same config dictionary, the more problematic Kafka's standard approach to configuration became because you could easily hit cases where there were conflicting settings. Not sure if a) that'll be an issue here or b) if we even want to support assignors that have _that_ much config, but something worth thinking about before committing to this specific approach to specifying assignors.
Sounds good. I think this is convincing :)
```suggestion + " for the following reason: ", ```
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.
req: I assume you do not want to test `handleAssignment()` here, so you should not specify behaviour verification on the mock. You could simply write ``` expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment))) .andStubReturn(singletonList(task00)); ``` `.andStubReturn()` is behavior that is not verified in the `verify()` method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test. Same applies to other similar locations in this test.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
validateStoreOpen() can be outside of lock block.
same question here and below about locking around a `volatile` variable. Is this the only reason to lock here? One would think so based on previous usage.
I don't think locking buys you anything w/r/t to that. The producer#send in the status backing store is asynchronous. So what would describe above can happen anyways, regardless of whether you lock this object. Of course, if it wasn't asynchronous things would be much worse. A bottleneck would be created by the lock, waiting for the `send` to finish in every locked block, so that's not an option. Wdyt? That's what I see at the high level without spending to much time on it, but see if you can check this assumption and we can return to this question.
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
`{@link TimestampedWindowStoreBuilder}` should probably be `{@link TimestampedWindowStore}`.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Nit: capitalization isn't quite right, see `reauthenticationLatencyMs` for example.
Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
nit: `toString` can be used implicitly
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
nit: would be nice to be consistent on the pattern we use here
nit: add a size? There are a few cases in here where we could do this.
nit: maybe worth adding `this == o` to these `equals` implementations. This applies to all of the similar classes in this PR.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
nit: could be useful to log the type of exception in the assertion message.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
nit: remove extra newlines
Compiler still gives some warning here, as you are erasing here the java generic type. Adding the `<>` infects the rest of your code with the `? super T` you have to add at the correct places.
Fixed this in my latest update of #4033.
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Not sure how many mirror maker tests we'll end up having, but would it make sense to have a `MirrorMakerTest` utility like the `KafkaTest` one, or does that end up being too minimal to be worth it (looking back at the `KafkaTest` one now, it looks like it's now just a few lines of code...)
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
This does a system call every time. We can use `InputStream`'s `readAllBytes`: https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/io/InputStream.java#L389
I think this can only catch `IOException`s. We can be more explicit.
Hmm, why did we do this? I thought we'd have a try/catch block.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
Why this and other 4 tests below cannot reuse the `runGenericBenchmark` common function? It seems it is not necessary to use a separate thread for these tests to distinguish them from others that can leverage this common function.
This line (314) could be renamed to `expected Key` as well.
Does this have to be a different variable name than line 131 above? Same below.
You can use the default constructor `ReplicationControlTestContext ctx = new ReplicationControlTestContext();`
nit: `Arrays.asList` a bit more concise.
I can't make out the difference between this test and the previous one. looks identical from the code.
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Nit: remove `this`
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
nit: add `final`
nit: you could just specify one `String` variable at the top of the method and set it accordingly if `topics` is `null` or not then have a single `return` statement. Just a personal preference though.
I've logged https://issues.apache.org/jira/browse/KAFKA-12380 for shutting down the worker's executor. Again, it's not an issue in runtime, but a *potential* issue in our tests.
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
That makes sense. I did not think about the reconfiguration case.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
Map.Entry<String, String> to avoid the check below
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
This should disappear with my suggestion.
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
cc @mjsax as well, LMK WDYT.
fair enough :)
Could we separate this assert by ```assertEquals```? It can produce more meaningful error message.
Could we address this issue in separate PR? It seem to me this issue does not belong to "trivial cleanups" :)
nit: `COGROUPKSTREAM-AGGREGATE -`
The output type is only `KTable<KR, VOut>` is we don't use windowing, right? Otherwise, it should be `KTable<KR, Windowed<VOut>>` ? Do we actually need two different `build()` methods for both cases? Maybe we can clean this up in the follow up PR though. Just something to think about
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
It will result in the same list of versions -- both equally good IMHO.
Nit: space missing before `timestamp_type`.
Actually from `0.10.0` to `0.10.1` requires a full stop, from `0.10.1` to newer version does not. But I think it is still fine to leave it out.
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks @vvcephei -- that is convincing.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
Changed it locally.
It doesn't seem that the client needs principalBuilder.
That's right, changed it locally.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
I don't think we need to mock `generation()` in this test.
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
above (some some more times below)
nit: both lines missing . at end
Nit `.` at the end
store not used
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
Same here. We should use `builder.stream().toTable()`
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
As above, I think we should create both tables using `toTable()` operator
```suggestion log.debug("New average number of connectors per worker rounded down (floor) {} and rounded up (ceil) {}", floorConnectors, ceilConnectors); ```
```suggestion log.debug("New average number of tasks per worker rounded down (floor) {} and rounded up (ceil) {}", floorTasks, ceilTasks); ```
We can use the fact that these are non-negative integers. ```suggestion int ceilTasks = floorTasks + ((totalActiveTasksNum % totalWorkersNum == 0) ? 0 : 1); ```
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
You can click on the below links named "Details" to see the failure message. Also try using `./gradlew checkstyleMain checkstyleTest` to check locally.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
req: no longer used
nit: add `final`
IMO, using reflection should be a last resort. It is pretty horrible and makes the tests harder to comprehend. I'm not a big fan of making methods visible just for testing, either, but I prefer this to having hacky test code using reflection. What would be better is if there was an easy way of testing this without either of the approaches.... That would require a fair amount of refactoring and smaller classes.
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
Can we actually just get rid of "test only" constructors? It couldn't be used by _that_ many different tests...
Maybe something like "No command specified" would be more descriptive
That was what I meant.
No need to mention the version -- it's also not clear how long we keep it -- it's at least up to the next major release -- but might be longer. I would rather point to the new method to be use: `@deprecated use {@link #topicSet()} or {@link #topicPattern()} instead` This explains users how to rewrite their code and is more helpful.
nit: I don't feel strong about the style here, but maybe we should consider align with other functions which has the first parameter on the same line with function name.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
@kkonstantine, thanks for the correction -- the admin properties for the sink worker task (and specifically the DLQ reporter for the sink task) should use a combination of the `producer.*` and `consumer.*` properties that are connector-specific.
This syntax is a bit hard to follow with the conditional at the end. Can you rewrite it to something like: ```python self.jmx_tool = None if jmx_object_names is not None: self.jmx_tool = ... ``` (and also check the attributes as mentioned above)
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
looks good, thanks
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
It's intentional to avoid build warnings about importing deprecated classes.
Make all params `final`
As this does not change, I wonder if we could direct initialize `consumerProps` when it's declared
Would it be better using a separate topic in order to keep a partition without any records? By changing this topic it affects existing checks in all tests
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
Sorry, you are right! My bad!
Add a check to verify, whether the iterator has no more elements.
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
Seems like a no-op
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: use `{}` instead of string concat for `retries`
nit: log.info("Suspended {}", state());
Nit: we don't need this tag before the parameter list.
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
`KeyValueStore` -> `TimestampedKeyValueStore`
Let's use `Map` on the left side instead of `HashMap`
Similar here, we can cache the result in case to be reused.
nit: we can use `map#compute` to replace getOrDefault + put.
```suggestion ``` Do we need both of these debug messages? After all, `worker.assign(...)` is just adding a `ConnectorTaskId` to a collection. How about keeping the first one since this is at this point an on-going process and we've not actually assigned anything to the actual worker node.
```suggestion log.debug("Assigning lost tasks to {} candidate workers: {}", candidateWorkerLoad.size(), candidateWorkerLoad.stream().map(WorkerLoad::worker).collect(Collectors.joining(","))); ```
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
nit: move `logContext` to its own line
if you have an unsigned ~~8-bit~~ 16-bit data source
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: add `a {@link Named} config`
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
A couple other potentially interesting test cases: 1. After starting in a resigned state, verify that the node will become a candidate after the election timer expires. 2. Verify that we can vote for new candidates when in the resigned state after beginning shutdown.
nit: add `final` -- applies to all other vars below too.
Personally I think it makes sense to just disallow calling `ofTimeDifferenceAndGrace(...).grace(...)` entirely, this seems like abusing the API
nit. I think there is `.` missing `since 3.0[.] Use`
`of` -> `or`
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
nit: Indentation of those lines seems to be off here.
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Nitpick: I'd call this `deserialize`.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
I know this is a bit picky, but we (mostly @mjsax and i) are trying to encourage everyone to use `final` where possible
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
`Note when the windowed serde class is used, one needs...`
please fix this: `use {@link } instead`
You can also remove the `@SuppressWarnings("deprecation")` at the top of the method.
I think that the driving condition of the test is the number of samples, not the time. But I thought I'd point out that time won't advance by default, but you can make it by using the `autoTickMs` constructor.
Do we need to wrap with the LinkedHashMap? Could we just do `Collections.unmodifiableMap(metrics.metrics());`
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
nit: remove empty line
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
We probably should name this sth like removeFromIsrAndMaybeChooseLeader.
Currently, the follower never removes the leader out of ISR. So, perhaps we should just throw an exception if this is not the case.
key -> topic
It would be nice if you made these `final` while you are doing this change.
add line: `TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();` and change to `RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
Can just return `name.startsWith(acl.resourceName())`
Not sure if it makes a big difference, but we could use EnumSet for these.
nit: need to update this since resource name is before pattern type now
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
Still not used
`tp` is not used anymore.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
nit: newline for better IDE debugging.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Very good catch. Thanks, @abbccdda .
Actually on a second thought.. if users configures `-1` it means they probably do not care about enforced processing, while on the other side the INFO entry may flood the logs here. So NVM.
Should we log INFO if we are indeed enforcing processing? I.e. there are some empty partitions.
It doesn't seem like we need this variable. Why not assign the result of `createFetchRequests` and call `size` on it? Less mutable variables that need to be tracked.
NPE: ![image](https://user-images.githubusercontent.com/925755/55269396-d55f3480-5292-11e9-9c29-78c524d63c65.png) I'm not using a topic pattern, equality should still work.
Schema? It's Field class...
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
nit: add `final`
nit: add `final`
nit: `final` params
as above mentioned, the `listStore.all()` is not closed here.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
@SupermanScott This change looks ok, but this is failing checkstyle: ``` :connect:runtime:checkstyleMain [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:280:1: File contains tab characters (this is the first instance). [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:282:21: '}' should be on the same line. [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:293:21: '}' should be on the same line. :connect:runtime:checkstyleMain FAILED ``` Looks like simple indentation cleanup. You can easily check that these are fixed by running `./gradlew :connect:runtime:checkstyleMain`.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
It should always be called from the same thread, though we've taken a simple approach to locking in this class where everything is protected as far as I can tell. This does technically introduce a behavioral change though, since `heartbeatThread` is being reset to `null` whereas it was not before, but this doesn't seem like a problem.
Since we acquire the same lock in `heartbeatThread.close()`, maybe we should just call `hb.close` within the lock to make the locking clearer. It's also consistent with `startHeartbeatThreadIfNeeded` and `disableHeartbeatThread`. We just leave the `join` outside the lock.
you are right :P
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
This is not a suggestion: we are sending `null` with the guarantee that we should have never forward for this key before. I think a good test case coverage would be to have a windowed aggregation emit final, followed by a join. The join results would need both the old/new values to be able to correct, and if emit final we should only emit once, with old value setting to null.
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.
typo: `consume` -> `restore`.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
I'd suggest we do not print the stack trace in either case; instead, we can just print the exception's `name()`, which is sufficient to tell it is the locking issue.
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
The order is not really that important here, either way works
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
Here you should just need a queue as for `clientLevelMetrics`. We need a map for the other levels because there can be multiple objects for each level, e.g., there might be multiple stream thread and each one manages its sensors under a key in the map. However, there is only one client on client level.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
nit: 4-space indention plus move `builder` down one line
No need to check null. We always have to forward oldAgg and newAgg, anyway.
add `final` twice
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
I think it might be ok for `windowSize` to be greater than `segmentInterval` as the window will comprise multiple segments, but I could be wrong about this. \cc @guozhangwang @mjsax
Is there a reason why we are using `no smaller` instead of `greater than or equal to`? I generally prefer to avoid logical negations in messages.
I would move line 328 and 329 to before this line.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
nit: "active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
We really need a bug fix release for this! \cc @guozhangwang
It might be nice to keep the tasks/topologies that have failed in another list entirely. Then when reprocessing after an exception we can run all the good tasks first and commit them before running the failures. This will be important for EOS as we con't commit only part of a transaction. The larger part of that doesn't need to be done it this PR but keeping the groups separate would be nice in my mind
It would be much simpler but unfortunately its not as simple as we first thought. The producer has only one transaction, so the records of the good tasks are mixed in with the records of the failed task and there is no way to separate them. So we need to take the tasks that we know will fail and process all the other tasks without them. That way we continue making progress. We can take the failing tasks and backoff and retry again later.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
Maybe mention the advantage of using this over `Thread.sleep`.
What is the reasoning for not throwing an exception if the condition is not met after the timeout? That would make the tests more concise.
Consider adding some small back-off time (say 100 millis) to avoid busy looping and possible log polluting.
ah, i missed that it was in the Validator and not ConfigDef itself.
same question about override here
Should probably add this as the first bit in this method: ```suggestion String strValue = (String) value; if (value == null || strValue.trim().isEmpty()) { return; } ```
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Yeah if it exists elsewhere let's just leave it as is for now.
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
I think we might want to consider dropping some of these `log.debug`s to `log.trace`. Some of the logs in error conditions make sense at `debug`, but logging every fetch request and response at `debug` might make changing from `info` to `debug` a bit overwhelming.
Should it be "...consumption...is coupled"? Currently it is "...consumption...are coupled".
`deteremined` => `determined`
I think `will go` should simply be `go`.
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
nit: `final` params
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular. So, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though 
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
spelling -> recrord -> record
The config name was chosen for consistency with the producer, but it is not an ideal name when considering the consumer in isolation. The consumer can block longer than this if a longer timeout is passed to any blocking API that accepts a timeout. Can we explain that this config is only used as the default timeout for operations which do not have an explicit timeout option? Similarly for the changes in the upgrade docs.
Could use the parameter variable here. METADATA_FETCH_TIMEOUT_CONFIG
The logic for a valid topic name is actually a little more complicated than this. The server side validation exists in `kafka.common.Topic.scala`. We also shouldn't duplicate the logic. So if we move it to the clients package, removing the Scala version and replacing its usages would be good too. Also #898 (KAFKA-3219) may change this logic a little bit too.
One issue is that the exception thrown by this method is `InvalidTopicException`. In this context, `IllegalArgumentException` seems like the right exception to throw to be consistent with the other things we check. Maybe we can catch the exception and rethrow it. Also, it would be good to mention it in a @throws clause.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
It seems you can move this line after line422.
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
nit: 'else' can be dropped
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Seems to fit in one line
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
typo: `per reach record`
as above nit: double space `to Kafka`
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
It seems like your indentation is set to 8 spaces instead of 4.
Can remove the `W extends Window` generic
We can remove the `<W extends Window>` now, right? Also it looks like the `initializer` input isn't needed anymore either
Yeah if it exists elsewhere let's just leave it as is for now.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
doesn't look a great name for its behavior. perhaps something like currentContext
nit: as in `position` below, `this` is not required
I see that you are actually printing the sink nodes. I'm wondering if this if condition is necessary since in line 85 this function will be skipped anyway.
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
For consistency: {@link KafkaStreams} instance
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
nit: We should use `groupId.idValue` here and in the others.
Yes, we can open a JIRA to do it later.
We can use `ApiResult.completed()`
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
the `null` is redundant.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Thinking about this, I am wondering if we should just change the FSM to allow this transition and simplify the code here? \cc @guozhangwang
Nit: the method name an logic does not align 100%
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
This class is surprisingly similar to org.apache.zookeeper.Login, have we copied from the same source? ;-)
Nit: unnecessary new line.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
4th overload makes sense to me as well. I like the idea of placing the serdes in `Consumed` into `Materialized`, but I'm trying to think would there ever be a case where they need to be different? I can't ATM, so I it's a yes for me.
We can call the static function of KStreamImpl directly and get rid of the additional function in `InternalStreamsBuilder`.
method name changes
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
AK convention is to not use `set` setters or `get` getters.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
@parafiend Looking at this again, adding to `failedSends` in `poll` can result in multiple disconnect notiifcations for a channel (we have to guarantee exactly one). `failedSends` are processed on the following `poll()`. But for the write exception here, we process `close()` in the catch block and the channel is added to `disconnected` list. In the next poll, when `failedSends` are processed, the channel will be added again to `disconnected` list. It would be better to set a flag rather than update `failedSends` and change the close in the catch block to: ``` close(channel, !sendFailed); ``` This will close the channel in the current poll without waiting to process any outstanding requests.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
@spena just ping to make sure you get this on the follow-up PR.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
Your words sound good to me, but they are directly in contradiction to the code here, which skips suspending and committing active tasks because it assumes they have already happened. In other words, there is an assumption here that the active tasks are in some kind of "committed" state, while the standbys are in either "created" or "running". If we're going to have a branch that explicitly assumes the task is already committed, then I'd like to verify it, otherwise experience says it will become false after refactoring, and we'd wind up trying to track down an IllegalStateException later on. On the other hand, if these transitions are idempotent, we can just include them in both branches (or rather move them outside the conditional block).
Hi all, thanks for the discussion. To start at the beginning, yes, I was advocating for throwing an IllegalStateException at the earliest possible moment when we can detect the illegal state. Right here in the code, we are making an invalid assumption. Namely, that if a task to be recycled is active, then it has already been suspended and committed, and if it is a standby, then it still needs to be suspended and committed. Why should this be true? Because some other code deep inside another set of nested conditionals thirty lines above looks like it does that right now? Experience says that this situation will not survive refactoring. We cannot test every branch, so we need to make assertions about the state being valid when it's in doubt. We could make an argument that if this assumption becomes incorrect, than we'll throw an exception later on before any state becomes corrupted, which would be good. We could make a stronger argument that the exception we throw later on will be perfectly crystal clear about the cause and therefore we won't spend weeks poking around a flaky test or a user bug report trying to figure out what happened. But both of those arguments would depend on even further assumptions about stuff that may or may not happen elsewhere in the code base. The best thing to do at all times is validate potentially dangerous assumptions. This looks very much like a potentially dangerous assumption. I'm asking that we validate it.
Much better name :)
These methods look like they are identical to those in the previous test class above
Definitely don't add an abstract class! Let's leave it as is for now, then.
initialize the `KStream` here and make it `final`
git: The sentence "So setting the strategy ... matching the given strategy name" reads a bit confusing here. I think we only need to state that when the change of the policy would take effects (the next time when compaction is triggered by the log cleaner thread), and emphasize that for "timestamp" and "header" we would always still retain the last record.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
remove var -- only used once.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
For compatibility, the mbean needs to be `kafka.controller:type=KafkaController,name=GlobalTopicCount`. That means you need to pass "KafkaController" here instead of "ReplicationControlManager".
I think it would be preferrable to call this "globalTopicCount" and call the gauge "globalTopicCountGauge" or something similar.
I think it would be preferrable to call this "globalPartitionCount" and call the gauge "globalPartitionCountGauge" or something similar.
@granthenke Heh, I didn't even mean for the actual credit. But sometimes it's useful in tracking down where an issue was first introduced, the reasoning for the way a particular chunk of code is written, etc. It's an inconvenience, not a serious problem, especially for a set of changes this size.
@granthenke Ahh, great point! I guess that is both nice to enforce and also annoying in this particular case.... Seems like we did some significant reformatting under the initial checkstyle addition as well, which unfortunately messes up git-blame, but I guess it's a one-time cost.
Could we use ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG etc instead of hand-coded strings? It is less error-prone for possible future changes.
Nit: fix line break
Would it make sense to set `position` explicitly to null if the `FetchState` does not expect to have it. For example, it seems currently when we reset the offset, we leave `position` at whatever value it had previously. If we were initializing, then it would be null. If we had an offset out of range, it would be non-null. It might be easier to reason about the logic if it is always null in the AWAIT_RESET state.
Nit: remove `this`
does always apply -> always applies
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
an -> a
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
method should be static
Maybe consider: ```suggestion final CountDownLatch latch = new CountDownLatch(1); kafkaStreams.setStateListener((newState, oldState) -> { if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) { latch.countDown(); } }); kafkaStreams.start(); try { latch.await(IntegrationTestUtils.DEFAULT_TIMEOUT, TimeUnit.MILLISECONDS); } catch (final InterruptedException e) { throw new RuntimeException(e); } ``` Then, this method won't return until Streams is actually started, which we've seen can increase test stability.
Ah, yeah, there are some limitations to the linter. Thanks for taking care of it.
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
rewrite test as above using `assertThrows()`.
nit: `final` params
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
How about using lambda? `(groupInstanceId.map(s -> ", groupInstanceId=" + s).orElse(""))`
nit: usually we write this like this: ```java this.groupInstanceId = requireNonNull(groupInstanceId, "group.instance.id can't be null"); ```
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
What's the benefits of using a callback here than calling `openIterators` directly? I think adding to `openIterators` outside of the constructor makes sense, but cannot think of the rationale of doing this upon closure.
If the object is created with the no-argument constructor, this will throw an NPE.
Good point, thanks!
Thanks for splitting out https://github.com/apache/kafka/pull/7076 @ableegoldman Please review the new PR, too, @pkleindl
Good point, thanks!
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
nit: line too long
I didn't think of that before, but now that you mention it, the change makes sense to me.
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
What is the reason for having `assertDoesNotThrow` here and below? The test will fail if an exception is thrown, so seems like unnecessary noise.
rewrite test as above using `assertThrows()`.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Ah, now I got it! Sorry! Makes sense! In that case we can reuse `REPLACE_THREAD` also for the global stream thread. Forgot about that!
I actually would be in favor of calling the enum value `REPLACE_STREAM_THREAD`. A stream thread is a stream thread and a global stream thread is a global stream thread. I am aware that the KIP calls the enum value differently, but we also have a config that is called 'NUM_STREAM_THREADS_CONFIG' and we have also 'addStreamThread()' and `removeStreamThread()`. So I guess, the name to the outside of this is stream thread and not thread. We have also other threads in Kafka Streams like the state directory cleaner thread and the RocksDB metrics recording thread.
nit: "can not" -> "cannot", same below
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
It was removed from the other versions of `group` but not from here.
Adding an extra "if" statement on every call to `read()` seems like a very heavy price to pay for this feature. We should look at where we're using the total length and check there, I think.
I think the right time to check this would be when we originally create the NetworkReceive object, not in the hot path for every read. Also, whatever we choose as the "upper limit" for a size to be interpreted as valid needs to become the new upper limit for `message.max.bytes` (which in turn, means that maybe this needs a KIP, since it changes a public config)
Are you referring to my fix? or my problem? I just would like my problem solved :) I'm not concerned about my fix - I was just mainly trying to demonstrate the problem.
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
Sorry for the dumb question, but I am curious if ``` if (partitionsPerTopic == null) return that.partitionsPerTopic == null; ``` Wouldn't be ``` if ((partitionsPerTopic == null && that.partitionsPerTopic != null) || (partitionsPerTopic != null && that.partitionsPerTopic == null)) return true; ```
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
Not sure if we want to provide information on a topic on which user does not have permissions.
don't think we need to log here . If a topic is not matching to the pattern than why bother logging here.
This condition should probably be like a few lines below: ``` java if (subscriptions.getSubscribedPattern().matcher(topic).matches() && !(excludeInternalTopics && TopicConstants.INTERNAL_TOPICS.contains(topic))) ``` So we would want to extract that condition to a helper method perhaps.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
nit: blank line missing here
nit: fits in one line
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
This computation of `broker_ids` can be simpler with a set comprehension like `{node for node in self.nodes if self.is_registered(node)}`
normally in the tests we use `wait_until` instead of manually doing retries, and it generally makes the entire thing a one-liner. it's also generally better since you can just set time-based termination conditions instead of coupling # of retries with the timeout.
this looks cleaner as `"[" + ", ".join(self.idx(node) for node in self.nodes)` + "]"
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
nit: we could set a final int for numRetries as: ``` put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), numRetries); ``` and use (numRetries + 1) here to clearly indicate we are trying to go beyond the retry limit.
Even though the config is invalid, the passwords may be valid, so it seems safer not to include them. It would be nice if the sensitive entries in the JAAS config would be communicated in some way to improve debuggability (in the future). Btw, a nit: we seem to use inconsistent capitalisation of the word JAAS in our messages. It would be nice to make that consistent.
Nit: unnecessary new line.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
You are right. Never mind.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
@satishd I re-read your code and I see that you are only de-allocating on exceptions instead of using a finally like in my suggestion. They both work (in my suggestion, we null the outer variable once we have used it to prevent it from being freed). One difference is that in your approach, we could end up calling `free.deallocate` twice if the first invocation throws an exception (which should not happen normally).
nit: extra line
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
same question around log level as above
same question around log level as above
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
nit: move to line above.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
nit: add `final
nit: use `IntegrationTestUtils#safeUniqueTestName` for the name
Nice coverage with different num.partitions, thanks!
Same here: we should leave this test here until we remove the deprecated API. (and just suppress the warnings for only this test method)
```suggestion assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD)).gracePeriodMs()); assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD + 1L)).gracePeriodMs()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
nit: add `final`
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Overall, confusing indentions and line breaks...
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
Harsha address this, I believe.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
Do we need to set OP_READ? It seems it's always on.
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
req: The names of this method and the previous method should be switched.
This is needed if you want to persist the limit across a reboot. But we are not rebooting here. Get rid of this.
These two statements don't have the same semantics. Previously we would block until the first line of output, which here probably also guarantees metrics are being reported via JMX. From what I can tell here, it looks like you could easily be running `start_jmx_tool` before the metrics are ready. I don't think the previous version is great, but I think it at least provides the guarantee since we're piping logs elsewhere -- one line of output should indicate consumption has started.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
nit: unnecessary newline
This causes a checkstyle failure
Yeah, we're still feeling out the best patterns for handling older versions.
I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular. So, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though 
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Could we rename this to something like "remainingPartitions"
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
Hmm.. Do we know how this is getting propagated in all cases? Some of the responses are handled using a `RequestCompletionHandler`, but NetworkClient currently eats any exception raised from this callback.
Thanks for the clarification @hachikuji
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
nit: additional new line
I think you should create an interface rather than passing in the `AssignedTasks` class, it seems you only need a single method. The contract should be on an interface rather than the class.
Instead of passing the whole AssignedTasks object and hence introduce this penetrated dependency, could we instead throw the MigratedException as is, and only add the task information in `updateNewAndRestoringTasks` and re-throw? Then the exception's message function would depend on whether or not the task field is null.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
I think we should have this in this PR already.
Might be overkill if this is the only use case, but we could also add a composite validator.
As above: need to keep default value.
Got it. Thanks. The patch LGTM.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
`timestamp` missing (three times)
nit: missing comma `headers[,]`
`timestamp` missing (twice)
```suggestion import java.util.Collection; ```
```suggestion * Options for {@link Admin#electLeaders(ElectionType, Set, ElectLeadersOptions)}. ```
I don't think we want star imports.
nit: add a newline here too.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
just `name` should be fine
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
Was the intention here to avoid the deprecation warning? If so, you can just call this method `name()` and do this: ```java @Override @SuppressWarnings("deprecation") // TODO remove this when {@link Joined#name} is removed public String name() { return name; } ``` Callers won't see the deprecation warning as long as they access the method via a `JoinedInternal` and not a `Joined`.
It should be something like "Sent by an (admin) client to get data about a specific consumers group like main information about members in such group."
It should be something like "Sent by an (admin) client to get data about consumers groups managed by a broker. To get a list of all consumers groups in the cluster, it needs to be sent to all brokers."
I think similar to produce and fetch, LIST_OFFSETS should be sent to the leader of the partitions
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
deliveryTimeoutMs should be mentioned
isFull is no longer used.
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
req: drop the `!caughtUpClients.isEmpty()` check here, if it's in the map it should have at least 1 caught-up client
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
req: we'll never hit this, as `taskToCaughtUpClients` only contains tasks _with_ caught-up clients IIUC. Can we just construct `unassignedTasksWithoutCaughtUpClients` as the set `totalTasks - taskToCaughtUpClients.keySet`? We can do that in `assignTasksWithoutCaughtUpClients` and remove `unassignedTasksWithoutCaughtUpClients` from `assignTasksWithCaughtUpClients` entirely
This might fix the issue, but don't you think it's a little weird that it's necessary? Wouldn't we have the same problem anywhere that we call `stop`? I'm wondering if we need to fix this in ducktape itself.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Delete this block - this was a specific check for ensuring log output in the test_console_consumer
Uggh, yeah, I forgot about this. We kind of inherit some annoying types from Kafka's config setup but I tried to ensure we're using String types where possible. It gets a bit hard to figure out what is valid where -- the JsonConverter actually gets passed a `Map<String, Object>` as that's what is returned by `AbstractConfig.originalsWithPrefix`, but in practice they are all `String` so type erasure allows this to work...
We are setting the default in the config already, we should not duplicate it here IMO. If the config is missing, we can probably throw an IllegalArgumentException.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
Cool, yeah that addresses my concern 
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
There is a built-in for this `Function.identity()`
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
nit: I would rather use the full name instead of using acronyms.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
typo: CompleteableFuture -> CompletableFuture
one liner? partitionsInFlight = sendInOrder ? new HashSet : null;
Just a note. This may need consideration together with #1707 where the metadata age may subject to change during producer startup.
Discussed offline with Becket - rework this patch to avoid the null checks elsewhere; i.e., make the accumulator more explicitly aware of the `sendInOrder` requirement
Nice catch. Reminds me though, why the second rebalance may not be deterministic in migrating tasks back? I thought our algorithm should produce deterministic results? cc @ableegoldman
Yes, I think so
@ableegoldman is it related to the UUID randomness? If yes please ignore my other question above.
Would it be slightly simpler to use `private long nextAllowedRetryMs = 0`? In general `long` seems simpler than `Long`
nit: Normally for getters we have the convention of dropping the `get` from the method name.
Checkstyle failure: ``` Name 'JITTER_MAX' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName] ```
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Same here. We should use `builder.stream().toTable()`
As above, I think we should create both tables using `toTable()` operator
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
aren't these just the defaults? if so, they can be omitted.
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
I think we can use a utility method provided by the `ConfigDef` class here: ```suggestion List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST); if (topics.contains(dlqTopic)) { ```
Should we log the topic name for this exception? For example, ```has a topic name (xxx) which```
```suggestion Arrays.setAll(topics, i -> topics[i].trim()); ```
nit: "can not" -> "cannot", same below
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
Nit: or empty **if** this worker ....
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
I think we can avoid this alignment style. It leaves us with significantly less space to write lambdas, etc. (another indicator is that this style is not applied elsewhere in the file). Two tab stops in the line below should be fine, even if the declaration above is where it is now.
Similar to the mute in `poll()` - the mute could be delayed until a buffer needs to be allocated? It is possible that the channel already has a buffer allocated, in which case, we want it to complete read.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states: 1. Handshake 2. Authentication 3. Waiting to receive a message (receive == null) 4. Received partial message size (receive != null, buffer == null) 5. Received size and partial message body (receive != null, buffer != null) 6. Muted after receiving size due to OOM 7. Explicitly muted 8. Disconnect The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState` is unnecessary if muting is done on allocation failure and that makes the code simpler.
Both of these cases are still testing the same thing. I think you are intending to set an invalid record count, but this is actually changing the size of the batch (i.e. the size in bytes). So whether it is 2 or 10, we're validating the same path.
as above -- guess some more below
As above: use `assertThrows` and verify error message
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
These can be final.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
`assertNull`s shouldn't be here but few lines bellow.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
This is the same code as in `KTableFilter` -- we should refactor and share code.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
Is this necessary? The leader epoch is -1 by default.
This was a separate bug, I guess? Might be worth mentioning in the PR description.
I'm ok saving this for #7409.
nit: move `windows` to next line
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
nit: 4-space indention plus move `builder` down one line
I see what you mean, and yea that is a fair point 
```suggestion // Emulate losing leadership in the middle of a non-atomic append by not writing ```
Thanks for cleaning up the code duplication.
Sounds legit. Thanks.
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
prop: You can remove this line. You only need `expectLastCall()` if you need to do some further expectation settings on a call that returns `void`, e.g., `expectLastCall().times(3)`
Could we fail the test right here? It doesn't seem like there is much benefit to returning the missing metrics from the method. That would let us simplify this a little. Instead of this: ```java Set<String> missingMetrics = getMissingMetricNames(expectedMetricNames, expectedGroup, expectedType); assertEquals(Collections.emptySet(), missingMetrics, "Expected metrics did not exist"); ``` we could have: ```java assertRegisteredMetrics(expectedMetricNames, expectedGroup, expectedType); ``` We could probably also drop `expectedGroup` since we only have `kafka.controller`.
nit: could use Utils.mkSet
nit: When this assert failed, we'll see the error messages: `Expected metrics did not exist` ==> expected: `emptySet`, but was: `missingMetrics` I think we should change the error messages, ex: `Expect no missing metrics` ==> expected: `emptySet`, but was: `missingMetrics`
Think you might have forgotten to remove some debugging here
Okay, sounds fine.
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
nit: ```suggestion private static final String storeName = "InMemorySessionStore"; ```
missing coverage on: * expired segments * retention time * fetchSession, which doesn't find a session * fetch
This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.
Should we guard against NPE? (same blow)
nit: add `final`
This could throw a NPE. I think we should guard against this, and throw `ParseException` if NPE happens `ts.split("T")` is redundant and should be extracted into a variable.
If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all.
The map is not used.
typo in the test name.
```suggestion * @param timeoutDuration timeout duration; must not be null ```
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
And why is this test deprecated as well? More generally it seems the `context#getStateStore` function was being deprecated but it was not explained in the KIP wiki.
This is not introduced by this PR: the name has a typo: through => throw
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
If we're just testing broker compatibility I don't think we even need this part of the test.
Nit: space missing before `timestamp_type`.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: it is naming a source node, not a processor node. -> `"source"`
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
why do we make lines longer? harder to read now
nit: empty line.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
nit: add `a {@link Named} config`
the method name changed to `windowedTable` and `windowSize` parameter is missing
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
Yeah, `FetchResponse` will most likely be the last one to convert because we'll have to figure out how zero-copy will work.
I hope we can get rid of those conversion in the future :)
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
req: This is unnecessary
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
this will never be called if one of the assertions fails
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
```suggestion // Most of the records will be an internal sink record, but the task could potentially // report modified or new records, so handle both cases if (record instanceof InternalSinkRecord) { ```
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
I'm wondering if we also need to delay the call to `client.wakeup()`? For example, consider the following sequence: 1. disableWakeups() 2. wakeup() 3. poll(0) 4. enableWakeup() 5. poll(Long.MAX_VALUE) The underlying wakeup on Selector would effect the `poll(0)`, but we would suppress the exception. Then there would be nothing to wake us up from the `poll(Long.MAX_VALUE)`.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Yes, the sensors are created in `Sender/Fetcher` to avoid knowledge of the different names to the network layer. Recording is done in the network layer since `Sender/Fetcher` don't see all responses (now that any response may be throttled) and to use common logic.
git: The sentence "So setting the strategy ... matching the given strategy name" reads a bit confusing here. I think we only need to state that when the change of the policy would take effects (the next time when compaction is triggered by the log cleaner thread), and emphasize that for "timestamp" and "header" we would always still retain the last record.
I tweaked this a little before merging.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
We should check that `inputRecords.hasNext` is false after the for loop, or any manners to make sure that pairing lists have the same size.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
Let's be consistent and just use string concatenation for both fields.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
This needs to be `totalAbortedThreads`
What do we want to achieve with this throttle? Do we just want to backoff for `THROTTLE_PERIOD_MS` whenever we can't find a connection you sent? I think we should simply use a `Thread.sleep` call. To be concrete, I recommend we instantiate a `org.apache.kafka.common.utils.SystemTime` class and use both its `sleep()` to sleep and `milliseconds()` to get the current time
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
Thinking a bit more. This is a bit tricky since we probably can't just continue here. For channels like SSL, we need to do the handshake after the socket is connected. Currently, the handshake will be triggered immediately after the connection is established through channel.prepare() and this has to be done in the same poll(). Otherwise, the selector may not be able to select the key again for initiating the SSL handshake. This applies to all those immediately connected keys not coming from the selector. So, to get around this issue. We can probably create a new KeyIterator that iterates both keys in this.nioSelector.selectedKeys() and those in connectableChannels. The iterator can return a <key, alreadyConnected> tuple. For keys coming from selectedKeys(), alreadyConnected will be false. For keys from connectableChannels, alreadyConnected will be true. Then, we just need to change line 291 to check if (key.isConnectable()) || alreadyConnected) and leave the rest of the code as it is. This way, keys in connectableChannels will be handled in the same as way as those from selectedKeys().
Maybe we should remove the `Collections.synchronizedList` wrapper(s).
Actually maybe we should wrap it in an `if hasPersistentStores` so that users won't get this warning if they don't have any persistent state
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
Same here, this exception message does not apply to the case this is trying to catch
How about: ```suggestion "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results.", ```
It might be simpler to just use `int transactionTimeout` -- Java will auto-cast to long in ``` if (transactionTimeout < commitInterval) { ```
That makes sense -- a `long -> int` cast is safe, but not the other way around.
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
Nit: let's not add an unnecessary extra line.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
We could probably just bump up the number of parameter limit in the checkstyle file to 14.
@hachikuji asked you to change the name originally: ```text hachikuji 5 days ago Contributor nit: the name is a little awkward. How about hasRemaining to match ByteBuffer? ``` :)
Sorry for the confusion. I thought `hasRemaining` made sense initially, but then I realized that the name should be more suggestive of its usage. I'd prefer something like `ensureNoneRemaining`, but it's not a dealbreaker for me.
> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later? It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
+1 for 100 tasks. Thanks.
I think we're testing `testDir` Occupied here, not `AppDir`.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Note this is indeed fixed in trunk but not in older versions.
Thanks for clarification, @guozhangwang !
We've observed from the actual logs that it's not actually.. and the reason is this: https://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception
Hmm... Yeah, maybe the call in the metadata listener is sufficient. But we definitely don't want to fetch metadata for all topics. Also, it seems unnecessary to request a metadata update blindly. I think `metadata.setTopics` was previously requesting the update only if the topics are not already contained in the Metadata.
@vahidhashemian Are you concerned about the fact that we will have some unneeded metadata cached? That doesn't seem like a problem as long as we clean it up eventually (i.e. on the next scheduled refresh).
@vahidhashemian Feel free to ping me on Google chat if you want to talk about this. It would probably go a little faster that way.
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
Nit: including "execute" here is completely unnecessary. ```suggestion throw new ConnectException("Fail to " + descriptionStr + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
I think we should clear `immediatelyConnectedKeys` at the end of each `poll()` as well.
Oops, that was actually my fault.
The need for this check is a bit unfortunate since it makes the api a bit unsafe (kind of tough to tell at a glance that all current uses are safe, though I think they are). Since we have only three valid options, I was wondering if we could replace the two booleans with an enum representing the disconnect state or something. Not too big of a deal since it's internal, but might be worth considering.
Why is this needed? This is worse than the previous approach as it opens, closes and reopens the file.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
There is a file lock on this file that causes the issue, which might be hiding another issue even on other platforms.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
ditto on (what I think is) the impossibility of this condition being false.
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
Yeah if it exists elsewhere let's just leave it as is for now.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
nit: should be `named` can't be null
I guess if you are removing `this.` above, you could remove it here as well for consistency.
I guess if you are removing `this.` above, you could remove it here as well for consistency.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
This is common enough that there's a util for that and is used extensively: ```suggestion Utils.closeQuietly(retryWithToleranceOperator, "retry operator"); ```
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
I'd suggest we do not print the stack trace in either case; instead, we can just print the exception's `name()`, which is sufficient to tell it is the locking issue.
Cool, I will create a JIRA ticket for now to keep track of it.
nit: `ERR_MSG` -> `ERROR_MESSAGE` (avoid abbreviations to increase code readability)
nit: this can be package private (what is more restrictive)
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
Hmm, but if the value is null, we won't hit those points: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java#L638. With this patch, both of these tests fail: ```java @Test(expected = ConfigException.class) public void testEmptyTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, ""); connector.start(sourceProperties); } @Test(expected = ConfigException.class) public void testNullTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, null); connector.start(sourceProperties); } ```
 fair enough
How about returning a Set instead of List? ``` return topics .stream() .filter(topic -> !topic.isEmpty()) .collect(Collectors.toSet()); ```
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
You only need to crate that instance once, right? It can be a member of the class
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
Blank line can be removed.
That's right. Thanks for the explanation.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: add a space so it is "StreamsMetadata {...} topologyName=xyz"
nit: add `final`
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
As before, we can use `assertThrows`.
We could get away with a single `*`
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
Sounds fine to me.
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
How about this. First, we can augment the message above to something like this: ```scala log.trace("Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`", position, nextPosition, completedFetch.partition, partRecords.size()); ``` This gives us enough information in the logs to see which partitions caused the `poll()` to return and it tell us exactly where in the log the aborted transaction/control records exist. Second, maybe we can add back your previous log line, but make it a little more terse and put it at trace level: ```scala log.trace( "Returning empty records from `poll()` since the consumer's position has advanced " + "for at least one topic partition") ```
`HashMap` can be replaced by `Map`.
nit: extra line
One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent. That would allow the "Skipping fetch for partition" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.
From the default implementation and netty implementation, it looks like they would never be null. But I guess it is ok to do the null-check here since we may make SSLEngines pluggable.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
i know it was motivated by findbugs, but this was probably a good refactoring anyway :) `RestServer.httpRequest` to *make* an http request has always been a bit awkward
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
Could also use `Collections.singletonList`, which would also make this immutable
With further reading, I think the timer should still be 7 days for pid expiration.
Adding a bit more on my thoughts about initTxns call: in eos it is per-task so we can delay the initTxn call as much as possible to make sure the pid does not get expired; in eosBeta however, initTxn has to be triggered at the thread-level, and suppose after that has been triggered and then a rebalance happens which assigned more tasks to you, and restoring them takes quite long time during which we would not process any active tasks and hence not calling txn APIs causing the pid to be expired -- note that since we still call poll regularly we would not be fenced on the consumer side. It might be a big issue if we still have large restoration time practically, and in near future what I can think of for tackling it are in two folds: 1) try optimizing our restoration time so that we are confident such thing should be very rare. 2) allow active task processing at the same time while restoring others, hence keep the txns going; note that it is not always comprehensive since if all actives are restoring then it would not help.
prop: do you think we should name this as `threadProducer` for readability? cc @guozhangwang
nit: needs a comma after the `{@link ...}`
Overall LGTM. But can we format it differently? We should start a new line for each sentence. If we update the docs later, it make the diff simpler to read.
extra space after `*` needs to be removed
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
`assertNull`s shouldn't be here but few lines bellow.
nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with `env.getCluster().nodeById(..)`.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
I was thinking we need to do something about the `ProcessorContext`, too. The interface is quite broad. I think what you have suggested is ok, but I'd probably like to see `register` moved elsewhere, too. At the moment the `ProcessorContext` is accommodating initialization and processing, which means we have methods on it that aren't valid in all circumstances. IMO it would be nicer to have more focussed interfaces that make it less likely that you can do something that isn't allowed. The interfaces can all be implemented by the same object, of course.
About `ProcessorContext`: yeah definitely not for this PR, just throwing it out here for discussion. About `punctuate`: that is a good point, and it makes me thinking if we should just change the return type from `R` to `void` then (and I know we need a KIP for that..).
I'd probably extract this to an inner class. I just find it a bit unwieldy having an anonymous class of this size. I find it a bit distracting. But i'm not overly bothered either way. Just a suggestion
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
As I understand it, handleResponse will always be called by AdminClientRunnable from the single 'network thread' (KafkaAdminClient.thread).
nit: format got unaligned. Please check the suggestion fixes it ```suggestion new ConcurrentHashMap<>()); ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
We can compute this once and pass it to `removeAttribute`.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
as above, your "link" markup
This is my first time looking at this class, but I noticed that pretty much all of the "payload" of these description nodes are strings. Should we consider returning a string here instead? In fact, if we did that, we could consider calling `toString` on the extractor instead of returning the class name. This would allow authors of the extractors to provide more information about the extractor than just its name. This might be especially useful in the case of anonymous implementations.
Personally I think the default form of `someObjectClassname@hashcodenumber` is fine as to identify the extractor, other than a single topic name, is used, which is probably the most important illustration from `SinkNode` anyways.
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Nit: remove `this`
In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
Making sample age super high sounds better to me, comparing to having a ballpark check.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
True, will we ever want to have this ability? But the change seems fine to me.
this global variable isn't great. Can't we hit some rest endpoint that can return this internal values out of the extension? probably makes for a better end to end test too.
another ideaa: add an assertion here? so that we are guaranteed that this method is called.
we can make method this public in `EmbeddedConnectCluster`.
Should be final
nit: Could just to `new ArrayList<>();`
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
Same here. We should use `builder.stream().toTable()`
As above, I think we should create both tables using `toTable()` operator
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Isn't this the same as: ``` tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue); ```
IMO, the code is easier readable if you name the variables consistently like `tagValueToClients` and `tagKeyToTagValues` or `clientsForTagValue` and `tagValuesForTagKey`. I prefer the former because it better visualises the mapping, but that is a matter of taste, I guess.
Yes, I think that makes sense. In this way you can also directly test the method. BTW, you can simply pass `statefulTaskIds` to this method instead of `statefulTasksWithClients`. The keys in `statefulTasksWithClients` should be the task IDs in `statefulTaskIds` and the values in `statefulTasksWithClients` are never used.
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
Is the following error code also retriable? 0x15 | KDC_ERR_CLIENT_NOTYET | Client not yet validtry again later
Would it be easier to understand if this handled all of the unwrap exceptions after the IOException? And then we could call this method `processUnwrapExceptionAfterIOException`.
If stop throws we won't count down the latch. No harm will result except there will be an erroneous log messages about exceeding the stop timeout.
We should name the thread so that thread dumps are a bit more informative. I _think_ these should be daemon threads because if we're prepared to basically ignore the non-return of `task.stop()` during runtime I don't see why we'd block jvm exit for them.
`toString` is not required.
This is needed if you want to persist the limit across a reboot. But we are not rebooting here. Get rid of this.
This should probably just return a boolean
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Same as above: need to check `clientResponse.hasResponse()`
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
As before, we can use `assertThrows`.
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
Thanks for cleaning up the code duplication.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Should have a comma after "for example"
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
nit: remove empty link
nit add `a {@link Named} config`
modifiers should be in the order `private final`
nit: both lines missing . at end
Nit `.` at the end
I'm not convinced we should allow this
It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in `RebalanceSourceConnectorsIntegrationTest` or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.
Why do we need to call `build()` here? (similar below)
Nit: fix line break
Nit: remove `this`
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
nit: `testAggregateRandomInput` to match up with other test names
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
Is it really worth having this catch here? I think it's best to just let the exception propagate. Any method under test can throw an unknown exception after all.
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
I asked you exactly that a few months ago :) You referenced some old PR but basically the takeaway was, a restoring task hasn't initialized anything but its state, therefore needs to close only the state manager
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
I think we're testing `testDir` Occupied here, not `AppDir`.
typo: we want to test the **case** that poll() returns no records.
```suggestion // This handles a tombstone message when schemas are enabled ```
"... is called previously... " without a subsequent call to `unsubscribe()`? Same below.
it treated => it is treated
it treated => it is treated
It is a shame we have to do it like this, but i don't see a viable alternative
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Add missing `<p>` tag
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
This is not used.
nit: "another thread wrote to ..."
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
Overall, confusing indentions and line breaks...
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
nit: `COGROUPKSTREAM-AGGREGATE -`
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
Nit: let's not add an unnecessary extra line.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
Changed it locally.
Is this test needed? It seems that loginContextName can never be null.
nit: new Integer(1) => Interger.valueOf()
we can maintain `context`, `actions` as class variables initialized in setup().
resourceWildcard => resourceWildcard
nit: fix typo `store[s]` ;)
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
nit: Indentation of those lines seems to be off here.
nit: What about creating a small helper to create a `ListOffsetTopicResponse` for a given `TopicPartition` & co? That would reduce the boilerplate code.
`new ArrayList<>` is suffice
Yes, does not hurt to leave it. Just for sure.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
extra new line.
I think this method can still block regardless of requestTimoutMs at least due to the following calls: ``` updateFetchPositions() -> coordinator.refreshCommittedOffsetsIfNeeded() -> fetchCommittedOffsets(missingFetchPositions) -> ensureCoordinatorReady() { // Using zero as current time since timeout is effectively infinite ensureCoordinatorReady(0, timeoutMs = Long.MAX_VALUE) }```
To clarify, there are two steps in `updateFetchPositions`. The first is fetching committed offsets; the second is lookup up missing positions. The latter is asynchronous now, but the former still blocks. We'll need to fix this once the KIP is approved, but it could turn out to be a little tricky.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
Please remove empty lines here and in the other test methods.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Nit: the same key.. ditto below.
Ditto above. I would recommend having consistent explanations here.
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
In that case could we just use the foreach loop after `ConsumerRecords#records` to get the filter list from the returned list? I just felt leveraging on `ConsumerRecords#iterator` is unnecessarily costly.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
If you use the `assertEquals` that takes a `double`, you can pass a `delta` value, which makes the code a lot more concise.
Seems like this should be a `long`.
We should have a constant rather than using '262' directly
We do not throw `InvalidTopicException` "if [the topic] is not found"
"of [an] ever-updating ..."
"of [an] ever-updating ..."
Seems like the indenting should be adjusted to the left, right? Applied to other changes in this file too.
This does not need to be a map, a list is good enough since we would not call `addList` with the same groupId, similar to `addError`.
original was better
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
nit: add `final`
We could port this function when it is actually needed.
nit: final on params here and methods below.
nit: I don't think the copier uses group instance ID (maybe we could add support for that separately?), so I don't think `FencedInstanceIdException` is possible at the moment.
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
nit: fill in `@return` docs
nit: `planned in` -> `planned for` ? (similar below)
an -> a
I know this is a bit opinionated, but ... I think we should make an effort to make all locals `final` where possible. It is just a few extra keystrokes (that intellij can do for you!), and it helps to eliminate a class of bugs.
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
I wasn't aware that `finally` blocks had a large impact on performance, and haven't been able to find anything definitive on the subject that indicates they are. Can you provide a reference that backs up this claim? Fwiw, using a `finally` block would be more concise and readable here: ```suggestion try { return Utils.newInstance(klass); } finally { compareAndSwapLoaders(oldClassLoader); } ```
The other benefit is the way we are loading converters it could technically be loaded in a different plugin loader that what are keyed in the map since we're loading the ConnectConfig after swapping the class loader in `startTask`. If we prefer to keep it the way it is here, we need to revert that change.
This method is called from within `newConverter`, `newHeaderConverter`, and `newConfigProvider`, so mentioning "converters" and getting the available converter implementation classes is actually wrong when used to find the config provider implementation class. One way to address that would be to pass in additional parameters to the method, but since we want to backport this to branches before `2.0` we have to make that work without method references. So one simple option is to rename the method to `converterClassFromConfig` and add a new method for `configProviderClassFromConfig` that does essentially the same thing but is tailored for config providers. Perhaps a better alternative is to dynamically determine the name and the `pluginNames(...)` based upon whether `klass` is a subtype of `Converter` or `ConfigProvider`. This keeps a single method, but is a bit more dynamic.
I don't think we need extra `toGiveUpTopicPartitions` to store the partitions to be deleted. We can log the warning message in L1103 here directly
OK, make sense.
nit: additional new line
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
The variable name should be changed.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
"... as producer is closed" Same elsewhere
ok - same thing three times. Maybe extract it to a method `verifyTransactionInflight`
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
prop: abortTransaction can also throw ProducerFenced.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this: ```java try { producer.beginTransaction() producer.send(...) producer.sendOffsetsToTransaction(...) producer.commitTransaction() } catch (Exception ) { producer.abortTransaction() } ```
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Sensor names don't appear in JMX.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
nit: we can make this a `static` function and rename it to something like `convertToVoters`.
It seems you can move this line after line422.
We could change other callers of `isGlobalSource` and use the index instead? E.g. `describeGlobalStores()`, and then this `isGlobalSource` itself could be removed.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
Nit: param alignment.
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
This does not need to be a map, a list is good enough since we would not call `addList` with the same groupId, similar to `addError`.
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
Ok, I took a closer look and had a bit of a flash-back to how confusing these metrics are... Maybe https://github.com/apache/kafka/pull/7057 will help.
Thank you @vvcephei !
This is not part of the PR but I realized that `CumulativeCount` is in `streams.processor.internals.metrics` but not in `common.metrics.stats`. Is this intentional? @vvcephei
Good point. I think it is still worth keeping the optimization, although typically the the producer will only allocate poolable batch size, so the actual memory allocation should not happen very often.
Yeah, it's not pretty. However, reducing the concurrency of `BufferPool` in the common path is not desireable. We definitely need to handle OOMs correctly, but they are relatively rare and it's OK if that path is slower.
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
So you want to advance `recordFactory` time too? (similar below)
We should initialize `TopologyTestDriver` with a fixed mock-timestamp and use this below instead of calling `System. currentTimeMillis()`
Is it intended that all records fall into a single window / segment? What was the root cause of the bug and how does this data/timestamp pattern cover it? (cannot remember)
For `shouldNotReturnDuplicatesInRanges` it seems best to use `processorContext.timestamp()` -- `processorContext` is passed in `init()` do you just need to add a member variable to the `Transformer` to store is so you can use it in `transform()`
old code sets `retainDuplicates=true` but new code sets `retainDuplicates=false`
It seems like we can migrate away from the deprecated method in this test.
nit: use `"table-source"` ? It's naming a source node, not a processor node.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
nit. Add `{ }` to block (we always use them). Same below.
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time.
Although, you simplified this code, you did not apply all simplifications I proposed in PR #7914. I think you can even simplify this code further as shown here: ``` return Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); }); ``` This simplification can be applied also to the other methods below. If you have any concerns about this simplifications please share your thoughts.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
The number of elements is not always 1. Each created thread-level sensor is added to this queue, e.g., `processLatencySensor`, `pollRecordsSensor`, etc. Check out the callers of `threadLevelSensor()`. Each queue contains all thread-level sensors for one single stream thread.
Yes, I think it's worthwhile to check the result of `Connector.config()` just in case `Connector.validate()` is overridden and the default check there is no longer used.
I was referring to the call to `connector.validate` two lines above. That is where the other null check in this patch in `Connector` would be applied, unless the user has overridden `validate`.
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
And again - i'd say it is definitely worth having what i mentioned above
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
`Integer.toString` is a slightly more concise way of doing this.
We also need a test to validate that some extensions can be ignored (neither valid nor error).
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
After discussion on #4713 I think this idea should actually work. Nit: Can we rename the lock to `LOCK_FILE_NAME + " -" + taskId` though.
extra new line.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
I think we should have this in this PR already.
Hmm... It seems a little inconsistent to use the offset of the first record. When the batch is empty, we use the base offset of the batch. Shouldn't we do the same here? Otherwise we will end up with some batches which have base offset smaller than the segment base offset. Note that the base offset is always preserved by compaction.
nit: why do we need to pass the base offset as a separate parameter? We are already passing `batch`.
We only need to write the DeleteHorizonTime if the batch contains a tombstone or a control record. This increases the chance for more optimized writeOriginalBatch case to happen.
nit: formatting -> should be in the line above.
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
nit: if unused, remove the variable instead of suppressing a warning.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Note that Kafka only supports kerberos as the SASL mechanism.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
I do not think you need to put an entry if you use mocks.
This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.
To get rid of the test failure, you need to change this: ```suggestion final KafkaMetric metric = metrics.metric(new MetricName("prefix-scan-rate", STORE_LEVEL_GROUP, "", tags)); ``` Sorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Perhaps instead we could add this to a mixin type. Then if we find cases where getting accessing to the `ApiMessage` generally would be useful, we could just use `instanceof` checks. These would ultimately go away after the conversions are finished.
Is there an advantage to pulling this up? Seems like we still need to update a bunch more classes. Until we have all the protocols converted, it might be safer to find another approach.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
Are there ever situations where users would want the old behavior (to have access to the `ProcessorContext` for the record that triggered the lookup, rather than the context for the record that's being looked up)? For example, if the topic name is relevant for the transformer and all records (including the current one that triggered the lookup and the one being processed) are from the same topic, then the old behavior gives access to the topic name but this new behavior doesn't.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
That makes sense. I did not think about the reconfiguration case.
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
ditto to `KStreamImpl`
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Add the stream task id prefix here as well for both exception message and the warning log entry.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
you don't need this. Junit gives you a new instance of the test class for every test method
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
nit: also add java doc for type `T, O` here
nit: in spite of the getter convention, I still prefer setters be prefixed with `set`
Since we're only using a batch size of one, we should be able to just verify that we got a list of size 1 and it contained what we wanted, right? Also, we should also try to be robust against malformed responses that don't include the group we asked for (this will currently throw an NPE if we get one of those).
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
Nit: "..and producer's {@link DefaultPartitioner}".
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
nit. I think there is `.` missing `since 3.0[.] Use`
Should we guard against NPE? (same blow)
nit: add `final`
Thanks for the follow-up.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
nit: `addMetadata` -> `put`
`setNeedsCommit` -> `{@link #setNeedsCommit}`
`needCommit` -> `needsCommit`
I'm not sure it actually matters since users are unlikely to construct this object manually, but it seems like we should use `ConfigSource.DEFAULT_CONFIG` if `isDefault` is true and `ConfigSource.UNKNOWN` otherwise. Then `isDefault()` will continue to work with this constructor.
I think you need `equals/hashCode` since `synonyms` is now included in the same methods for `ConfigEntry`.
nit: unneeded newline
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Checked with Jun and this is fine.
We probably shouldn't change this default to true -- we should override it in the specific test we need by calling `self.mark_for_collect(consumer, 'verifiable_consumer_stdout')` where `consumer` is the `VerifiableConsumer` instance. stdout for verifiable consumer can result in _huge_ log files, so collecting them by default will result in very large archived data for some tests.
I don't think this is a problem yet, but we should start thinking about these `Service` classes as at least semi-public interfaces. I know we rely on them in muckrake (although perhaps not this particular one yet) and I know others are starting to/planning to/want to be able to build tests on top of the pieces included in Kafka. While these definitely aren't the same as our client APIs, I think we should make an effort to provide some degree of compatibility.
A docstring for this method would be good :)
> I ignore 1 out of every 2 punctuations Uh. That's kinda painful... I think we need to discuss this in more details, ie, what semantics we want to provide. \cc @bbejeck @dguy @guozhangwang @miguno
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Doesn't this introduce the possibility of conflict between two plugins (or I guess specifically connectors, since those are the only ones we strip suffixes from) which have different fully-qualified class names, but the same simple class name? Or where they would have the same simple class name, except that one ends with `Connector` and the other doesn't? In practice this is unlikely to come up but if we support it at the moment, probably best to take care here to avoid introducing a potential regression, especially if someone for some reason wants to run, e.g., two different `MySqlSink` connectors on their worker.
Also, it seems like using a default of `PluginType.UNKNOWN` here might be suboptimal. If someone wants to the view the config for a REST extension, for example, they'll end up seeing an error message later on (in `AbstractHerder::connectorPluginConfig`) that says something like "Invalid plugin type unknown. Valid types are..." I think it'd be clearer to users if we could differentiate between these two cases: 1. User requests config for a plugin that does exist on the worker, but which we don't expose config information via the REST API for (such as a REST extension or a config provider) 2. User requests config for a plugin that doesn't exist on the worker Status-wise, In the case of 1, a 400 response probably makes sense, but for 2, a 404 response might be more applicable.
Could we just add a toString function to ProcessorNode / SourceNode / SinkNode so that this code just becomes: ``` print(node) { // node.toString // also include listing children names foreach(child <- children) print(child). } ``` And also you do not to maintain the mapping as well.
the message format => that message format
We could make this field access `public`
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
More specifically, there's no point in having this if we close the network client before we even get to the point where the request has reached the network layer.
Don't we have to do something with the futures returned by `send`? How do we know if the requests completed? Also, cc @hachikuji.
The consumer close code needs to block until the fetch sessions are closed. `KafkaConsumer` has `close()` and `close(long timeout, TimeUnit timeUnit)`. In the case of the latter one, we don't want to wait longer than the specified time.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
We should pass in `record.topic()` instead of `""` into `deserialize`.
why use `topicName` here? Should it not be `failed: key=X actual=A expected=B...` instead of `failed: key=X <topicName>=A expected=B...`
`start` is not used.
I think it is a bad idea to let callers concern about calling `maybeExpandBuffer`; previously the caller is abstracted from this away.
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
`if (ignoreWhenShuttingDownOrNotRunning && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING))`
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
Wonder if it might be simpler to initialize `partitionsToRetry` from the request key set.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
the fist argument is still "expected value" so we don't need to switch the args. There are many similar occurrence.
This change seems not needed.
please revert those changes as first argument should be "expected" value.
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
There's a pattern for all of the Trogdor JSON code where we don't use null anywhere. The problem with null is it gets annoying to check each collection for empty vs. null, each string for empty vs. null, etc. etc. null is also handled kind of inconsistently in Jackson. Sometimes Jackson will serialize a field that is null as `"foo": null` whereas sometimes it will just omit the field. (I think that `"foo": null` is actually not conforming JSON, by the way...) There are probably ways to configure all this, but null doesn't really provide any value 99% of the time, so it's simpler to just treat empty as null.
We don't use null entries in JSON, because it gets too confusing. You should check against empty string here.
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
records to it, and reading all records from it, such that
It would be best if this test used a replication factor of 2. With a replication factor of 1 we will have no regular replication traffic occurring when the producer writes messages. It would be good to have both throttled replication and non throttled replication happening at the same time.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
same here. let's make all method params as `final`
nit: creating restoredPosition is not required if !constinceyEnabled
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
nit: add `final`
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
NIT: I think we should keep the check consistent between subscribe(topic) and subscribe(TopicPartition). I am fine with either way of checking.
Hmm, but we're not actually calling the listener here. We do that separately.
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Overall, confusing indentions and line breaks...
nit: how about just put these two in one line? We usually only use multi-lines if there are 3+ parameters.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
Should be more specific about the type of error being caught -- catching all exceptions should be reserved for very special cases, like protecting the top stack frame of a thread to avoid uncleanly exiting the thread. I suspect that here you specifically want to capture `CalledProcessError`, which indicates an issue running the command on the remote host and/or `ValueError`.
Does mirror maker support multiple consumer configs? A quick glance at the code suggests it only supports one.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
It seems you can move this line after line422.
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
i don't think there's a good way to do this directly with `monitor.wait_until` because it was written originally to look for a fixed pattern (we've mostly used it to monitor startup where we know a specific message gets logged once the service is actually serving requests). you could either look for the fixed pattern (e.g. if this message only gets logged once and before the broker ids it has a known msg) or do a `wait_until` yourself. `wait_until` takes a function, so you can just wrap up the check in a local function that returns a boolean and pass that into `wait_until`.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
should this be `< 0`? Otherwise, we skip index 0
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
It seems better to say Producer.send() instead of send.
@sutambe I also think we should mention the scenario that a Record is added to a batch that is about to expire.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
nit: needs a comma after the `{@link ...}`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: missing `<p>` for new paragraph
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
This doesn't really work as a general pattern. What I mean is that for `ElectPreferredReplicas` there is no analog of `NewTopic`: The `AdminClient` API is just a `Collection<TopicPartition>`, so there's no handy class in which to put the conversion code. It might be worth having a consistent place to put these conversion functions (for AdminClient protocol messages, at least); either a single common class (`AdminClientProtocolUtil`), or a `ElectPreferredReplicasProtocolUtil` etc. The latter would have one benefit: With the use of nested classes there's the possibility for imports like `import org.apache.kafka.common.message.ElectPreferredLeadersResponseData.Result;` to collide when another protocol uses a nested class named `Result`, and using outer class qualified type names makes the code ugly. Having a `ElectPreferredReplicasProtocolUtil` etc would avoid this.
This is unrelated to this PR but the check brings up a good point. Maybe instead of throwing an error on the pattern subscription finding an overlapping topic and then restarting the thread and trying again. Maybe we can pause a topology until the topics are not longer overlapping? EDIT: I can see this in the other PR
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
as above: we need to remove adminPrefix configs
How about we get rid of the problem altogether -- define `WorkerConfig.rebalanceTimeout()` that returns null by default, and then override it in `DistributedConfig` to return `getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG)` since that will always be an integer. The latter's method is trivial, and the code in `RestServer` becomes simpler and easier to read. I think there's enough precedence in `StreamsConfig` and `ConverterConfig` to add getter methods (without `get` prefix). The fact that it cleans this up this significantly is also good justification.
Could also use `Collections.singletonList`, which would also make this immutable
static import maybe better
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@mjsax What you suggested sounds right to me.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Should not have an implementation but call overloaded method.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Nit: space before `:`.
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Oh, right, I forgot that the metrics registry returns the same copy of the sensor when all the name, description, and tags are the same... Thanks.
nit: I slightly doubt whether these exact match tests are necessary
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
Yes, that is the reason for the inconsistency. Obviously it was a mistake to let `OffsetAndMetadata` be serializable in the first place.
`while` seems to be missing
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think `will go` should simply be `go`.
Should have a comma after "for example"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
This doesn't read well. Assuming it needs to be as long as `inactivity-gap` plus `grace-period` then the following reads better to me: ```suggestion * @param retentionPeriod length of time to retain data in the store (cannot be negative) * Note that the retention period must be at least as long as * inactivity-gap plus grace-period. ```
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
I would move line 328 and 329 to before this line.
ditto about the log level (also for the below uses of `debug`)
naming nit: topicConfigsWithRetention
In this case I think we should include some error details here. In particular, the last seen error for each topic. I'm worried about cases where we try to create but the create times out but is eventually successful. We'd return an error back, but the user would have no way to know that setup failed because an internal topic already exists.
Nit: add `final`
as above: we need to remove adminPrefix configs
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
This is public API meant to be used by users. I don't mind if our tests are a bit more verbose but we should aim to have succinct public APIs
Does this need to be public? Making it private forces use of `of` method, which I think is good.
@ableegoldman is it related to the UUID randomness? If yes please ignore my other question above.
Yes, I think so
Nice catch. Reminds me though, why the second rebalance may not be deterministic in migrating tasks back? I thought our algorithm should produce deterministic results? cc @ableegoldman
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
How about we get rid of the problem altogether -- define `WorkerConfig.rebalanceTimeout()` that returns null by default, and then override it in `DistributedConfig` to return `getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG)` since that will always be an integer. The latter's method is trivial, and the code in `RestServer` becomes simpler and easier to read. I think there's enough precedence in `StreamsConfig` and `ConverterConfig` to add getter methods (without `get` prefix). The fact that it cleans this up this significantly is also good justification.
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
Yeah, the logic seems right to me.
nit: why double space? (similar below and further below)
Okay, sounds fine.
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
`long` -> `Long` is a binary incompatible change.
This is also related to the following PR https://github.com/apache/kafka/pull/1015 that uses sentinels in a number of places. It would be nice to be consistent on -1 versus null.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
Unused import - this is causing checkstyle failure in the PR build.
This method is usually called `setUp()`.
Ah ok fair enough -- thanks!
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
This reminds me that we should change the internal interfaces of `Segments` etc to use the new StateStoreContext, not the ProcessorContext anymore, will create a new JIRA.
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
nit: Please fix code style.
nit: Please fix code style.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
+1 for consistency
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
nit: maybe `disconnect` is a better name given actual behavior.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
Since this is basically just a pass-through method anyway, there isn't that much to test here -- you could simplify this test just to use a simple set of connectors you create yourself. There's a lot of code in this test when all you really want to see is that the set makes it back out of the call to ConnectorPluginsResource
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
`apiKey` is of type `ApiKeys` while `requestHeader.apiKey()` returns a `short`.
requestHeader.apiKey() can just be apiKey.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.
I'd go for being consistent with the other logging statements
Thanks. I will make another pass now.
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
This is also kind of unclear (what is a max right window?), but I get that we can't call it `previousRecordRightWindow` since we don't know that it is a previous record or not at this point. I think yet again, just keeping track of the previous record's timestamp as we iterate through the windows, will be the most clear; if `previousRecordTimestamp` is still null by this point, we know right away that we don't have to create a previous right window. And then we can actually drop the `rightWindowNecessaryAndPossible` check altogether, since we know the current record has to be in range of the right window of the previous record (since we're in `processEarly`). The one exception is if the previous record and current record are on the same timestamp, so we can actually skip the previous right window creation if `previousRecordTimestamp ` is `null` OR equal to `timestamp`
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
nit: we might want log different message if we're ignoring due to a fatal state instead of due to a bumped epoch or ID.
Discussed offline, but I guess the one case where it is not safe to adjust sequence numbers is when the number of retries are exhausted.
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
as above (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
Just curious, could we possibly call this function for the same node more than once? It seems yes as you are checking `!keyChangingOperationsToOptimizableRepartitionNodes.containsKey(node)` here, but I cannot tell from the code...
Generally speaking we should not rely on the caller to pass in parameters that are guaranteed to no pass the check. What I suggested (below) is to have a slightly modified recursion pattern which do not rely that the first caller would never satisfy the predicate.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Fair enough. Let's leave it as-is.
nit: add `final
Could just use `false`
I understand that. My question is whether there is some thinking on which configs make sense to be set by users. Ideally, we'd do that instead of being reactive.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
Nit `.` at the end
Should not have an implementation but call overloaded method.
nit: `{@code null}` (same below)
No worries. All good. :)
Should this change not be more "radical" calling `KStream#peek()`? In order to resolve the `PEEK_NAME` vs `PRINT_NAME`, we can add a private `peek()` that take the name as parameter.
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
Hmmm... Good point. Let leave it as-is. It's also covered in integration tests that the right store is returned.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Isn't MM2/Connect using at least once by default? ie, the producer in the runtime can cause duplicates.
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
Instead of doing: ``` if (condition) { return true; } else { return false; } ``` You can do: ``` return condition; ```
`brokerConfig` is a `Properties`, so we can call `getProperty()` to get back a `String` directly
nit: add `final`
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
@vahidhashemian, yes, that's what I mean.
I think we tend to use `forXxx` rather than `create` for mapping names to enums.
I think that the driving condition of the test is the number of samples, not the time. But I thought I'd point out that time won't advance by default, but you can make it by using the `autoTickMs` constructor.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
nit: The callers of this iterate the committed offsets twice. Once in order to check partition ownership and then a second time to invoke `updateLastSeenEpochIfNewer`. I wonder if we could consolidate these two loops.
This exception message is not really clear to me. What about `"Cannot commit offset of topic partition " + tp + " since the partition was not dynamically assigned to this consumer"` or similar? In the code such messages usually start with an uppercase letter.
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
Thanks @ewencp! Glad I'm still up-to-date on that. Happy to adjust per project and yes, unnecessary diffs are better to be skipped in non-cleanup/non-refactoring PRs. Huge fan of that.
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
Here's a nice blog re: exception costs and when they occur: https://shipilev.net/blog/2014/exceptional-performance
Is ```Exception``` in method signature necessary? the method signature of ```ErrorReporter#close``` don't include checked exception.
Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations: ```suggestion if (reporters.size() == 1) { return reporters.get(0).report(this); } List<Future<RecordMetadata>> futures = new LinkedList<>(); for (ErrorReporter reporter: reporters) { Future<RecordMetadata> future = reporter.report(this, callback); if (!future.isDone()) { futures.add(future); } } if (futures.isEmpty()) { return CompletableFuture.completedFuture(null); } return new ErrantRecordFuture(futures); ``` And since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a `LinkedList` instead to avoid excessive allocation when adding the first element to the `ArrayList`.
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
I am not an expert on this API, but I would expect that even if we commit using the producer, the consumer should still be able to read the metadata. Did you verify that we cannot retrieve the metadata if EOS is enabled? If this is the case, I would claim it's a bug that need to be fix.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
newuntil => newUntil
Changed it locally (and in one other similar place).
Changed it locally.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
nit: avoid inserting random blank lines.
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
nit: extra spaces after the `->`
How about updating `nullableSeenMetadata` only if we don't return records? ```java if (longPollShouldReturn(records)) { ... } else { if (nullableSeenMetadata == null) { nullableSeenMetadata = new HashMap<>(records.metadata()); } else { nullableSeenMetadata.putAll(records.metadata()); } } ``` The benefit from above code is that we don't need to handle duplicate metadata which exists on both `FetchedRecords` and `nullableSeenMetadata` when it succeed to get records and metadata in first loop.
Also, `n >= records.size()`? Seems we could clear `this.records` and use this fast path if they are equal. Would also remove need for checking `iterator.hasNext()` below.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states: 1. Handshake 2. Authentication 3. Waiting to receive a message (receive == null) 4. Received partial message size (receive != null, buffer == null) 5. Received size and partial message body (receive != null, buffer != null) 6. Muted after receiving size due to OOM 7. Explicitly muted 8. Disconnect The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState` is unnecessary if muting is done on allocation failure and that makes the code simpler.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
I wonder if we should move all of this into a new private method that takes the interceptor callback and intercepted record. The reason is that it's a bit easy to make a mistake and use `record` and `callback` instead of the intercepted ones (with the current approach).
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
ditto on the properties and the driver.
Ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
Still not used
`tp` is not used anymore.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
Please fix this, too.
nit: camel case
You need to fix these other methods also. If the exception is raised, the call to `release` will lead to invalid state.
the condition is always false if you don't add brackets. ``` throw new IllegalArgumentException("Topic pattern to subscribe to cannot be " + (pattern == null ? "null" : "empty")); ```
Just throwing on the first is probably fine. Alternatively, if you want to list them all, I'd suggest iterating through them and collecting them into a collection rather than using suppressed exceptions.
Might be better to add a proper POJO maybe `StreamsMetadata` or something that wraps the `streamTime` Long plus `ProcessorMetadata` instead of using `KeyValue` ? We might add new fields later on what is easier to do for a new POJO.
nit: is there a way to verify a function is never called? Like `consumer.commit()`
nit: we could use mkMap helper here as well.
Nit: how about "a new {@link KTable} ... as this {@link KTable}"? Ditto for all `through` calls of KTable and KStream
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
Can remove the first 3 null checks as they are covered in the new overloaded `aggregate` method
Also need to change `keyValueStore` method to return `StateStoreSupplier<KeyValueStore>`
Can remove the first two null checks as they are covered in the overloaded `reduce`
maybe: `inputKeySerde` and `inputValSerde`
As i said above, we should `requireNonNull(mapper, ...)`
`PrintForEachAction<>` to remove warning. Also in the `print` method
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
Yeah if it exists elsewhere let's just leave it as is for now.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably.
I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required.
Same here, I think `computeIfAbsent` would simplify the logic
Let's use `Map` on the left side instead of `HashMap`
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
nit: add one whitespace at the end after "...state"
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
We can implement that when handling a response, invalid cluster id are fatal unless a previous response contained a valid cluster id.
What should we do if we see this error in a response? It looks like it would hit `handleUnexpectedError` currently which just logs an error. That might be ok for now. I think there is a window during startup when we could consider these errors to be fatal. This would be helpful detecting configuration problems. We probably do not want them to be fatal in all cases though because that might result in a misconfigured node killing a stable cluster.
The KIP talks about bootstrapping the topicId for the metadata topic. Is that part done already? I don't see it included in this PR.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Nit: param alignment.
If not, we should move the exception capturing logic inside the dbAccessor as well.
recommended; ditto below.
as above: we need to remove adminPrefix configs
Use diamond (`<>`).
Yeah, it seems to me like we should remove it.
Atm we cannot say for sure, but it's likely that what's observed on KAFKA-9701 is a broker-side issue; we can either 1) add the check on broker-side across all members to make sure the selected protocol is consistent for everyone, so if the broker already made a wrong choice itself would log an ERROR, or 2) let it check on the client side. I think for trouble-shooting purposes option 2) is fine, and if we later discovered that this bug is actually on the client side I'm happy to revert this change after fixing it.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
Nice, thanks for the update. Looks good
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks @vvcephei -- that is convincing.
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
I guess it doesn't harm.
What we are getting here is not necessarily the bootstrap nodes that the user has passed as it depends on what the caller passes (the current implementation does that, but it could easily be changed). 3 options I can think of: - Leave the current implementation, but change the name of the field to something like `initialNodes` - Add an additional constructor parameter called `bootstrapNodes` - Leave as is
Passing the bootstrap cluster to Metadata sounds good. That's better than passing an additional constructor parameter called `bootstrapNodes` to `NetworkClient`.
Could this be completely fixed by ``` int startIdx = Utils.abs(this.randOffset.nextInt(Integer.MAX_VALUE)) % nodes.size(); ... for (int i = 0; i < nodes.size; i++) { int idx = (startIdx + i) % nodes.size(); } ```
I'm against the sprinkling of `final` keyword everywhere. But more than that, I'm in favor of consistency w.r.t. the surrounding code. A few lines above, you may observe that `close` has a similar loop without `final`. Every project has its idiosyncrasies and Connect is not dogmatic w.r.t to `final` for local variables.
@cyrusv Your example should print something like: `<SimpleClassName: blah blah blah>` But I don't see `<` or `>` in your example. So either the code example is wrong, or you didn't mean to use those "quotes" in the joiner. BTW probably not adding this decoration would be better.
This will end with a comma after the last transformation.
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
should this be null? perhaps throw an IllegalArgException
Another name might be `seek()`.
Style convention: this should be called `value()`.
Not really related to this line. Could you verify that the state store is closed in the unit test that tests line 148? The name of the test is `shouldThrowStreamsExceptionForOldTopicPartitions()`.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
+1 for 100 tasks. Thanks.
I think we're testing `testDir` Occupied here, not `AppDir`.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Should be final.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
I would say it's important _not_ to be able to create bogus requests. ;) We can introduce specific mechanisms for testing, but a public constructor for a request should do its own validation.
We could make this field access `public`
isFull is no longer used.
deliveryTimeoutMs should be mentioned
This is still not used
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
Should we clear the exception here? If not, then we'll have the exception thrown after the rebalance.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
variable exception unnecessary
We need to make sure the `fetch` bounds don't go into the negative. We only call `processEarly` if the record's timestamp is within the timeDifferenceMs, but here we search starting at timestamp - 2*timeDifferenceMs
I think this fetch might break if you go into the negatives, should just fetch starting from 0
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
Maybe I'm missing something, but this doesn't appear to be used anywhere.
Yeah, the throw is what I was looking for.
nit: We should use `groupId.idValue` here and in the others.
nit: we usually try to keep lines under 120 characters.
Yes, that's what I was thinking. Then we can remove the global references and use the same pattern consistently.
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
nit: final on params here and methods below.
nit: remove empty line
nit: add `final` (2x)
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
nit: line too long
nit: single parameter per line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
No need to check null. We always have to forward oldAgg and newAgg, anyway.
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
same here. let's make all method params as `final`
nit: creating restoredPosition is not required if !constinceyEnabled
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
The names are terrible. You don't suspendImpl something, you suspend it. You don't commitImpl, you commit.
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
This block can be moved outside of the `try-catch-block`
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
I think it would be better to make this a static factory method and keep the constructor for the case where we receive `FetchResponseData`.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
It looks like these are backwards. If I understand this method signature, even though iteration is reversed, the "from" key is still the lower bound, and the "to" key is the upper.
@vvcephei is correct. The `from` is still the lower bound and and `to` is the upper bound in the `reverseRange` method definition. We can confirm it by checking the parameter validation in the `CachingKeyValueStore#reverseRange` ``` public KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from, final Bytes to) { if (from.compareTo(to) > 0) { LOG.warn("Returning empty iterator for fetch with invalid key range: from > to. " + ... } ```
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
nit: Please fix code style.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Nit: too many blank lines.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
nitpick: we usually include a space before and after ":"
One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent. That would allow the "Skipping fetch for partition" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.
`HashMap` can be replaced by `Map`.
nit: extra line
nit: past control records or aborted transactions when isolation.level=READ_COMMITTED
Nit: missing articles & punctuation in this description. "not available in buffer" -> "not available in the buffer". "available currently in buffer else return empty" -> "available currently in the buffer, else return empty"
Are we missing the ApiException here? I.e. error returned from servers that are not recoverable.
Do you want `@DefaultValue` or whatever the appropriate annotation is? Same for rest of the `forward` flags.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
nit: Starting a message with lower case feels a little unusual.
It does seem like we could pass the node id to `RequestSend` without much issue.
We'll need to fix this in a follow-up so that followers send the right replicaId.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
and -> a
and -> a
The original motivation is to maintain the information of which thread was hitting this error, but the current implementation does not maintain that any more.. I'm not sure if it was lost somewhere in previous commit or it is never the case. But this is what I've in mind originally: ``` final String logPrefix = String.format("stream-thread [%s] ", threadClientId); final LogContext logContext = new LogContext(logPrefix); final Logger log = logContext.logger(LogAndContinueExceptionHandler.class); ```
Would be helpful to understand the reason as well. The problem with a mismatch is that it's harder to track down the source of a log message and it messes up line numbers when printed.
Yes they are designed to be public.
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
These two cases don't seem to be different. I'd recommend just always wrapping the exception and throwing (currently the else block). If we just re-throw the first exception, reading the stack trace becomes very confusing. Especially since a lot of those exceptions don't even include the stack trace.
In newest trunk we always call `task.closeDirty` .
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
We could perhaps add a `leaderId` in `PartitionMetadata`
nit: add a space before the `:`.
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
+1, we should just throw in this case.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
nit: both lines missing . at end
Nit `.` at the end
`keySerde` -> `valueSerde`
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
nit: "another thread wrote to ..."
maybe "Restoration completed for partitions:"
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
same here and below, we should assert false to `(config.unknown().contains("prefix.sasl.kerberos.kinit.cmd")` after using that config.
here, we should change to assert `unknown`, right? Same as below.
Should not have an implementation but call overloaded method.
Make all params `final`
remove this line -- not required.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Tiny nitpick: `TopicPartition.toString` does what you are doing manually here, so you could simplify it by just saying: ``` java "Batch containing " + recordCount + " record(s) expired due to timeout while requesting metadata from brokers for " + topicPartition ```
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
```If the thread is callback thread. Calling flush() from callback is not allowed since it makes deadlock.```
It seems to me ```IllegalStateException``` is more suitable for this case. Also, please update docs of ```flush()```
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
to me it seems like we can't possibly know what the constraints of all reporters would be and they don't provide an interface for validation, so it should be up to them to figure out how to substitute. but i've also asked some other folks to maybe chime in here who may have better context on how we've handled this elsewhere.
I know the naming thing has bit us in the past, is this same approach used elsewhere and/or how was it decided on? Specifically, metric name constraints really shouldn't be JMX specific if that is the case here, despite the fact that the metrics is so obviously JMX-inspired. I can easily find https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala#L46 but nothing else. Have we not had the same problems because metrics w/ topic names in them already have constraints on the naming? If I am remembering correctly, I think maybe both @gwenshap and @junrao were involved in some discussions, I think `-` vs `_` was a problem at some point? Maybe one of them could chime in here.
ah, right. nah, that's fine. just when reviewing I had the thought that if we guaranteed non-`null`/non-empty in the constructor, this wouldn't be necessary. i realized that it was actually intentional, but easy to miss when reviewing here and not getting the same highlighting as an IDE
This is part of KIP-516
nit: remove empty line
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
nit: extra line
Feel free to do a PR against https://github.com/apache/kafka-site
@mjsax Should we include indentation settings here: http://kafka.apache.org/coding-guide.html ? It's mentioned in the scala guidelines...
> Is there any official style guide or export of accepted settings? Not really :( Rule of thumb is, avoid reformatting if possible :)
To get rid of the warning, you can just copy the body of `close(long, TimeUnit)`. If you don't want duplication, you can make the deprecated method call the non-deprecated one.
Nit: It's probably a bit better to have this be near the `setPollException(...)` method.
Is this ok to do for both reasons this would be called? Do we actually want to request a commit when in the case of the connector throwing a RetriableException? That'll result in the connector being forced to flush all its data.
With the type promotion changes I think you'll need to pass in both the original value and the projected value to this method and this assertion will have to change to something like `assertEquals(expectedProjected, projected)`.
Expected value here should be a list of longs.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
I am confused. If we change `store.range(hello, "zooom");` should `hasNext()` not return `true` (start and end are inclusive). Thus, to me it seems the test is wrong and there must be a bug in the iterator code.
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
I know this is just how the other tests are doing it, but it's not really an airtight way to validate the expected results...if nothing is returned then we never enter the `while` loop and the test passes, even if we did in fact expect there to be actual output. The important thing here was just to make sure it didn't throw an exception so it still does that, but it would be good to fix this up maybe in a followup PR
If we do allow an `int` here, we should a) check for bounds and b) re-validate values after the cast. Otherwise you can end up with invalid results (which should never happen in practice given the range of sane values for replication factor, but down casts like this always warrant such checks). However, I suspect just shifting to using `short` all the way back to the configs is probably a better solution.
This shouldn't be possible, right? It wouldn't make much sense to put a topic in the result if it didn't have a corresponding `TopicListing`.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
the test passes without the second poll. the first poll finishes the sync ``` INFO Successfully synced group in generation ``` before the second poll is triggered. The second poll notifies the assignor and gets committed offsets which i don't think is necessary in this test
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
nit: could be useful to log the type of exception in the assertion message.
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
Ah, I see. Thanks for the explanation
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
nit: newline for better IDE debugging.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Very good catch. Thanks, @abbccdda .
Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
It looks like this is not used anywhere
We can use `List<Class<? extends Connector>` to avoid the warning
The number of elements is not always 1. Each created thread-level sensor is added to this queue, e.g., `processLatencySensor`, `pollRecordsSensor`, etc. Check out the callers of `threadLevelSensor()`. Each queue contains all thread-level sensors for one single stream thread.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
You can inline the value of variable `key` here and remove `key`. ```suggestion final String fullSensorName = CLIENT_LEVEL_GROUP + SENSOR_NAME_DELIMITER + sensorName; ```
This is unused too
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
We can use `List<Class<? extends Connector>` to avoid the warning
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
Nit: we typically just say `partition` in these cases. Same for the other `log.debug`.
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
nit: Indicate that this needs shallow iterations on the entries.
@hachikuji OK, I am fine with that. @vahidhashemian Can you remove the `needUpdate` change here? Thanks.
Hmm.. In the consumer, we only fetch metadata if we know we need to, and that is sometimes conveyed only through the `needUpdate` flag (as in the case I mentioned). One additional note: this flag alone is not sufficient to prevent metadata retries if the metadata max age has expired (which will be the case on initialization).
It is to avoid unnecessary connections being made to update metadata e.g. from `Sender` thread which will continue to retry if `needUpdate` is set. The flag will be set again if the application performs another operation (e.g. another send), but we are avoiding automatic retry.
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
Actually, it seems more than one figure needs updating. Makes me wonder if we should actually remove them altogether and let people read the code (which can't go stale) instead.
It's fine to create a new file under the kafka system tests for now. In general though, it's OK to submit PRs against ducktape and advance the version - releasing new versions is a fairly lightweight process, and we pin the kafka system tests to a specific version of ducktape anyway.
I think `file_exists` is duplicated from `test_console_consumer.py` We should probably put this into something like `remoteaccount_utils.py` and reuse
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
This should disappear with my suggestion.
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
Done now, thanks @ijuma
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
same here. let's make all method params as `final`
This debug message seems like it would appear _before_ we actually attempt to do any checking. It's probably worth keeping the old message (or something similar) _after_ the checking has been completed.
Hmmm. If that is the case then at least `kafka-leader-election.sh` won't exist in older versions of Kafka. I think we added it in 2.3.
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
@mjsax What you suggested sounds right to me.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
SGTM. If we find it flooding the logs and not helpful we can reconsider
@guozhangwang Yes, keeping the reference is fine. I was concerned by the check itself because we were calling `generation()` twice in the previous implementation thus we could get two different instances. ``` generation() != Generation.NO_GENERATION && !protocolName.equals(generation().protocolName) ``` I haven't thought about the error-log but it is also a good point.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
missing coverage on: * expired segments * retention time * fetchSession, which doesn't find a session * fetch
nit: ```suggestion private static final String storeName = "InMemorySessionStore"; ```
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
@hachikuji asked you to change the name originally: ```text hachikuji 5 days ago Contributor nit: the name is a little awkward. How about hasRemaining to match ByteBuffer? ``` :)
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
There is no need to test this as it is calling the same method as above.
This line is failing checkstyle.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
Nice, thanks for the update. Looks good
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
If we're just testing broker compatibility I don't think we even need this part of the test.
`topics` defined here and in next test, maybe move up to `init`
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Adding this constructor is quite dangerous because it's almost the same as the one that takes `partition` (the only difference is that one is a `Long` and the other is an `Integer`)
`of` -> `or`
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
As mentioned on the KIP discussion, `BytesDeserializer` should not be included.
nit: remove `this` (not required)
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
nit: log an error and include the relevant info (eg `windowStart` and `inputRecordTimestamp` at least). Same for the IllegalStateException in `processEarly`
Is this the same as in the non-early `process`? Maybe we can factor it out into its own `createRightWindowIfNeeded`(or whatever) method.
Minor typo, `were` -> `where`? Also, `10 <= start time <= 20` might be slightly better notation.
Fixed via https://issues.apache.org/jira/browse/KAFKA-13699
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
Got it. Thanks. The patch LGTM.
Maybe we should move "This is also effectively a cap on ... which may be different from this." to after "This setting will limit ... sending huge requests.". It seems like the latter describes the purpose of the setting while the former is an additional implication.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
Got it. Thanks. The patch LGTM.
I think this should be used in the define method below too.
add `final` twice
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
Updated this when merging.
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
"Should have thrown as producer is already initiailzed"
nit: might be better to set end to another timestamp.
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
and -> a
I'm not sure why this needs to be a field. It looks like it is used to measure latency, but then why not just use a local variable around the code that is being timed?, i.e, ``` final long start = time.milliseconds() // do some computations computeLatency(start) ``` Also, the name doesn't seem correct as it isn't really the current time. It is more like `timerStartedMs`
seeing as you are restricting visibility (thanks!) this one could be private
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
ditto on removing before/after.
nit: remove empty line
Ditto on removing these before/after methods.
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
I think we need to insert a `fail` here to fix this test
We should improve this test by moving this line outside/before `try`
This seems like it's a kind of weird restriction. I guess it'd be odd to use the same name for an internal topic and another, though if you know its going to be prefixed it might not be great to not be able to use that same name.
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
`instantiateConfigProviders` since this is potentially creating multiple providers
Yeah, that's fair. Just thought I'd mention it since it stood out while reviewing the patch.
It seems we never use the return value here? I also wonder if we should just remove the `connector` argument. This is only invoked from 2 places and I don't see a good reason to have used the list of connectors provided in the rebalance callback, which is the only place we don't use the `stopConnectors()` variant. We might want to validate the set in the callback and complain in a log if something looks wrong, but we always want to shutdown all connectors that are actively running. It would also mean we wouldn't need to check whether the connector exists in `stopConnector(String connName)`.
The return value is fine, I just noticed that it wasn't used, which made it feel unnecessary. Doesn't really matter either way as it can be added/removed easily. Re: `stopConnectors()`, rebalances will always revoke the entire assignment (at least as things stand today), so the two are equivalent. To be honest, this one is a bit up in the air anyway -- today we rely on the entire assignment being revoked, but in the future we may do a sort of incremental rebalancing. In that case, it isn't clear whether we would be able to use the callback to notify the listener exactly which ones are _actually_ being revoked or if that's too application-specific.
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
nit: double space
I'd suggest update the existing test directly in this PR.
`topics` defined here and in next test, maybe move up to `init`
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
nit: Please fix code style.
nit: Please fix code style.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
We should verify the actual timestamp.
I think it is probably worth adding as even if it is deprecated it is still supported
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
nit: not a big deal here, but for unit tests I think given the very low overhead it is better to separate out each of the cases into their own test as it can help make it more quickly obvious if issues are with a specific case or if it affects multiple cases.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
`HeaderConverter` and this method don't exist prior to AK 1.1.
This method is called from within `newConverter`, `newHeaderConverter`, and `newConfigProvider`, so mentioning "converters" and getting the available converter implementation classes is actually wrong when used to find the config provider implementation class. One way to address that would be to pass in additional parameters to the method, but since we want to backport this to branches before `2.0` we have to make that work without method references. So one simple option is to rename the method to `converterClassFromConfig` and add a new method for `configProviderClassFromConfig` that does essentially the same thing but is tailored for config providers. Perhaps a better alternative is to dynamically determine the name and the `pluginNames(...)` based upon whether `klass` is a subtype of `Converter` or `ConfigProvider`. This keeps a single method, but is a bit more dynamic.
Map.Entry<String, String> to avoid the check below
nit: add `final` -- same next line
Ouch! Sorry about that!
```suggestion final OffsetCheckpoint checkpoint = new OffsetCheckpoint( new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME)); ```
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Can we think of a better name for this? `pairs` doesn't really tell me anything.
nit: line too long
nit: single parameter per line
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
`Count is a {@link SampledStat} that maintains a simple count of what it has seen.` So with this stat, its value will be increased for the window period, then suddenly drops to zero, then start rising again. So it's hard to alert on such a metric, on the other hand `Rate(Count())` will record the average rate per time unit (here second), so users in practice can easily set a threshold for alerting, and even if they want "zero tolerance", setting the threshold to be 0 can still satisfy their needs.
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
This will probably be subjective, but I'm ok with "hop" for now.
Remove double blank.
Remove double blank.
either move `this` to next line, or fix indention.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
nit: avoid unnecessary `this.` prefix
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
Nit: add `final`
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
Though now I look at the message for `UINT16` I see it would be consistent with that. Still I think because there are two types involved here, the Java type and the network type, including both is clearest.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
even clearer: "Represents a signed integer"
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
Technically, this is `numDrainedRecords`.
Also, maybe we should assert that `numExceptionReceivedInCallback.get() > 0` if we expect at least one to fail (in theory, if `numSentRecords == 100`, there would be no exceptionReceivedInCallback).
Maybe we can have a numRecords variable for the `100`.
this problem also affect replication node fetch process? our server occur a error: [2019-04-05 23:59:46,084] WARN [ReplicaFetcherThread-1-1], Error in fetch kafka.server.ReplicaFetcherThread$FetchRequest@b22a64c (kafka.server.ReplicaFetcherThread) java.io.IOException: Connection to 1 was disconnected before the response was read but return to normal after a reboot kafka server.
This is a change in semantics, right? Before, we would never expire a connection that has been processed during the `poll` because we use `currentTimeNanos`. After this change, if something in `poll` takes long enough, we could end up expiring a connection in `completedReceived` for example.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Please avoid raw types, even when you don't need the type bound. ```suggestion for (final RankedClient<ID> rankedClient : rankedClients) { ```
I see what you mean. I do not have any heart feelings here. Would be interesting to see in experiments how the two approaches differ.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
Again, a bit more information would be more useful: ```suggestion // Then if we delete the connector, it and each of its tasks should be stopped by the framework // even though the producer is blocked because there is no topic ```
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
I think we can simplify this test. We only want to make sure the application directory gets deleted. Thus, we can create it manually, create the driver with corresponding config (passing in a empty Topology), call `close()` and check if the directory was deleted.
It reminds me that sometimes I did the same thing of piggy-backing re-orging in the code base with actual changes that makes the reviewers life harder :P All of us should be careful in the future. That being said, for this PR since Boyang and I have bite it already, I think we can save reverting the piggy-backed reordering and just merge as this.
Thanks. Should we instead make `referenceContainer` a field and access its members to get the references? The purpose of `AssignorConfiguration` is to parse the configs, not to be a general container for the configured values. Otherwise, we wouldn't need to assign any of the other fields here.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Thanks for cleaning up the code duplication.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
I think the requirement should be stronger, we should check the subject has a KerberosPrincipal to skip the kerberos login. Else we can run into a situation that there is an alternate login context that has nothing to do with Kerberos and we'll fail.
we are not using getters and setters. I think this should be named as serviceName
We already have a `serviceName()` method that returns the `serviceName` variable. This method is different as it's getting the values from the config or JAAS file. I think it's useful to use a different name. I used the `get` prefix, but we could use `find` or some other prefix.
This block can be moved outside of the `try-catch-block`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
nit: add `final`
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Nice, thanks for the update. Looks good
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
I think we may be able to get rid of some of the `bytesRead` bookkeeping. As far as I can tell, it is only used in the exception message below and it seems redundant there (i.e. sizeOfBodyInBytes - bytesRemaining = bytesRead).
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
It is a shame we have to do it like this, but i don't see a viable alternative
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
Re: your concern, I don't think we can assume that a user's state store's `init` method is idempotent. AFAIK nothing should change that's relevant to the state store registration, but if something does (eg TaskCorrupted) we'd have to wipe out everything and start it all again anyways
+1, we can rely on `storeManager#getStore` inside `StateManagerUtil` to check if the store is already registered.
Nah, I think we should actually keep this (although `IllegalStateException` seems to make more sense, can we change it?) -- we should just make sure we don't reach it
Yes, I had misread the code originally.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
I fixed this one to use the constant before merging.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
nit: `final` (also next line)
Would this be any clearer? ```java OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp); if (offsetRequestSpec == null) { future.completeExceptionally(error.exception()); } else if (shouldRefreshMetadata(error) { retryTopicPartitionOffsets.put(tp, offsetRequestSpec); } else { ... ``` Also, in the case of that we got back an unexpected partition, I think we can raise a new `KafkaException` and provide a clear message indicating what happened.
Currently in this case, we do the following: ``` future.completeExceptionally(error.exception()); ``` I am suggesting we do something like this: ``` future.completeExceptionally(new KafkaException("Unexpected partition in response...."); ```
The slf4j `{}` placeholders will not work here since we are constructing the message ourselves.
The only place we need this duration is ``` duration.negate().addTo(now); ``` `java.time.Duration` has `negated` but its `addTo` is taking a `Temporal`, and all its extends like `LocalTime` etc do not have the right API of `getTime`. So I think we just keep it like this.
@mjsax LGTM Both `kafka/tools/StreamsResetter.java` and `kafka/admin/ConsumerGroupCommand.scala` should be migrated to remove usage of `javax.xml.datatype.*`
In line 126 above, maybe we can just call `e.toString` which will include the message string if it contains? otherwise LGTM
Nit: let's avoid unrelated line additions.
How about keeping this `private` and adding a protected `isCancelled()` method? Currently, the `cancelled` field is encapsulated entirely within the `WorkerTask` class, and modified only via the public `cancel()` method. We can just as easily keep the encapsulation. OTOH, if we were to make `cancelled` protected, we'd lose that encapsulation and make it a bit more complicated if a future developer did want to add logic upon cancellation.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
`hop` vs `advance` is subjective :) I am fine with `hop`, too; it's called a `HoppingWindow` after all. `advance` is just a term that is quite common in literature so I am somewhat used to it.
I would prefer `advanceBy(long advance)`
Remove double blank.
Since we have three threads for this test, there can be multiple rebalances before the streams instance stabilize and start processing...
Why do we need this? This is logged anyway.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
This should not have the `sink` suffix
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
We should not be using mockito internal classes.
nit: unneeded newline
This was probably unintentionally reduced to debug level while fixing conflicts.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Hmm.. Are we safe to remove these given that this is a public API. It's _probably_ unlikely anyone is using them, but still..
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
Seems to fit in one line
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
I was thinking that this should be `Collections.unmodifiableMap(tempProducerDefaultOverrides)` to ensure that it is never mutated after it's assigned.
Nit: maybe we should call this `PRODUCER_DEFAULT_OVERRIDES` and the other one `CONSUMER_DEFAULT_OVERRIDES`.
Do we need to change these signatures from `ConnectorTaskId` to `String`? The `ConnectorTaskId` gives us the ability to define tasks-specific client configuration properties if necessary/desired. I'm afraid that switching to `String` will make it harder and more invasive to add that back in. Plus, if there's not a good reason to remove these, let's leave that for smaller PRs.
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
An alternative might be to let verifiable producer accept just one compression type. Then in your test case, you could create separate instances, each with a different compression type. Seems a little more intuitive that way to me.
Probably not necessary to report consumer startup time in this test? Also `start_consumer` is a misleading name, since it doesn't actually start the consumer :) Also - on line 88, change `self.consumer.stop_node(node)` -> `self.consumer.stop()`
```suggestion log.warn( ```
How about: ```suggestion "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results.", ```
If all we're doing here is instantiating a map and then placing a single property into it, is there any reason to do so in a separate `beforeAll()` method instead of moving everything into the `setup()` method? Additionally, if that's the case, we can make `pluginProps` a local variable instead of an instance variable.
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
It was removed from the other versions of `group` but not from here.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
nit: extra line
Sounds good. I think this is convincing :)
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
This was not addressed yet. the whole sentence can be removed
There is no `CoGroupedStream#reduce()` -- we can remove this
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Yeah, I was thinking it was fine to remove it since users should not create an instance of this apart from tests and it's been deprecated for a few release cycles. Or if we think the method is useful, then it would make sense undeprecate it.
`long` -> `Long` is a binary incompatible change.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
Same here, this exception message does not apply to the case this is trying to catch
ditto here, please add a separate check and exception
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
Why call it `now` and not `then` (or `previous`); `time.milliseconds()` give the current timestmap, ie, "now"
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
Nit: can you clean up the code further and add `{ }` (we prefer to use `{}` for all blocks. Thanks.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
Oh yeah, duh. Nevermind this 
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
super nit: extra blank line
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
nit: maybe utility methods can be moved to the bottom of the class.
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
add `final` twice
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)
I suggest only catching `KafkaException` and only close the client then. About other exceptions, we can probably try retrying or fail the thread, I don't have a strong preference. I don't think we should catch Throwable though, as that catches errors like `OutOfMemoryError`
I think that's fine, I don't think there's a way to recover (nor if it makes sense) from an OutOfMemoryError - https://stackoverflow.com/a/352842
Yes it is. I was asking whether we might want to alter it for the default case. I think it's fine to keep it as it is
Same here, I think `computeIfAbsent` would simplify the logic
nit: could use `computeIfAbsent` to simplify this
nit: ditto here, `cluster()` will reconstruct a new object on each call.
Should we consider including the error message of `e` in to the exception as well? also nit: capitalized `Fatal`.
Not sure what has changed here.
prop: I would use `taskId01.toString()` here, since you are not testing the `taskId01.toString()` method. Our assumption is that the folder has a name that is equal to the result of `taskId01.toString()` and not `0_1`.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
nit: line too long
nit: break line
We need to choose at least a live replica.
In the ZK based code, we also take live brokers into consideration when selecting a new leader.
We are generating an UnregisterBrokerRecord.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
same... `@link` > `@close` for this case.
I would use the full qualified name of `Filter` and `Cache`. If anybody wants to look them up it is easier. Additionally, I would enclose it into `{@code ...}`.
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
This is not related to change in this PR, but after second read, should we setup non-empty topology here. to prevent IllegalStateException to be thrown due to empty topology after some future change. ```suggestion testDriver = new TopologyTestDriver(setupSourceSinkTopology(), config); ```
Nit: this is really primarily where the configuration properties go, plus any optional config provider properties. How about: ```suggestion * @param originals the configuration properties plus any optional config provider properties; may not be null ```
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
It seems like we ought to just define `log` at the AbstractTask level and avoid having two almost identical `maybeInitTaskTimeoutOrThrow` method definitions.
This is the wrong format for this log message. The exception won't be logged. You have to format the string first: ```suggestion log.debug( String.format("Timeout exception. Remaining time to deadline %d; retrying.", deadlineMs - currentWallClockMs), timeoutException ); ```
nit: use `{}` instead of string concat for `retries`
\cc @lindong28 -- seem's you forgot to update this when dumping the version in `1.1` branch.
Why are these in a separate block when compared to `offsets-for-times-supported`, etc.
`topics` defined here and in next test, maybe move up to `init`
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
Are you going to update the other two to remove unnecessary operations? That's all that's left and then I can merge the PR. Thanks!
You made this stricter (+100 instead of +1000), I'd not do that given Jenkins variability.
Nitpick: `assertTrue` can be used instead of `assertEquals`.
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
we need to shutdown the executor service even if there is a test failure (either in a finally block or the teardown).
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Also, maybe we should assert that `numExceptionReceivedInCallback.get() > 0` if we expect at least one to fail (in theory, if `numSentRecords == 100`, there would be no exceptionReceivedInCallback).
ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
an -> a
method name changes
an -> a
Sensor names don't appear in JMX.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
can you please also check that the partition id gets set to -1
nit: I'm wondering if just using would suffice (IMHO slightly easier to immediately grok the meaning). ```java if (!cachingEnable) { context.forward(key, new Change<>(newValue, oldValue)); } ```
I think, we should first check for this condition, because we should only check the most inner store -- if an wrapping store would (be mistake) implement `TimestampedBytesStore`, we would return `true` even if the most inner store does not -- this would be incorrect.
> Why does it do this? Maybe it's a translation layer for other stores? In which case, is it correct for Streams to second-guess the implementation and break its own contract by ignoring the marker interface and delivering non-timestamped binary data? I don't think that would work. Note, on restore, we always get the most inner store and would not call this "translation layer wrapper store" (and thus it would break as we would insert our converter and hand timestamped-bytes to the store that does not understand them). If one want to implement a translation wrapper like this, she need to "hide" it from Kafka Streams and not implement `WrappingStore` (ie, the translation wrapper must be the most inner store).
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
Let's capitalize the log statements for consistency. There are a few of these.
Changed it locally.
It doesn't seem that the client needs principalBuilder.
That's right, changed it locally.
nit: `child` -> `toChild`
nit: remove (was tested already)
not used: can be removed
an -> a
nit: missing `<p>` for new paragraph
records to it, and reading all records from it, such that
I think we could reduce the change of this PR by reverting the numbering change which seems unnecessary.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
You are right. Never mind.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
I see. Guess it's good as-is.
This is unnecessary as junit always create a new test class for each test case.
Heh, I actually just looked it up, because I was surprised all those other `<p>` elements were not closed... Apparently, even in HTML, you don't _have_ to close `<p>` elements: > Paragraphs are block-level elements, and notably will automatically close if another block-level element is parsed before the closing \</p\> tag. > https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p Sometimes, you just have to stop and marvel at HTML...
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
Isn't this the same as: ``` tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue); ```
nit: `this method will....`
line/sentence formatting `{@code null}`.
nit: parameter/line formatting
```suggestion * @throws ConnectException if the configuration fails to be serialized or if the request could not be sent ```
Java doc for lifecycleListener.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Thanks for cleaning up the code duplication.
nit: it is a tad vexing to see all the `context` prefixes. I guess another option might be to define `RaftClientTestContext` as an abstract class so that the test method can define the test behavior within the scope of a subclass. For example: ```java new RaftClientTestContext(builder) { void run() { assertTrue(client.isShuttingDown()); ... } } ``` Not required, just an alternative to consider.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Hmm, but we're not actually calling the listener here. We do that separately.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
the condition is always false if you don't add brackets. ``` throw new IllegalArgumentException("Topic pattern to subscribe to cannot be " + (pattern == null ? "null" : "empty")); ```
Discussed offline with Becket - rework this patch to avoid the null checks elsewhere; i.e., make the accumulator more explicitly aware of the `sendInOrder` requirement
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
nit: There is an extra space before `=` here and below. I am not a huge fan of using `TestUtils.fieldValue`. Did you consider making both attributes package private or something like this instead? `ApiVersions` is passed to the constructor of `NetworkClient` so we could access it this way as well.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: use `"table-source"` ? It's naming a source node, not a processor node.
nit: I think it's better to just print the e.message in a single line.
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
Since we're specifying the key, we expect only to get back windows with the value for that key. The aggregation we specified is to sum all values for the key, and it comes out to `2` because we only write one value for each key; namely, the value is the same number as the key.
This is why the test was failing for you. The query is for a range of window start times, not record times. Since the window size is five minutes, the range `[now - 1 minute, now]` wasn't going to contain the actual window start time of `now - 5 minutes`. In other words, just a simple oversight :/ Sorry for the trouble.
It seemed like a good idea to check a few other query configurations, but none of them showed any problems.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Also: should only call onPartitionsLost on owned partitions that no longer exist
typo: moreq -> more
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
these 2 lines shouldn't be here
need a newline after the group before the underline, and a double newline after the underline
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I think you can do the zookeeper start in the `setUp` method
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
Can `LogManager.getRootLogger().getLevel()` be `null`? With other loggers you return the effective level if that's the case.
Is this the format required by the mbean stuff? It might be nice to return something more structured here.
this _technically_ changes the public interface and would require a KIP if we're being pedantic about the process. I personally think we can go by without a KIP but we obviously need a committer to say what they think
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
nit: "Null value is encoded..." -> "A null value is encoded..."
"encoded on two bytes using network byte order" -> "encoded using two bytes in network byte order"? Similar for the other integer types if you like the suggestion.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
We could get away with a single `*`
If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all.
Ditto here and elsewhere.
I see. Could we do something like this: first assign the partitions for all internal topics as the writing topology's number of tasks, i.e.: ``` // for all internal source topics, // first set the number of partitions to the maximum of the depending sub-topologies source topics for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) { Set<String> internalTopics = entry.getValue().interSourceTopics; for (String internalTopic : internalTopics) { Set<TaskId> tasks = internalSourceTopicToTaskIds.get(internalTopic); if (tasks == null) { int numPartitions = -1; for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> other : topicGroups.entrySet()) { Set<String> otherSinkTopics = other.getValue().sinkTopics; if (otherSinkTopics.contains(internalTopic)) { for (String topic : other.getValue().sourceTopics) { List<PartitionInfo> infos = metadata.partitionsForTopic(topic); if (infos != null && infos.size() > numPartitions) numPartitions = infos.size(); } } } internalSourceTopicToTaskIds.put(internalTopic, Collections.singleton(new TaskId(entry.getKey(), numPartitions))); } } } ``` And then update the Cluster metadata with `Cluster.withPartitions`, then in the `ensureCopartitioning` call, if after the first for-loop, `numPartitions` is still -1, it means all topics in this co-partition group are internal topics, and then in this case read from `metadata.partitionsForTopic` and took the maximum among all of them; and then later after calling `prepareTopic`, the metadata will be updated again with `metadata.withPartitions()`.
Minor typo "will is"
nit: "name" => "joinThisName"
Yeah, that works, too, and is more align with the current code.
AK convention is to not use `set` setters or `get` getters.
Right, so we should just reword Each connector gets their own dead letter queue topic to be more clear, so something like Each connectors dead letter queue is usually written to a different topic. Not super important, but Im just looking to eliminate potential usability issues.
This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.
Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
I think it is probably worth adding as even if it is deprecated it is still supported
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Incase => In the case
You can either make them non-static or pass `Logger` as a parameter. Makes no difference to me, but `log` won't work as a static field when you have multiple instances.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
ditto to `KStreamImpl`
nit: remove `this.`
I was looking at the `append` code and it seems a bit brittle. It assumes that: - The collection returned from `PartitionRecords.take` is safe to mutate even though the latter returns `Collections.emptyList()` if `records == null` - That `part.take` will always return at least one element This is fine today, but it may be worth making it a bit more robust to refactorings.
nit: o2[%s] was **equal** to o1[%s]
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
Please include TopicDeletionDisabledException here.
This exception can't be thrown by DeleteTopics.
AK convention is to not use `set` setters or `get` getters.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
How about `return admin.endOffsets(assignment);`
nit: don't need the type params on the next three lines
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
nit: `anf` -> `and`
It might be simpler to just use `int transactionTimeout` -- Java will auto-cast to long in ``` if (transactionTimeout < commitInterval) { ```
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
Can you explain this a bit further? Why do we return null when the group is stable, I assume stable means everyone is successfully on the same generation? So why return null rather than the actual generation.
I think you should have a separate method `generationIfStable` or something.
How much effort would it be to have a test case for this? We have a few LeaveGroup tests in `ConsumerCoordinatorTest`.
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
`keySerde` -> `valueSerde`
nit: both lines missing . at end
Nit `.` at the end
nit: Please fix code style.
see my question above about using mocks.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
```suggestion "The desired Unix precision for the timestamp. Used to generate the output when type=unix " + ```
`orderInGroup` param is duplicated for key & value converter
nit: fits in one line
This test went from checking for exactly one message, to checking at least one message. If you think that is fine (I dont have the full scope so I dont know) then LGTM! :)
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
Method should not be `final`. Additionally the `final` keyword for method arguments and local variables is not required and does not improve readability of the code here. Indeed Java does not distinguish between readonly and read-write variables. But unless an anonymous class is declared (this requirement is removed after Java 8) or the variable is used further down in the code (improved readability) marking every single readonly variable as final does not make things better IMHO.
nit: config is an overloaded term in the code, you might prefer to name this argument configInfos for instance.
Currently in this file the indentation style used is: ```java protected boolean maybeAddConfigErrors(ConfigInfos config, Callback<Created<ConnectorInfo>> callback) { ``` Still, once we move to single arguments per line it should be: ```java protected boolean maybeAddConfigErrors( ConfigInfos config, Callback<Created<ConnectorInfo>> callback ) { ``` I'd pick one of these. (First I confused `callback` for a local variable)
```suggestion info.userEndPoint(), taskManager().getTaskOffsetSums()) ```
req: Due to this deletion, line 327 becomes a no-op. Please remove it, too.
nit: line 365 above can be moved down now since it is only needed before line 379.
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
Sorry, my bad! It doesn't matter whether it is called or not, since the `IllegalStateException` is thrown.
This test misses the verification whether the `StreamThread#setUncaughtExceptionHandler()` is called. This has been missing also before this PR.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
add `final` (also all other methods below)
would be nice to mark all params final whilst we are changing this
add `final` twice
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
A docstring for this method would be good :)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
You should use try with resources here too.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@mjsax What you suggested sounds right to me.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
How do you feel about dropping `Number` from these names? This would be consistent with the methods we expose from `TransactionManager` itself (e.g. `lastAckedSequence()`).
Hmm.. In fact, there is not necessarily any relation between the partitions that are being consumed and those that are being written. Usually you would expect them not to overlap. I wonder if it actually makes more sense to track these offsets in a separate map.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
SGTM, thanks @mjsax to bring to our attention.
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
typo: we want to test the **case** that poll() returns no records.
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
I would maintain the terminology of "subscribe" to topics and "assigned" partitions, and say: "manually specify the partitions that are assigned to it"
I think `will go` should simply be `go`.
`while` seems to be missing
Pls use caps :) (see your `ErrorReporter` above).
Just thinking about is once more: why do we need to make this interface public? We have `Named` as public method to use `NamedOperation` and other public control objects (`Consumed` etc) that implement it -- but I actually think, users don't need to know about this interface? \cc @fhussonnois @bbejeck @guozhangwang @vvcephei @ableegoldman
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Or https://github.com/google/guava/blob/master/guava/src/com/google/common/math/IntMath.java#L56-L72 It is safe to look as it is Apache License 2.0.
Can you do something like: ```java static final int tableSizeFor(int cap) { int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1); return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } ```
nit: `addMetadata` -> `put`
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
We normally use `assertThat()` in new and refactored code. Please also change the other occurrences. ```suggestion assertThat(hasStateTransition(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING), is(true)); ```
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
`return` is not necessary
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
nit: 'else' can be dropped
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
`start` is not used.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Sensor names don't appear in JMX.
I think it's just a computer-sciencey matter of principle. `clientsByTaskLoad` is a linear collection, so every `offer` would become `O(n)` if we did a `contains` call on it every time. Right now, it's only `O(n)` when we need to remove the prior record for the same client, and `O(log(n))` otherwise. Does it really matter? I'm not sure.
Gah! You're right. We should also _remove_ the client from `uniqueClients` when we `poll`.
Ah, sorry about that @ableegoldman ; I wasn't able (or was too lazy) to follow the `git praise` trail through the class movement. Well, kudos to you, then. :)
I think @becketqin was suggesting that we can add a method to `Kafka` that would cause the thread waiting on `KafkaServer.shutdownLatch` to resume and call `System.exit()` with the appropriate exit status.
If there are multiple FatalExitError thrown, do we want call `System.exit()` only once instead of creating a thread for each of them? Maybe we can just log the error and return.
It should be 'false' by default
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Yeah if it exists elsewhere let's just leave it as is for now.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
Let's use try with resources here and the other test so that the file is closed after it's used.
Ah, nevermind, the topic is in the message's `toString()`.
typo: Woth -> With
Ideally, we'd always use brackets on control flow operators.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: add `final`
nit: add `final`
nit: add `final`
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Shouldn't need this line, it's handled by the superclass's constructor.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Note that Kafka only supports kerberos as the SASL mechanism.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
remove this line
we need to remove this line, too. It is taken car of in `mapValues(ValueMapperWithKey...)`
Ack. Thanks for clarification.
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
This seems to change the behaviour...
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
We should verify the actual timestamp.
nit: remove empty line
nit: add `final` (2x)
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Better name as "setCurrentNodeInProcessorContext"? And then in java docs mention that it returns the processor context with current node set.
nit: add `final`
nit: move to next line ("weird" formatting)
The name `restoreAvailableMemoryOnFailure` is a bit weird because we should always restore available memory on failure. Maybe we can name it `hasError` and set it to `false` right before `return buffer`.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
You are right. Never mind.
`consumer messages` should be `consume messages`
I suggested `alive` to be consistent with the method being called on the consumer. The main reason is that `init` and `start` are a bit too similar and it's a bit difficult to distinguish between the two. Anyway, ok to leave as is if you feel strongly about it.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Ok. Thanks for clarifying.
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
This can be static
This can be static
This can be static
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
```suggestion /** * The value of {@link StreamsConfig#APPLICATION_SERVER_CONFIG} configured for the Streams * client. * * @return {@link HostInfo} corresponding to the Streams client */ ```
I'm not sure of the original motivation behind setting it to `50`. But if we are going to define it as `1000` i think it might be better to remove this and `reconnect_backoff_min_ms_config` definitations from here as they are the same as what is set in the `ProducerConfig` and `ConsumerConfig`, so it seem pretty pointless overriding them
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
Is this `null` assignment needed? Don't see the variable used after this.
This seems unnecessary since we're throwing away the collection anyway.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
I would move line 328 and 329 to before this line.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
I would move line 330 and 331 to before this line.
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
Line too long (also some lines above and further below)
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
nit: it was correct before
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
Yeah, that works, too, and is more align with the current code.
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Thanks for the explanation. A bit subtle as you had said. :)
nit: extract the call to this.interceptors.onConsume(new ConsumerRecords<>(records)) above line 1258 - its result would always be used
While you're here, I suggest fixing `fetchablePartitions` so that it doesn't do two `remove` calls on an `ArrayList` (in the worst case, it has to shift all the elements in the underlying array twice). We could pass a predicate to `subscriptions.fetchablePartitions()` and make it more efficient.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
Nit: maybe this can call the newly introduced `withTransactionalRecords`.
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
Same as above mentioned, the validation didn't get handled in new API.
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
nit: full-stop after the description.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
I've been thinking about this and I wonder if this is the best way to handle this. It feels a bit error-prone to pass a `RecordMetadata` with almost nothing set. Have we considered passing the topic and nullable partition via the `exception` parameter? It's then easy to explain: if an error occurred, the second parameter is set, otherwise the first parameter is set. This would imply having a custom exception type that would include topic, nullable partition and the original exception.
@junrao The user would not have to cast because we would change the type of the second parameter from `Exception` to e.g. `RecordSendException`. This class would have 3 fields: `topic`, `partition` (nullable) and `cause` (original exception). It also means that if we ever need to add additional fields for the error case, it's easy. Wrapping and unwrapping the exception can be a bit annoying, but it's a common pattern in Java (`Future.get` throws an `ExecutionException`, which wraps the original exception for example).
The issue with passing topic/partition through exception is how the user would know which specific exception class to cast to. Also, when there is an error, the topic in general is available. However, partition may not be. So, you would still have the issue of including partial metadata. Passing any available metadata through RecordMetadata seems more natural to me.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
nit: `final` (also next line)
nit: move to line above.
Since this is at the end of runOnce, I'm wondering if it also makes sense to log whether we committed or punctuated, and whether/how many records we polled at the beginning of the method. Basically, it seems like, if it's a good idea to log some information once per cycle, then it's probably a good idea to summarize everything you'd want to know.
Is it important to make the distinction between "Processed zero records, going to poll again" and "Finished processing 0 records, going to poll again"? Also if we are not saying its processed anything maybe we should log what it is pulling from
Maybe we do this else where but can we log at DEBUG if there is nothing done and there is tasks assigned to the thread? It might be good to be able to confirm that its not getting stuck here or its actually not polling anything.
(Tbh that drives me crazy, I once spent like 4 hours debugging something only to realize that I wasn't using the correct TimeoutException  )
nit: move the import to the other `o.a.k.*` imports
Nit: move these two static factory methods above the non-static member variables, so all static and non-static members are together.
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
Not sure if we need this? If the shutdown is not clean, we logged an ERROR before in `StreamThread#run()`
nit: `clean flag` sound like an implementation detail, that may not be a good way to phrase it for an INFO log...
Ditto above: This is the only case where we pass in the parameter as `false`, but I think this check is not necessary as long as the state is guaranteed to be in `pending shutdown`.
Not really sure this has value if the test case expects the leader change correctly.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
Can this be a `byte`.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
the method name changed to `windowedTable` and `windowSize` parameter is missing
Since this is a general `toString()`, we probably should handle manual assignment as well.
You don't want to pay the cost for the computation of the hashCode if it's never used. I still think it would be better to make the hashCode very cheap by just using the `id`.
Nit: why reuse `appId` -- if there is a bug, and we set `APPLICATION_ID_CONFIG` by mistake, it would not be detected. Maybe better to set a different id.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
extra new line.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
Not sure what has changed here.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: break line
nit: line too long
This was the checkstyle error that was failing your build.
More explanatory error, as discussed.
Dropped this unnecessary duplicate code, as we discussed.
Existing issue, space should be after the colon.
We should deprecate this one too I believe.
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
nit: 4-space indention plus move `builder` down one line
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
prop: We could do ``` committed += taskManager.commitAllStandby(); if (committed > 0) ``` for the rest of the logic so that we don't need a separate `if (committed > 0)` later
Fine with me to keep the guard. Was just double checking.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Oh, nevermind. I didn't see the following line.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
adding the registraiton part into Config file doesn't look good to me. Generally speaking Config classes are only there to express the config members and validation around them but not to take any action. SecurityUtils.java would be a better place.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
ditto for the rest of the test
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
typo: `will does not`
Nit: remove empty line ;)
Seems the `fastTimeout` should be thread-safe since it will be accessed by both RequestSendThread and controller event thread. Perhaps `volatile` is a least requirement for memory visibility.
the test passes without the second poll. the first poll finishes the sync ``` INFO Successfully synced group in generation ``` before the second poll is triggered. The second poll notifies the assignor and gets committed offsets which i don't think is necessary in this test
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Instead of using `.format` and `+` to create the string, maybe use same way as `cmd` is constructed (using % to format, and multiline string without `+` but by ending with `\`)
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
I think we ditch the before/after methods as I previously recommended.
I wonder if we want to print the actual list of partitions here, which might be long. And do it twice. I see the same pattern is applied elsewhere. I understand the value of explicit listing.
nit: move below the shortcut return below.
I was thinking it was odd that this wasn't reusing `flushAndCommitOffsets` since they are basically the same. But I see the exception handling is different, so that makes sense. But then I noticed there's no call to `onCommitCompleted` if there's an exception during the flush, which we do for every other path during commits. _Then_ I realized that this is actually for the special case of closing, and that the callbacks that are invoked by `commitOffsets` in this special case are weird since the seqno will never match and it is probably always getting logged as an "error" at debug level. This isn't critical since it's just at debug level, but should we have `onCommitCompleted` check for the sentinel value and ignore the callback in that case? But also, I think the seqno handling is kind of unnecessary now. I think this is a holdover from possibly 2 separate things. First, we previously handled offset commit differently with different threads. Second, the semantics of offset commit in the new consumer were very unclear at the time this code was original developed (that was back when it wasn't even fully implemented and I was trying to sort out what semantics we wanted, so ended up being somewhat defensive in this code). I believe it is the case now that you cannot have callbacks for offset commit come back out of order (despite the fact that they are async) and that since we guard offset commits with a check on whether we are currently committing, we can only end up with multiple because we explicitly allow commits to expire to allow a newer one to be submitted. But will this ever actually help? Because of the way the protocol works, won't the offset commits just get queued up anyway? Would it make sense to adjust this so we have to wait for the commit to finish regardless, but that we might do something like pause processing if it takes too long? Basically, since we don't do it synchronously, we allow processing to continue since it's nice to not have to stall during the commit, but at some point if we couldn't commit, we shouldn't just cancel that commit, we should wait until it actually completes. (And, of course, we also need better handling if the commits repeatedly fail.)
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
Oh, right, I forgot that the metrics registry returns the same copy of the sensor when all the name, description, and tags are the same... Thanks.
nit: I slightly doubt whether these exact match tests are necessary
I would suggest we co-locate the description with metrics and refactor out the sensor creation part, this may help reduce code redundancy.
My minor concern is that KeyFactory and ValueFactory may be only specific to key-value stores, not not any general stores; for example for database stores you may have some functions like ``` withSchema() ``` that defines the data types for each column but not using "withKey / Value" any more. But since it is only for future improvements let's revisit this nested mechanism later.
And if they are not used yet, we can probably remove them for now.
nit: remove empty link
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
nit: remove empty line
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
nit. Add `{ }` to block (we always use them). Same below.
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
@ewencp Actually I'd probably prefer to use separate explicitly named classes. For example: `StartConnectorCallable` and `StopConnectorCallable`. The reuse of the status enum seems a little confusing.
Can this be reverted? The existing formatting appears to be correct. ```suggestion public class ErrorHandlingMetrics { ```
Nit: let's remove this blank line, since it's unrelated to other changes.
Maybe we should have a line for the old consumer as well. Regarding your question about the cluster id, the following line needs to be conditional on `from_kafka_version <= LATEST_0_10_0`: ``` assert self.zk.query("/cluster/id") is None ``` The code is just checking that the cluster id is generated after an upgrade from 0.10.0.x or lower to 0.10.1.x or higher.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
Can we change `subject` to `rockDdStore` -- it's a weird name IMHO.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
the method `restorePartition` is no longer used and can be removed
@mjsax What you suggested sounds right to me.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
This is the same code as in `KTableFilter` -- we should refactor and share code.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
"empty" should be clear enough. Will update it in #10092
If @jeqo has not objections, I think we should remove it for both cases.
just in case, latest consumer-group reset-offset is comparing against `ListOffsetsResponse.UNKNOWN_OFFSET` instead of `null`. https://github.com/apache/kafka/blob/0bc394cc1d19f1e41dd6646e9ac0e09b91fb1398/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L680-L681
I don't fully understand why we check for a `processId` less than or greater than the ones defined above.
`hasItem(task01)` -> `equalTo(Collections.singleton(task01))`
As above: add more "randomness"
`final` is not required here.
I've seen alternative solutions floating around that use a configurable source here. Basically, the configuration passed to configure() is consulted to find the "source cluster", rather than looking at the topic name. That approach lets you return an actual source here, which obviates the new canTrackSource() method etc.
createMetadataTopic() is no longer used.
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Don't need `Vin` here
Don't need `Vin` and `W extends Window` here
original was better
original was better
original was better
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
I think this could be done with `computeIfAbsent` like in "finishSnapshot" above
You might consider using `OptionalDouble`.
(very minor) nit: inconsistent naming of these in this class
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
super nit: extra blank line
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
Use `KafkaException` instead of `RuntimeException`
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
this is the same as above: we should extract to a method to avoid inconsistencies if one might get updated but the other one slips...
We should, and I think it was actually a bug that we did not do that before.
It's not about being trivial or not, but about consistency. Code duplication has the danger to only update one copy of the code...
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
I stand by what i said, but I'll leave it up to you. It isn't a deal breaker for me!
I think this should be four different tests. One for each of the methods you are testing. I probably said this before, but it is much nicer if you can just read the test names to understand what should/shouldn't be happening
Regarding 1) and 2), I think it is not needed, and hence we can use the same `streams` object. 3) fair enough. 4) sounds good, keep it as is then.
maybe: `inputKeySerde` and `inputValSerde`
nit: ".. select the grouping key and the value to be aggregated".
records to it, and reading all records from it, such that
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Same here. We should use `builder.stream().toTable()`
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Yeah, it seems to me like we should remove it.
We can use `ApiResult.completed()`
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
nit: maybe utility methods can be moved to the bottom of the class.
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
Could store `entry.getKey()` in a local variable since it is used several times
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
As mentioned above, we can make this constructor default access.
Yeah, builders are nice for the reasons you mention. However, since it wasn't discussed in KIP-222, I think keeping it package private for now might be better.
Curious why you made this change. Lists are a bit more convenient for users.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
nit: We could revert this change as it does not bring much.
nit: We could revert this change as it does not bring much and re-align like it was before.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
Nit. use `{ }` for all code blocks
nit: parameter/line formatting
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
We already import the class on L26, so let's remove this import.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
Need to check if `group` is `null` in both `k1` and `k2`. Using this on, e.g., `DistributedConfig` from Kafka Connect doesn't currently work.
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
Yes that's correct as this PR stands now. But if we put the name check back to what it was originally, then this line is not needed.
All the callers seems already handles the `name == null` case so this seems unnecessary.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: formatting -> single parameter per line (same next line)
nit: 4-space indention plus move `builder` down one line
nit: avoid unnecessary `this.` prefix
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
Yeah, that would be the closest for primitive types.
Good to know the subtle difference. "acknowledged" sounds good to me.
Why are we using `LinkedList`? It's very rare where it should be used.
`seems existing already but it doesn't` -- this might be confusion. What about: ``` Could not create topic {}. Topic is properly marked for deletion (number of partitions is unknown). Will retry to create this topic in {} ms (to let broker finish async delete operation first). Error message was: {} ```
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Nit: `.` full stop missing.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
It seems that we haven't added the topicId yet.
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception.
Are we missing the ApiException here? I.e. error returned from servers that are not recoverable.
Nit: missing articles & punctuation in this description. "not available in buffer" -> "not available in the buffer". "available currently in buffer else return empty" -> "available currently in the buffer, else return empty"
What is the offset commit is positive and invalid? cc @hachikuji
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
We don't need this for files, right? Just for directories (because of `file.deleteOnExit`)
We should update the Scala `TestUtils` to call this method.
This change is not needed.
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
As above: add more "randomness"
Can't you use `createClient`? This call confused me... (same below)
I believe this is fixed in my next PR.
Do we need to set OP_READ? It seems it's always on.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
should we not just add 10 records in a row (offsets `0...9`) and simulate commit marker at offset 10 via setting endOffset to 11 ? This setup would indicate, that the record at offset 10 is either a non-transactional message (we could have mixed writes) or it's in pending state (neither committed not aborted) and thus endOffset could not be 11L as "last stable offset" is 9 or 10 and endoffset must be smaller than "last stable offset")
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
Sounds good. I just want to avoid someone trying to simplify the tests in the future without understanding that this test is verifying both features work together.
The "Swallowing the exception" is a bit alarming at first. It's true that if this method returns false then `TestUtils.waitForCondition` (which is using this method as the test condition) will fail at line 137. Pretty minor, but how about the following? ``` // Log the exception and return that the partitions were not assigned log.error("Could not check connector state info.", e); return false; ```
extract to variable
typo `direcctly` -> directly
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
I don't think we really need this function any more... we can just submit to the executor from the other function.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
It's a bit better if you move this inside the `try` and remove the `return` from the catch.
I think we're testing `testDir` Occupied here, not `AppDir`.
Is this line intentional? Unit tests normally don't need to print out to console.
This should probably just return a boolean
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
req: this member should be removed.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
Can we just add https://checkstyle.sourceforge.io/config_sizes.html#LineLength so we never have to have this discussion again? :)
Are the `storeType` supposed to have the dash already? If not this should be `storeType + "-" + STORE_ID_TAG`.
In `threadLevelTagMap(String...)` there is a check: ``` if (tags != null) { if ((tags.length % 2) != 0) { throw new IllegalArgumentException("Tags needs to be specified in key-value pairs"); } ``` Should we do the same here
Seems this duplicates `L733`. Might be good to extract into a small helper method.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
Should we be checking for null here? It's probably OK as this is only called from `abortIncompleteBatches`, but I thought I'd ask.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
That would work for me, but I'm OK with leaving as is as well.
Right, so we should just reword Each connector gets their own dead letter queue topic to be more clear, so something like Each connectors dead letter queue is usually written to a different topic. Not super important, but Im just looking to eliminate potential usability issues.
Nit: reword to avoid "log" being ambiguous as verb or noun: "Writes errors and their context to application logs."
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
nit: might be helpful adding a little helper since we do this a few times in here
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
nit: extra newline here
Is this necessary? The leader epoch is -1 by default.
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
Typo: should be "or larger than the number of available brokers"
Please include TopicDeletionDisabledException here.
We do not throw `InvalidTopicException` "if [the topic] is not found"
Also, are we fine with the config logging being bumped up to `info` (which is what `logAll` does) vs `debug` (which is what it was here).
This could be `ConfigException` instead. With the `ConnectException` hierarchy we get a little bit stuck because we can't have `ConnectException` be the base of all our exceptions and not redefine some. There are some from core Kafka that are fine if triggered from within the framework, and `ConfigException` falls under that category -- the biggest value aside from standardization that asking for `ConnectException`/`RetriableException` from connectors/tasks is that it is a signal for how to handle the exception, but the runtime code doesn't require that everything inherits from those (and in fact we handle other exceptions from connectors as well, we just would prefer that the connector think about how it wants the framework to handle possible errors).
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
nit: `testAggregateRandomInput` to match up with other test names
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
Nit: Maybe `Use {@link #localThreadsMetadata()} to retrieve runtime information.`
We should mention it gets deprecated by two functions: 1) to get the static full topology description, one should use `Topology#describe()`; 2) to get the runtime thread metadata, one should use `#localThreadsMetadata()`;
I meant to have all "singulars" for consistency, i.e * Creates a * Starts the * Shuts down this * Does a clean up I'm OK with imperative style.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
Yeah. Even if it's "compatible" and does not break anything, it's still fall into the "public api change" category...
"or null if this was not redirected"
What is the value in separating the `set` and `done` methods? The one place where I can see that they are separated by some other code it looks like moving the `set` next to the `done` doesn't affect the tests at all.
Thanks! Will push this shortly.
Yeah it makes sense. Sorry for not getting back to you sooner
It seems relevant to the implementer whether the configs have been validated or not. If they're not guaranteed by the contract to have been validated then the implementer might need to duplicate some of the validation logic in this method.
Nit: using a new paragraph makes this stand out more. ```suggestion * <p>For backwards compatibility, the default implementation will return {@code null}, but connector developers are ```
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
ditto for the rest of the test
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
I think the name of the function is better defined as `interleaveTasksByConsumers`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
My bad, mis read the lines :)
By catching Throwable, aren't we silencing the error and not immediately failing the test? Also we should not be catching Throwable, if the JVM throws an Error we want it to fail the test and never want to handle it. There're a bunch of them in these files
Just let the Exception flow, that will automatically fail the test
What do we do if there's an exception? If it's expected, let's make it clear
Yes, that is the reason for the inconsistency. Obviously it was a mistake to let `OffsetAndMetadata` be serializable in the first place.
nit: I think parenthesis are a little more conventional.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
What you had is fine.
Sorry for not catching this earlier, shall we try to be consistent when it comes to `toString` output? Here's an example of a recent Java class: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClientResponse.java#L67 What do you think? Also, it would be good to include an example of the logging output.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
nit: add a size? There are a few cases in here where we could do this.
Typo, should be: "an ever-updating" counting table"
Typo: "new instance of [an] ever-updating KTable" Also, we do we say "ever-updating"? I'd remove it. Also, in other parts of the docs we do not mention "ever-updating" when we talk about KTable (see e.g. the `aggregateByKey` variant right above this one).
Ah, I see the difference: I suppose "ever-updating" refers to this method returning a _non-windowed_ KTable? If that's right, then we should perhaps clarify this a bit in the docstring (it's easy to miss -- these methods all have similar names).
nit: use `assertThat` instead
nit: I would remove this line
nit: This last check is not needed, since it verifies functionality of the `Map` returned by `Collections.unmodifiableMap()` and not of the code under test.
We can remove this field now that it's unused
It seems ```exitProcedure``` and ```haltProcedure``` can be local variable
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
nit: move this `if` statement below/above version check on line 62
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
nit: seems these could be final? Same in `ConsumerUserData`.
Let's use `Map` on the left side instead of `HashMap`
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Yeah with KIP-429, during the rebalance some tasks would still be processed and hence records being sent out; so the resuming tasks could be in RUNNING state as well.
I think you need this line before line 32 to have the imports lexicographically sorted.
req: this member should be removed.
This needs to be updated with the new constructor that accepts two arguments.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
Nah, I think we should actually keep this (although `IllegalStateException` seems to make more sense, can we change it?) -- we should just make sure we don't reach it
+1, we can rely on `storeManager#getStore` inside `StateManagerUtil` to check if the store is already registered.
Re: your concern, I don't think we can assume that a user's state store's `init` method is idempotent. AFAIK nothing should change that's relevant to the state store registration, but if something does (eg TaskCorrupted) we'd have to wipe out everything and start it all again anyways
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Typo: "you can create [a] windowed ..."
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
nit: remove empty line
Can we unify the `dataLength` computation into a single place? It's a little bit scattered at hard to follow. Also with different versions, I like a more explicit pattern using switch: ``` switch (version) { case 0: encodeV1(); break; case 1: encodeV2(); break; default: throw... } ```
nit: maybe use `finally` to restore this value.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
Hmm.. The function is aimed to be a public API in `Topology` in KIP-120 (which I think will be renamed from the current `TopologyBuilder`?), but here it is only used as the function of the `InternalTopologyBuilder` which seems incorrect.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
prop: abortTransaction can also throw ProducerFenced.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
Same here, for verifying the thrown cause
We should also verify the thrown cause
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
Actually I think #2456 does not update the printed html for marking these as deprecated, so we still need to rebase + merge this PR after that one is merged.
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
Should be two tests instead? (Leave it up to you to decide -- don't insist on a change)
nit: add `final` to parameters
Consider naming the topic "topic2" since there are only two topics in the test
Ok, let's just keep it in our back pocket for now.
We could detect if the processorTopology contains only `StaticTopicNameExtractors` and still throw in that case if the topic name isn't in the topology.
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
Hmm, maybe even better: ``` java long startWaitNs = time.nanoseconds(); long timeNs; boolean waitingTimeElapsed; try { waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS); } catch (InterruptedException e) { this.waiters.remove(moreMemory); throw e; } finally { long endWaitNs = time.nanoseconds(); timeNs = Math.max(0L, endWaitNs - startWaitNs); this.waitTime.record(timeNs, time.milliseconds()); } ```
The name `restoreAvailableMemoryOnFailure` is a bit weird because we should always restore available memory on failure. Maybe we can name it `hasError` and set it to `false` right before `return buffer`.
We can probably remove this line `restoreAvailableMemoryOnFailure = false`.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
It's better to keep the parameters aligned (having same indentation)
We may as well use the more specific type `TimeoutException` given the name of the method.
I'm wondering if it still makes sense to consider metadata timeout and request timeout to determine the timeout for `selector.poll`, but maybe it can be discussed in another PR..
We probably want all these `Long`s to be `long`s.
Ah, you're right. I misread the second check.
Not really sure this has value if the test case expects the leader change correctly.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Nit: If we do fix up the above example of this it makes sense to fix this up too.
Personally I'd prefer to have decodeTasksData in which we hard-code the logic of doing both prevTasks and standbyTasks, we do not code-share for these two task sets but we share code of constructing the set for version two and version three. I guess we cannot get both code sharing, and since it is really a nit I'm fine either way :)
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
Yeah, we're still feeling out the best patterns for handling older versions.
I would say it's important _not_ to be able to create bogus requests. ;) We can introduce specific mechanisms for testing, but a public constructor for a request should do its own validation.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
'valueInZK' is poor name here - it's the name of the wildcard-suffixed pattern. 'intput' is actually a concrete resource name. `valueInZK` won't have the '*' suffix, (as discussed on the thread). - update the java doc accordingly too. Personally, I see a lot of scope for this being called in code where it shouldn't, (as it currently is). A more defensive API might be for the signature to be: ``` public static boolean matchWildcardSuffixed(Resource wildcard, Resource concrete) public static boolean matchWildcardSuffixed(ResourceFilter wildcard, Resource concrete) ``` And then to have the method throw if the wrong types of `wildcard` and `concrete` are passed in, i.e. `wildcard` should always have type `WILDCARD_SUFFIXED` and `concrete` should always to `LITERAL`
Hey @junrao, It was discussed on the dev channel that we shouldn't store or pass around the '*' the end as this then requires places to validate the resource name ends in a '*' on API calls and when loading from ZK. Also, with the rebrand of this from 'wildcard-suffixed' to simply 'prefixed' then I think we can drop the '*' completely. e.g. the user would add an ACL to any topic resource the has the prefix 'foo'. Look mum, no asterisks! This also helps separate this from the current 'wildcard' support i.e. '*'.
Ok. Then the KIP wiki needs to be updated.
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Fair enough, let's just leave it as is then. Thanks for the explanation.
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
nit: could be useful to log the type of exception in the assertion message.
nit: remove empty line
nit: add `final` (2x)
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Is this used anywhere? I see we have changed client code to use the other C'tor.
Nice catch. And since we're here, why not make these `private`, too? They're currently not used outside of this class.
Nice tidy up of this test class :-)
Fair enough. Let's leave it as-is.
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
nit: remove (was tested already)
At least we try to not abuse Thread.sleep() ;-)
check `source != null` not necessary. In doubt add an assertion.
I'd also consider extracting: `source.getTimestampExtractor() != null ? ...` into a local as the line is quite long and it will make the code a bit easier to read.
add line: `TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();` and change to `RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
Do we need to set OP_READ? It seems it's always on.
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
I think we want to make the string computation lazy, but not lose info. We could do that by overriding `toString`.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Yeah I know that :) I was referring to the one with `SUM` not `COUNT` too but I copied the wrong name. We already have a public `addRateOfSumMetricToSensor` which is only used for rocksDB today, and I was wondering if we can leverage that.
nit ```suggestion expectedSensor, StreamsMetricsImpl.CACHE_LEVEL_GROUP, tagMap, hitRatio, HIT_RATIO_AVG_DESCRIPTION, HIT_RATIO_MIN_DESCRIPTION, HIT_RATIO_MAX_DESCRIPTION ); ```
It was removed from the other versions of `group` but not from here.
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
We really need a bug fix release for this! \cc @guozhangwang
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
nit: "active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
@ijuma Sorry, I don't know of a standard way of doing this,
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This is not necessary, since the for loop below would be a no-op.
```suggestion * A byte array comparator based on lexicographic ordering. ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I could not find any unit tests for this method.
Maybe we should add one case where `position > 0`.
It would be good to verify that the buffer contents are correct as well.
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
nit: remove empty line
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
We just need to make sure the extracted generation from all three processors are the same.
is it important to have 3 brokers for this test? I'm wondering if the tests would be more resilient and faster with just one broker and replica of each topic.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Ok. Thanks for clarifying.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Shouldn't you verify if topology 1 still produces output records at this point? When I read the test name I would expect that verification here.
Nice coverage with different num.partitions, thanks!
nit: insert space `String... expected`
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
The other constructor calls the parameter `sampledStat`. We should be consistent.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
We should use `aasertThrows` and verify the error message similar to the `TransformerSupplier` test
as above -- guess some more below
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
Ah, I see. Thanks for the explanation
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Would it be simpler to check channel.isMuted() instead of channel.isInMutableState()? Then, the latter can be a private method in KafkaChannel.
Selector is shared between client and server. So, it's better not to mention server here.
This can be static
nit: What about extracting the construction in a small helper method `prepareDescribeLogDirsResponse` that create a response for one LogDir and TopicPartition? It seems that the same block of code is used in many tests.
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
i think we should just stick with `joiner` for the name of this param. here and elsewhere
super nit: move method params up one line to start after `join(`
super nit: ditto from above
nit: avoid inserting random blank lines.
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
nit: extra spaces after the `->`
I'm not sure of the original motivation behind setting it to `50`. But if we are going to define it as `1000` i think it might be better to remove this and `reconnect_backoff_min_ms_config` definitations from here as they are the same as what is set in the `ProducerConfig` and `ConsumerConfig`, so it seem pretty pointless overriding them
I think I would say the second sentence as: "This avoids repeatedly sending requests in a tight loop under some failure scenarios." As it's phrased, it could be interpreted as meaning that we would repeatedly fail the same request.
spelling -> recrord -> record
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
I think we'd want to override `parseResponse` for `API_VERSIONS` only.
That alternative looks good to me :)
> it doesn't seem particularly beneficial It seems to me the benefit is the error happens from runtime/test_runtime to build time (generate message code).
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
the type of pollTimeMs is *Duration*. It seems to me that the "ms" is a bit redundant.
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
Cool, yeah that addresses my concern 
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
This could throw a NPE. I think we should guard against this, and throw `ParseException` if NPE happens `ts.split("T")` is redundant and should be extracted into a variable.
nit: add `final`
Should we guard against NPE? (same blow)
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
nit: this can be final
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
Actually, WDYT about adding this class in the "add configs" PR and then rebasing this PR on top of that? Then I could do the same (since I need this class in my next PR as well)
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
Hmm, this sounds to me that the StreamProducer's own `partitionsFor` did not return the num.partitions so we ended up calling `send` with `partition == null`, since otherwise we will get the `partition` as ``` partition = partitioner.partition(topic, key, value, partitions.size()); ``` where `partitioner` is the `StreamsPartitioner` and the producer's own partitioner should not be used.
Thanks for the explanation. Make sense.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
nit: add `final
nit: add `final
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
Like I said, not a big deal and I'll commit as is. But by using entrySet, I didn't mean not to use key at all, just that something like: ``` for (Map.Entry<String, String> entry : configKeys.entrySet()) { String configName = entry.getKey(); Type type = entry.getValue(); ``` would avoid doing the extra lookup and would still let you use the key directly as needed.
I see, I guess `type` in `ConfigDef.convertToString` can be `null` due to this call, but why would we even use `ConfigDef.convertToString` in that case? It seems weird to use something from `ConfigDef` to do conversions when we know there isn't a type or `ConfigKey` associated with the field.
I believe that including the name of the property in the error message is redundant as that information will be available already in the REST response. I also think we may want to be clearer about the error message here. Users can't supply null values, but developers (by specifying `null` as the default value for a property in a `ConfigDef`, for example) definitely can, and we may want to make it clear which variety we're prohibiting. What do you think about this? ```suggestion .map(configEntry -> new ConfigValueInfo(configEntry.getKey(), "The JSON literal `null` may not be used in connector configurations")) ```
We also don't want to accept null addresses, right? I was thinking we could do this: ```java if (address == null || address.equals(NON_ROUTABLE_ADDRESS)) { throw new IllegalArgumentException("Invalid address " + address); } ```
I think we probably want to include both `host` and `url`. Maybe something like: ``` java log.warn("Removing server {} from {} as DNS resolution failed for {}", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host) ```
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
Why did you remove the `return`? Without the `return`, the code will reach the `finally` block and log that the assignment took place.
Why is this an ERROR? If we receive a SHUTDOWN signal, it's just an "eager exit" to not create tasks IMHO. We might want to log a DEBUG though. Would also update the message: ``` log.debug("Skipping task creation in rebalance because we are already in shutdown phase."); ```
```suggestion log.debug("Skipping task creation in rebalance because we are already in {} state.", ``` (minor suggestion)
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
Should be larger
Typo: should be "or larger than the number of available brokers"
What is the offset commit is positive and invalid? cc @hachikuji
Missing newline character.
Hmm. I think we're just following the same pattern used elsewhere, but I think the version check is redundant. We already know it's a valid version because the request constructor verifies it.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
Might be simpler to use the mock deserializer only for values.
nit: 'else' can be dropped
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
`error` is unused
Relatedly, I think there might be some sort of checks in unit tests in maybe the producer or consumer that validate metrics are unregistered, might be able to use a similar approach here.
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
@mjsax Got it. Thanks for your response!
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
req: This is unnecessary
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
@ijuma Sorry, I don't know of a standard way of doing this,
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
line is too long
Nit: add `{}` to block
This is deprecated by scala (scala 2.12, not 0.12 :)). Here's the note from scala doc in 2.12.0, to explain why it is deprecated: > The transparent conversions provided here are considered fragile because they can result in unexpected behavior and performance. > Therefore, this API has been deprecated and JavaConverters should be used instead. I think that time, `CollectionConverters` haven't existed, so suggest to use `JavaConverters` instead. REF: https://www.scala-lang.org/api/2.12.0/scala/collection/JavaConversions$.html
nit: `Arrays.asList` a bit more concise.
We can use `TestUtils.assertFutureThrows()` here too
nit: We could use `TestUtils.assertFutureThrows` here.
Passing in the current listener seems a little weird. I wonder if we're trying a little too hard to reuse the subscribeTopics method in SubscriptionState. If instead we had a method like SubscriptionState.subscribeMatchingTopic (or something like that), then we wouldn't need the flag and we wouldn't need to pass a listener. You could change this line to this: ``` subscription.subscribeMatchingTopics(topicsToSubscribe); metadata.setTopics(topicsToSubscribe) ```
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
nit: ```suggestion = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(new DBOptions(), new ColumnFamilyOptions()); ```
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
Can you elaborate? What do you mean by > otherwise the state won't proceed
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Do we need this extra variable? It seems like `defaultValueBytes` is not used
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
Yeah if it exists elsewhere let's just leave it as is for now.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
That's correct. The implementation becomes a bit tricky, as we can't just use `Arrays.asList` and be done.
To a client of api its preferable if the contract is visible from the api signature. If we want this restriction, I think better to make it visible in the signature.
nit: Update the unit test to use this new method. Jose changed them to use C'tor.
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
You only need to crate that instance once, right? It can be a member of the class
`PrintForEachAction<>` to remove warning. Also in the `print` method
Nit: can be `final`
Nit: can be `final`
Nit: both parameters can be `final`
super nit: extra blank line
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
Same for all such calls.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Same as above: need to check `clientResponse.hasResponse()`
`ClientRequest.destination` is the broker id.
Fair enough. Let's leave it as-is.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
@rajinisivaram The login callback handler class isn't getting its #configure(Map<String, ?>, String, List<AppConfigurationEntry>) invoked. Perhaps it might be better to treat the login callback handler class the same way the client and server callback handler classes are treated, which is to create/configure them in SaslChannelBuilder? Note that the login callback handler class is potentially used both on the client side **and** on the server side (it is used on the broker when the mechanism is the inter-broker protocol).
Hah, I got the last word! Just kidding, fwiw I'm not trying to block this PR on the matter so it's fine by me if you merge as-is. Only wanted to make sure we're being fair to our assignor friends :P
Fine with me (although it does slightly detract from the opt-out possibility). WDYT about adding a retry backoff though? I'm a bit concerned we might just end up stuck in a loop of useless rebalancing, and waiting the full `probing.rebalance.interval` doesn't feel right either
Probably a better alternative to the marker interface is just an abstract assignor class that adds the lag info to the `ClientState` which the HATA and potentially future custom assignors could implement
should this be `&&`? As it is, this loop could terminate even if we always return null and never non-null
we can get rid of the else block here and save some indentation. if we threw an exception then we're out of the function
nit: add `final`
Thanks for the background @rajinisivaram. By the way, I noticed one of the checks is defined inconsistently: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java#L344.
Was the boundary check wrong? You changed `>=` to `>`.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
ditto for the rest of the test
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
This should not have the `sink` suffix
Ack, I get it now. Thanks for clarifying.
same for the store
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
I think this can be written as a for loop over `requests` followed by `requests.clear()`.
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Nit: the same key.. ditto below.
Ditto above. I would recommend having consistent explanations here.
@guozhangwang Yep, sounds good to me.
it treated => it is treated
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
@cmccabe I think you missed this change.
Did you mean: ```suggestion setBrokerId(2). setBrokerEpoch(100). ```
Nit: you can remove `value =`
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Yes, this seems fine then.
assignedTopicPartitions could be updated concurrently and we are accessing it without lock protection here.
It seems that it's more natural for producerManager to create a CompletableFuture and return it through publishMessage() rather than passing in a CompletableFuture from the caller.
Hmm, this callback will be called from Producer's Sender thread and consumerManager.waitTillConsumptionCatchesUp() blocks until the timeout. This will block the Sender thread, which is not ideal.
Maybe we could use a different value here.
You might consider using `OptionalDouble`.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
yes, it seems to be not what this test is checking on. I think we can drop it here.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: add `final`
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
Also, not sure if we can make this easy, but it'd really be ideal if we could keep the logging line as a single line of code without requiring additional logic by the caller. (Maybe even if this requires allocating a special object to do that.)
Does it still make sense to have the if/else here? we log debug anyway and the exception should contain enough information to figure out the type of error.
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
extra new line.
+1 to this
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
remove try-fail-catch and rewrite to ``` final StreamsException expected = assertThrows(StreamsException.class, () -> collector.flush()); assertTrue(expected.getCause() instanceof TimeoutException); assertTrue(expected.getMessage().endsWith(topic1TimeoutHint)); ```
Not 100% sure how `OnSubsequentCall` is meant either. But what you say seems to make sense and thus it should be different test. Thanks for the extra mile splitting them up!
nit: This last check is not needed, since it verifies functionality of the `Map` returned by `Collections.unmodifiableMap()` and not of the code under test.
Also need to change `keyValueStore` method to return `StateStoreSupplier<KeyValueStore>`
Can remove the first two null checks as they are covered in the overloaded `reduce`
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
nit: extra newline here
Ditto here for different exception types.
Thanks for the follow-up.
I think `nanoseconds` should be `milliseconds`. It's a little more idiomatic to do it like this: ```java time.sleep(autoCommitIntervalMs); coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); ```
This test case passes without the fix. It doesn't look like it even goes through the auto-commit path.
The name should mention the fact that this case covers auto-commit.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
This statement is a bit misleading, how about "to the format indicated by the given magic value".
The try/catch is over the whole block, so the message seems a little odd. If this can only happen while reading the headers, we should probably move the try/catch to `getHeaders`. We should also probably rename `geHeader` to `readHeaders` or something like that.
No, I'm talking more about the `encode*` and `castTo*` methods in this class. I'm not suggesting we make them public, but instead I'm asking whether it would make sense to make these *protected* and *non-static* so that it would be easier to create subclasses. I guess the problem with that, though, is that they are implementation details and not part fo the public API. If anybody did create a subclass and we later changed the implementation, their subclass would no longer compile and they'd have problems running on different versions of Connect. Given that, I think it's fine the way it is: private non-static or private static doesn't really matter except for subclasses.
We can inline this like you've done for the byte[] case below
Is this always true? Can't the method be called with `schema` set to null? (The method does check for that condition.)
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
Just wondering if this test is good. Seems like a "RoundRobinAssigner" might compute the exact same assignment. Some more "randomness" would be good IMHO. (Not a must though.)
nit: use static imports to get rid of `Assert.`
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
same here -- sounds like CachingKeyValue with TimestampStore
prop: I find `assertThat()` better readable than `assertEquals()`.
Btw, we should take the chance and make `version` and `commitId` final. Something like: ```java private static final String version; private static final String commitId; static { Properties props = new Properties(); try (InputStream resourceStream = AppInfoParser.class.getResourceAsStream("/kafka/kafka-version.properties")) { props.load(resourceStream); } catch (Exception e) { log.warn("Error while loading kafka-version.properties :" + e.getMessage()); } version = props.getProperty("version", "unknown").trim(); commitId = props.getProperty("commitId", "unknown").trim(); } ```
nit: just simplify to `throws Exception`
nit: add `final` -- same next line
Ouch! Sorry about that!
```suggestion final OffsetCheckpoint checkpoint = new OffsetCheckpoint( new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME)); ```
typo: "partitions that *are* no longer assigned"
I think we should not say that the commit can be retried. I would just say that the rebalance needs to be completed by calling `poll()` and that offsets to commit can be reconsidered after the group is rejoined.
nit: If a user tries to commit during an eager rebalance, the `CommitFailedException` will still be thrown -- we should consider making this explicit, I worry users might read this and think it's ok to commit during an (eager) rebalance as long as they catch the `RetriableCommitFailedException`
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Maybe a little subjective, but I think the test case would be more readable if we list these brokers directly. For example: ```java List<Integer> allBrokers = Arrays.asList(1, 2, 3, 4, 5); List<Integer> brokersToKeepUnfenced = Arrays.asList(1); ```
Perhaps we can use `Uuid.randomUuid`? It's a little weird for all brokers to have the same incarnationId.
nit: it's a small thing, but the assertion failure message is more useful if we use the `Errors` type. ```java assertEquals(Errors.NONE, Errors.forCode(createTopicsResponseData.topics().find("foo").errorCode())); ```
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
@vahidhashemian, yes, that's what I mean.
I think we tend to use `forXxx` rather than `create` for mapping names to enums.
We shouldn't return `null`, but instead return a "unknown query" result.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
Not sure what has changed here.
extra new line.
nit: move below the shortcut return below.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
Maybe we shouldn't say "error" when it's not an error. I'm just imagining the mailing list questions that will start pouring in... ```suggestion log.info("Received version probing code {}", AssignorError.VERSION_PROBING); ```
Nit: remove `this`
nit: break line (way too long)
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
This should say `AdminClient`, not `Consumer`.
Personally, yes, I prefer one call, but I leave it up to you.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
For `all()` function, its returned type should be `KafkaFuture<Void>`; ditto for other two Results as well.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
We could refactor out a helper function here.
nit: maybe worth adding `this == o` to these `equals` implementations. This applies to all of the similar classes in this PR.
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
instead of creating a new set, thoughts on just returning an empty collection? (`Collections.emptyNavigableSet()`)
nit: Provide a message to the `IllegalStateException` constructor
Oh, I see. Well, either way works for me.
Nit: space missing before `timestamp_type`.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Thanks @vvcephei -- that is convincing.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
What value does this test provide? Seems it verifies that `setup()` method is correct? Seems unnecessary to me.
IIUC, `rocksDBMetricsRecordingTriggerThread` is either going to be `null` because it didn't meet the criteria for creating the thread or it is going to be non null because we did create it. Given that, checking for `null` is a nicer way to determine if we need to shutdown because as long as we have a thread, regardless of the condition that created it (e.g. `RecordingLevel.DEBUG`), we should shut it down. This is also safer if the creation conditions ever change and we forget to update them here.
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
maybe instead to check that `rocksDBMetricsRecordingTriggerThread` is `null` instead? Here and elsewhere.
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
The number has changed and 5 is no longer relevant.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
Nit: we don't normally use exclamation marks in Kafka log messages.
We should just throw an "InvalidArgumentException" here.
should we put `kafka` and `numThread` directly into `props` and reduce number of parameters here (would also simplify all the "passing through" code.
nit: additional new line
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
I think this can be simplified a little bit more by getting rid of this variable. How about this? ```java boolean userConfiguredTransactions = this.originalsContainsKey(TRANSACTIONAL_ID_CONFIG); boolean idempotenceEnabled = this.getBoolean(ENABLE_IDEMPOTENCE_CONFIG); if (!idempotenceEnabled && userConfiguredTransactions) { throw new ConfigException("Cannot set a " + ProducerConfig.TRANSACTIONAL_ID_CONFIG + " without also enabling idempotence."); } return idempotenceEnabled;
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Fair enough, let's just leave it as is then. Thanks for the explanation.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
I used `equalToObject()` because it makes the intent more explicit.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
This is a nitpick, but perhaps we should not use `at the same time` in `close` or `configure` because it won't actually happen at the same time (it will most likely happen sequentially, but we don't have to specify that).
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
nit: rename `ivtwks`
nit: add `final`
nit: add `final`
True, will we ever want to have this ability? But the change seems fine to me.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
Is this designed to have thread-level be the parent of the cache-level? I think originally we want to have task-level be the parent of cache-level (but there is a bug for that so it may not actually be the case).
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
I don't see bucketing
a tab is missing here for alignment too.
nit: probably checkstyle won't catch this, but if you format the block, there's an extra tab that you can remove here too.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
another ideaa: add an assertion here? so that we are guaranteed that this method is called.
we can make method this public in `EmbeddedConnectCluster`.
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
not be => not be able to
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
public access? I can see this being accessed by another package too (such as `rest.resources`)
Oh, and a typo which I would like to make KNOWN (or UNKNOWN?! ... I would pick a pun over clarity any day :) )
This doesn't need to be declared outside the loop (it can be final at the assignment).
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Yes, this seems fine then.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
If you pass the new one, then you can probably get rid of `changedTopicId`
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
This is an internal class -- no need to mark as deprecated.
nit: add `final`
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
