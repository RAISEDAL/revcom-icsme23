Nit: insert `<p>` tag to actually get the new paragraph rendered. Nit: `Topology -> `{@link Topology}` It's not really clear what "deterministic" means. We should elaborate more.
`incompatible runtimes and unexpected results` -> `incompatible runtime code and unexpected results or errors.`
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
This line is a bit misleading, as it is referring subject and object to the same class. I'd suggest just remove this line.
nit: {@link KGroupedTable}
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
We should not use wildcard imports. Check your IDE setting to disable this auto-rewrite.
actually, rather than using an enum i think we could use a functional interface something like: ``` interface ProcessSupplierGetter { KStreamAggProcessorSupplier apply(Aggregator) ``` Please come up with a better name! we can then remove the `switch` from `doAggregate` and the enum
Btw, we should take the chance and make `version` and `commitId` final. Something like: ```java private static final String version; private static final String commitId; static { Properties props = new Properties(); try (InputStream resourceStream = AppInfoParser.class.getResourceAsStream("/kafka/kafka-version.properties")) { props.load(resourceStream); } catch (Exception e) { log.warn("Error while loading kafka-version.properties :" + e.getMessage()); } version = props.getProperty("version", "unknown").trim(); commitId = props.getProperty("commitId", "unknown").trim(); } ```
looks like this is not passed to Metrics object. we can use reporter instance at below line.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
Updated this when merging.
`TaskManager#tasks` has to be accessed through the state change thread. It can't be accessed here by incoming requests since there is no lock or synchronization. Probably the easiest thing to do is to have a CreateTasks runnable which does what you want.
nit: move to line above.
If not, we should move the exception capturing logic inside the dbAccessor as well.
nit: 'else' can be dropped
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
deliveryTimeoutMs should be mentioned
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Ditto here, for this test we can still reuse the shared streams.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
Should we use "NO_CURRENT_THREAD" to replace "null"? It seems to me "NO_CURRENT_THREAD" is more readable.
`threadId` is no longer used.
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
This class is public API, so we cannot remove `setTimestamp` but can only deprecate it. We also need to update the KIP to mention the deprecation and the newly added methods.
Since the interrupt status has been cleared by using `Thread.interrupted()` and `InterruptedException` is caught here, the outer `run` call further up the call stack knows nothing about Sender thread had been interrupted and will keep running. An alternative way is not to capture `InterruptedException` and let it be thrown the outer `run`.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think we can.
Hmm. I feel the `final` would be worth capitalizing the var name.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Good point, thanks for clarifying.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
I was going to leave this, but since there's one other change to be made, I think you can just do: ```java recordsLag.add(this.metrics.metricName(name + "-max", ``` And similarly for `avg`. That avoids having to compute `toString` on the topic partition again.
We can use the `replace` overload that takes a `char`.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
@spena just ping to make sure you get this on the follow-up PR.
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
I was using this test to print the topology and it shows two sub topologies while it should be one (seems the reason is that you use the same `StreamsBuilder` as in `setup()` method. Also, the naming of the operators seems to be incorrect. Also wondering if `KGroupedStream#cogroup()` needs on overload that takes a `Named` parameter? Maybe not, but the specified `Named` from `aggregate()` would need to be used for other processors, too. Atm there is this weird `COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test` ``` Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 (topics: [topic]) --> none Sub-topology: 1 Source: KSTREAM-SOURCE-0000000001 (topics: [one]) --> COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> test <-- KSTREAM-SOURCE-0000000001 Processor: test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> KTABLE-TOSTREAM-0000000005 <-- COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: KTABLE-TOSTREAM-0000000005 (stores: []) --> KSTREAM-SINK-0000000006 <-- test Sink: KSTREAM-SINK-0000000006 (topic: output) <-- KTABLE-TOSTREAM-0000000005 ```
nit: why not `k2` ? Should we use `A`, `B`, `C`, `D` for the values to make it easier to understand the expected result? It's unclear which A is which below.
`WithOverlappingKeys` (or `WithSharedKeys`)
only one parameter should be `null` -- otherwise it's unclear what this test actually does
What value does this test provide? Seems it verifies that `setup()` method is correct? Seems unnecessary to me.
We should also mention somewhere that we do not support concurrent transactions.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
```suggestion * Options for {@link Admin#electLeaders(ElectionType, Set, ElectLeadersOptions)}. ```
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
For `all()` function, its returned type should be `KafkaFuture<Void>`; ditto for other two Results as well.
This is a breaking change, right? Same for the other `create` method in this class.
Just want to check my understanding. The user may attempt to commit offsets while the broker has a JoinGroup in purgatory. In this case, we would send the older generation which would be doomed to fail with `ILLEGAL_GENERATION` once the join completes. In this case, should we still reset the generation as we do below? I am wondering if it is useful to remember the generation that an offset commit was sent with (perhaps inside `OffsetCommitCompletion`) so that we only reset if necessary.
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
If the goal is preventing generic "BufferUnderflowExceptions" I think its worth adding. Doesn't need to be this patch if you don't want to though.
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
Detail: just to be sure, I would initialize it to `this.generation`, to make sure the generation always increments.
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
nit: should be removed (similar below)
Changing old version encoding would break our upgrade path, ie, all existing `encodeVersionX()` methods cannot be modified.
I felt this refactoring is a bit messy compared to its benefits, so how about: ``` OutputStream baos = new ByteArrayOutputStream(); // first write the version as byte array directly before switch baos.write(version); // then wrap with compression if it is newer version if (version >= 5) { baos = new GZIPOutputStream(baos) } try (final DataOutputStream out = new DataOutputStream(baos)) { switch (usedVersion) { // ... } } ```
Should require non-null for `fetchPosition`
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
Can you please elaborate why we no longer read the header during construction? It seems to me that `checkHC` could be a constructor parameter and then we could keep it as a private and final variable and less changes would be required. But maybe I am missing something. Note that public and mutable variables are generally avoided in Java.
Both `GZipInputStream` and `SnappyInputStream` read the header in the constructor, so it would make sense to me to remain consistent in that respect.
All these tests could also be parameterized - would be a lot less code, but would likely need reflection to look up the constructor.
My understanding is that Jun is suggesting that we should set `checkHC` to true for the Java client if message format is 1.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
The purpose of the `Map<TopicPartition, Long>` was to avoid adding a new object type. But since we're doing that anyway what about just making the call: ``` public void commit(List<OffsetMetadata> offsets, CommitType type) ``` where now the `OffsetMetadata` class includes the `TopicPartition`? This is arguably no more complex than the original call in the case where you aren't giving metadata. Also does `OffsetMetadata` imply metadata about the offset whereas in fact this is both the offset and metadata? Other options would be `OffsetCommit` or `OffsetInfo` or `PartitionOffset`. Don't have a strong feeling on this one.
@ewencp As you say, it does hinge on whether this `commit` overload is used often or not. If it is not, then having this _and_ having `commitWithMetadata` seems excessive. You guys have a better handle on this, so I'll leave it to you. :)
We're returning the collection directly here which violates the synchronization. Actually there's some inconsistency between `groupSubscription` and `subscription`. The latter is effectively immutable in the sense that we do not update the set once it is created. Perhaps we can do the same for `groupSubscription` which would fix this problem.
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
I think `will go` should simply be `go`.
`while` seems to be missing
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
Add to the end, "as long as they still match the subscribed pattern"
> Just clarifying: After the group has formed, both leader and follower can still trigger a rebalance: leader will trigger a rebalance if the existing topics number of partitions has changed (including the topic is deleted); follower will trigger a rebalance if the subscription has changed (both due to a metadata refresh with regex pattern or user called subscribe again). Is that right? Yes, right. > And if we change the consumer coordinator to allow passing regex to the leader, I think joinedSubscription can be removed completely and only leader need to trigger rebalances unless users call subscribe again on any of the consumer member, is that right? The leader will still have to deal with the potential for a metadata update during a rebalance, so I'm not sure we can remove `joinedSubscription`. At least we won't need this funky logic to try to change `joinedSubscription` after the rebalance though.
I'm not sure this works. The purpose of the `joinedSubscription` field is to remember the exact list of topics that were used when joining the group. If a metadata update arrives after the rebalance has begun, then we can notice the fact that the joined subscription does not match the current subscription and we can trigger another rebalance. With this change, we will no longer be able to detect this case, which means that consumption from a topic matching the subscribed regex will be delayed (perhaps indefinitely). It seems the behavior we want is to only add to `joinedSubscription` those topics which were added to the assignment by the leader.
I'm puzzling a bit over whether we should be using `joinedSubscription` here instead of `subscriptions.subscription()`, or whether it matters. Seems it should be `joinedSubscription`. Suppose that `joinedSubscription` contained only [A] at the time of rebalance. Topic B is then created by the leader and assigned. Before the rebalance completes, the consumer discovers topic B, and `subscription` is updated to [A, B], while `joinedSubscription` is still [A]. The consumer then receives the assignment containing partitions from both A and B, but `addedTopics` will be empty (since B was already added to `subscription`). The consumer will then notice that `joinedSubscription` is [A], while `subscription` is [A, B], and request an unneeded rebalance.
Should be `Optional.of(generation)`. `int` values are not nullable.
Strictly speaking, this shouldn't be necessary as `SCHEMA_TYPE_CLASSES` should have a `Schema` instance for all `Schema.Type` literals. And with `SchemaBuilder` a connector or converter cannot create a schema instance with a null `Schema.Type`. However, it is possible to construct a `ConnectSchema` instance with a null `Type` reference (like what `FakeSchema` essentially does in the existing test), which of course without this change would result in this method returning a null list. So +1 for this line change since it simplifies the error handling in the calling code.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Much better name :)
We lack unit test coverage for this case
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
- It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm. - And we know that KafkaStreams never writes headers into changelog topics. Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case. We can also do a version check as suggested by Guozhang.
I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content. Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
If I understand correct, a record read from the changelog topic should only be: 1) having no headers at all (old version) 2) having a singleton header with `v --> byte`. All other cases should indicate a bug in the code. So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value: 1) byte == 1: do as below. 2) otherwise: to not support forward compatibility, we can just throw unsupported.
It's a little confusing that this is named newValue, but is sometimes actually priorValue
nit: `final` params
not used: can be removed
nit: remove (was tested already)
nit: remove -- not used
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
We avoid using time-based operations in integrations since they usually leads to flaky tests. Consider using `TestUtils.waitForCondition()` with a timeout.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
nit: move .collect to new line
Btw: all those test only set differn window size and grace and thus it seems we can share more code (ie, setting up the topology). Only the window definition is different.
`TimeWindows .of` is deprecated, right? Do we need to test old API? If yes, I think we need a `@SuppressWarning` to make the build pass
Instead of using `.format` and `+` to create the string, maybe use same way as `cmd` is constructed (using % to format, and multiline string without `+` but by ending with `\`)
Unless I'm wrong, we move to this directory and that's where we execute all the rest of the commands (such as the echos in output files below). Just want to make sure this is what we want (which looks like it is)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Shouldn't you verify if topology 1 still produces output records at this point? When I read the test name I would expect that verification here.
Nice coverage with different num.partitions, thanks!
nit: insert space `String... expected`
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
For create / destroy maybe that's okay, but `process` is at the very critical path so we have to be careful not to incur any overhead.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
nit: add `final`
Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.
and -> a
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
as above: we need to remove adminPrefix configs
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: blank line missing here
nit: fits in one line
nit: extra blank line
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
validateStoreOpen() can be outside of lock block.
`validateStoreOpen()` can be outside of lock block.
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
This statement is always false.
as above: we need to remove adminPrefix configs
If we did as I suggested above, then we could make the inverse of this as the loop condition.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
This is still not used
No longer used.
I'd probably just `return error` here (I'm not a fan of `return`, but `break` isn't much better and I assume you wrote it like this to avoid a call to `getSuperclass()` in the very common case where we find the `Errors` on the first attempt). While I am nitpicking: space after `if` is missing.
It may be worth explaining what happens if both a subclass and superclass have a mapping (the subclass mapping is used).
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
Ah, you're right. I misread the second check.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
nit: full-stop after the description.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
Could use `equalsIgnoreCase` directly.
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Ack, I get it now. Thanks for clarifying.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
The other constructor calls the parameter `sampledStat`. We should be consistent.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: use the `SecurityManager` interface
Do we need to check if it is null here? I think it is probably ok if it doesn't throw any exceptions? Obviously it would be better if we could check that `loginManger.release()` was only called on the first invocation, but i appreciate that involves further refactoring
typo: `per reach record`
as above nit: double space `to Kafka`
nit: forward. EDIT: I realized it may be inherited from the overloaded function, we could fix both.
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
Can we add this to all other methods, too? (ie, all that take a supplier, Processor or Transformer?) And maybe also to `ProcessorSupplier#get()` and `TransformerSupplier#get()` -- I think it would be best to provide redundancy :)
The `CachingKeyValueStore` doesn't throw an NPE here, rather `org.apache.kafka.common.utils.Bytes.LexicographicByteArrayComparator.compare` does. We probably should add `Objects.requireNonNull(...)` to `CachingKeyValueStore#put(..)` etc
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
You can remove this `assertThat` and the loop below as you've already proven this is true in the test above. So no need to assert it again.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
Ok, it's your call. I think this might make the tests flaky, but I guess we can figure that out later.
How about also augment the built `Topology` with processor APIs to add more processors, and check that as long as we do not call addStores the resulted topology would still be stateless.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
This might not be robust enough if the streams app decides to store the directory elsewhere right? I'm thinking we need another test as part of this file that checks a stateful topology and confirms the storage is actually there. Then the pair of tests would be more robust to changes.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Maybe just check that `minikdc` is not None here
Does it make sense to set `self.kdc` in the constructor? And then having non-None self.kdc would be part of the logic in `has_sasl_kerberos`
If we're adding all of one list to another list, we should use `extend`, e.g. ```python sasl_mechanisms.extend(self.additional_sasl_mechanisms) ```
nit: two spaces between code and `#`
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Instead of pulling the value out with a regex, what do you think of `streamsString.contains("appId")`. Although what you have works as well.
Is it really worth having this catch here? I think it's best to just let the exception propagate. Any method under test can throw an unknown exception after all.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Could just use `false`
could use `assertFalse`
`joinThisName` is used in the store of `thisWindow` which is to be queried by the other stream, so my personal understanding is that: 1. for inner-join (`rightOuter` = false, `leftOuter` = false): both window-store has `JOINTHIS_NAME-store`. 2. for outer-join (`rightOuter` = true, `leftOuter` = true): both window-store has `OUTERTHIS_NAME-store`. 3. for left-join (`rightOuter` = false, `leftOuter` = true): the left window-store `THIS_NAME-store` and the right window-store `OUTERTHIS_NAME-store`, since we will join with `null` if the right window-store returns null (hence "outer"), but not vice-versa.
Sounds good. We can consider this resolved.
This should not have the `sink` suffix
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
same for the store
Just curious, could we possibly call this function for the same node more than once? It seems yes as you are checking `!keyChangingOperationsToOptimizableRepartitionNodes.containsKey(node)` here, but I cannot tell from the code...
Generally speaking we should not rely on the caller to pass in parameters that are guaranteed to no pass the check. What I suggested (below) is to have a slightly modified recursion pattern which do not rely that the first caller would never satisfy the predicate.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
nit: ```newPosition``` can be created lazy.
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
Adding the exception is fine, but you can just throw it directly: ``` throw new OffsetOutOfRangeException(...)` ``` Not need to assign it to variable first :)
Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
nit: add `final`
Yeah if it exists elsewhere let's just leave it as is for now.
Another reason for having these classes in common (i.e. KAFKA-5265) is that they can potentially be used by the Authorizer interface when we move it to Java.
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
You might consider using `OptionalDouble`.
@vahidhashemian, yes, that's what I mean.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
An alternative might be to let verifiable producer accept just one compression type. Then in your test case, you could create separate instances, each with a different compression type. Seems a little more intuitive that way to me.
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
This is not necessary, since the for loop below would be a no-op.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
nit: remove empty line
This is not necessary, since the for loop below would be a no-op.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
Shouldn't need this line, it's handled by the superclass's constructor.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
Here it is better to use to distinct records, because if the code contains a bug that adds a record twice, you would not discover it.
nit: remove empty line
Add a check to verify, whether the iterator has no more elements.
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
Sorry, you are right! My bad!
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We can use `StringDeserializer.class.getName()`
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
Doing in `@Before` is fine. But we don't need to call `new` each time. `this.props` will be a new empty `Properties` instance anyway -- we don't need the second object.
By the way, I wonder if we should just say it should be idempotent? Seems redundant to mention KafkaProducer.
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
nit: remove extra newlines
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Note this correction
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
We seem to have lost the `info` message from the original.
Nope, up to you. Just thought this might be more readable: ```java maybeInvokePartitionsAssigned(addedPartitions, firstException); ```
nit: "User rebalance callback **threw** an error"
Let's call this "StdoutMonitor" since that makes it more clear what it is doing. We may want to pass things other than status eventually. Also, the instance variable is called `stdoutMonitor`, which suggests that this is a better description.
How about just ``` log.error("{}: (stderr):{}", id, line); ```
Please add the exception at the end of the line so that we can get a stack trace, including any "cause" exceptions. Example: ``` log.error("{}: Failed to start the external process.", id, e); ```
Please include the problem in the errMsg which is used to complete `doneFuture`. For example: ``` errMsg = "Failed to start the external process: " + e.getMessage(); ```
Maybe something like "No command specified" would be more descriptive
No need to reorder imports.
No need to reorder imports.
Nit: let's avoid moving all of these imports around. Our convention is to place the `java` and static imports at the end, and moving them unnecessarily just complicates maintenance.
Can this be reverted? The existing formatting appears to be correct. ```suggestion public class ErrorHandlingMetrics { ```
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
Could we rename this to something like "remainingPartitions"
Won't that result in a dead coordinator and be handled by the first `if` block.
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
`a graph containing`, and correct space between `1. Build`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
nit: remove `this` (not required)
As mentioned on the KIP discussion, `BytesDeserializer` should not be included.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
Maybe we can just a better name for `path` since it makes this code look suspicious.
@hachikuji Yes, that sounds good to me. It should reduce heap usage when there are lots of requests requiring down-conversions while still avoiding blocking the network thread.
This name seems backwards.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
I like this parity check. :+1:
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
Oh you mean `UnsupportedForMessageFormatException`? That doesn't seem to be added.
It seems to be added on line 703.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
why removing this line? this test is used to make sure we can't initialize the `recorder` multiple times with different task id.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
I like this parity check. :+1:
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
cc @mjsax as well, LMK WDYT.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
Since condition is just a comparison, you can put the comparison here directly
This can be moved to ConsumerRecords class
nit: when records2 is empty, you can return immediately.
nit: extract the call to this.interceptors.onConsume(new ConsumerRecords<>(records)) above line 1258 - its result would always be used
Use log object if this is to be kept
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
As above: use `assertThrows` and verify error message
While existing test are written this way, we try to move off this pattern and not use the `expected` annotation. Instead, we should use `assertThrows` and also verify the exception error message.
Also set the store name in this test.
We should use `to()` and use a `TestOutputTopic` instead of the processor
nit: 4-space indention plus move `builder` down one line
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
ditto to `KStreamImpl`
@hachikuji Why don't we catch the exception here? If the task is being stopped, a wakeupexption is kind of noise in log especially for the application which needs to monitor error log.
Is this log line needed? Seems like we get all this info in `onCommitCompleted`.
I think we're missing a `{}`. Same in `doCommitAsync`.
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
placeholder may not be required for exception
If we throw InvalidTopicException directly, be change public API (possible exception are part of the API) and thus, this would require a KIP.
Just throwing on the first is probably fine. Alternatively, if you want to list them all, I'd suggest iterating through them and collecting them into a collection rather than using suppressed exceptions.
Yeah, it's a good question. `IllegalArgumentException` doesn't feel quite right. Another option would be `IllegalStateException`. Also, we should probably mention the configuration property in the exception message.
nit: an aync
it treated => it is treated
need a check for null on `obj` here as well
A common pattern for classes like this without any state is to create a static instance. ```java public static final UnknownAddressSpec INSTANCE = new UnknownAddressSpec(); ```
nit: unneeded parenthesis
Style convention: this should be called `value()`.
nit: add `final`
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
It looks like this is not used anywhere
We can use `List<Class<? extends Connector>` to avoid the warning
We don't need this field, this could be a local in `startClusters()`
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
`windowSize` should be `Duration`
the method name changed to `windowedTable` and `windowSize` parameter is missing
Also not clear why "numSegments - 1" here.
original was better
nit: "another thread wrote to ..."
original was better
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
I meant to have "Note that enabling idempotence requires this config..." before "Allowing retries...". And break the two parts with a paragraph.
Hello and thanks for reviewing! The callout about record ordering was originally requested within Confluent; the same is mentioned at this page: https://developer.confluent.io/tutorials/message-ordering/kafka.html. We think this would also benefit AK docs.
maybe `then the records in the second batch` is a bit better
Ah, you're right. Not sure how I missed that.
typo: with message ordering preserved for any allowable **vlaue** --> **value**
Hmm, normally `IllegalArgumentException` indicates that the argument to a function is bogus, right? That's not really the case here-- the function argument was fine, but the topic wasn't set up correctly. This can probably just be a generic `RuntimeException`, since we don't have a need to make it something fancier.
Thanks for the follow-up.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
That's right, changed it locally.
It doesn't seem that the client needs principalBuilder.
Checked with Jun and this is fine.
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
No need to pass the spec in the constructor here, as all the connections have access to the internal spec.
nit: `{@link KeyQueryMetadata}`
nit: line too long. `final` not required -- a static method cannot be overwritten anyway
`function` -> `method` ? `{@code null}`
This method does not return a `String`. Maybe ``` @return StoreQueryParams a new {@code StoreQueryParams} instance configured with the specified partition ```
The primary functionality of this method is to _set_ the partition... I am confused by `with stale(standby, restoring) stores added via fetching the stores` -- what does this mean? Maybe just simplify to: ``` Set a specific partition that should be queries exclusively. ```
We normally use `assertThat()` in new and refactored code. Please also change the other occurrences. ```suggestion assertThat(hasStateTransition(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING), is(true)); ```
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
The order is not really that important here, either way works
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
I would prefer defaulting to range just for consistency. We have seen similar cases in the producer where the behavior of partitioner's hashing function changes a bit, causing offset manager migrated for mirror-makers and hence resetting offset and data duplicates.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
Might be overkill if this is the only use case, but we could also add a composite validator.
I don't think we want this in the base class -- its risky as it leads to accidental sharing as we had before. Instantiating it once statically in `SourceConnectorConfig` and `SinkConnectorConfig` seems fine, but keeping it here seems like we're inviting people to accidentally define additional parameters on it. It looks like we only use this in one place (in this file) so probably not a big deal to remove it.
We avoid using time-based operations in integrations since they usually leads to flaky tests. Consider using `TestUtils.waitForCondition()` with a timeout.
nit: move .collect to new line
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
`TimeWindows .of` is deprecated, right? Do we need to test old API? If yes, I think we need a `@SuppressWarning` to make the build pass
Btw: all those test only set differn window size and grace and thus it seems we can share more code (ie, setting up the topology). Only the window definition is different.
We are tracking the LEO in two places: 1. In `ReplicatedLog::endOffset`. This gets increase every time the log gets appended: https://github.com/apache/kafka/blob/28ee656081d5b7984c324c3ea3fc9c34614d17db/core/src/main/scala/kafka/log/Log.scala#L1302 2. The `LeaderState` also stores what is now the LEO. One suggestion is for `LeaderState` to instead store the "flush offsets". In `LeaderState` the follower's flush offset is the LEO but for the local replica the "flush offset" may not be the LEO. An example of the high-watermark increasing but the LEO not changing: 1. follower: LEO = 10 2. leader: LEO = 100, FlushOffset = 100, HW = 0 Follower successfully fetches for offset 10 => Leader: LEO = 100, FlushOffset = 100, HW = 10. Follower successfully fetches for offset 20 => Leader: LEO = 100, FlushOffset = 100, HW = 20. In this example if the leader already flushed to the LEO then there is no need to flush again when increasing the HW.
The invariant that the leader most satisfy is that the `highWatermark <= flushOffset`. The current implementation satisfies this by flushing after every append and implicitly defining `flushOffset == logEndOffset`. At a high-level, I think the goals is to allow `highWatermark <= flushOffset <= logEndOffset`. On the follower, things are a little different. On the follower the `flushOffset == logEndOffset` before a `Fetch` request can be sent. This is because the leader assumes that the fetch offset in the `Fetch` request is the offset that the follower has successfully replicated. The advantage of appending without flushing as soon as possible replication latency. The leader cannot replicate record batches to the followers and observers until they have been appended to the log. I am not exactly sure how exactly we want to implement this since I haven't looked at the details but I think you are correct that on the leader side of things we want to increase the `flushOffset` in the `Fetch` request handling code as the leader attempts to increase the high-watermark.
Got it. For future readers the function is `maybeCompleteShutdown`.
Yeah. You want to force an epoch change in the case that the old leader stays leader and partially replicated data was lost. This would force followers to truncate to the new leader's log state.
Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage). I am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the `Fetch` and `BeginQuorum` protocol and not from the state machine (Kafka Controller) itself. It is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.
request "got" re-sent to the control
Should be larger
Typo: should be "or larger than the number of available brokers"
Please include TopicDeletionDisabledException here.
This exception can't be thrown by DeleteTopics.
nit: `isolate ... form other client configs` -> ``` override ... for the main consumer client from the general consumer client configs. The override precedence is the following (from highest to lowest precedence): 1. main.consumer.[config-name] 2. consumer.[config-name] 3. [config-name] ``` Ditto below for other two.
Nit: it's also to distinguish from other client configs -- not just consumer configs.
recommended; ditto below.
as above: we need to remove adminPrefix configs
This shouldn't be possible, right? It wouldn't make much sense to put a topic in the result if it didn't have a corresponding `TopicListing`.
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
+1 to this
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
Should we have a separate `UnsupportedVersionException` for multiple endpoints? For example, we could have multiple plaintext endpoints.
nit: we can use `map#compute` to replace getOrDefault + put.
I hope we can get rid of those conversion in the future :)
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
nit: align parameters.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
We should update the Scala `TestUtils` to call this method.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Same here, we can cache the result of `Builder.getPartitions(data)` for re-use.
typo: byteArrray -> byteArray
`< Callback >` this explicit type is not necessary.
nit: we could split this lone line by different key, value by new line to make it clear. ex: ``` String[] args = new String[] { "--topic", "Hello-Kafka", "--num-records", "5", .... }; ``` Same as below.
nit: remove the redundant line. Same as below.
redundant type arguments `<ProducerRecord<byte[], byte[]`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
Nit: why not use `boolean`
Nit: maybe `("Topic: " + topic)`
`rebalancing()` should never throw an `InvalidStateStoreException` as it is just constructing the `CompositeReadOnlyKeyValueStore` wrapper. The underlying stores should not be accessed until `get`, `range`, or `all` are called. So, i think this is safe to leave it as it is
typo: Woth -> With
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
We actually don't need to name the store. This could be `.count()` plus updating the name for the repartitioning and changelog topic.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
nit: empty line.
I think it is still possible. Here's one scenario: 1. last checkpoint at offset 100; all writes goes to old CF. 2. continue writes to old CF til offset 110, but no checkpoint written yet. 3. non-graceful shutdown happens, and upon restarting new CF is used. 4. we start restoring from offset 100 to log-end-offset 110, to the new CF. Now we ended with data of offsets 100-110 in both CFs.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
You are right. Never mind.
Can we not do: ```java if (appendResult != null) return appendResult; else { ... } ```
Seems like we can put this back into the previous line.
isFull is no longer used.
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
We lack unit test coverage for this case
These two cases don't seem to be different. I'd recommend just always wrapping the exception and throwing (currently the else block). If we just re-throw the first exception, reading the stack trace becomes very confusing. Especially since a lot of those exceptions don't even include the stack trace.
In newest trunk we always call `task.closeDirty` .
Not sure. PENDING_SHUTDOWN indicates a clean shutdown while this lets the thread fail.
The fallback should be taken from `StreamsConfig` and not be earliest all the time.
We should add `else` and throw `StreamsException` with cause using original `NoOffsetForPartitionException` (ie, `ex`). Furthermore, the error message should explain in detail what happened and how a user can fit it. Something like: ``` No valid committed offset found for input topic T (partition P) and no valid reset policy configured. You need to set configuration parameter "auto.offset.reset" or specify a topic specific reset policy via KStreamBuilder#stream(...) or KStreamBuilder#table(...). ``` The reason for throwing an exception is, that Streams has no change to start reading a topic/partition in a meaningful way. The user needs to fix this issue! This is also current behavior, because `NoOffsetForPartitionException` would be raise for `auto.offset.reset=none` and missing committed offsets, too.
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later? It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.
As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too. Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
Are we intentionally not logging the exception as the extra parameter? If the exception wraps a more useful exception, we won't see any information about the wrapped exception unless we can see the stack trace in the warning log message.
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
Nit: ```suggestion final long maxAttempts = maxRetries + 1; ```
```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a *. {@link org.apache.kafka.connect.errors.RetriableException} * before retrying again; must be 0 or more ```
Nit: maybe `("Topic: " + topic)`
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Sounds right to me.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
We could refactor out a helper function here.
@eliaslevy Since this part is covered in other unit test case, we want to remove redundant coverage to leave the unit test as succinct as possible.
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
nit: preserve empty line after `checkAndClearProcessResult`
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
as above (similar below)
nit: it would improve readability to factor out some functions for some of the work here. Here we can have a separate function with a nice name for building the assignments
nit: `Short.MAX_VALUE` is 32767
I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
The order is not really that important here, either way works
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
nit: line too long
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: simplify `InterruptedException, IOException` to `Exception`
nit: just simplify to `throws Exception`
This is unnecessary as junit always create a new test class for each test case.
Or we make each test create task instead of creating task in ```setup```
Couldn't we simply wait for the current state to become `RUNNING`? ```suggestion private void waitForRunning() throws Exception { waitForCondition( () -> kafkaStreams.state() == KafkaStreams.State.RUNNING, DEFAULT_DURATION.toMillis(), () -> String.format("Client did not transit to state %s in %d seconds", expected, DEFAULT_DURATION.toMillis() / 1000) ); } ```
super nit: no `.` at the end or start sentence with `[T]he` :)
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
nit: add `a {@link Named} config`
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
This is not a feedback: as we are changing the main loop of StreamThread, we may need to carefully benchmark if this change along with the main loop changes will have unexpected performance penalty: with low traffic input stream, we are effectively sending sync commit requests more frequently. cc @mjsax If it does become a problem for performance, we could consider making the commit request async, and consider a commit only completed after the commit response is returned. Of course it means more complicated logic.
Even without EOS and assuming KIP-211 is in place, we'd probably still to commit regularly as a pre-requisite to fix https://issues.apache.org/jira/browse/KAFKA-6502.
Do we need `transactionInFlight` here? Previously we just considered if `eosEnabled` for committing a transaction.
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
This part and the line 525-529 below can be extracted out of if condition.
Not introduced in this patch: "is non" => "as non"
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
We should not include this along with the unit tests since it's not a unit test.
Is this line intentional? Unit tests normally don't need to print out to console.
Could also be `final`
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
Nit: let's not add an unnecessary extra line.
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
Is it valid to have a blank string as the replacement value? If not, then the `replacement` config should have a validator that prevents using invalid values, and it probably would be good to succinctly describe the limitations in the doc string. And it may be better to only set `this.replacement` to a non-null string that should always be used. This would centralize the logic of determining whether it should be used in one place, and line 135 becomes a lot simpler and more efficient: ```suggestion if (replacement != null) { ```
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
records to it, and reading all records from it, such that
an -> a
I think `will go` should simply be `go`.
Would it make sense to validate here that idx not in self.nodes_clean_shutdown? (although we might want a set instead of a list). Seems like it should be an error to receive a valid message after receiving a "shutdown_complete" message
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Delete this block - this was a specific check for ensuring log output in the test_console_consumer
I'm not sure in this. So in case of two properties (k1=v1 and k2=v2) do you generate the `--consumer-property k1=v1 --consumer-property k2=v2` string? The repeated property didn't work when I tried it out and it only picked up the first one. I think you need to pass `--consumer-property "k1=v1,k2=v2"` as with some other commands. In case the "k1=v1,k2=v2" format is needed, there is a nice one-liner way to do it: ``` ','.join("%s=%r" % (key,val) for (key,val) in k.iteritems()) ```
This is two methods calls, thus it should not be a one liner (test would pass if `getStateStore()` would throw `UnsupportedOperationException`. Should be: ``` @Test public void shouldNotAllowInit() { final StateStore store = globalContext.getStateStore(GLOBAL_STORE_NAME); try { store.init(null, null); fail("Should have thrown UnsupportedOperationException."); } catch(final UnsupportedOperationException expected) { } ``` Similar below.
Calling `maybe_start_jmx_tool` after each line that gets read from the producer process doesn't seem quite right. I think we want the behavior to be: - start producer "asynchronously" - start jmx tool asynchronously - wait for producer to finish - process each line of producer output I think it would look something like: ``` cmd = "same_as_before &" # now the cmd is "async" node.account.ssh(cmd) wait_until(producer is alive) self.start_jmx_tool(node) wait_until(producer is finished) for line in node.account.ssh_capture("cat /mnt/producer-performance.log"): # same as the previous for loop ```
This debug message seems like it would appear _before_ we actually attempt to do any checking. It's probably worth keeping the old message (or something similar) _after_ the checking has been completed.
Can we rename it to something like "getState", assertAssignment is kind of misleading.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
One issue with c. is that it works for new environments. If users already have MM2 running, it's using topics with the current names.
Why are we generating the connect internal topic names here? if there's a rule for the topic naming convention it should be defined in one place which is `ReplicationPolicy`.
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
Yes, it makes sense to return a range for an ApiKey instead of a single version. I was just wondering if this method is redundant given NodeVersions.apiVersionRange(ApiKey api). Also, it feels a bit weird for a public facing class to reference Protocol, which is not a public facing one.
But `toString` by default returns `name()`. So, I don't understand why we are overriding it.
Answering here the question about name changes. This is probably the best example, it has been renamed a couple of times: `ConsumerCoordinatorRequest` -> `GroupCoordinatorRequest` -> `FindCoordinatorRequest`. Also, I'd like to rename `Produce` to `ProduceRecords` and `Fetch` to `FetchRecords` so that all protocol APIs are consistent.
Hmm, that's annoying.
You can use `EnumMap`.
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
nit: I don't spot any, but safer to avoid typos by just having constants for these
Nitpick: I'd call this `getOrCreateFileChannel`.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
@ijuma Sorry, I don't know of a standard way of doing this,
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
Harsha has done this.
Even though the config is invalid, the passwords may be valid, so it seems safer not to include them. It would be nice if the sensitive entries in the JAAS config would be communicated in some way to improve debuggability (in the future). Btw, a nit: we seem to use inconsistent capitalisation of the word JAAS in our messages. It would be nice to make that consistent.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
nit: `Older` -> `older`; `topic Id` -> `topic ID`; `TopicId` -> `topic ID`; `Epoch` -> `epoch`.
It might be worthwhile having a separate case which goes through the sequence described in the jira. Basically this: 1. Receive metadata response with topicID A. 2. Receive metadata response with UNKNOWN_TOPIC error. 3. Receive metadata response with topicID B.
nit: `foo` reads a bit weird here. I would just remove it. Similarly, `topic A` is weird because the topic is name `topic`.
This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.
Ah, you're right. I misread the second check.
This is fine for everything past 1.0.0, but if we do want to make things releasable using this script on trunk (similar to how people use the merge script probably), then we'd want to maintain support for the 4-component version and validate ones starting with 0 vs >= 1. I'm fine going either way (not maintaining that for simplicity and bug fixes on older branches will need to use the older release script, or just adding in a bit more logic here).
yeah, seems valid. we already chop off whatever we don't need in `docs_version`, I guess just leftover from the `0.` days...
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Sounds good. Actually I've seen the same situation for our current caching layer flushing logic as well: e.g. `put(A, v)` -> `delete(A)` and both only hit the cache layer. When flushing we tried to read the old value and found its null bytes, so we know nothing was flushed for `A` and nothing written to downstream before so we can skip putting a tombstone to underlying store as well as downstream. For suppression buffer though, it is harder since you do not have an underlying store to fetch the old value, and of course reading the whole changelog to see if there's any updates on this key `A` costs you everything. But suppose we always have a persistent buffer, this may be an easier task.
an -> a
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
an -> a
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
Let's avoid making whitespace changes like this, since they tend to make the change harder to read
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
nit: this can be final
What do we want to achieve with this throttle? Do we just want to backoff for `THROTTLE_PERIOD_MS` whenever we can't find a connection you sent? I think we should simply use a `Thread.sleep` call. To be concrete, I recommend we instantiate a `org.apache.kafka.common.utils.SystemTime` class and use both its `sleep()` to sleep and `milliseconds()` to get the current time
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
Changed it locally.
not be => not be able to
Changed it locally.
newuntil => newUntil
Changed it locally (and in one other similar place).
nit: unneeded newline
nit: can we make this debug level? Otherwise it will make this test a little spammy.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
What do we do if there's an exception? If it's expected, let's make it clear
This isn't our fault. When we added the timestamped stores, we chose not to make SessionStores timestamped because the session bounds already have the end timestamp available, which is identical to the timestamp we would have stored in the value.
This is why the test was failing for you. The query is for a range of window start times, not record times. Since the window size is five minutes, the range `[now - 1 minute, now]` wasn't going to contain the actual window start time of `now - 5 minutes`. In other words, just a simple oversight :/ Sorry for the trouble.
Since we're specifying the key, we expect only to get back windows with the value for that key. The aggregation we specified is to sum all values for the key, and it comes out to `2` because we only write one value for each key; namely, the value is the same number as the key.
It seemed like a good idea to check a few other query configurations, but none of them showed any problems.
Since I was fixing stuff anyway, I went ahead and fixed a bunch of formatting issues that I didn't bother mentioning before.
I think we should test for the exception -- if we change the behavior intentionally, we should remove the `fail` as well as the `try-catch` instead of allowing both behavior to pass (it's an either or form my point of view). IMHO, tests should be on an as narrow code path as possible: if we change behavior and a test fails, we are forces to reflect on the change, what is good as it guards against undesired behavior changes...
Do we need this? The test doesn't actually use or test it so it seems irrelevant
nit: use `ConsumerConfig .SESSION_TIMEOUT_MS_CONFIG`
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
Is there similar behavior in the map parsing? I see a similar comma consume call being ignored. Consider the following test: ``` SchemaAndValue schemaAndValue = Values.parseString("{foo:bar baz:quux}"); assertEquals(Type.STRING, schemaAndValue.schema().type()); assertEquals("{foo:bar baz:quux}", schemaAndValue.value()); ```
I was not aware of the restriction on JSON object keys, and that seems like a fine standard to follow. I can't imagine it being too useful.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
I think I'd just do: ```java List<String> lines = asList(loginType.contextName() + " { ", jassConfigProp, "};") Files.write(jaasConfigFile.toPath, lines); ```
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
nit: would be nice to be consistent on the pattern we use here
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
Better be `cooperative-sticky`? `cooperative` is too general I think.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
Ouch! Sorry about that!
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
> That being said, I get that this is confusing. Do you think changing the check to `if (endTime == windows.TimeDifferenceMs() && !isLeftWindow(next))` would make it seem cleaner? Haha no, I don't think saying `if (!isLeftWindow(next)): then next = latestLeftTypeWindow` would be less confusing. If we call a variable `leftTypeWindow` then it should _always_ be a left type window. That said, I now see what you meant here and it's the same problem as above, with the same fix of replacing `latestLeftTypeWindow` with `previousRecordTimestamp`. In that case I think we can just remove this check entirely (ie, don't explicitly check if it's the combined window), and all we need to do is make sure `previousRecordTimestamp` is set correctly
I think with this replacement then we might be able to get out of doing any kind of special handling for the combined window outside of `processEarly`
Yeah sorry I didn't mean that we shouldn't have any conditionals here whatsoever, I just meant that we don't need the combined window check (or really anything other than what we need to accurately set `previousRecordTimestamp`)
Yeah, you're saying that we just always keep track of the `previousRecordTimestamp`, but before we go ahead and create a left window for the current window we just actually verify that the previous record is within range? That makes sense to me, actually if anything I feel like it will make `rightWindowNecessaryAndPossible` even more clear to put it in terms of `previousRecordTimestamp`. What I'm realizing from this is that it's easier to understand these boolean checks in terms of the actual record locations, in general. Maybe it's just my mental model, I still picture a rectangle sliding over boxes on a timeline 
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
req: This is unnecessary
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
`final` is for the var `activeTasksMetadata` (not for the method). We try apply a "use `final` whenever possible" policy. It's just some nit.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
Nit: use `{ }` for the loop body.
This is not required as contained in the check next line.
Nit: add `final`
This adds an additional `get` when compared to the previous solution.
One extra line.
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
Also, same as above (and elsewhere), mixture of `final` and not `final` locals. They could all be `final` - i don't really care either way, but consistency would be good
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
nit: unneeded newline
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
As an alternative, which might align better with Kafka in general, would be to set the timeout via `StreamsConfig`. This keeps the API clean. @enothereska argument that `close()` should not have any arguments is quite valid to keep APIs consistent within Kafka.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
The test case `BrokersToIsrsTest.testNoLeader` suggests that it is a possible case. It looks like the path through `ReplicationControlManager.handleNodeDeactivated` could result in a `PartitionChangeRecord` which has leaderId set to -1.
It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.
Hmm, why do we need to remove for -1 broker? It doesn't seem that brokersToIsrs tracks that.
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too. Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.
Be careful about changing this -- it should probably be a LinkedHashMap or some similar order-preserving Map if we do this. Depending on the serialization format, they may be sensitive to field ordering so being able to preserve the ordering will be important. Even within a single process a converter could end up randomizing schema field orders and breaking equivalence if an order randomizing map was used. The alternative would be to add an extra Set to SchemaBuilder, which requires extra allocations but doesn't require jumping through hoops to preserve the behavior of the `fields()` method.
Possibly my favorite aspect of this PR is that you have reminded me that `SchemaBuilderException` is even a thing that exists...
Some fields might be safe to just shallow copy, but I think a number of these need to be deep copied to avoid accidentally modifying the original schema. I think `parameters and `defaultValue` at a minimum need to change. Schemas seem like they should be fine since they'd just be fully replaced, not modified, anyway.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
nit: avoid `this` if not required.
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
We should log an error that prints out what the two configs actually are
super nit: extra blank line
That makes sense, however you might be able not include the new field from the hash to prevent a chaotic assignment if you wanted
Nit: line too long
Thinking about this, I guess we could improve `StickyTaskAssinger`. If I am not off, load balancing on stream basis is not optimal -- but I am also not sure if the effort to improve it is worth it... If we extend this test to assign more tasks, let's say 12, client `p2` will get 7 tasks assigned and `p1` get 5 tasks assigned (while it would be better to assign 8 tasks to `p2` such that all 3 thread get 4 tasks each). The problem is, that the capacity factors are not considered: `p2` should get twice as many tasks assigned as `p1` -- but the algorithm says only "more" -- and this more is determined be the diff of the capacity (ie. in this case `p2` will get at most 2 more tasks assigned than `p1`. Or maybe my analysis is wrong (I did not run the code and step through it.)
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
If we expect no warmups, we can assert it here with: ```suggestion assertValidAssignment(0, allTaskIds, emptySet(), clientStates, new StringBuilder()); ```
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
This is still not used
No longer used.
This docstring could be improved. Perhaps we can just mention that we will may have a memberId before we are part of a group generation.
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
Thanks for bringing this up @dguy @enothereska , thinking about this more I feel it is Okay to state that users should not call any public APIs of the `KafkaStreams` object inside this callback, if they do then undefined behavior.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
Should this be `num_lines=3` (cf. L116 and L126)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Not for this patch, but we should do a KIP to add support for batch topic creation.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
and -> a
records to it, and reading all records from it, such that
and -> a
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
nit: remove empty link
I suggest passing all of the variables we access in this constructor via`SustainedConnectionWorker.this.spec` to be switched to parameters we pass upon instantiation
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
This won't log the error. We want to use the `public void error(String msg, Throwable t);` overload, so we need to change `e.getMessage()` -> `e`
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
How about "Custom partitioning is an advanced topic" -> "If not specified the underlying producer's {@link org.apache.kafka.clients.producer.internals.DefaultPartitioner} will be used to determine the partition".
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
No worries, let's keep the scope small for now. Just wanted to raise the question
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
This is the essential line of the test that verifies the change in #7097. Without the fix in #7097, the listener will get an update with *all 3 task IDs*, and to pass this line would then need to be changed to: ``` configUpdateListener.onTaskConfigUpdate(Arrays.asList(TASK_IDS.get(2), TASK_IDS.get(0), TASK_IDS.get(1))); ``` However, we don't want *all* task IDs to be updated. Instead only want only the task ID(s) for the tasks that were indeed updated. That's why this test case expects that the task config update only includes the one ID of the task that is actually updated (i.e., `TASK_IDS.get(2)`). So as is, this test method will fail unless the fix for #7097 is actually applied.
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
Looks good. I like the additional checking that you're doing here.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
nit: use `{}` instead of string concat for `retries`
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
I'm not sure this is a good idea. If we're unlucky, the partition we're interested in may not be listed. Since this is an exceptional case anyway, I would suggest using the more verbose message.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
Not something we have to do here, but one way we could improve this in the future is by taking into account leader epoch information from individual partitions. We can ensure that epochs increase monotonically in order to prevent using stale information during retry. Another thing we could do is reduce the topics we are fetching metadata for as the ListOffsets requests complete. Ideally we'd only be refetching metadata for topics with metadata errors.
There is an inconsistency with above code: `Map.Entry` vs `Entry` -- I guess we should use `Map.Entry` everywhere.
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
nit: `if (max = value) else (max = Math.max)`.
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
WDYT about this: ```suggestion long millisRemaining = Math.max(0, end - System.currentTimeMillis()); if (millisRemaining > 0) { Utils.sleep(millisRemaining) } ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
Should we use this check as a condition to wait? Sleeping 10 secs feels pretty brittle
Seems this could be a function as well. For example: ``` java Map<TopicPartition, PartitionInfo> partitions(Map<String, InternalTopicMetadata>); ``` (I'm looking for small independent chunks of code that can be taken out of this function.)
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Seems you can replace this with this: ``` java topicPartitions.addAll(partitionsForTask.get(id)); ``` Same below.
Are the two loops below equivalent to this? ``` java taskIds.addAll(state.activeTasks); taskIds.addAll(state.standbyTasks); ```
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
Ouch! Sorry about that!
I think we're testing `testDir` Occupied here, not `AppDir`.
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
Nit: `.` full stop missing.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
nit: {@link KGroupedTable}
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
Can you elaborate? Seems to be orthogonal to the timestamp fix.
If not, we should move the exception capturing logic inside the dbAccessor as well.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
Nit, suggested rewording ```java log.info("Fetch offset {} is out of range for partition {}. We only have log segments in the range of {} to {}. Resetting offset.", fetchOffset, tp, partition.logStartOffset, partition.lastStableOffset); ```
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
There's a space missing after `offset`, I'll fix it before pushing.
There's an interesting edge case where `record.isValid()` could throw a `IndexOutOfBoundsException` if the buffer size is smaller than 4 (i.e. we fail when we try to extract the checksum from the buffer). Obviously, this means that the record size field was itself corrupt (since it should never be that small), but it can happen. I'll file a separate PR for that.
nit: remove newlines
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
I think we'd want to use `updateLastSeenEpochIfNewer`. The provided epoch just gives us a lower bound on an acceptable leader epoch.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
We probably have to keep the `size() == 0` behavior for compatibility.
`deteremined` => `determined`
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
No worries, let's keep the scope small for now. Just wanted to raise the question
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
nit: add `final`
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
I think using a string and the `NonEmptyString` validator would be a little clearer. That would allow us to skip any additional checks in `start`.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
`orderInGroup` param is duplicated for key & value converter
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
There is https://github.com/ben-manes/concurrentlinkedhashmap. Guava and Caffeine (Java 8 required) also have Cache implementations with the same underlying behaviour.
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
style nit: normally we'd use braces around blocks unless they're a single line
nit: add a space before the `:`.
What you had is fine.
strictly speaking, you don't need this yet. Still needed when we evolve I suppose
I don't have all the context, but isn't `3` pretty low? We don't do exponential back-offs, so the recommendation for no data loss is typically higher.
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Well, we wouldn't want to just remove that clause -- in the case of `SerializationException` you want to maintain the `SerializationException`, but if there's some other `RuntimeException` (which there can easily be for serializers that aren't aggressively catching exceptions and converting to `SerializationException`) then you still need to convert it to a basic `KafkaException`. I think you could do this: ``` try { // parse record } catch (SerializationException e) { throw new SerializationExceptionException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } catch (RuntimeException e) { throw new KafkaException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } ``` as long as we're confident there aren't any other `KafkaExceptions` we'd want to handle differently (i.e. any other more specific types of `KafkaException` where we'd want to preserve the same type instead of generalizing to `KafkaException`).
A quick look shows that other code has access to e.g. `topic` and doesn't include it in the exception message. Seems like having the fields there could help with better exception messages.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Thinking about this, I am wondering if we should just change the FSM to allow this transition and simplify the code here? \cc @guozhangwang
Sure. Why not. I just would avoid the term "invalid" because it might confuse users (they may thing something bad happens, but it's expected and not bad).
This might be miss leading for users who don't know the details. What about: ``` log.debug("Ignoring request to transit from PENDING_SHUTDOWN to {}", newState); ```
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Preexisting, but should this be an error log? (seems to be implied by the text of the log)
This is fine for everything past 1.0.0, but if we do want to make things releasable using this script on trunk (similar to how people use the merge script probably), then we'd want to maintain support for the 4-component version and validate ones starting with 0 vs >= 1. I'm fine going either way (not maintaining that for simplicity and bug fixes on older branches will need to use the older release script, or just adding in a bit more logic here).
yeah, seems valid. we already chop off whatever we don't need in `docs_version`, I guess just leftover from the `0.` days...
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
I really like this class.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
use `return CODE_TO_VALUE.getOrDefault(code, UNKNOWN)`
I really like the fact that we are separating Resources from ResourcePatterns! Great job.
Ditto on removing before/after
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
Ditto on removing these before/after methods.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
Typo: should be "or larger than the number of available brokers"
We do not throw `InvalidTopicException` "if [the topic] is not found"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Should have a comma after "for example"
This adds an additional `get` when compared to the previous solution.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
Thanks for the explanation. Make sense.
OK, makes sense. I was missing that context.
`replicaing` -> `replicating`
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
req: This is unnecessary
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
Nit: I think the state transition should be done before the INFO log.
Won't that result in a dead coordinator and be handled by the first `if` block.
Ah, yes, the magic is hardcoded here.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
may be use Objects.requireNonNull
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
Also not clear why "numSegments - 1" here.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
It's a bit confusing, but it can: http://slf4j.org/faq.html#paramException (if I understood your point correctly).
Do we _know_ that it will resolve the problem? Maybe better: ``` Changing the location of state.dir may resolve the problem ```
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
Should we consider including the error message of `e` in to the exception as well? also nit: capitalized `Fatal`.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
Nit `.` at the end
nit: both lines missing . at end
nit: `{@code null}` (same below)
above (some some more times below)
Ditto here about error message
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
We lack unit test coverage for this case
These two cases don't seem to be different. I'd recommend just always wrapping the exception and throwing (currently the else block). If we just re-throw the first exception, reading the stack trace becomes very confusing. Especially since a lot of those exceptions don't even include the stack trace.
In newest trunk we always call `task.closeDirty` .
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
Good question. I can't think of any right now (maybe someone else can?) so I would lean towards keeping it package-private for now.
Sorry, you're right. The thing being tested should be second.
I don't think we need to mock `generation()` in this test.
It would be good to verify that the buffer contents are correct as well.
We usually avoid writing to standard out in test cases. A few more of these.
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
exception not used.
This is not a suggestion for change: while working on removing `KStreamBuilder` and `TopologyBuilder` I realized in some unit tests we may still need this class to access the internal topology builder. So probably we cannot remove it even after that, but we can discuss this later in the cleanup PR.
Better name as "setCurrentNodeInProcessorContext"? And then in java docs mention that it returns the processor context with current node set.
nit: add `final`
This line is a bit misleading, as it is referring subject and object to the same class. I'd suggest just remove this line.
nit: move to next line ("weird" formatting)
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
At least we try to not abuse Thread.sleep() ;-)
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
If we follow the same pattern as `ConsumerGroupOperationContext`, then this could be a static method which takes the response as a parameter.
nit: usually we drop the `get` prefix on getters.
Maybe doesn't matter, but seems a little more intuitive to check that the error is not NONE.
Given usage, this could probably be a Set.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
Ah, you're right.
nit: we could define this transition list in a variable to be reused.
nit: extra space after second COMMIT
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
I do not think we need to emulate the behavior of the java uncaught exception handler here. I do not see why a user should try to reset the uncaught exception handler. If we receive this requirement, we can still add it. I have the impression the "reset" makes this code unnecessarily complex.
```suggestion public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler uncaughtExceptionHandler) { ``` We prefer to resist the urge to abbreviate, especially in the public-facing APIs.
Oh, sure. Now I know why you picked this name :)
"with a read-only key"
with a read only key
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
Can we think of a better name for this? `pairs` doesn't really tell me anything.
There is no `CoGroupedStream#reduce()` -- we can remove this
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
nit: unintentional? one more in `subscription`
Effectively what we are doing is sorting the assignments for each partition using the generation and picking the latest two for the current and previous assignments. Is that right? Could we simplify the logic by building a `SortedMap<Integer, String>` for each partition where the key is the generation and the value is the memberID? Then the current assignment would be the last entry and the previous assignment would be the one prior.
@guozhangwang @dguy we cannot guarantee that all the entries for one key will necessarily precede the entries for the next key. The following code still fails with this patch, and only returns `0001` and `0003`, since the key for `("a", "0005")` will come after the key for `("aa", "0004")` ``` final RocksDBWindowStoreSupplier<String, String> supplier = new RocksDBWindowStoreSupplier<>( "window", 0x7a00000000000000L, 2, true, Serdes.String(), Serdes.String(), 0x7a00000000000000L, true, Collections.<String, String>emptyMap(), false); windowStore = supplier.get(); windowStore.init(context, windowStore); windowStore.put("a", "0001", 0); windowStore.put("aa", "0002", 0); windowStore.put("a", "0003", 1); windowStore.put("aa", "0004", 1); windowStore.put("a", "0005", 0x7a00000000000000L - 1); final List expected = Utils.mkList("0001", "0003", "0005"); assertThat(toList(windowStore.fetch("a", 0, Long.MAX_VALUE)), equalTo(expected)); ```
Note that `WindowStore.fetch()` should return `WindowStoreIterator` where the key is `Long` indicating timestamp and `value` is the value.
Should we also check that ``` final long start = SessionKeySerde.extractStart(bytes.get()); final long end = SessionKeySerde.extractEnd(bytes.get()); return end >= from && start <= to; ``` Although the current impl of RocksDBWIndowStore naturally checked that for us, it does not guarantee all underlying store impls guarantee that.
It's unusual to hold a reference to an abstract class like this. I believe the intent is to be able to transparently handle either `KeyValueSegments` or (I'm guessing) `KeyValueTimestampSegments`. The full expression would be to have a `Segments` interface implemented by `AbstractSegments`, which is then extended by your two implementations. Then this line would reference `Segments<S>`. It's fine to collapse this into just the abstract class (although questionable in the presence of package-protected fields). But to maintain transparency, I'd name the abstract class `Segments` instead of `AbstractSegments`. That way, to an outsider class (like this one), you're still just interacting with the interface (i.e., the public interface of the class), rather than specifically an abstract class. Adhering to this pattern leaves the door open in the future to extract `Segments` into a full interface without having to change any outside code (which is what I meant by maintain transparency).
minor performance nit, it might be worthwhile to have something like SessionKeySerde.inWindow(bytes, binaryKey, from, to) to avoid creating three bytebuffer objects for each comparison.
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
nit: this is not introduced in this PR but, other places capitalize the first letter after log prefix.
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
This was originally to try and prevent a dead-lock, i.e, the `UncaughtExceptionHandler` is triggered by Thread-1. The user calls close (still on `Thread-1`). Thread-1 join will never return as we are executing on `Thread-1`, but we already know it isn't running. I think we still need this check
Why don't we need this check anymore? It's still done for `globalThread`.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
A docstring for this method would be good :)
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
nit: Provide a message to the IllegalStateException constructor
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
I think we should stick with host:port for the near term unless there is specific requests in the KIP feedback.
My inclination is to provide more flexibility to the user, but add some guidance on what a "typical" setting would look like (think: `host:port`).
Seems like the indenting should be adjusted to the left, right? Applied to other changes in this file too.
The last commit added a bunch of indenting changes. We should revert them.
I guessed the npathcomplexity thing. :) My question is why we have a class instead of just a method.
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
Sounds good. I'm also ok with a more incremental change if it ends up being more complex than I suggested.
Coordinator changes are _usually_ associated with disconnects, but not necessarily. We have a `coordinatorDead()` function in `AbstractCoordinator` and I was thinking we could make a call to `client.failPendingSends(coordinator)` or something like that at the same time that we set the coordinator to null. I think that would make the behavior the same as the existing code. There may be in-flight sends to the coordinator at the time that we call `coordinatorDead()`, but that is true currently as well, and at least this would prevent any new requests from being sent.
Thanks for the explanation. A bit subtle as you had said. :)
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
Good question. I can't think of any right now (maybe someone else can?) so I would lean towards keeping it package-private for now.
generally in assertEquals, the thing being tested comes second. If there is an error message, the first thing appears as the "expected value"
I actually had a similar thought, but I am torn though. Using two variables required to keep them "in sync" was is not great. However, using `null` is less explicit... Thus overall I am fine either way as both seems to provide the overall same good/bad ratio.
Thanks. Understood. It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information. In `close()` before we actually "loose" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`). This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies.
nit: keep fields with the same access level together
We should only store `handleSnapshotCalls` since `handleSnapshotCalled` is always equal to `handleSnapshotCalls > 0`.
`volatile`? It is potentially being accessed by different threads and the synchronization is on different objects
nit: We could revert this change as it does not bring much and re-align like it was before.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
nit: We could revert this change as it does not bring much.
Yes, we can open a JIRA to do it later.
nit: We should use `groupId.idValue` here and in the others.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
That's a good idea. Note: Kafka does not use this JUnit functionality yet (i.e. no use of ExternalResource, ClassRule, Rule as far as I can tell). @ijuma: Would it ok for us to introduce this? There's no additional dependency etc., it's just using a new JUnit feature that was introduced in 4.7 (we're on 4.12).
We should limit this suppression to the method for which we really need it instead of the whole class
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
remove try-fail-catch and rewrite to ``` final StreamsException expected = assertThrows(StreamsException.class, () -> collector.flush()); assertTrue(expected.getCause() instanceof TimeoutException); assertTrue(expected.getMessage().endsWith(topic1TimeoutHint)); ```
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
nit: style seems to be to not include braces when there is only one if or else statement
Nit: we don't normally use exclamation marks in Kafka log messages.
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
nit: missing `<p>` for new paragraph
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
@wicknicks would be useful to have some unit test for this class.
Is this the format required by the mbean stuff? It might be nice to return something more structured here.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
Can `LogManager.getRootLogger().getLevel()` be `null`? With other loggers you return the effective level if that's the case.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
We should also mention somewhere that we do not support concurrent transactions.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
This exception can't be thrown by DeleteTopics.
Please include TopicDeletionDisabledException here.
Typo: should be "or larger than the number of available brokers"
Should be larger
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
add `final` twice
nit: `final` params
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
ditto on removing before/after.
Ditto on the properties and the driver.
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
Use `KafkaException` instead of `RuntimeException`
We should add a `null` check to allow closing a deserializer that was not properly setup
super nit: extra blank line
Nit: go with single parameter per line.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
We cannot rely on the rebalance protocol from `setRebalanceProtocol` in task manager to determine the logic, since during a rebalance, one can send with two supported protocol but the `EAGER` protocol is still used --- only the leader knows which protocol is used when doing the assignment. At the moment it is almost impossible to let the listener know which protocol was exactly used (not what protocol was supported); but on the other hand I feel may be it is not necessary as well: If the COOPERATIVE protocol is used, then we are effectively suspend tasks in onPartitionRevoked, and then immediately close those suspended tasks in onPartitionsAssigned since they are called consecutively in `onJoinComplete`, and no one would ever be resumed at all (i.e. that `resumeSuspended` call would always be no-op with no passed in task parameters). So I'd suggest we make the listener to also be agnostic to the rebalance protocol, and just follow the current logic of suspending resuming, and we can even see if we could in later releases remove the whole suspending/resuming all together regardless of the protocols.
Nit: move these two static factory methods above the non-static member variables, so all static and non-static members are together.
That makes sense, I think keeping it as-is is better.
Think you might have forgotten to remove some debugging here
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Maybe get the value of this expression on the line above. IMHO it will make it easier to see what the `assertThat` is doing.
As above: add more "randomness"
As above: use `assertThrows` and verify error message
While existing test are written this way, we try to move off this pattern and not use the `expected` annotation. Instead, we should use `assertThrows` and also verify the exception error message.
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
Also set the store name in this test.
We should use `to()` and use a `TestOutputTopic` instead of the processor
I guess in the end we will find these classes a better place (currently they are a bit scattered, e.g. `KeyValueStoreFacade` is in the materializer class).
Seems you got the same :) and ditto elsewhere.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
as above: we need to remove adminPrefix configs
typo: `consume` -> `restore`.
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: line too long
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
do we need both the `expectedStarts` and `expectedRestarts`? It seems like the former should be just one more than the other.
Yeah it makes sense. Sorry for not getting back to you sooner
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
and -> a
`than` -> `that`
nit: than -> that
the method name changed to `windowedTable` and `windowSize` parameter is missing
`keySerde` -> `valueSerde`
`windowSize` should be `Duration`
Blank line can be removed.
We probably have to keep the `size() == 0` behavior for compatibility.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: line too long
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
This is the same code as in `KTableFilter` -- we should refactor and share code.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
Not introduced in this patch: "is non" => "as non"
No need to check null. We always have to forward oldAgg and newAgg, anyway.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
Let's rename `headers1` and `headers2` here too
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
Sorry, you are right! My bad!
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
prop: abortTransaction can also throw ProducerFenced.
@thspinto I think i am running into the same issue which you pointed here , so the source consumer group has different topic and is active and the target consumer group is idle but having different topic, but the code only checks for the topics and partitions matching the target site and adds them only when the target offset is less than source, it ignores other topics at the source
I would like to propose the following change to take care of the source consumer group changes ```suggestion for (Map.Entry<TopicPartition, OffsetAndMetadata> convertedEntry : convertedUpstreamOffset.entrySet()) { TopicPartition topicPartition = convertedEntry.getKey(); for (Entry<TopicPartition, OffsetAndMetadata> idleEntry : group.getValue()) { if (idleEntry.getKey() == topicPartition) { long latestDownstreamOffset = idleEntry.getValue().offset(); // if translated offset from upstream is smaller than the current consumer offset // in the target, skip updating the offset for that partition long convertedOffset = convertedUpstreamOffset.get(topicPartition).offset(); if (latestDownstreamOffset >= convertedOffset) { log.trace("latestDownstreamOffset {} is larger than convertedUpstreamOffset {} for " + "TopicPartition {}", latestDownstreamOffset, convertedOffset, topicPartition); continue; } } } offsetToSync.put(convertedEntry.getKey(), convertedUpstreamOffset.get(topicPartition)); ```
I wonder if more of this code is generic and should be pushed somewhere else.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
It seems that we can just use one level of if/else.
response version should be 1.
response version should be 1.
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
nit: could probably simplify these two assertions a little: ```java assertEquals(Collections.singleton(error), errorCounts.keySet()); ```
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
It was removed from the other versions of `group` but not from here.
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
empty line needed
Oh right duh I was thinking it was just a single bit but it's a byte. In that case we should have a test that verifies it goes from `0` to `1` to `2`, etc -- might be good to verify the behavior on overflow as well, if you call `subscriptionUserData` the max_value number of times
Yeah, it should be `0` the first time you call it, then `1` the second time, and then back to `0` again on the third call
nit: the mocked task manager would just call `getTaskOffsetSums(...)` so maybe we can just call `taskManager.getTaskOffsetSums()` here? Ditto elsewhere.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
nit: move this `if` statement below/above version check on line 62
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Nit: space before `:`.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
Should this be simply `builder`? The current name sounds like it builds the definition.
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
\cc @lindong28 -- seem's you forgot to update this when dumping the version in `1.1` branch.
I think you should only do this when the version actually is published.
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
It will result in the same list of versions -- both equally good IMHO.
We already import the class on L26, so let's remove this import.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
nit: add `{ }`
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: would be nice to be consistent on the pattern we use here
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
I think the name of the function is better defined as `interleaveTasksByConsumers`
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
Not sure if we need to call out EOS in particular? It's incorrect in any case.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
req: `subscription` -> `subscription info`
nit: extra line
this will never be called if one of the assertions fails
Do we really want to do this? Might it be better to have a config for this? Or just run it with a fixed number of threads
Yeah, that works, too, and is more align with the current code.
nit: java style
`counts` is not used
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
`KeyValueStore` -> `TimestampedKeyValueStore`
nit: remove empty link
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
nit: add `a {@link Named} config`
Use diamond (`<>`).
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
nit: would be nice to be consistent on the pattern we use here
We should emphasize that this partitioner should be stateless as it can be shared across topics / sink nodes.
Great catch, thanks @showuon !
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
nit: why not `k2` ? Should we use `A`, `B`, `C`, `D` for the values to make it easier to understand the expected result? It's unclear which A is which below.
I think we should rewrite this to make sure the NPE is thrown in `get()` but not in the constructor. ``` final KTableTransformValues kTableTransformValues = new KTableTransformValues<>(parent, new NullSupplier(), QUERYABLE_NAME); try { kTableTransformValues.get(); fail("..."); } catch (final NPE expected) {} ```
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
nit: should be `named` can't be null
nit: add `final`
Is it correct to set parent node name as `this.name + "GROUP_BY"`? Seems the parent node name is not set in this way. Ditto below.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
Thanks for the explanation @dguy, very helpful to understand where caching and sequence numbers come into play. It might be worthwhile to put this in a JIRA somewhere. I do think it would be a useful optimization to have eventually, as fetches have some setup / teardown overhead.
@xvrl there is no `get` on `WindowStore`. We could add one and it would work in scenarios where we don't have duplicates, i.e., the key for a WindowStore is (recordkey, timestamp, sequenceNumber) - if the store doesn't have duplicates the sequence number is always 0. If the store does have duplicates then we don't know what the sequence number is. Without a KIP to add a `get()` to `WindowStore`, the only thing we could do is add a bit of a hack to see if the inner most store is a `RocksDBSegmentedBytesStore` and then we could call `get(..)` on that. If it isn't, then we'd still need to call `fetch`. For the DSL this would work as the only time we have duplicates in the `WindowStore` is for joins and we disable caching for those so it skips this code path. However, for the PAPI, we would need to always disable caching if duplicates are set. Which we probably should do anyway as it won't work as is.
I thought the timestamp would uniquely define the segment in which that key is stored.
Here we need to do: `final Agg oldValue == newValue == null || sendOldValues ? fetchPervious(..) : null;` This is because `SessionWindows` have a dynamic time range, the the start is always fixed. So we need to send deletes for the previous smaller window when a window is merged, i.e, a simple count: a@0 -> SessionKey(key=a start=0, end=0), 1 a@5 -> SessionKey(key=a start=0, end=0), null (delete this as it is merged) SessionKey(key=a start=0, end=5), 2 (this is the new merged session)
nit: move to line above.
nit: if you want a new paragraph you need to add `<p>`
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
`deteremined` => `determined`
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
nit: add `a {@link Named} config`
nit: remove empty link
nit add `a {@link Named} config`
`KeyValueStore` -> `TimestampedKeyValueStore`
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
Ah, sorry to say, one more thing slipped by me before. We should `verify(inner)` at the end of both of these tests. It should actually fail for `shouldNotPutIfSameValuesAndGreaterTimestamp` because we should _not_ call `inner.put` in this case. To fix that, we would just delete L202, where we set up the mock for `inner.put`. Then, the mock would be initialized to expect no calls, and the verification would fail if we did call it.
nit: move to line above.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Note that Kafka only supports kerberos as the SASL mechanism.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Do we still need these 2 blocks? In `setup()` we already consumed all messages
`replicaing` -> `replicating`
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
this overwrite of mm2config should go in the setup method, IMHO
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
yeah, other stuff from `RuntimeMXBean` was just a suggestion, obviously i wouldn't limit to that. classpath was the main one i saw that is useful. i don't think that it's too confusing since it is also alongside a bunch of other general system info. also, we still support the classpath approach, we just won't fix conflicts. we've definitely had cases on the mailing list that ended up being classpath issues that we probably could have spotted the likely root cause more quickly if we had had access to the classpath info.
Should we add some more stuff to round this out (and make use of all the support for reporting more than one value...), e.g. some other `RuntimeMXBean` info? For example, classpath info seems like it'd be useful (probably more so before `plugin.path`, but still probably handy from time to time).
I'd suggest moving this static method after the non-static methods.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
We may be able to save all this hassle by defining alias group as a map of `target` to a list of `deprecated` configs? We defined this as a 2-dim array but we always convert it to lists...
Oh... Thanks for the reminder. @RivenSun2 , could you summit a small KIP for this change? Thanks.
Why not use the copy constructor? It could even be done inline: ``` java for (Map.Entry<String, Object> entry : new TreeMap<>(this.values)) ```
Is this punctuate function necessary? Since we do not `schedule` in the init functions I think this punctuation will never be triggered.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
Actually a more general question is that for assign(), is checking subscription.isEmpty() sufficient or not. Today we allow subscribe(empty_list) whose semantics is different from unsubscribe(), but they will leave the same state in subscription map.
I think the semantics of subscribe(empty-list) should be similar to pause(all), but leave the consumer still registered as member of the group with coordinator; for unsubscribe() the consumer means "do not talk to coordinator anymore" and moving forward we may add a leave-group request (there is already a ticket I think). As for now let's keep the original approach to check the assignment upon each commit call; thoughts? @hachikuji @onurkaraman
nit: remove newlines
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
You actually do not need `this` here, right? The values are the default initialization values in Java. And `super` is also called in the default constructor. So actually, you could remove this constructor completely.
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
nit: Starting a message with lower case feels a little unusual.
I think that's the optimal compromise. Keep `null` since it's a keyword and avoid starting a sentence with it. *Any* seems to work fine here.
Seems to fit in one line
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
Seems like a no-op
Nit: let's avoid moving all of these imports around. Our convention is to place the `java` and static imports at the end, and moving them unnecessarily just complicates maintenance.
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
Also: should only call onPartitionsLost on owned partitions that no longer exist
We no longer need `DEFAULT_OUTPUT_TOPIC_NAME` if all caller will use `createTopic` now
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
yes, it seems to be not what this test is checking on. I think we can drop it here.
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
prop: Use `assertThat()` here and in the other overload of this method.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Not for this patch, but we should do a KIP to add support for batch topic creation.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
`start` is not used.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
Would it be worth having the rate limiter have 2 levels: the first level logs every error and the second logs every nth error? My concern is that simply cutting off logs entirely once we hit a certain # of messages can mask later messages. You don't want to mask those entirely, you just want to cut them off. I think any approach using timestamps is probably going to get too complicated. But I think still printing every 1000th message would be useful so you eventually see the problem.
There is a built-in for this `Function.identity()`
Why do we need to do this? `log` is private and is not used
Avoid concatenation? This should still log the exception with stack trace: `log.warn("{} caught an exception: ", what, e)`
I'd suggest moving this static method after the non-static methods.
Add the stream task id prefix here as well for both exception message and the warning log entry.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
There's already an expected exception, we can remove the `fail(...)` call here.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
nit: `fetchOffset` instead of `fetchedOffset`? I think this is meant to describe the offset from the fetch request.
Yes, the sensors are created in `Sender/Fetcher` to avoid knowledge of the different names to the network layer. Recording is done in the network layer since `Sender/Fetcher` don't see all responses (now that any response may be throttled) and to use common logic.
nit: do we still need this function? Could we just reference the `metricLock` object directly? Since it is private my understanding is that it was not intended to be used outside this class.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Sensor names don't appear in JMX.
I don't think this is necessary to add here. AFAICT `StreamsKafkaClient` is only used in `InternalTopicManager` and it can just be constructed there
typo: `consume` -> `restore`.
nit: add `a {@link Named} config`
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
`windowSize` should be `Duration`
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
nit: we're missing a period and space following the partition. I can fix this when merging.
`[because] the tool`
typo: `will does not`
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
`statestore` -> `local stores` (to use same terminology as above)
Nit: remove empty line ;)
ditto for the rest of the test
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
I don't have the full context on the history, but it would not be easy to change the API... I talked to Jason about it, and it seem we can just move forward with this PR as-is, and could do a KIP later that allows us to store metadata as `byte[]` type if we really need to change it. Atm, the metadata is just a few bytes and the overhead does not really matter IMHO.
We should return `RecordQueue.UNKNOWN` instead.
Nit: (simplify to) `"Unsupported offset metadata version found. Supported version {}. Found version {}."`
Same as above: Also, we should log a WARN message there, that the the found metadata is corrupted and cannot be decoded.
Nit: `log.warn("Could not decode offset metadata.")` I think it's better not to log `encryptedString` as we don't know what's in it, and we could potentially leak sensitive information.
This message should say "Consumers earlier than 0.10.1.0..." since KIP-74 was part of 0.10.1.0.0.
Hmm, we can't use shallowEntry.offset() since not all messages will be retained. Perhaps we could just maintain maxOffset as we add messages to retainedEntries.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Hmm I think a better test would be to skip the linger wait time. This would show that calling `forceDrain` causes us to forego the linger: ```java assertFalse(acc.needsDrain(time.milliseconds())); acc.forceDrain(); assertTrue(acc.needsDrain(time.milliseconds())); assertEquals(0, acc.timeUntilDrain(time.milliseconds())); ```
The try/catch is over the whole block, so the message seems a little odd. If this can only happen while reading the headers, we should probably move the try/catch to `getHeaders`. We should also probably rename `geHeader` to `readHeaders` or something like that.
```suggestion public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates, final Set<TaskId> allTasks, final Set<TaskId> statefulTasks, final AssignmentConfigs configs) { ```
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
would be nice to mark all params final whilst we are changing this
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
add `final` (also all other methods below)
@becketqin Yes, my preference, as mentioned above, is to deal with that problem separately. We should not make behavioral changes without first raising the issue at least in a separate JIRA. The unintuitive thing about the proposed behavior to me is the fact that although the consumer's position remains at the offset of the failed record, the next returned record will be from the offset after that position. You can see this in the test case below: the consumer's position is at 1, but the returned record is at offset 2. This makes the behavior less deterministic. It would be nice to maintain the invariant that the next fetched record is always the first record at an offset greater than or equal to the current position.
Please update the test case as I suggested. Thinking about the current patch. If there is an exception parsing or validating one of the records, we will update `PartitionRecords.nextFetchOffset`, but we will not change the current position (i.e. what is returned by `consumer.position()`. That means in the next call to `poll()`, we will simply discard the rest of the records. So there is no behavior change here and my suggestion above simply makes the behavior explicit. You can confirm this by updating the test case.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
nit: we're missing a period and space following the partition. I can fix this when merging.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Yeah good catch, see above
Very good catch. Thanks, @abbccdda .
nit: newline for better IDE debugging.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
Thanks for the catch!
nit: ditto here, `cluster()` will reconstruct a new object on each call.
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
req: this member should be removed.
Use `File.separator` instead of `/`
Just some more nits. Can you add `final` wherever possible: `ClassLoader`, `String filename`, `BufferedReader`, `for(final String...)`, `Exception`
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
It seems a little harsh to set a fatal error condition because the process logged something on stderr. A lot of applications use stderr as an output. Maybe we should just log this with `log.error`.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
True, will we ever want to have this ability? But the change seems fine to me.
why do we make lines longer? harder to read now
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
I didn't think of that before, but now that you mention it, the change makes sense to me.
What is the reason for having `assertDoesNotThrow` here and below? The test will fail if an exception is thrown, so seems like unnecessary noise.
That's a good point.
This catch is a bit weird to me. Could you create a true `CompletableFuture` carrying the exception `new TimeoutException()` instead of mocking object? For example: ```java CompletableFuture<RecordMetadata> future = new CompletableFuture<>(); if (success) future.complete(new RecordMetadata(new TopicPartition("tp", 0), 0, 0, 0, 0, 0)); else future.completeExceptionally(new TimeoutException()); ```
The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).
nit: not a big deal here, but for unit tests I think given the very low overhead it is better to separate out each of the cases into their own test as it can help make it more quickly obvious if issues are with a specific case or if it affects multiple cases.
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
`selector.connected()` is cleared after each `poll()`. So we should keep track of the total number and compare the total against `conns`.
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
nit: missing `<p>` for new paragraph
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Thanks for verifying @vvcephei!
FWIW - this is probably something that could be tested easily with good usage of a real Mocking framework (rather than hand coded stubs). We should probably start making use of something in streams as it would save a lot of effort and with correct usage lead to a better overall design.
Is this the reason that `threadProducer` is not `private`. I'm not 100% sure, but if it is maybe there is another way of testing this so that it can be `private`, i.e., if you are using the per task model you know that the `clientSupplier` should be called n times. However, with the thread model it should only be called once.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
looks like this isn't used
If that was the case wouldn't we get an exception due to the topics not being co-partitioned? We should have a test for this case.
Yeah if it exists elsewhere let's just leave it as is for now.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
nit: add `{@link Named}` here and elsewhere below
nit: missing `<p>` for new paragraph
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
`KStream` => `{@link KStream}`
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
Yep. Thanks, @bbejeck !
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
same for the store
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
Might be excessive to add `final` everywhere. I'd say if it's self-evident that things aren't mutated I could skip final.
We do not need to mention "partition" here since it is supposed to be abstracted from users, ditto below.
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
as above nit: double space `to Kafka`
True, will we ever want to have this ability? But the change seems fine to me.
This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.
nit: initialize `appId` with the prefix you want (eg `appID_`, or something more descriptive like `TaskMetadataTest_`) and then just append testId, ie ``` appId = appId + testId; ``` Just makes it easier to locate what the prefix is (same with `inputTopic` below)
```suggestion processed = new AtomicBoolean(true); ```
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
Not sure what the intent is here, to increment the number between each test, or between each instance of this integration test class within the JVM... It actually does the latter.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
Hmm.. Do we know how this is getting propagated in all cases? Some of the responses are handled using a `RequestCompletionHandler`, but NetworkClient currently eats any exception raised from this callback.
nit: would be nice to be consistent on the pattern we use here
Nit: ```suggestion @Override public void close() { ```
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
`error` is unused
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
may be use Objects.requireNonNull
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
I'm ok saving this for #7409.
There's also https://github.com/apache/kafka/pull/11128 to consider.
Same for generation and member.id. We could just call `request.data().generationId()`
Hmm. I think we're just following the same pattern used elsewhere, but I think the version check is redundant. We already know it's a valid version because the request constructor verifies it.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
This loop is a little puzzling to me. Is it not the same as the following: ```python assert all_captured_preferred_read_replicas[non_leader_idx] > 0, ... ```
see my question above about using mocks.
See my question above regarding using mocks.
nit: Please fix code style.
Just wanted to say I really like the way this unit test is written! With the right usage of mocks we would avoid having any time-dependent flakiness.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
`Count is a {@link SampledStat} that maintains a simple count of what it has seen.` So with this stat, its value will be increased for the window period, then suddenly drops to zero, then start rising again. So it's hard to alert on such a metric, on the other hand `Rate(Count())` will record the average rate per time unit (here second), so users in practice can easily set a threshold for alerting, and even if they want "zero tolerance", setting the threshold to be 0 can still satisfy their needs.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Sensor names don't appear in JMX.
We could refactor out a helper function here.
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
We should pass in `record.topic()` instead of `""` into `deserialize`.
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
`consumerProps` and `producerProps` are of type `Map`, therefore the `.toString()` is probably not readable. So you'd need to convert these into a comma-separated list sth like `K1=V1,K2=V2,...Kn=Vn`.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content. Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
If I understand correct, a record read from the changelog topic should only be: 1) having no headers at all (old version) 2) having a singleton header with `v --> byte`. All other cases should indicate a bug in the code. So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value: 1) byte == 1: do as below. 2) otherwise: to not support forward compatibility, we can just throw unsupported.
- It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm. - And we know that KafkaStreams never writes headers into changelog topics. Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case. We can also do a version check as suggested by Guozhang.
It's a little confusing that this is named newValue, but is sometimes actually priorValue
nit: `final` params
Your understanding is correct @mjsax .
Add the stream task id prefix here as well for both exception message and the warning log entry.
The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).
req: typo unknown Pid
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
seems like it should at least be info, if not warn
It seems like we ought to just define `log` at the AbstractTask level and avoid having two almost identical `maybeInitTaskTimeoutOrThrow` method definitions.
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
This is the wrong format for this log message. The exception won't be logged. You have to format the string first: ```suggestion log.debug( String.format("Timeout exception. Remaining time to deadline %d; retrying.", deadlineMs - currentWallClockMs), timeoutException ); ```
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
Currently the code iterates over the active tasks and assigns all standby tasks for each active task. If the standby tasks cannot all be assigned, we might end up with all standby tasks assigned for some active task but none for others. What do you think about to assign one standby task for all active task and then assign the second standby task for all active task, and so on. In this way, it is more likely that at all active tasks have at least one standby task assigned. I am aware that the default standby assignor has the same drawback.
When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting.
nit: we can directly break the while when `numRemainingStandbys == 0`, so that we don't need to run the redundant `findClientsOnUsedClientTagDimensions` in the last run. Ex: ```java countOfUsedClients++; numRemainingStandbys--; if (numRemainingStandbys == 0) { break; } clientsOnAlreadyUsedTagDimensions.addAll( findClientsOnUsedClientTagDimensions( clientOnUnusedTagDimensions, countOfUsedClients, rackAwareAssignmentTags, clientStates, tagEntryToClients, tagKeyToValues ) ); ```
Wouldn't this be equivalent and maybe a bit more concise? ```suggestion UUID lastUsedclient = activeTaskClient; do { fillClientsOnAlreadyUsedTagEntries( lastUsedclient, countOfUsedClients, rackAwareAssignmentTags, clientStates, tagEntryToClients, tagKeyToValues, tagEntryToUsedClients ); final UUID clientOnUnusedTagDimensions = standbyTaskClientsByTaskLoad.poll( activeTaskId, uuid -> !isClientUsedOnAnyOfTheTagEntries(uuid, tagEntryToUsedClients) ); if (clientOnUnusedTagDimensions == null) { break; } clientStates.get(clientOnUnusedTagDimensions).assignStandby(activeTaskId); countOfUsedClients++; numRemainingStandbys--; lastUsedclient = clientOnUnusedTagDimensions; } while (numRemainingStandbys > 0); ```
nit: including the acked offsets to checkpoint as well.
nit: add `final` (we add `final` to all variables when possible -- applies multiple time -- please update all variables (inc. loop-variables)
remove var -- only used once.
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Can we remove the `AndHighAvailabiltiyEnabled` suffix from the test name? And/or just generally shorten it if you have a better idea
Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
Thanks for the explanation. A bit subtle as you had said. :)
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
This is not required as contained in the check next line.
nit: move below the shortcut return below.
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
`advanceMs` is not the same as provided input parameter `advance` -- this would make the error message miss leading.
We should call out explicitly that this is setting the grace period to 0, which means that out of order records arriving after the window end will be dropped. Otherwise it's too easy to just use this method without thinking any further about the grace period and what it means/whether you want it
Same as above mentioned, the validation didn't get handled in new API.
nit. I think there is `.` missing `since 3.0[.] Use`
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Changed it locally.
not be => not be able to
Changed it locally.
newuntil => newUntil
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
nit: we can use `map#compute` to replace getOrDefault + put.
You can use `EnumMap`.
Similar here, we can cache the result in case to be reused.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
There's an interesting edge case where `record.isValid()` could throw a `IndexOutOfBoundsException` if the buffer size is smaller than 4 (i.e. we fail when we try to extract the checksum from the buffer). Obviously, this means that the record size field was itself corrupt (since it should never be that small), but it can happen. I'll file a separate PR for that.
There's a space missing after `offset`, I'll fix it before pushing.
Also: should only call onPartitionsLost on owned partitions that no longer exist
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
Ack, I get it now. Thanks for clarifying.
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
nit: might be better to name it windowedKTable
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
nit: Starting a message with lower case feels a little unusual.
nit: I know it was already like that, but since we are now passing the actual class object, you might want to refer to the class object as `klass` (I like the keystrokes on this one) or `clazz`, which are common naming conventions when using class objects. Then call the String field `className` or similar. Of course JsonProperty will continue to be called `class`. Up to you.
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
Same thought w/r/t performing assertions on the complete set of returned plugins: ```suggestion Set<Class<?>> excludes = Stream.of( ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES, ConnectorPluginsResource.TRANSFORM_EXCLUDES ).flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of( SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS, CONVERTER_PLUGINS, HEADER_CONVERTER_PLUGINS, TRANSFORMATION_PLUGINS, PREDICATE_PLUGINS ).flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(false)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ```
Typo: should be "or larger than the number of available brokers"
We do not throw `InvalidTopicException` "if [the topic] is not found"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Should have a comma after "for example"
Maybe use log parameters instead? ``` java log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), t); ```
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
Actually I think it works: http://www.slf4j.org/faq.html#paramException.
Code convention nitpick: there should be a space before the colon.
Should we check the `key` is not null here? Since in later callers e.g. `final KeyQuery<Bytes, byte[]> rawKeyQuery = KeyQuery.withKey(keyBytes(typedKeyQuery.getKey()));` we do not check if `getKey()` is null or not, and `keyBytes` function could throw if it is.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
We really need a bug fix release for this! \cc @guozhangwang
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
nit: since we're not doing anything in the EAGER case, couldn't we simplify this: ```java if (protocol == COOPERATIVE) adjustAssignment(ownedPartitions, assignments) ``` Similarly in `onJoinPrepare`
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
Please use string interpolation. There are a few other places like that.
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
Changed it locally.
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
Ditto above. I would recommend having consistent explanations here.
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
Thinking about this some more, not sure there's a lot of value in forcing users to set `idempotence=false` in cases where they're setting `acks=1|0` or `retries=0`. So, I'd change the warning to info for these cases. `max.in.flight.requests.per.connection` is different since it's an implementation constraint that `idempotence` doesn't work when it's > 5 vs inherent to the configuration. For this one, I'd have a warning and mention that it will become an error in Kafka 4.0.
I think it would read better if we remove `config` from the sentence.
createTime -> creationTime
We are passing `now` everywhere else. Maybe we can just keep the argument name the same.
Still not used
`tp` is not used anymore.
Thanks! Will push this shortly.
Minor but I'm not sure if we'd prefer the builder's toString or the underlying request struct's toString as was formerly the case.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
nit: We should use `groupId.idValue` here and in the others.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
This is still not used
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
This test case passes without the fix. It doesn't look like it even goes through the auto-commit path.
The name should mention the fact that this case covers auto-commit.
I referred to the value serializer.
Nit: rename to `shouldThrowOnInvalidTopicNames`
nit: use `"table-source"` ? It's naming a source node, not a processor node.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
nit: ".. select the grouping key and the value to be aggregated".
remove "on a window basis"
remove "on a window basis"
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Hmm, we're using a raw type here and a few other places. This is discouraged (type checking is disabled in these cases). If we don't want to propagate the generics when we use the superclass, we should probably drop them.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Nit: let's avoid unrelated line additions.
adding `final` in every argument here was just noise but with not much added value. Given that we don't write constructors like this in Connect I think we should revert and add the new argument without the `final` specifier.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
It actually would be helpful to include the exception's error message in this line, since the message alone might be bubbled up via the REST API. ```suggestion log.error("{} Error converting message value in topic '{}' partition {} at offset {} and timestamp {}: {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e); ```
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
Up to you I guess. No need to expand the scope even further for a tiny nit.
It seems like we can migrate away from the deprecated method in this test.
This is definitely a nitpick, but can you put this one fewer lines? 2 lines should be enough for this. Same for the ones below.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
And why is this test deprecated as well? More generally it seems the `context#getStateStore` function was being deprecated but it was not explained in the KIP wiki.
This is not introduced by this PR: the name has a typo: through => throw
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: blank line missing here
nit: fits in one line
nit: extra blank line
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
ditto to `KStreamImpl`
nit: missing . at end
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
nit: add a space before the `:`.
style nit: normally we'd use braces around blocks unless they're a single line
I would call this one `topics()` as you did already in the request.
strictly speaking, you don't need this yet. Still needed when we evolve I suppose
Do we really need to print `super.toString`? Ditto above.
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
doesn't look a great name for its behavior. perhaps something like currentContext
nit: as in `position` above, `this` is not required
empty line needed
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
This is not related to your PR at all. It seems that if `offsetRequestSpec` is `null` here, `future` will be `null` as well cause `futures` is initialised based on `topicPartitionOffsets`. If it turns out to be correct, it may be better to just log a warning here like we do in `createTopics()`.
I just noticed that we don't ensure that all futures of the current broker are completed. It would be great to ensure it by using `completeUnrealizedFutures` method if `retryTopicPartitionOffsets` is empty. We already do this in `alterReplicaLogDirs()` if you want to see an example.
This logic seems not correct? If there's a global error, we would fail all the internal futures. But that global error could be set because only one of the member response has an error as well.
nit: align parameters.
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
My understanding is that Jun is suggesting that we should set `checkHC` to true for the Java client if message format is 1.
We should add a `null` check to allow closing a deserializer that was not properly setup
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
The purpose of the `Map<TopicPartition, Long>` was to avoid adding a new object type. But since we're doing that anyway what about just making the call: ``` public void commit(List<OffsetMetadata> offsets, CommitType type) ``` where now the `OffsetMetadata` class includes the `TopicPartition`? This is arguably no more complex than the original call in the case where you aren't giving metadata. Also does `OffsetMetadata` imply metadata about the offset whereas in fact this is both the offset and metadata? Other options would be `OffsetCommit` or `OffsetInfo` or `PartitionOffset`. Don't have a strong feeling on this one.
@ewencp As you say, it does hinge on whether this `commit` overload is used often or not. If it is not, then having this _and_ having `commitWithMetadata` seems excessive. You guys have a better handle on this, so I'll leave it to you. :)
This one is still not using a tab.
Is this necessary? The leader epoch is -1 by default.
We could make this field access `public`
We could remove this function
It seems that we are logging at the debug level. I am wondering if we should log at WARN as before in ZK based appoach. ``` if (exception instanceof ApiException) { log.debug("{}: failed with {} in {} us", name, exception.getClass().getSimpleName(), deltaUs); return exception; } ```
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
this line is still a bit long... You could try a static import for `singletonList`.
cosmetic: extra space at the start
this is creative :)
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
-> `storeName()` (without `get`)
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
```suggestion final Map<TaskId, Integer> tasksToRemainingStandbys = computeTasksToRemainingStandbys( numStandbyReplicas, statefulTasksWithClients ); ```
Something does not work as expected in this algorithm. According to this doc, the assignor should fall back to distributing tasks on least-loaded clients. However, the following test case fails: ``` @Test public void shouldDistributeTasksOnLeastLoadedClientsWhenThereAreNoEnoughUniqueTagDimensions() { final Map<UUID, ClientState> clientStates = mkMap( mkEntry(UUID_1, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_1), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_0)), mkEntry(UUID_2, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_2)), TASK_0_1)), mkEntry(UUID_3, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_2)), mkEntry(UUID_4, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_1_0)) ); final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates); final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, CLUSTER_TAG, ZONE_TAG); new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs); assertEquals(1, clientStates.get(UUID_1).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_2).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_3).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_4).standbyTaskCount()); } ``` The standby task for active task 0_0 can be put on client UUID_2 and the standby task for active task 0_1 can be put on client UUID_1 without breaking rack awareness constraints. Standby tasks for active tasks 0_2 and 1_0 cannot be put on any client without breaking rack awareness, so they should be distributed on least-loaded clients. However, that does apparently not happen, because client UUID_3 and UUID_4 are not assigned any standby.
is old metadata missing expected after we start off? Might be useful to add a debug log or trace if this is not normal.
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly: ``` log.error("Could not initialize partition time. Committed metadata is corrupted.", e); ```
nit: move below the shortcut return below.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
nit: add `final
prop: Would it make sense to also have an overload with just a flat list, i.e., ``` void runTestWithDriver(final List<TestRecord<Long, String>> expectedResult) ``` Maybe it would simplify the code of some of the tests. Hopefully, you can share some of the code in the overloads.
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: could make access private and get an accessor.
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
nit: top of class
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: Indicate that this needs shallow iterations on the entries.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
Do you need this `int` cast? I thought I removed that in my branch and it worked fine.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
nit: add `final`
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
Just wanted to say I really like the way this unit test is written! With the right usage of mocks we would avoid having any time-dependent flakiness.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
ah, right. nah, that's fine. just when reviewing I had the thought that if we guaranteed non-`null`/non-empty in the constructor, this wouldn't be necessary. i realized that it was actually intentional, but easy to miss when reviewing here and not getting the same highlighting as an IDE
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
to me it seems like we can't possibly know what the constraints of all reporters would be and they don't provide an interface for validation, so it should be up to them to figure out how to substitute. but i've also asked some other folks to maybe chime in here who may have better context on how we've handled this elsewhere.
I know the naming thing has bit us in the past, is this same approach used elsewhere and/or how was it decided on? Specifically, metric name constraints really shouldn't be JMX specific if that is the case here, despite the fact that the metrics is so obviously JMX-inspired. I can easily find https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala#L46 but nothing else. Have we not had the same problems because metrics w/ topic names in them already have constraints on the naming? If I am remembering correctly, I think maybe both @gwenshap and @junrao were involved in some discussions, I think `-` vs `_` was a problem at some point? Maybe one of them could chime in here.
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
nit: empty line.
We can remove the code block line 82-85 above since it will be called here.
nit: align parameters.
For compatibility, we cannot change any message format of old versions, so I think we need to bump up the version to 5 and only do compression at 5 while no compression at version 4. Think about the case: when a streams application with multiple instances are rolling bounce to be upgraded, maybe some instance are already on version 5 and hence sends the assignment back compressed while some other instance do not recognize this version at all. We need to distinguish the cases when to compress and when to not compress. @mjsax has done the version probing protocol and he can provide more.
I think the versions of subscriptionInfor / assignmentInfo are coupled, so if we bump up on one side we should do the same on the other.
I think this config property key seems a misfit, and probably reflects an earlier incantation of the design before KIP acceptance. It might be worth - in a separate PR - renaming this to something like `errors.tolerance` to better align with its purpose.
We'll need a separate AK issue, then.
nit: matches the old behavior is very relative
Maybe use a semicolon instead: "task failure; 'all' changes..."
The first part of the doc sounds incomplete.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
same here -- sounds like CachingKeyValue with TimestampStore
I'd suggest rename `ProcessorName` to `GraphName` to be consistent with the base `StreamGraphNode`, also to distinguish with the physical topology's `XXProcessorNode`. Ditto else classes.
typo: `per reach record`
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
as above nit: double space `to Kafka`
Make all params `final`
And again with `final` if you don't mind
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
line too long
This line is failing checkstyle. I think we need a space after the first semicolon.
Would it be worthwhile to also add an empty collection at the end of the map as well to make sure that case is covered? You could use LinkedHashMap to ensure order.
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
Yeah, the logic seems right to me.
We want the exception to be thrown in either case right? If requestTimeoutMs is greater than either sessionTimeOutMs or fetchMaxWaitMs an error should be thrown? I think this a difference between the english meaning of "and" and the programatic meaning of "&&".
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
Ah! I misread this as turning `logAll` *on* instead of *off*. Now I get it :)
Was just thinking about how long a. transaction might possibly be open. 1 minute SGTM
nit: this loop is a little unconventional. Maybe we could use `pollFirstEntry` instead of the iterator? Similarly in `setNumKip500BrokerNodes`.
You might consider using `OptionalDouble`.
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
As `KafkaStreams` implements the `AutoCloseable` interface now, `close()` should be called automatically when the `try {}` block is left -- that is the whole purpose of `AutoClosable` and try-with-resource construct -- it frees you up to call `close()` explicitly (so you cannot forget any longer).
I missed the fact that we moved the `waitForCondition` check _inside_ of the try-catch block... For this case, we need to call `close` explicitly of course, as we are still in the block and `close()` is not auto-called yet... Sorry for the confusion.
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
nit: add `final`
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
I think it is probably worth adding as even if it is deprecated it is still supported
Is this punctuate function necessary? Since we do not `schedule` in the init functions I think this punctuation will never be triggered.
Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.
Yes, does not hurt to leave it. Just for sure.
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
Nit: can be `final`
Nit: Please use `{ }` (even for one line blocks)
Nit: can be `final`
This should be three tests.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
nit: we can put kafkaStreams in a try block.
Nit: should the method be named `testOptionsDoesNotIncludeWadlOutput()` instead? The point of this PR is to prevent including WADL in the OPTIONS output, but the existing method name makes it seem like we're testing the content of the WADL output.
I think a default mock would be enough to get a better error message. Strict mocks also check the order of the calls which is not needed here. I would also pass `new MockTime(1)` instead of increasing the timeout, because the test retrieves the time twice. `new MockTime(1)` would progress time 1 ms each time. Hence, we would be within the 100 ms of the global `max.poll.interval.ms` and since we control the time progression the test would be more robust than by increasing `max.poll.interval.ms` to a higher value. With `new MockTime(1)`, we also avoid running the test indefinitely in case of a mistake in the production code, since after 50 time retrievals, the test would run into a timeout and the admin client mock would throw the unexpected method call error. Probably, it would be even better to use something like `new MockTime((Integer) config.get(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)) / 3)`.
After this PR is merged, I will open a PR that changes also the other usages of `Time.SYSTEM` and `createNiceMock()` since I think that would make all tests more robust. I do not do it now because I am still working on that code for KIP-698 and I do not want to run into too many merge conflicts.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
Nit: move these two static factory methods above the non-static member variables, so all static and non-static members are together.
It looks like this is not used anywhere
As above: need to keep default value.
Might be overkill if this is the only use case, but we could also add a composite validator.
`Note when the windowed serde class is used, one needs...`
We should log an error that prints out what the two configs actually are
`orderInGroup` param is duplicated for key & value converter
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
Nit: `.` full stop missing.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
Looks like all of these fields can be package private
I just find it odd to mutate an array that is a parameter to a method. Besides, using an `ArrayList` is more straightforward than using an array and maintaining an index ;-P
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
nit: 'else' can be dropped
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: add `final` (same line below)
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Nit: move into the `if` block where it is used. (also add `final`)
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
Nit: new line is unnecessary, and there's a misspelling: ```suggestion log.error("Encountered an error while awaiting an errant record future's completion."); ```
nit: initialization is not required
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
Hi, may I ask why do you do `@link` instead of `@see` annotations? :)
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
nit: move to line above.
req: typo unknown Pid
nit: remove empty line
nit: add `final` (2x)
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
Don't need `Vin` and `W extends Window` here
Don't need `Vin` here
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Can we call this something like `ensureCopartitioning` or `processRepartitions` or something? My take is that the copartitioning is the main point of this method so that's probably good to include in the name
@vamossagar12 Up to you I guess. I'm ok doing it all here since the changes seem pretty small.
I think the original KIP stated that the LeaderChange message would encode the set of voters that had voted for the leader. We thought this might be useful for debugging. Later on, we had a change of heart and decided it would just be the set of voters. Now I'm thinking it might be useful to have both. The log will always remember who the voters were at the time of the election and which voters had granted the leader's candidacy, which could be helpful in case of misconfigurations. For the set of voters which voted for the current leader, I think what we want is `CandidateState.grantingVoters`. However, by the time `onBecomeLeader` is fired, we have already dropped the `CandidateState`. One option is to carry `grantingVoters` over to `LeaderState`. We might also be able to pass it through `onBecomeLeader`. This will be easier if we get rid of the call to `onBecomeLeader` in `initialize()`. Following KAFKA-10527, it is not possible to initialize as a leader, so we could raise an exception instead.
Yeah, if you don't mind, it seems like a gap. Thanks!
Fair enough, we could relax later when we experiment out the static quorum changes.
Thanks for doing this. I also noticed it was missing and fixed it in this PR: https://github.com/apache/kafka/pull/10085/files#diff-1da15c51e641ea46ea5c86201ab8f21cfee9e7c575102a39c7bae0d5ffd7de39R134-R137 Maybe reconcile the two changes and we can merge this one.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
For consistency: {@link KafkaStreams} instance
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
nit: if you want a new paragraph you need to add `<p>`
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
@spena just ping to make sure you get this on the follow-up PR.
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
nit: double space
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
We just need to make sure the extracted generation from all three processors are the same.
Should this be `num_lines=3` (cf. L116 and L126)
Cheating the compiler, woohoo!
Nit: could just throw the exception directly here; doesn't appear to be much benefit to putting that in a separate `setup` method.
This can be final.
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
Seems to fit in one line
We shouldn't return `null`, but instead return a "unknown query" result.
Dropped this unnecessary duplicate code, as we discussed.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
Looks like we were not handling the right query variant before, but it didn't come up yet because other tests were failing before we got to this point.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
just `name` should be fine
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
same for the store
Well, if you want to match the use of `DESTROYED`, `RUNNING` probably makes the most sense since that is the target state you want the connector/task to be in. But I'm not picky, either one works.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Shouldn't this result in a change to a failed status? I think we'd either need to do that here (if thread safe) or have some way to communicate it to the original thread? We can defer this to another patch if we already weren't handling this case properly.
We should limit this suppression to the method for which we really need it instead of the whole class
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
Can we also have some boundary case tests for the other branches in that method? Since it's not obvious that `now` can be before the other values, it's good to have test cases for that.
Hmm, we're using a raw type here and a few other places. This is discouraged (type checking is disabled in these cases). If we don't want to propagate the generics when we use the superclass, we should probably drop them.
To make this really interesting, we would need to add some sequence number bookkeeping. Really its the sequence/epoch bookkeeping which makes the implementation so complex.
Similar to the offset commit path, it would be useful to validate here that each partition that was written to was first added to the transaction properly.
Maybe `Producer epoch...`. Also, not sure the exception message adds anything given what's already logged. Maybe we should remove that.
Another class of failure that we can simulate is when a request reaches the broker and gets handled, but the connection is lost before the response is sent.
nit: it was correct before
Nit: let's avoid adding new lines in code otherwise unaffected in the PR.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
Also, topic can never be `null` if it's coming from a parsed config value that doesn't have `null` as its default value. (another way to think of that is that you can't pass a `null` value from properties)
would be nice if this was a method in the config
Can just return `name.startsWith(acl.resourceName())`
Not sure if it makes a big difference, but we could use EnumSet for these.
nit: need to update this since resource name is before pattern type now
nit: `final` is redundant for private static method
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
nit: line too long
nit: line too long
Ditto on removing these before/after methods.
Ditto on removing before/after
nit: should we inline these? The variable names are barely shorter than the method names.
Maybe use log parameters instead? ``` java log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), t); ```
Actually I think it works: http://www.slf4j.org/faq.html#paramException.
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
Code convention nitpick: there should be a space before the colon.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
Nit: this can be written more concisely by using `Arrays.asList`.
Nit: this brace should be on the previous line.
Nit: there should be a space before and after the colon.
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
This exception can't be thrown by DeleteTopics.
Please include TopicDeletionDisabledException here.
We do not throw `InvalidTopicException` "if [the topic] is not found"
Typo: should be "or larger than the number of available brokers"
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
It was removed from the other versions of `group` but not from here.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
I think we can move this logic into ValueOrOtherValue as another static constructor.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
(especially given that below you use the simple name)
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
Thanks @ewencp! Glad I'm still up-to-date on that. Happy to adjust per project and yes, unnecessary diffs are better to be skipped in non-cleanup/non-refactoring PRs. Huge fan of that.
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
The number has changed and 5 is no longer relevant.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
Hmm, if we performs an unclean leader election, the only replica in ISR should just be the new leader since the data in existing ISR is not guaranteed to match with the new leader.
Sorry for being late on this. Populating static variables from a non-static method is generally not a good practice since it's generally not thread-safe. Given how JMH works, it's fine for those fields to be non-static right? Also, it seems more realistic since the data is isolated per run.
@original-brownbear I understand that part, but IMHO it's best if we stick to some basic guidelines for our tests/benchmarks. To reduce the GC impact how about we decrease the cache size to 5, and the number of records to insert 25 or so? Or just create an array inline with the keys and declare as a private variable? Either way, we should be able to do the work in the `setUp` method.
But `toString` by default returns `name()`. So, I don't understand why we are overriding it.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
Yes, it makes sense to return a range for an ApiKey instead of a single version. I was just wondering if this method is redundant given NodeVersions.apiVersionRange(ApiKey api). Also, it feels a bit weird for a public facing class to reference Protocol, which is not a public facing one.
Fair enough :)
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
Can you elaborate? What do you mean by > otherwise the state won't proceed
`replicaing` -> `replicating`
nit: `{@link KeyQueryMetadata}`
`it is` -> `they are` (we user is a person :))
This method does not return a `String`. Maybe ``` @return StoreQueryParams a new {@code StoreQueryParams} instance configured with the specified partition ```
nit: line too long. `final` not required -- a static method cannot be overwritten anyway
It might be simpler to add a private constructor that allows to specify all 4 parameters: ``` return new StoreQueryParams<>(storeName, queryableStoreType, partition, staleStores); ``` Similar in other methods
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
Should this ever happen? If it does happen should we consider it a bug? Ditto for the other `hasNextCondition`.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
When a stream is created from multiple topics, we do not have any ordering semantics across those topics, but within a single topic we still follow the within-partition ordering.
remove "(if any)"
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
This line is a bit misleading, as it is referring subject and object to the same class. I'd suggest just remove this line.
Should not have an implementation but call overloaded method.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
It was a good idea to remove the `processOutstanding` field from `CloseMode`, but I think this would be a little clearer if we kept the `notifyDisconnect` field.
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
nit: Indicate that this needs shallow iterations on the entries.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: all the other collections are initialized in the constructor.
nit: "can not" -> "cannot", same below
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
Do we need to do this `close` and `open` here? We do it also on lines 283 & 286
If not, we should move the exception capturing logic inside the dbAccessor as well.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
if (comparator.compare(nextNoTimestamp.key.get(), nextWithTimestamp.key.get()) == 0) we need to advance on both ends while only returning the one from with-timestamp iterator, otherwise we may get duplicates returned.
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
I find it strange that this method closes the consumer it received.
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
Just let the Exception flow, that will automatically fail the test
We can use `Collections.singletonMap()`
This is the callback from the `StreamThread`s so it will be called from multiple threads, i believe. See the inner class `StreamStateListener`
The state that is being updated is private data of `KafkaStreams`. It should be responsible for synchronizing access to its data, not external classes.
Probably want to make this a `ConcurrentHashMap` or `synchronize` access to it. It can be modified etc from multiple threads
Hmmm... Even if we use `ConcurrentHashMap` I guess this would not be enough. In `StreamStateListener#onChange()` we check the state of each thread to compute the new state for KS -- this most be atomic. Using `ConcurrentHashMap` would not prevent a race condition there. So maybe `StreamStateListener#onChange()` should have `synchronized` keyword -- it a single instance we hand into into each thread.
I had a similar thought first. But `StateListener.onChange()` is only called within `KafkaStreams#setState()` which is synchronized.
`return stream(null, null, keySerde, valSerde, topics);` Do the call directly instead of the cast.
topics -> topic. This may well be elsewhere in the java-doc, too
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
guaranteed -> guarantees
Make all params `final`
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
`hasFetchedRecords` avoids the cost of populating the map returned by `fetchRecords`. No locking is needed for that. So, if I understood the suggestion right, it would look something like: ```java if (fetcher.hasFetchedRecords && client.hasPendingWakeup()) client.poll(0, ...) ``` I guess by calling `poll(0, ...)`, we don't have to expose `maybeTriggerWakeup()`.
The current solution seems more direct than what I was suggesting, which is good. A bit unfortunate that we have to expose `maybeTriggerWakeup()`. I guess we could keep the synchronization around `pollNoWakeup` that you've added and still do the `hasFetchedRecord()` approach. Probably it would look like this: ``` if (fetcher.hasFetchedRecords()) { client.poll(0); return fetcher.fetchedRecords(); } ``` The downside is that it `hasFetchedRecords()` may not be trivial to implement since it would require doing the work that `fetchedRecords()` is doing, but without updating the position. Given that, exposing `maybeTriggerWakeup` doesn't seem too bad.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Should we clear the exception here? If not, then we'll have the exception thrown after the rebalance.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
@granthenke Heh, I didn't even mean for the actual credit. But sometimes it's useful in tracking down where an issue was first introduced, the reasoning for the way a particular chunk of code is written, etc. It's an inconvenience, not a serious problem, especially for a set of changes this size.
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Consider naming the topic "topic2" since there are only two topics in the test
as above: flip arguments
as above. (please fix in `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks`, too)
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
Might not a big problem, but I wonder if we should check for the authentication exception before the `wait` as well? It is possible that `awaitUpdate` returns before the authentication failure happens. A subsequent call may then begin with the `authenticationException` not null which would cause a needless `wait`. I think an easy solution is to move this line up to the beginning of the `while` block.
This message seems a little low level for something which will get propagated back to the user. An alternative to consider would be to let `awaitUpdate` return a boolean indicating whether the update happened or not. That would allow us to raise an exception with a producer-specific message from `send()`.
nit: Normally for getters we have the convention of dropping the `get` from the method name.
Checkstyle failure: ``` Name 'JITTER_MAX' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName] ```
Similar logic exists in `ClusterConnectionStates.updateReconnectBackoff`. Maybe we could extract it to a utilities class.
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
Let's rename the method to be more explicit about what we're updating here. ```suggestion public void updateCallRetryContext(CallRetryContext failedCallRetryContext) { ```
Making timeouts configurable could be a good idea, but it's better done in a general way in its own PR.
It seems a bit ad-hoc to have this environmental variable for one test only.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
nit: use `{}` instead of string concat for `retries`
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I think, we should first check for this condition, because we should only check the most inner store -- if an wrapping store would (be mistake) implement `TimestampedBytesStore`, we would return `true` even if the most inner store does not -- this would be incorrect.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
Is this necessary? The leader epoch is -1 by default.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
and -> a
and -> a
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
calc -> calculate
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
`a graph containing`, and correct space between `1. Build`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
we could use `computeIfAbsent`
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
as above mentioned, the `listStore.all()` is not closed here.
I referred to the value serializer.
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
nit: remove empty line
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
I think we can use a utility method provided by the `ConfigDef` class here: ```suggestion List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST); if (topics.contains(dlqTopic)) { ```
Should we log the topic name for this exception? For example, ```has a topic name (xxx) which```
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
```suggestion Arrays.setAll(topics, i -> topics[i].trim()); ```
This could be `ConfigException` instead. With the `ConnectException` hierarchy we get a little bit stuck because we can't have `ConnectException` be the base of all our exceptions and not redefine some. There are some from core Kafka that are fine if triggered from within the framework, and `ConfigException` falls under that category -- the biggest value aside from standardization that asking for `ConnectException`/`RetriableException` from connectors/tasks is that it is a signal for how to handle the exception, but the runtime code doesn't require that everything inherits from those (and in fact we handle other exceptions from connectors as well, we just would prefer that the connector think about how it wants the framework to handle possible errors).
Nit: var should be named `deserializeValue`
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
the `null` is redundant.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
nit: toString not necessary
nit: `--topic` and `--partition` could be extracted as helper static functions
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
nit: we could just build the expected result as a whole set and compare
typo: byteArrray -> byteArray
nit: also add java doc for type `T, O` here
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
`while` seems to be missing
These changes are going to break existing users. For example, I have connectors with a few settings prefixed with `consumer.`. I wonder if we could keep the old behaviour (even if partially broken) while adding the proper prefixes
It's unfortunate for admin we use `source.admin` as the prefix ... So we'd be left with a configuration like: ``` "source.cluster.bootstrap.servers": "localhost:9092", "source.cluster.security.protocol": "SASL_SSL", "source.cluster.producer.some-producer-setting": 123, "source.cluster.consumer.some-consumer-setting": 123, "source.admin.some-admin-setting": 123 ```
I don't think the format mentioned in https://github.com/apache/kafka/pull/9313#discussion_r498298987 would break compatibility.
Yes I'm running MM2 in a Connect cluster.
I meant add support for that format. We obviously want to keep supporting the existing formats
Add a reference to KIP-511 here
Same here, we can cache the result of `Builder.getPartitions(data)` for re-use.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
Yea, my suggestion would be to reuse the existing constructor as the construction of the `AlterConfigsResponseData` seems non trivial for a caller to do, compared with passing a map of errors.
I think the name of the function is better defined as `interleaveTasksByConsumers`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
nit: again this is not introduced in this PR, but let's use `AssignorError.NONE` here and elsewhere to be less vulnerable to enum changes.
Is it ever possible that the `clientState.ownedPartitions().get(partition)` is not null but `allOwnedPartitions.contains(partition)` is false? It seems the second condition is redundant (and hence the `allOwnedPartitions` parameter throughout the `assign` function seems not needed).
My bad, mis read the lines :)
> Just clarifying: After the group has formed, both leader and follower can still trigger a rebalance: leader will trigger a rebalance if the existing topics number of partitions has changed (including the topic is deleted); follower will trigger a rebalance if the subscription has changed (both due to a metadata refresh with regex pattern or user called subscribe again). Is that right? Yes, right. > And if we change the consumer coordinator to allow passing regex to the leader, I think joinedSubscription can be removed completely and only leader need to trigger rebalances unless users call subscribe again on any of the consumer member, is that right? The leader will still have to deal with the potential for a metadata update during a rebalance, so I'm not sure we can remove `joinedSubscription`. At least we won't need this funky logic to try to change `joinedSubscription` after the rebalance though.
I'm not sure this works. The purpose of the `joinedSubscription` field is to remember the exact list of topics that were used when joining the group. If a metadata update arrives after the rebalance has begun, then we can notice the fact that the joined subscription does not match the current subscription and we can trigger another rebalance. With this change, we will no longer be able to detect this case, which means that consumption from a topic matching the subscribed regex will be delayed (perhaps indefinitely). It seems the behavior we want is to only add to `joinedSubscription` those topics which were added to the assignment by the leader.
Add to the end, "as long as they still match the subscribed pattern"
I'm puzzling a bit over whether we should be using `joinedSubscription` here instead of `subscriptions.subscription()`, or whether it matters. Seems it should be `joinedSubscription`. Suppose that `joinedSubscription` contained only [A] at the time of rebalance. Topic B is then created by the leader and assigned. Before the rebalance completes, the consumer discovers topic B, and `subscription` is updated to [A, B], while `joinedSubscription` is still [A]. The consumer then receives the assignment containing partitions from both A and B, but `addedTopics` will be empty (since B was already added to `subscription`). The consumer will then notice that `joinedSubscription` is [A], while `subscription` is [A, B], and request an unneeded rebalance.
Should be `Optional.of(generation)`. `int` values are not nullable.
`The default "all" setting` -> `The default setting "all"`
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
`while` seems to be missing
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
deliveryTimeoutMs should be mentioned
isFull is no longer used.
This is still not used
No longer used.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
@rondagostino are we ok with merging this to trunk? Since this is not required for existing tests which either use ZK or PLAINTEXT brokers, not planning to backport to older versions.
@rajinisivaram Yes, merging to just trunk seems fine to me.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
A docstring for this method would be good :)
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Great. I will review and merge #1808 first and this PR then can be rebased.
Yeah that is fine. My bad.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
I don't think it makes a big difference either way. The intent of the offset commit interval is just to make sure committed offsets don't fall too far behind. It does not need to be a strict schedule. It seemed more intuitive and simpler to me to reset the interval. In any case, we should get rid of this relative tracking of the next commit. If we use an absolute time, then we will not have problems like this in the future.
Is there any reason not to accept this suggestion? I will go ahead and push an update to this PR next week if there are no further responses so that we can get this fix into the next release.
Maybe this is a little simpler? ```java nextCommit = now + offsetCommitIntervalMs; ```
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
Ok, it looks better now. Let's leave it this way, with two lines.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should log an error that prints out what the two configs actually are
There's already an expected exception, we can remove the `fail(...)` call here.
It's good refactor to use `assertThrow`, but the failed message is missed. Please add it in `assertThrow`. Thanks.
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
rewrite test as above using `assertThrows()`.
As I understand it, handleResponse will always be called by AdminClientRunnable from the single 'network thread' (KafkaAdminClient.thread).
OK, let's keep the change to that one field for now.
I wonder if we can just get a Map with the default size. I don't expect this code path to be very hot
nit: also add java doc for type `T, O` here
Here too it seems like we can use the generic version of `prepareOffsetCommitResponse`.
Does this test ever encounter this exception? I don't think we will be able to backport this test to < 2.6 because the method won't exist at all, much less generate the exception that is being caught here. If anything, this generates a less informative NPE later in `put`, and hides the actual root cause.
Seems to fit in one line
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
if you have an unsigned ~~8-bit~~ 16-bit data source
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
nit: remove empty link
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
Please remove empty line.
Should we still log it, perhaps as a warning? If I understand the background, this case is unexpected except with 0.10 brokers, so it seems like swallowing it could mask an important condition.
Thanks for the follow-up.
Ditto here for different exception types.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks @vvcephei -- that is convincing.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
I don't think this is necessary to add here. AFAICT `StreamsKafkaClient` is only used in `InternalTopicManager` and it can just be constructed there
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
nit: "another thread wrote to ..."
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
if we keep ending up with this pattern, it might be clearer to create a `Listener` implementation that delegates to a list of listeners instead of chaining them manually this way
same question as other pr -- this is `sink-task-metrics` instead of `sink-tasks-metrics` in the KIP
this state is missing from the KIP, it should be added
paused -> running
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
Filed this one: https://issues.apache.org/jira/browse/KAFKA-12607.
nit: ".. in epoch {}"? Similarly for other logs.
You could return Optional[String] probably where defined string would be the rejection reason. This would mean renaming the methods slightly though.
One tricky aspect is that we now generate the string even if logging is disabled. Let me think about that a bit more.
nit: move below the shortcut return below.
Adding to `connectorProps` won't change the already instantiated `config`.
If a public API change like this is required, you will need to propose a small KIP. I'm unclear why it's required tho, and ideally we would not alter the existing API if possible. If a new method is required, I think "track" is too ambiguous and should not be used here.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
I just took another look at the definition of streamTime, and it actually looks like it might be computed wrongly. The way it works is that the "stream time" for a task is computed most of the time in `org.apache.kafka.streams.processor.internals.PartitionGroup#nextRecord`, i.e., it's the max timestamp of any record _polled from the PartitionGroup_. However, when we commit, we commit the "partition time" for each TopicPartition, which is set when we move a record into the head position for that queue. During restoration, we read these committed timestamps for each TopicPartition, and we (incorrectly) set the "stream time" to be the maximum over the "partition time" of each partition in the PartitionGroup (aka Task). This is incorrect in two ways: 1. it should be the minimum, not the maximum (since we would choose the record with the minimum timestamp to process next) 2. the timestamp of the _head enqueued_ record (partition time) is not the timestamp of the _last dequeued_ record (stream time). I'll file a Jira ticket capturing all this. In the mean time, I'd suggest that we just update the docs to reflect the correct definition of "stream time": `which is defined as the largest timestamp of any record processed by the task`. Then, we can fix the code to make this true all the time. Currently, it's only true in steady state, not immediately after restoration.
Ah, perfect! This is much simpler than what I was thinking. Thanks.
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
nit: `final` (also next line)
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
The order is not really that important here, either way works
We don't need this for files, right? Just for directories (because of `file.deleteOnExit`)
Cleaner to just check if `tasks.isEmpty` after the loop is over.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
nit: `leaveReason = "consumer poll timeout has expired..` So that the whole log entry would read as `Member sending leaveGroup request to coordinator due to consumer poll timeout has expired ..`.
I was thinking that we can just pass in the statement in the above warn as the root cause into `maybeLeaveGroup`.
Maybe "consumer poll timeout" would be clearer than "heartbeat poll timeout"? The problem is the delay between calls to `Consumer.poll`.
This may be a bit misleading: how about `Heartbeat poll timeout has expired; it means the caller thread has been stalled too long, will explicitly leave the group to trigger a rebalance`.
Shall we consider a detailed log message like we do for commit failed exception? Also, is this just an `info` or should it be a `warn`? ```scala "Commit cannot be completed since the group has already " + "rebalanced and assigned the partitions to another member. This means that the time " + "between subsequent calls to poll() was longer than the configured max.poll.interval.ms, " + "which typically implies that the poll loop is spending too much time message processing. " + "You can address this either by increasing the session timeout or by reducing the maximum " + "size of batches returned in poll() with max.poll.records." ```
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
nit `stays at 2` seems to be correct -- it's `equalTo(2)` below.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
nit: move to line above.
Your understanding is correct @mjsax .
Please increase timeout to 30 seconds.
add `fail()` in next line to make sure we hit the timeout and throw an `AssertionError`
nit: add `final`
If we rewrite to use "absolute position" instead of delta, we can remove this.
Yeah that is fine. My bad.
`assertNull`s shouldn't be here but few lines bellow.
nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with `env.getCluster().nodeById(..)`.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: Empty line could be removed.
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
use `return CODE_TO_VALUE.getOrDefault(code, UNKNOWN)`
I really like the fact that we are separating Resources from ResourcePatterns! Great job.
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
Thanks for the clarification, makes sense.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
Actually, I now understand why the logic is where it is, and why the logical conversion doesn't need to be done. However, I still think the above logic using the schema's default and/or checking whether the schema is optional needs to only be performed when the `schema` is not null.
Should be final.
I think it would be more intuitive if we would reorder parameters to "topic, pattern, topic, pattern".
It seems you can move this line after line422.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
In the other constructor we have some nice preconditions and defensive copies, should we do the same here? Or maybe this constructor is meant to be private to `cloneWithFencing` below
Q: I might have missed the discussion. Why does an unknown offset result in `1` and not in `Long.MAX_VALUE`? Sorry if you have already answered this question elsewhere.
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
I see what you mean. I do not have any heart feelings here. Would be interesting to see in experiments how the two approaches differ.
Please avoid raw types, even when you don't need the type bound. ```suggestion for (final RankedClient<ID> rankedClient : rankedClients) { ```
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Not sure how many mirror maker tests we'll end up having, but would it make sense to have a `MirrorMakerTest` utility like the `KafkaTest` one, or does that end up being too minimal to be worth it (looking back at the `KafkaTest` one now, it looks like it's now just a few lines of code...)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
use `try-catch` instead of `expected` annotation -- not a single line test.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
Nit: both parameters can be `final`
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
store not used
nit: add `final`
nit: add `final`
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
We lack unit test coverage for this case
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Existing issue, space should be after the colon.
nit: Starting a message with lower case feels a little unusual.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
empty line needed
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Should be final.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
@mjsax I think I'm sold on your arguments, let's keep them as WARN then :)
Good point, thanks!
nit: additional new line
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`before or after`
`out-of-order` `window closed`
nit. I think there is `.` missing `since 3.0[.] Use`
`if (ignoreWhenShuttingDownOrNotRunning && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING))`
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Nit: fix line break
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
The fact that we're ignoring it suggests that it's not an `error`. For something that indicates something unexpected has occurred, but that we can handle it and continue correct execution, we should use `warn` level. Also, the type of exception will be captured in the stacktrace, so I would just say something more descriptive like, "Channel closed unexpectedly before lock release."
Might be better to use an `Exception` variable `firstException` and rethrow at the end if not `null` -- IIRC, behavior is undefined if we throw a second exception (ie, `finally` would executed after the first (outer) `catch` block.
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
@mjsax is right. Just to clarify, state store / changelogs today only do header-agnostic serde, so the scope of this PR is only for sink nodes.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
an -> a
nit: move to line above.
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Nit: remove the `:` after "type", since this forms a readable sentence.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
Is it valid to have a blank string as the replacement value? If not, then the `replacement` config should have a validator that prevents using invalid values, and it probably would be good to succinctly describe the limitations in the doc string. And it may be better to only set `this.replacement` to a non-null string that should always be used. This would centralize the logic of determining whether it should be used in one place, and line 135 becomes a lot simpler and more efficient: ```suggestion if (replacement != null) { ```
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
We can inline this like you've done for the byte[] case below
Why remove this? Do we need to instantiate this class now? (I only see static members still).
That makes sense, however you might be able not include the new field from the hash to prevent a chaotic assignment if you wanted
If we expect no warmups, we can assert it here with: ```suggestion assertValidAssignment(0, allTaskIds, emptySet(), clientStates, new StringBuilder()); ```
These utility methods can be static.
@rhauch I don't believe that's the effect the code here has. In the method call: ```java assertEquals( connectorName != null, connectorName != null ? context.startsWith("[" + connectorName) : false ); ``` if `connectorName` is null, then both arguments are guaranteed to evaluate to `false`. I think your intent may have been something like this: ```java assertEquals( connectorName != null, context.startsWith("[" + connectorName) ); ``` which would probably be acceptable, but may also benefit from a `message` that clarifies the expected behavior, possible something like `"Context should begin with connector name if and only if connector is non-null"`
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
nit: might be helpful adding a little helper since we do this a few times in here
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
@becketqin is right-- you should handle this case. Perhaps the server sent back bad data. The way to handle it is not to throw an exception, but to complete the relevant future(s) with an error. There are a few other cases where we handle bad server data by completing a future with failure in AdminClient.
If we do not expect this to happen. Shouldn't we throwI IllegalStateException? In this case, if the broker returned a replica that is not in the request, the broker may have somehow misplaced a replica. We should probably alert in this case.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
nit: fix alignment.
Nit: too many blank lines.
It's intentional to avoid build warnings about importing deprecated classes.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Are there ever situations where users would want the old behavior (to have access to the `ProcessorContext` for the record that triggered the lookup, rather than the context for the record that's being looked up)? For example, if the topic name is relevant for the transformer and all records (including the current one that triggered the lookup and the one being processed) are from the same topic, then the old behavior gives access to the topic name but this new behavior doesn't.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
Could it print the origin exception also? ```log.debug("xx", e)```
a tab is missing here for alignment too.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
Also: should only call onPartitionsLost on owned partitions that no longer exist
typo: moreq -> more
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
`instantiateConfigProviders` since this is potentially creating multiple providers
Nice tidy up of this test class :-)
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
Ditto on removing these before/after methods.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
nit: should we inline these? The variable names are barely shorter than the method names.
Ok, it looks better now. Let's leave it this way, with two lines.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
nit: I would rather use the full name instead of using acronyms.
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
Nit: remove unnecessary `this`.
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Good point, thanks for clarifying.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
nit: add `final` to cleanup code
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
not used here (InvalidTopicException is used instead)
This exception can't be thrown by DeleteTopics.
Please include TopicDeletionDisabledException here.
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
req: I assume you do not want to test `handleAssignment()` here, so you should not specify behaviour verification on the mock. You could simply write ``` expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment))) .andStubReturn(singletonList(task00)); ``` `.andStubReturn()` is behavior that is not verified in the `verify()` method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test. Same applies to other similar locations in this test.
req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
```suggestion private static final String ACCEPTABLE_RECOVERY_LAG_DOC = "The maximum acceptable lag (number of offsets to catch up) for a client to be considered caught-up enough" + ```
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
Should probably add this as the first bit in this method: ```suggestion String strValue = (String) value; if (value == null || strValue.trim().isEmpty()) { return; } ```
ah, i missed that it was in the Validator and not ConfigDef itself.
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
Similar questions below.
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
nit: The sentence sounds slightly better if you remove `the`
Not sure. PENDING_SHUTDOWN indicates a clean shutdown while this lets the thread fail.
The fallback should be taken from `StreamsConfig` and not be earliest all the time.
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
You can use `EnumMap`.
You can use `EnumMap`, which is a lot more efficient.
I think similar to produce and fetch, LIST_OFFSETS should be sent to the leader of the partitions
It should be something like "Sent by an (admin) client to get data about a specific consumers group like main information about members in such group."
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
nit: add `final`
nit: add `final`
nit: remove double space after `=`
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
nit: 4-space indention plus move `builder` down one line
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
ditto to `KStreamImpl`
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
We should limit this suppression to the method for which we really need it instead of the whole class
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
This can be final.
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
Let's capitalize the log statements for consistency. There are a few of these.
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
As an alternative, which might align better with Kafka in general, would be to set the timeout via `StreamsConfig`. This keeps the API clean. @enothereska argument that `close()` should not have any arguments is quite valid to keep APIs consistent within Kafka.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
nit: remove empty line
ditto on removing before/after.
I think we ditch the before/after methods as I previously recommended.
Ditto on removing these before/after methods.
exception can be improved a bit - "failed to flush within X ms, successfully completed Y/Z batches". wuold help distinguish between slow connection and no connection.
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
Do you mean it should NOT be included...
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
@mjsax What you suggested sounds right to me.
the method `restorePartition` is no longer used and can be removed
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
That's right, changed it locally.
It doesn't seem that the client needs principalBuilder.
Checked with Jun and this is fine.
Better be `cooperative-sticky`? `cooperative` is too general I think.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
nit: maybe utility methods can be moved to the bottom of the class.
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
nit: add a space so it is "StreamsMetadata {...} topologyName=xyz"
nit: add `final`
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
rewrite test as above using `assertThrows()`.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
How about "Custom partitioning is an advanced topic" -> "If not specified the underlying producer's {@link org.apache.kafka.clients.producer.internals.DefaultPartitioner} will be used to determine the partition".
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
nit: I think it's better to just print the e.message in a single line.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
You only need to crate that instance once, right? It can be a member of the class
Should this change not be more "radical" calling `KStream#peek()`? In order to resolve the `PEEK_NAME` vs `PRINT_NAME`, we can add a private `peek()` that take the name as parameter.
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
calc -> calculate
we could use `computeIfAbsent`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
`a graph containing`, and correct space between `1. Build`
nit: maybe it's more helpful to use the error directly here since no one knows the codes.
We can probably output the error instead of the error code (the field name should be changed appropriately.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
@hachikuji Yes, that sounds good to me. It should reduce heap usage when there are lots of requests requiring down-conversions while still avoiding blocking the network thread.
nit: Indicate that this needs shallow iterations on the entries.
nit: full-stop after the description.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
`windowSize` should be `Duration`
the method name changed to `windowedTable` and `windowSize` parameter is missing
I am wondering why this is not an assertion. Would the broker ever be expected to return only a subset of the partitions in a full fetch request? To be honest, I think it would be fine to skip these checks and just assume the broker gives us the right thing.
I am wondering if this can be lowered to `DEBUG` since it is handled internally.
Since topicPartition doesn't exist in next if we get here, there is no need to remove it.
Hmm, when is prevMetadata ever set to INCREMENTAL? From the code, it seems that prevMetadata is only set to response.metadata(), which always starts with FULL.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
nit: this loop is a little unconventional. Maybe we could use `pollFirstEntry` instead of the iterator? Similarly in `setNumKip500BrokerNodes`.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
I have a small preference for `Future<Map<Integer, Future>>` because it seems more aligned to how we do it for other APIs but I don't feel strong about it.
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
nit: need to update this since resource name is before pattern type now
Can just return `name.startsWith(acl.resourceName())`
We should not be using mockito internal classes.
Not sure if it makes a big difference, but we could use EnumSet for these.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
Aha! Thanks. Yeah, I'd be in favor of coding defensively here as well.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Map.Entry<String, String> to avoid the check below
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
Might be good to add a debug log message right before this.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
nit: I don't spot any, but safer to avoid typos by just having constants for these
Could also use `Collections.singletonList`, which would also make this immutable
This method name doesn't follow Kafka code conventions.
This exception is no longer possible since the constructor is taking `ObjectName`.
We can remove only if mbean.metrics.isEmpty. This line should be added in metricRemoval() method after unregister call.
We can compute this once and pass it to `removeAttribute`.
We typically avoid the `get` prefix in Kafka, so `objectName` seems fine.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Discussed this with @junrao. The main challenge with this option is having the top level error field for every response. This would probably affect a lot of code: 1. We would need to handle this top level error code everywhere. 2. A bunch of protocols that currently have a top level error code would no longer have them, so a bunch of code would have to be updated as well. So, it doesn't seem appropriate to do this as part of this KIP.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
the message format => that message format
Should be final.
We could make this field access `public`
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
I find it strange that this method closes the consumer it received.
Just let the Exception flow, that will automatically fail the test
We can use `Collections.singletonMap()`
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
Add missing `<p>` tag
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
That is correct: ``` {@link org.apache.kafka.streams.kstream.KTable KTable} ``` will show on java docs as `org.apache.kafka.streams.kstream.KTable`, while the above will show as `KTable` whose ref links to `org.apache.kafka.streams.kstream.KTable` still.
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Let's keep the existing `trace` and `error` log lines in the `else` block. My suggestion is to add a line at the debug or trace level in the `if` block so users can know if an error is ignored.
We can use `==` to compare enums.
The framework already retries this step if there is a retriable exception. I'm not sure if the operator is needed in the source connector (especially after resetting the processing context, which removes all useful context for the reporters).
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
```suggestion return futures.stream().allMatch(Future::isDone); ```
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
There is probably a generic pattern that you could factor out here. Basically you are providing a translation from `Map<K, V1>` to `Map<K, V2>` in both of these cases. So you could construct a generic `MapView` or something like that with a `Function<V1, V2>` and save some of the duplication.
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
nit: as in `position` above, `this` is not required
nit: as in `position` below, `this` is not required
nit: as in `position` above, `this` is not required
nit: in kafka we usually don't use `get`/`set` prefixes
For a nice example where caps make sense see right below, where two sentences are included.
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
We should test that delete twice in a row fails with `IllegalStateException`
This does work, but for a reason that is a bit obscure. When using an `ImplicitLinkedHashMultiCollection`, `remove` will remove the element b such that a == b, if it exists. This is necessary since if it just took the first element where `a.equals(b)`, it might be a different one than expected. It might be clearer to directly call `removeElementAtSlot`, since we already know the slot number.
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
Nit: Please use `{ }` (even for one line blocks)
Nit: can be `final`
Instead of adding headers each time, maybe we can pre-create the headers list and pass to ProducerRecord() constructor.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
I think we ditch the before/after methods as I previously recommended.
nit: remove empty line
ditto on (what I think is) the impossibility of this condition being false.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
this is creative :)
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
I used `equalToObject()` because it makes the intent more explicit.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Fair enough, let's just leave it as is then. Thanks for the explanation.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
This shouldn't throw an exception. The existing logic where we fail the task is what we want to do here.
Cleaner to just check if `tasks.isEmpty` after the loop is over.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
I don't think we really need this function any more... we can just submit to the executor from the other function.
`TaskManager#tasks` has to be accessed through the state change thread. It can't be accessed here by incoming requests since there is no lock or synchronization. Probably the easiest thing to do is to have a CreateTasks runnable which does what you want.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
Ideally, we would exit the loop after a certain amount of time.
Have you verified that this change is forward compatible as well? Older versions of the sticky assignor need to be able to work with the updated version. The client definitely has access to the group's generation. The question is whether and how to expose it to the assigner.
It certainly makes sense, however, I do not see much added benefit compared to the more or less arbitrary generation counter you implemented in this PR. If it was already exposed, you could use it, but I would not expose it just for this use case.
@vahidhashemian when I first proposed the generation counters, I looked at the coordinator code, and I had the impression that you have only access to the current / your own generation counter, but not the ones used/known by the other generation counters. So my interpretation is, that if you make this generation accessible to the `Assignor` you can only use it as an input source to for the generation counter we send in every ConsumerUserData. So I think its value is rather limited.
`hasFetchedRecords` avoids the cost of populating the map returned by `fetchRecords`. No locking is needed for that. So, if I understood the suggestion right, it would look something like: ```java if (fetcher.hasFetchedRecords && client.hasPendingWakeup()) client.poll(0, ...) ``` I guess by calling `poll(0, ...)`, we don't have to expose `maybeTriggerWakeup()`.
The current solution seems more direct than what I was suggesting, which is good. A bit unfortunate that we have to expose `maybeTriggerWakeup()`. I guess we could keep the synchronization around `pollNoWakeup` that you've added and still do the `hasFetchedRecord()` approach. Probably it would look like this: ``` if (fetcher.hasFetchedRecords()) { client.poll(0); return fetcher.fetchedRecords(); } ``` The downside is that it `hasFetchedRecords()` may not be trivial to implement since it would require doing the work that `fetchedRecords()` is doing, but without updating the position. Given that, exposing `maybeTriggerWakeup` doesn't seem too bad.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Should we clear the exception here? If not, then we'll have the exception thrown after the rebalance.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
For transition to `NOT_RUNNING`: the instance will only shutdown if the user uncaught exception handler decides to shutdown the whole instance, by calling `close()`, in this case it will still go through the `PENDING_SHUTDOWN` transition first? For `REBALANCE -> REBALANCE`, this is related to the thread-level `partition revoked -> partition revoked`, which I'm still wondering if we can avoid. Let's sync a bit on that.
Did we mean to swap `REBALANCING` and `RUNNING` around? If people were depending on the `ordinal` then this will break them
Again, could you describe: 1) Running -> Running 2) Partition Revoked -> Partition Revoked 3) Partition Revoked -> Dead 4) Assigning Partitions -> Dead
I'd probably pass this in via the ctor. `setStateListener` is always immediately invoked after construction so might as well just add the param to the ctor and do away with this method
for proper rendering, we should introduced list html markup
Doing in `@Before` is fine. But we don't need to call `new` each time. `this.props` will be a new empty `Properties` instance anyway -- we don't need the second object.
ditto on the properties and the driver.
I think we ditch the before/after methods as I previously recommended.
ditto on removing before/after.
Ditto on removing these before/after methods.
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
Ack. By bad.
I know this is a bit opinionated, but ... I think we should make an effort to make all locals `final` where possible. It is just a few extra keystrokes (that intellij can do for you!), and it helps to eliminate a class of bugs.
key -> topic
nit: add a blank line before this one to make it easier to read
nit: I understand the change here for compactness, but I find it a little hard to follow. This is subjective however so feel free to keep as is.
ok, I get that though I think that's just tech debt. for any test related files, we really shouldn't be using anything other than `PERSISTENT_ROOT` so that we can at least attempt to ensure each test/service gets a clean workspace
If this is allowed to shutdown gracefully now, should the subsequent `join()` on the worker thread have a timeout? Otherwise it could hang indefinitely if the worker thread doesn't exit properly. I think some of these unlimited timeouts have carried over from some initial test code I wrote originally, but they really should have timeouts.
I'm wondering if we should log this exception in case `thread_dump` raises an unexpected error. We don't want to lose the original error.
I think you need to do something like `assert not self.worker_threads[self.idx(node)-1].isAlive()` after this since `join()` always returns `None`.
We should probably use jcmd in the kafka shell scripts as well. In a separate PR, of course.
`KeyValueStore` -> `TimestampedKeyValueStore`
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
nit: remove empty link
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
Why is this needed? This is worse than the previous approach as it opens, closes and reopens the file.
After discussion on #4713 I think this idea should actually work. Nit: Can we rename the lock to `LOCK_FILE_NAME + " -" + taskId` though.
Maybe we should add one case where `position > 0`.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
nit: use "{}.x." vs. string concatenation
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
That has its own complications because if my provider is only providing TrustManagerFacotry.PKIX then it can't provider other services+algorithms that might be expected in calls like SSLContext.getInstance(String protocol, String provider). SSLContext.getInstance might look for TLSv1.1 or TLSv1.2 etc which my provider doesn't really have and I don't have a way to fallback anymore once I go route of specifying "provider" in getInstance() calls. In short - once we have a Provider providing a Standard service+algorithm we may have to implement other services+algorithm also otherwise it may not work (like I mentioned for SSLContext)
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
So let us say - we already have a single provider for JSSE but for Kafka we need one more to override with some of the common services+algorithm (example only override TrustManagerFactory.PKIX). We are at the mercy of the initialization sequence of calls, isn't it? Here the different providers could be owned by different teams and when we are in bigger infra setup it may be difficult to overcome this technical limitation. Only for Kafka applications the init sequence is different vs rest of the infra in the company. Basically, we are making an assumption which may not hold true and then we will be really stuck. What I am suggesting is - If we are calling out that limitation with these changes it is okay but otherwise it will result in a bug.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
A docstring for this method would be good :)
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
remove unnecessary newline
Ah I see, all good then.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
We need to check again after we grab the lock, otherwise the store might get closed after we check but before we grab the lock. Once we get the lock, we're guaranteed that this block is serialized wrt close(). But we can still check beforehand to avoid grabbing the lock if it is closed.
It would be sufficient. I assumed that we were checking before the lock to avoid synchronization overhead, so I left that in place.
We can extract a helper, but maybe we can do _that_ in a follow-up at least, since the concurrency controls needs to be standardized across all the caching stores anyway.
validateStoreOpen() can be outside of lock block.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
Unless i'm misunderstanding something, I think these test names are inverted. ```suggestion public void shouldParseUnquotedEmbeddedMapKeysAsStrings() { ```
It would be good to have some assertions.
nit: don't need the type params on the next three lines
Is there similar behavior in the map parsing? I see a similar comma consume call being ignored. Consider the following test: ``` SchemaAndValue schemaAndValue = Values.parseString("{foo:bar baz:quux}"); assertEquals(Type.STRING, schemaAndValue.schema().type()); assertEquals("{foo:bar baz:quux}", schemaAndValue.value()); ```
Ah, yes, the magic is hardcoded here.
nit: add `final`
nit: `final` I know it wasn't before, but let's stick to making params as final
I'd probably extract this to an inner class. I just find it a bit unwieldy having an anonymous class of this size. I find it a bit distracting. But i'm not overly bothered either way. Just a suggestion
I was thinking we need to do something about the `ProcessorContext`, too. The interface is quite broad. I think what you have suggested is ok, but I'd probably like to see `register` moved elsewhere, too. At the moment the `ProcessorContext` is accommodating initialization and processing, which means we have methods on it that aren't valid in all circumstances. IMO it would be nicer to have more focussed interfaces that make it less likely that you can do something that isn't allowed. The interfaces can all be implemented by the same object, of course.
About `ProcessorContext`: yeah definitely not for this PR, just throwing it out here for discussion. About `punctuate`: that is a good point, and it makes me thinking if we should just change the return type from `R` to `void` then (and I know we need a KIP for that..).
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
In order to reproduce this issue, we need to reset the generation via `maybeLeaveGroup` before the `onJoinComplete(gen.generationId, gen.memberId, gen.protocol, memberAssignment);` is triggered, but after the join-group response handler is exercised to set the generation id. I think this can still be doable with a single thread, to execute in the following ordering: 1. we only prepare the join-group response in the mock network client, but not the sync-group response, and we also make the MockTime to be able to advance time automatically, then by calling `joinGroupIfNeeded` with a small timeout instead of Long.MAX_VALUE, it should be able to finish the join-group round trip, trigger the handling logic to set the generation id, and then send the sync-group request, but then time out on https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L400 waiting for the sync-group response and return false. 2. Then we call `maybeLeaveGroup` within the same thread. 3. Then we prepare the sync-group response in the mock network client, and call `joinGroupIfNeeded` again, this time the response would be received, and the rest of the logic https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L406-L415 would be executed.
Not clear why we want to use a separate thread to call `joinGroupIfNeeded`? In unit test we would try to avoid any unnecessary multi-threading as it can easily cause flaky tests.
Thanks for looking into this!! I tested it locally and it passes here.
Also ran client:tests and all extended `BaseConsumerTest` and all passed.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
This statement is a bit misleading, how about "to the format indicated by the given magic value".
nit: Indicate that this needs shallow iterations on the entries.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
Can this be a `byte`.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
I'm fine as well, will make a reference to 10055 of this PR
Might be better to add a proper POJO maybe `StreamsMetadata` or something that wraps the `streamTime` Long plus `ProcessorMetadata` instead of using `KeyValue` ? We might add new fields later on what is easier to do for a new POJO.
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
format: no need for curly braces
format: ``` if (exception != null) throw exception; ```
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
AK convention is to not use `set` setters or `get` getters.
Yeah, that works, too, and is more align with the current code.
nit: as above, use `@link`
```suggestion * Example objects needing to be closed include {@code org.rocksdb.Filter} and {@code org.rocksdb.Cache}. ```
nit: insert `<p>`
And then here ``` Any object created with @{code new} in {@link #setConfig} and that inherits from {@code org.rocksdb.RocksObject} should have {@code close()} called on it here to ```
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
@jeffchao traditionally Kafka used key,value pairs in properties and pass it everywhere and each implementation takes look at this config and pulls their interested key,value pairs. Example, authorizer interface https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/security/auth/Authorizer.scala#L35 . The pluggable class when it gets instantiated a configure method will be called and all the key,value in server.properties will be passed and it will pick whats relevant to the class. We can do the same here instead of asking users append key,values into the a config which is hard to configure and hard to get it right.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
So let us say - we already have a single provider for JSSE but for Kafka we need one more to override with some of the common services+algorithm (example only override TrustManagerFactory.PKIX). We are at the mercy of the initialization sequence of calls, isn't it? Here the different providers could be owned by different teams and when we are in bigger infra setup it may be difficult to overcome this technical limitation. Only for Kafka applications the init sequence is different vs rest of the infra in the company. Basically, we are making an assumption which may not hold true and then we will be really stuck. What I am suggesting is - If we are calling out that limitation with these changes it is okay but otherwise it will result in a bug.
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
`long,long` is used for `WindowStore` while `Instance,Duration` (or `Instance,Instance` if we correct it) is use for `ReadOnlyWindowStore` that return the same iterator.
nit: remove empty link
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
the method name changed to `windowedTable` and `windowSize` parameter is missing
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
req: I think we want to introduce some `acceptableLag` config within which a task is considered caught-up, otherwise this is way too strict. ie the condition should be `lag <= acceptableLag`
req: drop the `!caughtUpClients.isEmpty()` check here, if it's in the map it should have at least 1 caught-up client
```suggestion * @return map from tasks with caught-up clients to the list of client candidates ``` or something similar to make it clear the map only contains tasks with caught-up clients
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
req: we'll never hit this, as `taskToCaughtUpClients` only contains tasks _with_ caught-up clients IIUC. Can we just construct `unassignedTasksWithoutCaughtUpClients` as the set `totalTasks - taskToCaughtUpClients.keySet`? We can do that in `assignTasksWithoutCaughtUpClients` and remove `unassignedTasksWithoutCaughtUpClients` from `assignTasksWithCaughtUpClients` entirely
Why the qualification that says "that generates stateful processors"? AFAIU nothing prevents the user from supplying a state-less transformer (supplier), even though yes, you can also provide the names of state stores when calling this method.
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
`KStream` => `{@link KStream}`
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
I think we ditch the before/after methods as I previously recommended.
ditto on removing before/after.
Ditto on the properties and the driver.
Ditto on removing these before/after methods.
Ditto on removing before/after
`schemaType` is null means that the value type is not supported by the Connect's Data API. May be we should throw an exception with a message indicating this.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
super nit: extra blank line
Should we have a separate `UnsupportedVersionException` for multiple endpoints? For example, we could have multiple plaintext endpoints.
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
nit: extra space.
Regardless of where the `lookupCoordinator` is triggered, we are only raising it from `ensureCoordinatorReady`, so I am not sure I follow the point about raising from other contexts. Note there doesn't appear to be any logic preventing multiple listeners from getting attached to the future. I think it would be better to always attach the listener when the future is created.
Shouldn't this be `timeoutMs - (time.milliseconds() - startTimeMs)`? Also, it's not too big of a deal, but the checks for `Long.MAX_VALUE` seem like overkill.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
the method `restorePartition` is no longer used and can be removed
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
@mjsax What you suggested sounds right to me.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
Maybe we could use a different value here.
Seems to fit in one line
We should have a constant rather than using '262' directly
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Fair enough, let's just leave it as is then. Thanks for the explanation.
I used `equalToObject()` because it makes the intent more explicit.
@tadsul We could perhaps convert this to a immutable map and store `in `originals` like we do for `values`.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
How about "runs an external command for the worker."
I don't think limiting the pool to 2 threads is really what you want, since you have 3 runnables that you are creating later on. Check out some of the other newXYZThreadPool functions. There is at least one that doesn't limit the number of threads.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
Suggest using `Executors#newCachedThreadPool` rather than manually counting threads. We don't need a `ScheduleExecutorService` here either... just a plain `ExecutorService` is fine.
Maybe something like "No command specified" would be more descriptive
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
`KStream` => `{@link KStream}`
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
nit: missing `<p>` for new paragraph
same... `@link` > `@close` for this case.
I would use the full qualified name of `Filter` and `Cache`. If anybody wants to look them up it is easier. Additionally, I would enclose it into `{@code ...}`.
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
And then here ``` Any object created with @{code new} in {@link #setConfig} and that inherits from {@code org.rocksdb.RocksObject} should have {@code close()} called on it here to ```
I would rather write the one line description in imperative. ``` Close any user-constructed objects that inherit from {@code org.rocksdb.RocksObject}. ```
I actually had a similar thought, but I am torn though. Using two variables required to keep them "in sync" was is not great. However, using `null` is less explicit... Thus overall I am fine either way as both seems to provide the overall same good/bad ratio.
Thanks. Understood. It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information. In `close()` before we actually "loose" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`). This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies.
nit: keep fields with the same access level together
We should only store `handleSnapshotCalls` since `handleSnapshotCalled` is always equal to `handleSnapshotCalls > 0`.
`volatile`? It is potentially being accessed by different threads and the synchronization is on different objects
Nit: can be `final`
Nit: both parameters can be `final`
Nit: can be `final`
Nit: I think `"not-null"` might be confusion. I think a better naming would be `"null-encoding-that-is-not-just-'null'"`
Nit: `dir` only used once -- can be removed
Great catch, thanks @showuon !
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
I don't know what has changed here.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
This could probably also be an exception since this shouldn't be called with invalid target states.
Shouldn't this result in a change to a failed status? I think we'd either need to do that here (if thread safe) or have some way to communicate it to the original thread? We can defer this to another patch if we already weren't handling this case properly.
Just a small nit: this is how my brain works -- otherwise the condition is "reverse" and I need to flip it in my head
Maybe this looks better? ```suggestion // we're at the end of the input. if (queryResult.getResult().value() == batch1NumMessages - 1) return; ```
So we basically do 10 re-tries? Is this intended? Or should be just sleep for a hard-coded "backup time"
Why not handling the ```InvalidStateStoreException``` in the helper method ```until```
which method can throw ```InvalidStateStoreException``` in this case? It seems to me the potential methods are caught by ```assertThrows```
Why the qualification that says "that generates stateful processors"? AFAIU nothing prevents the user from supplying a state-less transformer (supplier), even though yes, you can also provide the names of state stores when calling this method.
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
nit: missing `<p>` for new paragraph
Thanks for verifying @vvcephei!
I haven't seen this struct being used.
`...build a graph to calculate partition number of repartition topic, and numOfRepartitions of underlying TopicsInfo is used for memoization.`
nit: we could require non-negative value for `numOfRepartitions`
do a link: `method setRepartitionTopicMetadataNumberOfPartitions` -> `{@link #setRepartitionTopicMetadataNumberOfPartitions}`
is a topic node
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
I think you can do the zookeeper start in the `setUp` method
Not sure if this will actually be cleaner or end up more complicated, but you may be able to reuse some of the `StickyTaskAssignor` code here which does similar things
maybe also add a `lastAssignedTask(List<TaskId>)` helper to clean up `source.get(source.size() - 1)` used here and below
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
req: we'll never hit this, as `taskToCaughtUpClients` only contains tasks _with_ caught-up clients IIUC. Can we just construct `unassignedTasksWithoutCaughtUpClients` as the set `totalTasks - taskToCaughtUpClients.keySet`? We can do that in `assignTasksWithoutCaughtUpClients` and remove `unassignedTasksWithoutCaughtUpClients` from `assignTasksWithCaughtUpClients` entirely
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
Thanks for the explanation: checking per-commit is indeed easier. Moving forward we can even make them two separate PRs for other reviewers to easily review.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
`Integer.toString` is a slightly more concise way of doing this.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
I fixed this one to use the constant before merging.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
@becketqin Yes, my preference, as mentioned above, is to deal with that problem separately. We should not make behavioral changes without first raising the issue at least in a separate JIRA. The unintuitive thing about the proposed behavior to me is the fact that although the consumer's position remains at the offset of the failed record, the next returned record will be from the offset after that position. You can see this in the test case below: the consumer's position is at 1, but the returned record is at offset 2. This makes the behavior less deterministic. It would be nice to maintain the invariant that the next fetched record is always the first record at an offset greater than or equal to the current position.
Please update the test case as I suggested. Thinking about the current patch. If there is an exception parsing or validating one of the records, we will update `PartitionRecords.nextFetchOffset`, but we will not change the current position (i.e. what is returned by `consumer.position()`. That means in the next call to `poll()`, we will simply discard the rest of the records. So there is no behavior change here and my suggestion above simply makes the behavior explicit. You can confirm this by updating the test case.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
nit: we're missing a period and space following the partition. I can fix this when merging.
`KeyValueStore` -> `TimestampedKeyValueStore`
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
an -> a
and -> a
an -> a
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
An alternative might be to let verifiable producer accept just one compression type. Then in your test case, you could create separate instances, each with a different compression type. Seems a little more intuitive that way to me.
Hmm, is this required? Normally `render` should automatically have access to all fields of the object it's being called on. I'm not sure why the `security_config` one is there either.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
I didn't think of that before, but now that you mention it, the change makes sense to me.
IIUC, this changes the behavior of the `WorkerConnector` created below. Prior to this PR, the `WorkerConnector` was always created with the `Worker.offsetBackingStore`, even for sink connectors. However, with this PR, the `WorkerConnector` will be instantiated with a null `offsetReader` parameter, which will cause a NPE in `WorkerConnector#doShutdown()` and `WorkerConnector#cancel()` since `WorkerConnector` does not check for a null parameter there.
Suggestion: ```suggestion // Set up the offset backing store for this connector instance ```
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Can/Should these be immutable values? Maybe you can use the singleton `Collections.emptyList()` to avoid making memory allocations.
@adyach I have noticed that this "hidden" logic is what already happens today for the `ConsoleConsumer` tool for example (in the related `ConsumerConfig` class) so I think that we can live with that. FYI as part of the refactoring for #3453 (for having it more testable), I decided to use that PR for introducing the `CommandOptions` class even getting your good ideas here. In this case when the #3453 will be closed we should have a first version of some of the common components we need for tools refactoring.
@adyach having the parsing here could be a good idea but exiting from the application maybe not. This should be the base class for other command options classes but changing the application flow could be misleading here (i.e. Exit.exit(1)).
I've been doing the delete / reset offset path in all of my tests and it seems to be going fine. For the output topic I delete and do create in a loop because the list command sometimes does not show the topic but then create fails because it thinks it still exists. We could get the committed offset at the beginning and fail if it is greater than 0.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
This definitely doesn't cover the full space of errors that are possible here -- `asSubclass` could throw a `ClassCastException`, `newInstance` could also throw `SecurityException`. I think the `catch` was broad because this ensures that except for extreme cases like other `Throwables` or `Errors` we get everything converted to `KafkaExceptions`.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
@jeffchao traditionally Kafka used key,value pairs in properties and pass it everywhere and each implementation takes look at this config and pulls their interested key,value pairs. Example, authorizer interface https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/security/auth/Authorizer.scala#L35 . The pluggable class when it gets instantiated a configure method will be called and all the key,value in server.properties will be passed and it will pick whats relevant to the class. We can do the same here instead of asking users append key,values into the a config which is hard to configure and hard to get it right.
Similarly, better to declare `throws IOException, ReflectiveOperationException`.
nit: would be nice to be consistent on the pattern we use here
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
Is the goal to have a version number for this schema? If so, perhaps we could just add it to the sticky assignor schema. Something like this: ```java public static final Schema STICKY_ASSIGNOR_USER_DATA_V0 = new Schema( new Field("version", Type.INT16), new Field("previous_assignment", TOPIC_PARTITION_ASSIGNMENT_V0)); ``` That would remove the dependence on the consumer protocol. If we add a version, the next question is what kind of compatibility we are attempting to provide. Forward compatibility is probably the only useful thing to provide in this assignor. When you're doing a rolling upgrade to the newer assignor version, there's no way to ensure that only the newer member becomes a group leader, which means backwards compatibility is not enough. So if we can't have forward compatibility, then perhaps we can just leave the version out. To get it, we would basically need the parser to just ignore version numbers which are higher than it expects.
Does this really need to be `Serializable`? Same for the other comparators above.
Better be `cooperative-sticky`? `cooperative` is too general I think.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
nit: maybe utility methods can be moved to the bottom of the class.
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
We can use `ApiResult.completed()`
Yes, we can open a JIRA to do it later.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
nit: We could revert this change as it does not bring much.
nit: We should use `groupId.idValue` here and in the others.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
I liked this refactoring a lot, thanks @vvcephei !
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
nit: we do this same thing in the other `#resize` for thread count changes, can you factor it out into a helper method? Then I think we can narrow the scope and make only that helper synchronized (should double check that though)
Nit: we typically just say `partition` in these cases. Same for the other `log.debug`.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
@hachikuji Yes, that sounds good to me. It should reduce heap usage when there are lots of requests requiring down-conversions while still avoiding blocking the network thread.
I think perhaps the error message should be more generic. An invalid number of headers is one possibility, but it could also just be incomplete data.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
It was removed from the other versions of `group` but not from here.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
maybe "Restoration completed for partitions:"
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
This is an interesting question. One low-fi solution would be to think about using `equals()` (I think to pull this off, we'd need to introduce a requirement that serde/serializer/deserializers implement equals in a way that would be semantically sound for us. This would not be a back-ward compatible change. On the other hand, since callers actually subclass `Serde<T>` with a fixed type like `Serde<String>`, it actually should be available at runtime. I don't remember the hoops you have to jump through to get it right now, but I'll revisit it tomorrow.
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: remove unnecessary `this`.
nit: is it necessary for triggering a warning? My personal preference is that if a single line is not too long, then it is okay to do so.
nit: 4-space indention plus move `builder` down one line
I was debating the same thing. Won't `NetworkClient` keep the node under the `CONNECTING` state though? It seems like either approach involves a change in the contract that could affect users who are not expecting it. It's an internal class though, so we just need to make sure that the affected Kafka code is updated (if necessary). It would be nice to include a test for this so that we can verify that things truly work under this scenario.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
Not sure about this. `SslTransportLayer#hasBytesBuffered` returns true if there is any data in `netReadBuffer`. If more data is needed to unwrap and no data arrives from the client, I think the handling of `keysWithBytesBuffered` results in a tight polling loop with timeout=0.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
previous message => previous receive
@parafiend Looking at this again, adding to `failedSends` in `poll` can result in multiple disconnect notiifcations for a channel (we have to guarantee exactly one). `failedSends` are processed on the following `poll()`. But for the write exception here, we process `close()` in the catch block and the channel is added to `disconnected` list. In the next poll, when `failedSends` are processed, the channel will be added again to `disconnected` list. It would be better to set a flag rather than update `failedSends` and change the close in the catch block to: ``` close(channel, !sendFailed); ``` This will close the channel in the current poll without waiting to process any outstanding requests.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
Ouch! Sorry about that!
We don't want to remove the `stateDir` entirely, because then `stateDir.exists` will return false and we won't actually call `listFiles` which is what this test is trying to test. Can we just rename to `"state-renamed" + TestUtils.randomString(5)`? Or, clean up the previous contents at the beginning of the test? That said, `cleanUp` should be called after every test, so there shouldn't be any left over state...right? cc/ @guozhangwang
That's true, but I suspect that this leftover folder is not from this test suite but likely from others? I think the right fix here should be to check which test (highly doubt it is one of the `StateDirectoryTest` because of `cleanup` ) has this leftover and fix that one instead.
typo: we want to test the **case** that poll() returns no records.
Ditto on removing before/after
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
nit: add a space before the `:`.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
Any reason you change to import all classes under `java.util`? I think we should import what we used in this class only.
I know we're violating this a few places (due to the initial code import), but I think we want to avoid converting to `*` imports.
`log` not used
Could we use ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG etc instead of hand-coded strings? It is less error-prone for possible future changes.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
Thanks for cleaning up the code duplication.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
nit: it is a tad vexing to see all the `context` prefixes. I guess another option might be to define `RaftClientTestContext` as an abstract class so that the test method can define the test behavior within the scope of a subclass. For example: ```java new RaftClientTestContext(builder) { void run() { assertTrue(client.isShuttingDown()); ... } } ``` Not required, just an alternative to consider.
I don't think it can be. It needs to be a TimelineHashMap to work and needs to receive the snapshot registry in the constructor.
nit: we can use `map#compute` to replace getOrDefault + put.
We could make this field access `public`
Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case.
Yes, the alterIsr doesn't change leader, but generates a PartitionChangeRecord. On replaying this record, the code following code bumps on leaderEpoch? ` PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);`
Ah ok fair enough -- thanks!
Seems to fit in one line
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
We can use `List<Class<? extends Connector>` to avoid the warning
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
method name changes
`windowSize` should be `Duration`
and -> a
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
Is this really worth it? It seems like a `forMagic` without a transactional id gives you most of the benefit.
Same as above: need to check `clientResponse.hasResponse()`
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
Thanks! Will push this shortly.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
Missing a space after the comma, which is causing checkstyle to fail.
Since shouldBeSinkNode.get(0) may not be SinkNode, consider renaming the variable
I think one call of storeToChangelogTopic.keySet().removeAll() outside the loop should be equivalent to what you have now.
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
This was a separate bug, I guess? Might be worth mentioning in the PR description.
This doesn't seem to be used.
This doesn't seem to be used.
nit: add a space before the `:`.
We'll need to fix this in a follow-up so that followers send the right replicaId.
Thanks for the explanation @dguy, very helpful to understand where caching and sequence numbers come into play. It might be worthwhile to put this in a JIRA somewhere. I do think it would be a useful optimization to have eventually, as fetches have some setup / teardown overhead.
Here we need to do: `final Agg oldValue == newValue == null || sendOldValues ? fetchPervious(..) : null;` This is because `SessionWindows` have a dynamic time range, the the start is always fixed. So we need to send deletes for the previous smaller window when a window is merged, i.e, a simple count: a@0 -> SessionKey(key=a start=0, end=0), 1 a@5 -> SessionKey(key=a start=0, end=0), null (delete this as it is merged) SessionKey(key=a start=0, end=5), 2 (this is the new merged session)
@xvrl there is no `get` on `WindowStore`. We could add one and it would work in scenarios where we don't have duplicates, i.e., the key for a WindowStore is (recordkey, timestamp, sequenceNumber) - if the store doesn't have duplicates the sequence number is always 0. If the store does have duplicates then we don't know what the sequence number is. Without a KIP to add a `get()` to `WindowStore`, the only thing we could do is add a bit of a hack to see if the inner most store is a `RocksDBSegmentedBytesStore` and then we could call `get(..)` on that. If it isn't, then we'd still need to call `fetch`. For the DSL this would work as the only time we have duplicates in the `WindowStore` is for joins and we disable caching for those so it skips this code path. However, for the PAPI, we would need to always disable caching if duplicates are set. Which we probably should do anyway as it won't work as is.
I thought the timestamp would uniquely define the segment in which that key is stored.
nit: I'm wondering if just using would suffice (IMHO slightly easier to immediately grok the meaning). ```java if (!cachingEnable) { context.forward(key, new Change<>(newValue, oldValue)); } ```
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
We should also create a client connection with one of the newly disabled protocols like TLSv1.1 and verify that the client connection fails.
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
nit: 'else' can be dropped
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
`deteremined` => `determined`
and -> a
and -> a
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
I think a better test scenario is to move the logic in `close()` call, i.e. when the stream thread is being shutdown, and topology is closing, we call `processorNode.close()` in which we wait for a while and then tries to access the global store. It mimics the case where in closing the store cache is flushed and hence tries to access the global store again.
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
Is this just to prevent it from processing anything until you're ready to proceed? It seems like you can/are doing that just by controlling when to produce input messages and doing so one at a time (if that's accurate, then WDYT about renaming `process` to `processed` and flipping the boolean so it more clearly serves the purpose of indicating whether a record has yet been processed)
Nit: might be worth adding a simple assertion on the result just to make sure.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: Empty line could be removed.
nit: We could use `TestUtils.assertFutureThrows` here.
Something does not work as expected in this algorithm. According to this doc, the assignor should fall back to distributing tasks on least-loaded clients. However, the following test case fails: ``` @Test public void shouldDistributeTasksOnLeastLoadedClientsWhenThereAreNoEnoughUniqueTagDimensions() { final Map<UUID, ClientState> clientStates = mkMap( mkEntry(UUID_1, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_1), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_0)), mkEntry(UUID_2, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_2)), TASK_0_1)), mkEntry(UUID_3, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_2)), mkEntry(UUID_4, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_1_0)) ); final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates); final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, CLUSTER_TAG, ZONE_TAG); new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs); assertEquals(1, clientStates.get(UUID_1).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_2).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_3).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_4).standbyTaskCount()); } ``` The standby task for active task 0_0 can be put on client UUID_2 and the standby task for active task 0_1 can be put on client UUID_1 without breaking rack awareness constraints. Standby tasks for active tasks 0_2 and 1_0 cannot be put on any client without breaking rack awareness, so they should be distributed on least-loaded clients. However, that does apparently not happen, because client UUID_3 and UUID_4 are not assigned any standby.
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
Q: Why do you use a mock here? In the ticket you said you just want to replace `MockStreamsMetrics` with `StreamsMetricsImpl`. req: If there is no specific reason, I would propose to either create a common mock that can be used everywhere as I proposed or to consistently replace `MockStreamsMetrics` with `StreamsMetricsImpl`.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
I think we should rewrite this to make sure the NPE is thrown in `get()` but not in the constructor. ``` final KTableTransformValues kTableTransformValues = new KTableTransformValues<>(parent, new NullSupplier(), QUERYABLE_NAME); try { kTableTransformValues.get(); fail("..."); } catch (final NPE expected) {} ```
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
store not used
nit: add `final`
nit: add `final`
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
As above: need to keep default value.
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
nit: add `final`
nit: add `final`
nit: add `final`
This is not part of the KIP any longer
above (some some more times below)
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
Might be better to use an `Exception` variable `firstException` and rethrow at the end if not `null` -- IIRC, behavior is undefined if we throw a second exception (ie, `finally` would executed after the first (outer) `catch` block.
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
Sounds like a good idea
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
I had a look at this and your are right. It seems that keeping `TopicPartition` is better and difficult to change. In this case, have you considered pushing the conversion to the `Builder` by providing an overload of `setTargetTimes` which accepts a `Map<TopicPartition, ListOffsetPartition>`? That could make the code in the `Fetcher` a bit cleaner.
I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,
Yeah, we're still feeling out the best patterns for handling older versions.
nit: "another thread wrote to ..."
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
`replicaing` -> `replicating`
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
@rajinisivaram, hmm, I'd rather us specify the details or link to a config that specifies them. With security, people often struggle so the more information we can provide, the better.
methods => `mechanisms`
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
adding the registraiton part into Config file doesn't look good to me. Generally speaking Config classes are only there to express the config members and validation around them but not to take any action. SecurityUtils.java would be a better place.
@vahidhashemian, yes, that's what I mean.
This part will have some conflicts with @mjsax 's PR, just a note.
You mean `now`? :) If yes please feel free to resolve the ticket when you merge this.
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
The second param seems redundant.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Nit: add `final`
Nit: use `{ }` for the loop body.
cosmetic: extra space at the start
`final` is for the var `activeTasksMetadata` (not for the method). We try apply a "use `final` whenever possible" policy. It's just some nit.
`to fetch list of stores` -- that describe some internal implementation detail. Better: `Get the store partition that will be queried.`
guaranteed -> guarantees
remove this line -- not required.
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
This should be the only method with actual code. All other overloads should call this one.
Should not have an implementation but call overloaded method.
Yeah, it's not pretty. However, reducing the concurrency of `BufferPool` in the common path is not desireable. We definitely need to handle OOMs correctly, but they are relatively rare and it's OK if that path is slower.
Good point. I think it is still worth keeping the optimization, although typically the the producer will only allocate poolable batch size, so the actual memory allocation should not happen very often.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
ditto on removing before/after.
I think we ditch the before/after methods as I previously recommended.
Ditto on removing these before/after methods.
Personally I think we should call out the specific method that was removed so we don't cause undue panic. It was only a single method, after all.
> It seems to me a lot of work for little result to research all methods removed without deprecation period. Exactly -- we shouldn't push this to every user on every upgrade. It's annoying, so, we should just do it exactly once, and only before considering an upgrade.
I would be more concrete: ``` RocksDB version will be bumped to version 6+ via KAFKA-8897 in a future release. If you use `org.rocksdbOptions#WhatEverMethodWasRemoved`, you will need to rewrite your code after KAFKA-8897 is resolved because those methods were removed from `org.rocksdbOptions` class without a deprecation period. ``` Or something like this.
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
nit: extra line
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Good point, thanks for clarifying.
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
Similar here, we can cache the result in case to be reused.
nit: add a space before the `:`.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Right, sorry I misread that line.
How do you feel about dropping `Number` from these names? This would be consistent with the methods we expose from `TransactionManager` itself (e.g. `lastAckedSequence()`).
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
nit: I don't spot any, but safer to avoid typos by just having constants for these
Hmm.. Do we know how this is getting propagated in all cases? Some of the responses are handled using a `RequestCompletionHandler`, but NetworkClient currently eats any exception raised from this callback.
The issue is that local cache may not contain this topic metadata yet or not up-to-date, and that's why we may want to send an `MetadataRequest` in these two function calls.
API changes will call for a KIP I'm afraid. We'll also want to ensure that we preserve the old method for compatibility.
I think the root cause of the hanging is that, the `list offset` response returns empty indicating the broker did not know about this topic, and the client will hence retry forever. So what I suggest is that just making the metadata refresh once in `position()`. As for the flaky unit tests @baluchicken , we can use waitForConditions in those test rather than block waiting until it succeeds.
Actually when I was discussing with @huxihx on the JIRA itself what I was thinking is the following: 1. The call traces of `Fetcher#resetOffsets` which use `INT_MAX` are from two public APIs: ``` Fetcher#resetOffsets -> Fetcher#updateFetchPositions / Fetcher#resetOffsetsIfNeeded -> KafkaConsumer#updateFetchPositions -> KafkaConsumer#poll(timeout) / KafkaConsumer#position() ``` So we should pass in a `remaining` in the internal call from `KafkaConsumer#updateFetchPositions` all the way down to `Fetcher#resetOffsets` to replace `INT_MAX`; As for the public APIs, `poll` already have a timeout, we just need to let its internal `pollOnce` to pass in the updated `remaining` to `updateFetchPositions`, for `position` since it is defined as a non-blocking call, we should pass `0` to `updateFetchPositions` and if the call fails to return the offset in a single trial, it means no offset is cached locally and the remote offset request failes as well, we can throw `InvalidOffsetException` directly. By doing this we do not need a KIP. Admittedly it may not be optimal to treat the non-blocking `position()` call like this, but this is to adherent with its semantics; if we do want to change its public behavior then I'd prefer option 1) from @hachikuji to modify the semantics of "non-blocking" calls of consumer as we did for producer.
Yes I think after we update metadata, we should do a sanity check on the number of partitions. If we have stale information (i.e. the topic's state prior to deletion), then the partition will still exist and we can retry. But once we have updated state, we will see that the topic has a fewer number of partitions and we can raise an exception. Then how we handle it depends on the context.
It seems you can move this line after line422.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: maybe iterate over `entrySet()` instead.
It should be public as it will be used in `producer` APIs.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
It was removed from the other versions of `group` but not from here.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
We should not print the stacktrace imho, but verify the exception message using `assertThat`.
typo: CompleteableFuture -> CompletableFuture
nit: add `final`
Should we guard against NPE? (same blow)
Oh, gotcha--in that case, should we do a check somewhere else, since this will be triggered potentially multiple times for a single plugin? For example, if there are three copies of a connector, the warning will be logged twice right now, with different values for `inner.keySet()` each time. Also, it may also help to log exactly which one we're going to use either instead of or in addition to the complete set of discovered versions of the duplicated plugin.
I think this parameterization is a pretty good idea, and I can add it to my delete topics PR. But if we are going to change the public API, we should update the KIP and potentially update the mailing list with the changes.
We need to keep the Admin API backwards compatible. An application that was written using the 2.7.0 should not break if it is compiled with a 2.8.0 clients jar. You can always add an internal class with shared code to avoid duplication, but the public API itself needs to remain compatible.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
Since this is a protected method in a public API, we should probably keep the method and deprecate. It can just invoke the method below.
For `all()` function, its returned type should be `KafkaFuture<Void>`; ditto for other two Results as well.
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
as per previous `assertThat(..., instanceOf(...))` would be better
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
There is no need to test this as it is calling the same method as above.
I'm ok saving this for #7409.
I think we'd want to override `parseResponse` for `API_VERSIONS` only.
In the parsing logic, we still convert to struct first before calling `AbstractRequest.parseRequest`. I think we could bypass the `Struct` conversion by changing `AbstractRequest.parseRequest` to take the `ByteBuffer` instead of the `Struct`. ```java public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, ByteBuffer buffer) { ``` Then in the fetch case, we could just call this method.
Add a reference to KIP-511 here
This doesn't seem to be used.
We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
I don't see bucketing
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
I personally was on the side of always using task stream time everywhere but more people feel that we should use processor stream time :P Anyways, all I'm trying to say is that we need to make an educated decision here, and if we concluded that either 1) we rely on task time here, but still use processor time on other expiration logic, or 2) we rely on processor time on all logic, or 3) we rely on task time on all logic, we have a good rationale for whichever we choose.
Do we need to maintain it manually? Could we use `context.streamTime()` instead? Note that `context.streamTime()` might be slightly different because we advance it for every input record. Thus, if there is a filter before the join, the join might not get all records and thus it's locally observed stream-time could differ from the task stream-time. It's a smaller semantic impact/difference and it's unclear to me, if we should prefer processor-local stream-time or task stream-time? \cc @guozhangwang @vvcephei
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
@spena just ping to make sure you get this on the follow-up PR.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: would be nice to be consistent on the pattern we use here
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: "User rebalance callback **threw** an error"
Please add the exception at the end of the line so that we can get a stack trace, including any "cause" exceptions. Example: ``` log.error("{}: Failed to start the external process.", id, e); ```
Please include the problem in the errMsg which is used to complete `doneFuture`. For example: ``` errMsg = "Failed to start the external process: " + e.getMessage(); ```
Maybe something like "No command specified" would be more descriptive
It seems a little harsh to set a fatal error condition because the process logged something on stderr. A lot of applications use stderr as an output. Maybe we should just log this with `log.error`.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
`replicaing` -> `replicating`
@mjsax Got it. Thanks for your response!
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
nit: The mention of join group comes out of nowhere
Also add `@params topics`
nit: remove `which is`
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
i.e., add `fail` after this line
Nice tidy up of this test class :-)
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Also could we try to parameterize this part as well? Like passing processing mode flags to each test
From the comparison here: https://wikidiff.com/common/generic, the opposite of `generic` is `specific`, which in this case I guess you want to express is `mutual`. So IMHO `common` or `mutual` tests are more accurate here.
Thanks for the explanation, makes sense.
`final` ? All of the fields should be `final` really
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
nit: we can define a static ``` private static final String[] NO_PARENTS = {}; ```
null check not needed it is inside ensureExtended
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
@guozhangwang Yep, sounds good to me.
We shouldn't return `null`, but instead return a "unknown query" result.
Dropped this unnecessary duplicate code, as we discussed.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
Looks like we were not handling the right query variant before, but it didn't come up yet because other tests were failing before we got to this point.
This is the reason I exploded the `serdes` reference in favor of functions for deserializing the key and value. When we're handling queries for non-timestamped stores, we need to be able to adapt the value deserializer.
We should include `startPosition` in the message.
I don't think we need the `null` checks.
Nit: "The file channel position..."
What's the purpose of this warning? It doesn't seem needed.
I don't think we need this `null` check either.
`KAFKA-13046: Improve the test coverage for stickyAssignor` is created. Let me handle it! :)
I'm thinking we can have a test for package scope `partitionsTransferringOwnership` in `AbstractStickyAssignorTest`. I found we didn't test it before. We can verify the doubly assigned partitions and other revoked partitions are put into `partitionsTransferringOwnership` correctly.
@ableegoldman , I reviewed it again, and found we forgot to sort the `unfilledMembersWithExactlyMinQuotaPartitions` list here, to have deterministic result.
NVM, I realized it should never happen.
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
nit: add `final` and line too long
Can you elaborate? What do you mean by > otherwise the state won't proceed
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
exception not used.
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
Should be final.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
Harsha has done this.
nit: use `logContext
AK convention is to not use `set` setters or `get` getters.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
Nit: `.` full stop missing.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
nit: simpler or not? ```java Map<String, Uuid> newTopicIds = topicIds.entrySet().stream() .filter(entry -> shouldRetainTopic.test(entry.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); ```
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
By the way, one of the downsides to using the __consumer_offsets topic, is that it effectively makes the `listConsumerGroups` API dependent on having Describe access to this topic.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
I think this config property key seems a misfit, and probably reflects an earlier incantation of the design before KIP acceptance. It might be worth - in a separate PR - renaming this to something like `errors.tolerance` to better align with its purpose.
We'll need a separate AK issue, then.
nit: matches the old behavior is very relative
Maybe use a semicolon instead: "task failure; 'all' changes..."
The first part of the doc sounds incomplete.
same here -- sounds like CachingKeyValue with TimestampStore
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
nit: parameter/line formatting
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
Was the intention here to avoid the deprecation warning? If so, you can just call this method `name()` and do this: ```java @Override @SuppressWarnings("deprecation") // TODO remove this when {@link Joined#name} is removed public String name() { return name; } ``` Callers won't see the deprecation warning as long as they access the method via a `JoinedInternal` and not a `Joined`.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Missing a space after the comma, which is causing checkstyle to fail.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
Uggh, yeah, I forgot about this. We kind of inherit some annoying types from Kafka's config setup but I tried to ensure we're using String types where possible. It gets a bit hard to figure out what is valid where -- the JsonConverter actually gets passed a `Map<String, Object>` as that's what is returned by `AbstractConfig.originalsWithPrefix`, but in practice they are all `String` so type erasure allows this to work...
Sounds good! There's no rush, but I'll make sure we have your new PRs reviewed and merged quickly whenever they are ready, since you've worked so hard on this already. I'm sorry I wasn't able to make another pass on your original PR, but hopefully this won't be too much of a bother.
Ah, I see the confusion. The `#isTopologyOverride` method checks whether the config has been overridden for the specific topology, ie has been set in the Properties passed in to `StreamsBuilder#build` -- it's not looking at what we call the `globalAppConfigs` which are the actual application configs: ie those passed in to the `KafkaStreams` constructor. So basically there are two sets of configs. The value should be taken as the first of these to be set by the user, in the following order: 1) `statestore.cache.max.bytes` in `topologyOverrides` 2) `cache.max.bytes.buffering` in `topologyOverrides` 3)`statestore.cache.max.bytes` in `globalAppConfigs` 4) `cache.max.bytes.buffering` in `globalAppConfigs` Essentially, using `#getTotalCacheSize` on the `topologyOverrides` if either of them is set (which this PR is doing) and on the `globalAppConfigs` if they are not (which is the regression here). On that note -- we also need to move `##getTotalCacheSize` out of StreamsConfig, because it's a public class and wasn't listed as a public API in the KIP (nor should it be, imo). I recommend creating a new static utility class for things like this, eg `StreamsConfigUtils` in the `org.apache.kafka.streams.internals` package. There are some other methods that would belong there, for example the `StreamThread` methods `#processingMode` and `#eosEnabled` should be moved as well Hope that all makes sense -- and lmk if you don't think you'll have the time to put out a full patch, and I or another Streams dev can help out 
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
If there are race conditions, we need to fix them. Remember that Streams itself has to use AdminClient.
This is a one-node cluster, though, right? > /** > * Runs an in-memory, "embedded" Kafka cluster with 1 ZooKeeper instance and 1 Kafka broker. > */ > public class EmbeddedKafkaCluster extends ExternalResource { >
I'm a bit concerned using `listTopics` than using `JavaConverters.seqAsJavaListConverter(brokers[0].kafkaServer().zkClient().getAllTopicsInCluster()).asJava())` as only the latter can get the source-of-truth on ZK while the former may be subject to race conditions (e.g. if you create the topic and then call listTopics, it may not be included if the metadata was not propagated yet).
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
We should have a constant rather than using '262' directly
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
nit: "User rebalance callback **threw** an error"
extension name must not be empty
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
nit: should be removed (similar below)
Changing old version encoding would break our upgrade path, ie, all existing `encodeVersionX()` methods cannot be modified.
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
nit: The sentence sounds slightly better if you remove `the`
Here I'd suggest doing the opposite: `poll(0)` since it is during the normal processing, not during restoration; so we can afford to not having some time in a few iterations. Instead, we want to proceed to the next iteration to call the normal-consumer.poll sooner to not be kicked out of the group.
`replicaing` -> `replicating`
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
nit: these three can be package private
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
I would move line 330 and 331 to before this line.
I would move line 328 and 329 to before this line.
Yeah I think that makes sense here
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
I'm not sure how `removeMembersFromConsumerGroup` would behave if you passed in `""` as the `group.instance.id`, do you know? If not then let's just be safe and check what `streamThread.getGroupInstanceID()` returns, and skip this call if there is no group.instance.id (ie if not static)
Ok, this is going to be a little tricky...`removeMembersFromConsumerGroup` is async so we have two options. (1) just ignore the returned result and hope that it succeeded, or (2) check the returned `KafkaFuture` and wait/make sure that it succeeded. Probably we should go with (2) and just apply the remaining time of the timeout. If you haven't mucked around with the KafkaFuture class before, I believe `KafkaFuture#get(long timeout, TimeUnit unit)` is what you'd need here
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
original was better
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Does this actually buy us anything here? If `forceClose` is false, then doesn't that mean that there are no more in-flight requests? I think we still increment the in-flight request count even if the client doesn't expect a response.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
typo in the test name.
nit: remove empty line
nit: add `final` (2x)
nit: final on params here and methods below.
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
We could port this function when it is actually needed.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
Also: should only call onPartitionsLost on owned partitions that no longer exist
nit: why double space? (similar below and further below)
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: preserve empty line after `checkAndClearProcessResult`
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
Another tab here that should be replaced.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
these 2 lines shouldn't be here
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Oh, I just noticed. Then `synchronized` is not needed anymore.
It was removed from the other versions of `group` but not from here.
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
If we decide to not turn on bulk loading, then we should be able to not close and restart again after restoring is done.
yeah, the existing `IllegalStateException` is confusing and we should fix it.
I am wondering if we should throw `KafkaException`. This is an expected state since the producer is designed to block in `send()` to await metadata and there is not really any way for a user to avoid it. To be consistent, we could also raise `KafkaException` from `RecordAccumulator` in the similar scenario.
This message seems a little low level for something which will get propagated back to the user. An alternative to consider would be to let `awaitUpdate` return a boolean indicating whether the update happened or not. That would allow us to raise an exception with a producer-specific message from `send()`.
nit: top of class
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
nit: single parameter per line
nit: move parameter to next line
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
nit: I'd make this final with no assignment, then assign it in both branches below.
nit: log.info("Suspended {}", state());
req: no longer used
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
Again, a bit more information would be more useful: ```suggestion // Then if we delete the connector, it and each of its tasks should be stopped by the framework // even though the producer is blocked because there is no topic ```
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
One issue with c. is that it works for new environments. If users already have MM2 running, it's using topics with the current names.
> only when MM2 is running in standalone mode. They are created in any mode if there is no value for DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG > If the user is running MM2 in connect mode, the user is responsible for configuring DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, DistributedConfig.CONFIG_TOPIC_CONFIG, etc. It is what you are meaning. Right? Small clarification, users can use `DistributedConfig` with any mode (even standalone) to override the name of these topics. And they always had the power to do so, even before KIP-690, and if this new topic name didn't match the `isInternalTopic` policy, it would replicate. The PR's approach is trying to control the Connect topics that MM2 needs to set up using the separator; this is where I am not sure it's a minor fix or something that requires a KIP that follows KIP-690. My suggestion, is to introduce the minor fix first and propose another KIP if you believe that Connect internal topics created by MM2 Workers should to be controlled by the separator as well.
I'm wondering if we should make this `info` or `warn` level. It doesn't seem like it would be very verbose, and it might be nice to see by default because it will have secondary effects later on when we try to start a new transaction, but get blocked. But I also don't feel strongly about it, so I leave it to your discretion.
req: This is unnecessary
ok - same thing three times. Maybe extract it to a method `verifyTransactionInflight`
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
nit: if you want a new paragraph you need to add `<p>`
nit: extra space before `anyObject`
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
req: Please also verify `stateDirectory.unlock("0_2")`. Only verifying `lockedTaskDirectories()` seems too weak to me.
Looks good. I like the additional checking that you're doing here.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Would it make sense to do this check even before checking if the coordinator is known? Moreover, it seems that we could skip calling `doCommitOffsetsAsync` entirely by returning a completed future directly. What do you think? ``` if (offsets.isEmpty()) return RequestFuture.voidSuccess(); ```
I considered this but I think that it is clearer when kept separated.
I checked again, and I think it's OK. Thanks.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
This exception message is not really clear to me. What about `"Cannot commit offset of topic partition " + tp + " since the partition was not dynamically assigned to this consumer"` or similar? In the code such messages usually start with an uppercase letter.
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
nit: move parameter to next line
I think we should have this in this PR already.
Same thing here as above: probably need to use `worker.getConnectorType(className)` here.
Same issue here wrt connector name vs type. We probably need some chain like `connectorType(connectorClass(map))`.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
IMO, this would be a bit cleaner with a separate executor with a single thread for the status updater.
Yeah might as well change it I think, it results in shorter code
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
How about just ``` log.error("{}: (stderr):{}", id, line); ```
Let's call this "StdoutMonitor" since that makes it more clear what it is doing. We may want to pass things other than status eventually. Also, the instance variable is called `stdoutMonitor`, which suggests that this is a better description.
nit: we can do without the curly braces here and above. However, these will soon be replaced by the actual impl
the key type of `topics` is `Uuid` and hence this check is weird. Maybe it should be replaced by `topicNameToId`
This could be more concise: ``` topicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment); ```
Oh, indeed, I missed that. And in this case you'll need to have an extra line with put.
Another way to write this, that reduces a couple lines of code would be: ```java if (allTopics.remove(topicName) == null) { future.completeExceptionally(new UnknownTopicOrPartitionException(String.format("Topic %s does not exist.", topicName))); } else { future.complete(null); } deleteTopicsResult.put(topicName, future); ```
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
nit: add `final` and line too long
Can you elaborate? What do you mean by > otherwise the state won't proceed
this test doesn't seem to throw `InterruptedException` as well
exception not used.
We should also verify the thrown cause
I'm happy for this to be merged if @hachikuji is happy fwiw.
You mean if the offset is out of range? I'm not sure we have a good way to check this at the moment. It can't be done on the coordinator because we don't know what the valid offsets are for each topic partition, so that leaves the client where the check may end up stale anyway. By the way, there are a couple `commitSync` overloads that may need to be updated as well.
What is the offset commit is positive and invalid? cc @hachikuji
```suggestion * session timeout, errors deserializing key/value pairs, your rebalance callback thrown exceptions, ``` Same reasoning.
`deteremined` => `determined`
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
Adding to `connectorProps` won't change the already instantiated `config`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We can use `StringDeserializer.class.getName()`
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
Thanks for verifying @vvcephei!
Same as above, e.g. `shouldNotThrowWithoutPendingShutdownInRunOnce`
nit: remove empty line
instead of `fail` we should use ``` assertThrows( ClassCastException.class, () -> producer.send(record); ); ``` For this case, we can remove the `catch` block below.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
Could we rename this to something like "remainingPartitions"
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
nit: "User rebalance callback **threw** an error"
nit: would be nice to be consistent on the pattern we use here
```suggestion log.trace("Topic creation by the connector is disabled or the topic {} was previously created." + ```
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Should we include the brokers instead of removing them? Same below.
same question around log level as above
same question around log level as above
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
@guozhangwang Yep, sounds good to me.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
Would it be easier to understand if this handled all of the unwrap exceptions after the IOException? And then we could call this method `processUnwrapExceptionAfterIOException`.
req: This is unnecessary
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
nit: 'else' can be dropped
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Any idea why these processors changed order? It could indicate a deeper problem.
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
Adding to `connectorProps` won't change the already instantiated `config`.
If a public API change like this is required, you will need to propose a small KIP. I'm unclear why it's required tho, and ideally we would not alter the existing API if possible. If a new method is required, I think "track" is too ambiguous and should not be used here.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
I just took another look at the definition of streamTime, and it actually looks like it might be computed wrongly. The way it works is that the "stream time" for a task is computed most of the time in `org.apache.kafka.streams.processor.internals.PartitionGroup#nextRecord`, i.e., it's the max timestamp of any record _polled from the PartitionGroup_. However, when we commit, we commit the "partition time" for each TopicPartition, which is set when we move a record into the head position for that queue. During restoration, we read these committed timestamps for each TopicPartition, and we (incorrectly) set the "stream time" to be the maximum over the "partition time" of each partition in the PartitionGroup (aka Task). This is incorrect in two ways: 1. it should be the minimum, not the maximum (since we would choose the record with the minimum timestamp to process next) 2. the timestamp of the _head enqueued_ record (partition time) is not the timestamp of the _last dequeued_ record (stream time). I'll file a Jira ticket capturing all this. In the mean time, I'd suggest that we just update the docs to reflect the correct definition of "stream time": `which is defined as the largest timestamp of any record processed by the task`. Then, we can fix the code to make this true all the time. Currently, it's only true in steady state, not immediately after restoration.
Ah, perfect! This is much simpler than what I was thinking. Thanks.
@zhuchen1018 : But the issue is that we already processed the response. Next time, when we come back to handleCompletedReceives() again, we will get a response intended for a request different from what's in the head of inFlightRequests.
@becketqin : Yes, if it's a bug, it's going to be hard to auto fixing it in the client code. Just propagating the exception to the caller is probably the best that we can do.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
I don't think we really need this function any more... we can just submit to the executor from the other function.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
I'm wondering if we also need to delay the call to `client.wakeup()`? For example, consider the following sequence: 1. disableWakeups() 2. wakeup() 3. poll(0) 4. enableWakeup() 5. poll(Long.MAX_VALUE) The underlying wakeup on Selector would effect the `poll(0)`, but we would suppress the exception. Then there would be nothing to wake us up from the `poll(Long.MAX_VALUE)`.
Since we have several things that all need to be closed, perhaps we could use `ClientUtils.closeQuietly`? Maybe something like this ```java try { for (String id : connections) close(id); } finally { AtomicReference<Throwable> firstException = new AtomicReference<>(); closeQuietly(nioSelector, firstException); closeQuietly(sensors, firstException); closeQuietly(channelBuilder, firstException) if (firstException.get() != null) throw firstException.get() } ```
Same issue here wrt connector name vs type. We probably need some chain like `connectorType(connectorClass(map))`.
Same thing here as above: probably need to use `worker.getConnectorType(className)` here.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
Ok, it looks better now. Let's leave it this way, with two lines.
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
Why is the order of these methods different than in `ConnectorStatusListener`? Also, the `TaskStatusListener` methods always forward the method to the delegate _last_, whereas the methods of the `ConnectorStatusListener` use a mixture. Let's make them consistent.
This is going to be modified and accessed on potentially different threads, right? If so, we should add the `volatile` modifier here.
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
Not sure this is what people will generally mean by lag -- while the committed offset matters, normally if the consumer is in the process requesting the lag I think it'd mean the FetchRequest lag, i.e. how far behind *processing* the records is the consumer in comparison to what the broker indicates is the most recent offset. in other words, I think i'd update this at the end of each `put()` and change from `committedOffsets` to `processedOffsets`.
nit: as in `position` above, `this` is not required
Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations: ```suggestion if (reporters.size() == 1) { return reporters.get(0).report(this); } List<Future<RecordMetadata>> futures = new LinkedList<>(); for (ErrorReporter reporter: reporters) { Future<RecordMetadata> future = reporter.report(this, callback); if (!future.isDone()) { futures.add(future); } } if (futures.isEmpty()) { return CompletableFuture.completedFuture(null); } return new ErrantRecordFuture(futures); ``` And since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a `LinkedList` instead to avoid excessive allocation when adding the first element to the `ArrayList`.
```suggestion return futures.stream().allMatch(Future::isDone); ```
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
same question here and below about locking around a `volatile` variable. Is this the only reason to lock here? One would think so based on previous usage.
I don't think locking buys you anything w/r/t to that. The producer#send in the status backing store is asynchronous. So what would describe above can happen anyways, regardless of whether you lock this object. Of course, if it wasn't asynchronous things would be much worse. A bottleneck would be created by the lock, waiting for the `send` to finish in every locked block, so that's not an option. Wdyt? That's what I see at the high level without spending to much time on it, but see if you can check this assumption and we can return to this question.
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
nit: can we make this debug level? Otherwise it will make this test a little spammy.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
It isn't about blocking vs non-blocking. It is about how a Kafka broker behaves to a non-authenticated request on SASL ports... we need to test it (since it is the requirement of this JIRA). We hope that SASL behaves exactly the same as PLAINTEXT, but we don't know that it does without a test.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
Shouldn't we be using `SASL_PLAINTEXT` as this point? Also, it may be worth including a test with `SASL_SSL` as well for this case.
Good call. I'll update the code to throw an exception like in `Worker` when creating a source and sink task.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
```suggestion WorkerSourceConnectorContext(OffsetStorageReader offsetStorageReader) { ```
Shouldn't the indentation be as follows? ``` log.debug( "Encountered assignment error during partition assignment: {}. Will skip the task initialization", streamThread.assignmentErrorCode ); ``` The first parameter is one or two characters too long, though.
as above. (don't make lines longer)
There are two different error codes: - `VERSION_PROBING`: for this case we continue and rejoin the group; also the assignment is empty - `INCOMPLETE_SOURCE_TOPIC_METADATA`: for this case we shut down, and this case is already handled above. Hence, I think we don't need to log anything for this case? (Note that we log the version probing in StreamThread later and the metadata error is logged above already)
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Should we make a copy of this collection, if the purpose is to save them before they're updated? It looks like otherwise this is a reference to the same collection as `active.runningTaskIds()`.
Make all locals `final`
add `final` wherever possible
Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)
The test is fine -- but what's the value in testing overwrite the default extractor two times.
as per previous `assertThat(..., instanceOf(...))` would be better
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
Nit: space before `:`.
Nit: space missing before `:`. There are other cases like this in the file.
Since it's tied to the deprecated name, the error message should probably include both the deprecated and new name. Otherwise it'll be more difficult to track down the source of the conflict.
nit: newline after if condition, also space before and after `!=`, and space after `if`.
These iterators need to be closed or they'll leak resources (it's the same for IQv1 as well).
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Maybe this looks better? ```suggestion // we're at the end of the input. if (queryResult.getResult().value() == batch1NumMessages - 1) return; ```
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Would it help to actually list the method that was used, in case somebody thought they were using basic? ```suggestion log.trace("Request credentials used {} authentication, but only {} supported; ignoring", BASIC, method); ```
nit: `addMetadata` -> `put`
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: top of class
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: "Assigning tasks to streams clients: ..."; also better to be `log.debug`.
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
I think we need to skip all the code in this else block if the partition is no longer assigned.
@enothereska Yes, I think that is a better solution. But I think @hachikuji was right that we don't need to cover both if/else branches. We just need to cover the `parseCompletedFetch`
@enothereska The trunk code does not need to access `subscription.position`, instead it uses `PartitionRecords.nextInlineOffsets` which should be the same as position because the position is updated to this value every time after a successful `fetchRecords()`. The big try/catch is to make sure the the exception from `fetchRecords` will also be caught and not result in loss of non-empty `fetched`.
@enothereska Looking at the previous patch, it seems like the scope of the try/catch is unnecessarily large. I think it only needs to cover the call to `parseCompletedFetch`. If we do that, then there should be no need to access `subscriptions.position` down this path.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Could you please add some line breaks? This and some of the other verifications are too long.
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
Nit. use `{ }` for all code blocks
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
nit: parameter/line formatting
nit: you could just specify one `String` variable at the top of the method and set it accordingly if `topics` is `null` or not then have a single `return` statement. Just a personal preference though.
nit: also add java doc for type `T, O` here
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
nit: in spite of the getter convention, I still prefer setters be prefixed with `set`
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
We would want to check for `InvalidMetadataException` here as well. We need to go back to the Metadata call if we find a metadata error. This is similar to the cases when we call `rescheduleFindCoordinatorTask`.
Technically we may still have a connection. Do we need to close the client first before we decrement these? (ditto for others)
I suggest only catching `KafkaException` and only close the client then. About other exceptions, we can probably try retrying or fail the thread, I don't have a strong preference. I don't think we should catch Throwable though, as that catches errors like `OutOfMemoryError`
I think that's fine, I don't think there's a way to recover (nor if it makes sense) from an OutOfMemoryError - https://stackoverflow.com/a/352842
Yes it is. I was asking whether we might want to alter it for the default case. I think it's fine to keep it as it is
This won't log the error. We want to use the `public void error(String msg, Throwable t);` overload, so we need to change `e.getMessage()` -> `e`
I feel we could actually simplify the test by calling `streamsProducer.kafkaProducer()` every time for the check for the internal producer, instead of keeping a reference here, as the `eosAlphaMockProducer` and `eosAlphaMockProducer` look quite similar.
Not necessarily, we could pass in both processing mode and the expected output as parameters, if the test workflow looks essentially the same.
Sounds more like `common tests`
From the comparison here: https://wikidiff.com/common/generic, the opposite of `generic` is `specific`, which in this case I guess you want to express is `mutual`. So IMHO `common` or `mutual` tests are more accurate here.
Also could we try to parameterize this part as well? Like passing processing mode flags to each test
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
I don't mind diff noise if it makes things better btw (even slightly)
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
Add a reference to KIP-511 here
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
We could remove this function
Though now I look at the message for `UINT16` I see it would be consistent with that. Still I think because there are two types involved here, the Java type and the network type, including both is clearest.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
even clearer: "Represents a signed integer"
"encoded on two bytes using network byte order" -> "encoded using two bytes in network byte order"? Similar for the other integer types if you like the suggestion.
nit: Values 0 and 1 *are* used..
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
This can be initialized here and be `final`
Do we need the if/else? Since this is a unit test, it seems OK to just assert that the first element is the rate and the second is the total.
As above: need to keep default value.
For backward compatibility, we need to keep the old default value. Btw: we don't do any ordering here yet -- just above. so no need to reorder anything here.
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Thanks for the clarification @hachikuji
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
I just noticed that we don't ensure that all futures of the current broker are completed. It would be great to ensure it by using `completeUnrealizedFutures` method if `retryTopicPartitionOffsets` is empty. We already do this in `alterReplicaLogDirs()` if you want to see an example.
Yes, we can open a JIRA to do it later.
It would be good to elaborate on why we need to do this as it's not obvious by just reading the code.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
nit: Could we move `new DeleteTopicsRequestData.DeleteTopicState().setName(topic1)` to a new line in order to align it with the following `new DeleteTopicsRequestData`? There are few other cases like this.
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
Missing newline character.
We could iterate through v1 to v5 here to test every case.
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
That is right, thanks @omkreddy .
Thanks @omkreddy. I should have expanded the diff to see the docs.
nit: it was correct before
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Could you please already open the follow-up PR with scaffolding and link it here? I think otherwise we risk to forget about it.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
should we just do this per partition? if there were any negative values, this could still give incorrect information.
Nitpick: We don't need to explicitly check for `other == null` -- `instanceof` (which is required for the casting logic anyways) will return false if its first argument is null.
The `KeyValue` class allows null values for `key` and `value` (at least I didn't see input validations such as throwing IAE in its constructor when either key or value are null). So we must guard against nulls / NPEs here.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
nit: add `final`
It's unusual to hold a reference to an abstract class like this. I believe the intent is to be able to transparently handle either `KeyValueSegments` or (I'm guessing) `KeyValueTimestampSegments`. The full expression would be to have a `Segments` interface implemented by `AbstractSegments`, which is then extended by your two implementations. Then this line would reference `Segments<S>`. It's fine to collapse this into just the abstract class (although questionable in the presence of package-protected fields). But to maintain transparency, I'd name the abstract class `Segments` instead of `AbstractSegments`. That way, to an outsider class (like this one), you're still just interacting with the interface (i.e., the public interface of the class), rather than specifically an abstract class. Adhering to this pattern leaves the door open in the future to extract `Segments` into a full interface without having to change any outside code (which is what I meant by maintain transparency).
Also not clear why "numSegments - 1" here.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
How much effort would it be to have a test case for this? We have a few LeaveGroup tests in `ConsumerCoordinatorTest`.
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
nit: use `{}` instead of string concat for `retries`
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
Double-brace initialization is an anti-pattern. It would be preferable to use `mkProperties`.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Do we need to pass in all admin client configs? Actually, we only need retries. Also, I am not sure if we would set the correct default here. If nothing is specified, we would use `StreamsConfig` retry as default, but we actually should set `AdminClientConfig` default -- atm, both might be the same so it doesn't matter too much. Just is doesn't seems to be "correct" (in a very strong sense).
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
The change makes sense to me. I don't think anything would stop the auto-commits from going through. Even if there was such a mechanism, it seems better to explicitly disable it.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Couldn't we simply wait for the current state to become `RUNNING`? ```suggestion private void waitForRunning() throws Exception { waitForCondition( () -> kafkaStreams.state() == KafkaStreams.State.RUNNING, DEFAULT_DURATION.toMillis(), () -> String.format("Client did not transit to state %s in %d seconds", expected, DEFAULT_DURATION.toMillis() / 1000) ); } ```
I think nicer to just `return requests.first()` here
The `connectorStatus(connector)` call might fail with a `NotFoundException` if the connector was removed after the `configState.connector()` method is called but before the status for the removed connector is asked for. Although this shouldn't happen within the process (since requests are handled sequentially by a single thread), it may be possible that this herder is not the leader, that the leader performed the change, and that this herder's config state backing store read that change after the `configState.connector()` method was called but before the `connectorStatus(connector)` method is called for that connector. Should be easily handled with a try-catch, and if the connector with the specified name is not found then simply continue to the next connector name. Something like: ```suggestion try { out.put(connector, connectorStatus(connector)); } catch (NotFoundException e) { // do nothing with connectors that were just removed } ``` Note that if a connector is *added* with similar timing, the new connector name will not be returned from `configState.connectors()` and the new connector will not be included in the results. I think that's fine, considering the result of this method call would still be consistent with the state at the time the `configState.connectors()` call is made. Call it again, and you'd see the new connector.
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
I think this can be written as a for loop over `requests` followed by `requests.clear()`.
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
nit: I think it's better to just print the e.message in a single line.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
just `name` should be fine
nit: remove var `newJoined` (also not used for left-hand-side code)
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
Yes that's correct as this PR stands now. But if we put the name check back to what it was originally, then this line is not needed.
All the callers seems already handles the `name == null` case so this seems unnecessary.
My bad. My suggestion inserted a typo. At least I saw it before I start the build. ```suggestion return new HashMap<>(connectorConfigCallback.get(herderRequestTimeoutMs, TimeUnit.MILLISECONDS)); ```
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
This seems to change the behaviour...
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
Strictly speaking, this shouldn't be necessary as `SCHEMA_TYPE_CLASSES` should have a `Schema` instance for all `Schema.Type` literals. And with `SchemaBuilder` a connector or converter cannot create a schema instance with a null `Schema.Type`. However, it is possible to construct a `ConnectSchema` instance with a null `Type` reference (like what `FakeSchema` essentially does in the existing test), which of course without this change would result in this method returning a null list. So +1 for this line change since it simplifies the error handling in the calling code.
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
Actually, I now understand why the logic is where it is, and why the logical conversion doesn't need to be done. However, I still think the above logic using the schema's default and/or checking whether the schema is optional needs to only be performed when the `schema` is not null.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
Let's use `Map` on the left side instead of `HashMap`
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
nit: typo in description
line is too long
Nit: add `{}` to block
`[because] the tool`
Nit: why not use `boolean`
Good find! > However, gradle does not allow the usage of Base64 since it "could not be found" according to the compiler anyways. How did you try to use it? Everything from the standard library should be available... I would prefer to use Base64 if we can. If not possible, we can still fall back to using String, but I would really like to avoid it if we can.
@guozhangwang @bbejeck @vvcephei Do you think it's worth to add a version number for the binary format of the committed offsets (I tend to think we should add a version number). I would also not encode the timestamps as `String` but as 8-byte binary long.
To be future prove, we should encode a version number as prefix in case we ever what to change this metadata. What about `<version>:<partitionTime>` with version number "1" ? Also, line is too long. Move both parameters to their own lines.
Input parameter `partitionTimes` should always contain the correct partition time, hence, we can just get it: ``` final long partitionTime = partitionTimes.get(partition); ```
This is not a feedback: as we are changing the main loop of StreamThread, we may need to carefully benchmark if this change along with the main loop changes will have unexpected performance penalty: with low traffic input stream, we are effectively sending sync commit requests more frequently. cc @mjsax If it does become a problem for performance, we could consider making the commit request async, and consider a commit only completed after the commit response is returned. Of course it means more complicated logic.
I'm assuming this is just extracting the inlined function and hence skipped and did not compare line by line :)
```suggestion log.debug("The offsets have been reset by another client or the group has been deleted, no need to retry further."); ```
That's right. Thanks for the explanation.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
Let's rename `headers1` and `headers2` here too
this test doesn't seem to throw `InterruptedException` as well
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
nit: final and also below
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
nit: Just added some more information to the messages. ```suggestion log.error("Could not remove static member {} from consumer group {} due to: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new StreamsException("Could not remove static member {} from consumer group {} the following reason: ", e.getCause()); ```
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
as above. `requireNotNull` not necessary any longer
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
Good point, I think we should add `this.nano += TimeUnit.NANOSECONDS.convert(autoTickMs, TimeUnit.MILLISECONDS)` in `nanoseconds()` as well.
My reasoning is that the tests that expect the clock to auto-tick would be affected if the code under test changed from `milliseconds` to `nanoseconds`. Am I missing something? And is there a reason to only auto-tick `milliseconds`? `FastClock` is an interesting idea, it seems to depend less on how often `milliseconds` is invoked by the code under test. It seems more realistic too (in a sense, it's like reducing all timeout values by a multiplier).
This seems to overlap with #4095 -- should not be part of this PR IMHO.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
nit: I'm sure these fit in a line shorter than the one below
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: fits in one line
nit: blank line missing here
nit: extra blank line
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
nit: these three can be package private
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
you don't need this. Junit gives you a new instance of the test class for every test method
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
You should also compare `expectedValues`.
this is not needed as every test method gets a new instance of the class
We can use `TestUtils.assertFutureThrows()` here too
nit: Indentation of those lines seems to be off here.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
We can remove `requireNonNull` here, because `getter.keySerde()` would already throw a `ConfigException` if the default serde is null.
as above. `requireNotNull` not necessary any longer
as above. `requireNotNull` not necessary any longer
nit: here and the next line remove the `<` and `>` to be consistent
nit: 4-space indention plus move `builder` down one line
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
Yeah, I don't think it's worth doing it for broker properties at the moment.
Hmm, this doesn't seem great.
adding the registraiton part into Config file doesn't look good to me. Generally speaking Config classes are only there to express the config members and validation around them but not to take any action. SecurityUtils.java would be a better place.
That has its own complications because if my provider is only providing TrustManagerFacotry.PKIX then it can't provider other services+algorithms that might be expected in calls like SSLContext.getInstance(String protocol, String provider). SSLContext.getInstance might look for TLSv1.1 or TLSv1.2 etc which my provider doesn't really have and I don't have a way to fallback anymore once I go route of specifying "provider" in getInstance() calls. In short - once we have a Provider providing a Standard service+algorithm we may have to implement other services+algorithm also otherwise it may not work (like I mentioned for SSLContext)
typo: byteArrray -> byteArray
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
nit: we have assertions like this in many test cases. With a more direct api to update quorum state, we can move these assertions into that api.
It's useful if the simulation test is deterministic. That way failures are easy to reproduce. Perhaps we can use a shared `Random` instance (between this class and the coordinator) with a defined seed.
This type is not parameterized. It's generally better to list the parameters when you reference a parameterized type.
I don't think we need this `null` check either.
I don't think we need the `null` checks.
Nit: "The file channel position..."
We should include `startPosition` in the message.
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
`The default "all" setting` -> `The default setting "all"`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
yes, it seems to be not what this test is checking on. I think we can drop it here.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: add `final`
store not used
nit: add `final`
Minor but I'm not sure if we'd prefer the builder's toString or the underlying request struct's toString as was formerly the case.
We don't need to log this on every request. Perhaps in ApiVersions.update(), we can log in debug level of any request in nodeApiVersions that's older than the version the client has. This way, this is only logged every time a client connects to the broker.
Yes, this looks good to me.
I wonder if we can raise this to info? This is a really important event to see in the logs, and we are always left guessing about it when the user does not have debug logging enabled. The frequency at the rate of the request timeout should keep it from being too spammy.
nit: typo `snapshoId`
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
nit: remove `this` (not required)
Update return type to `L` (if we introduce `L`)
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
If using hamcrest matchers, I think it's better to use static imports to make them more concise.
Oh, I just noticed. Then `synchronized` is not needed anymore.
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
need a check for null on `obj` here as well
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
nit: add `final`
nit: add `final`
ditto on removing before/after.
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
nit: remove `this` (not required)
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
I guess another way would be to let this configuration be a delta which is added to the the current time. That way we wouldn't have a lot of messages with the same timestamp, which might be a little uncommon in practice.
Ah, ok. That works.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
Actually, I'm not sure we necessarily even _need_ to call on the `FallbackPriorTaskAssignor`, we just need to schedule the followup and remove the affected tasks from the assignment
I was tempted to say we should just return an empty assignment, which would prompt everyone to rejoin again immediately, but I think the FallbackPriorTaskAssignor is a preferable alternative. IIUC, we should be able to rely on the precondition that any previously assigned tasks we correctly initialized _before_ they were assigned initially, right? So we know they are all safe to keep working (if possible) while we wait a suitable backoff period before trying to create these topics again. I could see the idea to instead just remove any tasks we couldn't initialize instead of calling the FallbackPriorTaskAssignor, but if I'm reading this code right, we might just have failed to verify that the topics exist, not only fail to create topics we know didn't exist. So, we might actually remove tasks that were previously assigned if we do this. It's not clear which strategy is better, since it would depend on the exact nature of the failure, but maybe at a very high level, it's better to continue processing existing work and delay starting new work than potentially to start new work but delay processing existing work. Or we could try for the "best of both worlds", where we assign the union of all previously assigned tasks and any new tasks we _were_ able to set up. Finally, even if we re-assign previously assigned tasks, I'm not sure if we actually need/want to use the FallbackPriorTaskAssignor in particular. There doesn't seem to be anything wrong with just computing a new assignment for a subset of the tasks while we also schedule a re-attempt to set up the rest of the tasks after a back-off period.
For 441 we added a `nextScheduledRebalance` field to the assignment in order to signal when a followup rebalance is needed. Can we leverage that here as well so we don't have to go through the whole ordeal of `onPartitionsLost`? Check out the call to `fetchEndOffsets`in `StreamsPartitionAssignor#populateClientStatesMap` where we schedule a followup rebalance on the leader if the `listOffsets` request fails. I think we can reuse the same logic/code path and keep track of a general flag like `adminClientRequestSuccessful` so the assignor can still finish the assignment
> assign the union of all previously assigned tasks and any new tasks we were able to set up I was worrying about the case where some internal topics got deleted, and we would cause trouble for the previous owner of the corresponding task. But I suppose if the topic was deleted randomly in the middle of processing, the thread would die anyway, so the odds of the original owner _not_ dying on internal topic deletion is pretty low. With this strategy, we would at least contain the blast radius to just that current owner since once it dies, that task has no previous owner and would not be assigned. So I'm pretty strongly in favor of this idea. Arguably we could just incorporate this into the existing `FallbackPriorTaskAssignor` since it will just reduce to the current one in the case all topics have been validated. I'm not sure if that would be more work or less, though.
That does not sound right. If we throw a `StreamsException` the thread will die.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
nit: add `final` -> `for (final Map.Entry...`
nit: add `final`
nice cleanup -- the original code was quite clumsy...
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Also discussed offline with Becket, but this test can probably be simplified significantly
Technically, this is `numDrainedRecords`.
Nit: would generally prefer to see this made explicit (i.e., `drain` twice) as opposed to loop (which in case of some bug could in the worst case be non-terminating).
Maybe we can have a numRecords variable for the `100`.
Yes, and more generally, my understanding is that we also need to consider if the consumer may return a "super-list" of the expected values, for example (still assume the above expected `{A, 1}, {A, 2}`): * `{A, 1}, {A, 2}`: this should be correct. * `{A, 1}, {A, 1.5}, {A, 2}`: this should be correct. * `{A, 1}, {A, 2}, {A, 2.5}`: this should be wrong, since `{A, 2}` should be the last for the key. * `{A, 2}, {A, 1}`: this should be wrong. * `{A, 2}, {A, 2.5}, {A, 1}`: this should be wrong.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
Ok, it looks better now. Let's leave it this way, with two lines.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
nit: use `{}` instead of string concat for `retries`
Not sure about this -- why not add two generics to the store, one for "wrapped" and one for "root" and keep this method that return the root type? I would also rename `inner()` -> `root()`
One thing I would suggest we do is to create an intermediate struct to store all the parameters, in case later we need to add more fields for sensor creation.
nit: for method _calls_, we usually format like this: ``` addAmountRateAndTotalMetricsToSensor( sensor, ...); Breaking the first parameter already reduced line length and seems preferable.
`Moving average duration` may be a bit confusing to readers, maybe just `Average duration of ..`.
I would suggest we co-locate the description with metrics and refactor out the sensor creation part, this may help reduce code redundancy.
Yes, each time the method is invoked (I.e., once per store), a separate copy of the string is placed on the heap. I previously didn't think this would be a big factor, but someone in the community profiled the memory usage of a long-running topology and found that these strings tend to accumulate over time. There's no need to worry about this for short-scoped strings like exception messages, but metrics are long-lived objects, and we benefit from making them static constants.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
One extra line.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
Thanks for the explanation. Make sense.
Hmm.. I'm wondering how did we succeed in this test case, since in the above code `send()` call is only captured with `TimeoutException`? Note that we only set the KafkaException in the callback while here we throw exception directly. And in fact, you changed the expected exception from StreamsException to KafkaException in line 128 above.
nit: align parameters.
We can remove the code block line 82-85 above since it will be called here.
nit: empty line.
I think the versions of subscriptionInfor / assignmentInfo are coupled, so if we bump up on one side we should do the same on the other.
For compatibility, we cannot change any message format of old versions, so I think we need to bump up the version to 5 and only do compression at 5 while no compression at version 4. Think about the case: when a streams application with multiple instances are rolling bounce to be upgraded, maybe some instance are already on version 5 and hence sends the assignment back compressed while some other instance do not recognize this version at all. We need to distinguish the cases when to compress and when to not compress. @mjsax has done the version probing protocol and he can provide more.
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Can we also assert that the state gets to `RUNNING` after the new thread has joined
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
nit: Use braces & separate lines
You actually do not need `this` here, right? The values are the default initialization values in Java. And `super` is also called in the default constructor. So actually, you could remove this constructor completely.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Nit: remove `this`
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
Nit: fix line break
It'd be more powerful to do an assertion on the complete set of returned plugins, since that will only require one test run to discover all differences between the expected plugins and the actual ones: ```suggestion Set<Class<?>> excludes = Stream.of(ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES) .flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of(SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS) .flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(true)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ``` (This assumes we split out `CONNECTOR_EXCLUDES`, but the same general strategy should apply even if we don't).
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
It seems like we're duplicating some of the logic contained in `Plugins` into this class by tracking class alias names and pre-computing plugin type based on them. Did you consider a `Herder` method that only accepted the name of the plugin, and took on the responsibility of deducing the plugin type itself? ```java List<ConfigKeyInfo> connectorPluginConfig(String pluginName); ``` In `AbstractHerder`, we could do something like this: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { Object plugin = Plugins.newPlugin(pluginName); PluginType pluginType = PluginType.from(plugin.class); List<ConfigKeyInfo> results = new ArrayList<>(); ConfigDef configDefs; switch (pluginType) { case SINK: case SOURCE: configDefs = ((Connector) plugin).config(); break; case CONVERTER: configDefs = ((Converter) plugin).config(); break; // ... Rest of switch statement follows same pattern, and rest of the method remains unchanged } ``` And in `Plugins` we could do this: ```java public Object newPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends Object> klass = pluginClass(delegatingLoader, classOrAlias, Object.class); return newPlugin(klass); } ``` Or alternatively, we could introduce a common interface for plugins that expose a `ConfigDef`: ```java interface DefinedConfigPlugin { ConfigDef config(); } ``` Which could really simplify some of the `AbstractHerder` logic: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { DefinedConfigPlugin plugin = Plugins.newDefinedConfigPlugin(pluginName); ConfigDef configDefs = plugin.config(); // No switch statement on plugin type necessary // ... Rest of the method remains unchanged } ``` And the change to `Plugins` would be lightweight as well: ```java public DefinedConfigPlugin newDefinedConfigPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends DefinedConfigPlugin> klass = pluginClass(delegatingLoader, classOrAlias, DefinedConfigPlugin.class); return newPlugin(klass); } ``` Worth noting that if we want to differentiate to users between "this plugin is not on the worker" and "we don't expose config information for this type of plugin", we'd have to make a few further tweaks.
Same thought w/r/t performing assertions on the complete set of returned plugins: ```suggestion Set<Class<?>> excludes = Stream.of( ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES, ConnectorPluginsResource.TRANSFORM_EXCLUDES ).flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of( SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS, CONVERTER_PLUGINS, HEADER_CONVERTER_PLUGINS, TRANSFORMATION_PLUGINS, PREDICATE_PLUGINS ).flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(false)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ```
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
The message doesn't seem to match the condition above.
Same as above - use StringBuilder and rename method.
Use `StringBuilder`? Possibly also change the name of the method since it is not a getter.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
Since I was fixing stuff anyway, I went ahead and fixed a bunch of formatting issues that I didn't bother mentioning before.
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Nit: dowrade -> downgrade.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
nit: add `{ }` to block
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
Same as above mentioned, the validation didn't get handled in new API.
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
I would move line 328 and 329 to before this line.
We can save this for a follow-up, but it doesn't seem too difficult to allow the consumer to seek to the offsets that were successfully fetched. I think we just need to change the type to something like `Map<TopicPartition, Either<Errors, OffsetAndMetadata>>`.
This condition should probably be like a few lines below: ``` java if (subscriptions.getSubscribedPattern().matcher(topic).matches() && !(excludeInternalTopics && TopicConstants.INTERNAL_TOPICS.contains(topic))) ``` So we would want to extract that condition to a helper method perhaps.
Need to fix indentation below.
nitpick: we usually include a space before and after ":"
don't think we need to log here . If a topic is not matching to the pattern than why bother logging here.
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
this is creative :)
We should log an error that prints out what the two configs actually are
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
We can reuse `streams`.
same as above with try/catch
Why is this changing? `infos` are not used below.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
This is a little odd. I wonder if we can instead add an abstract `hasRemaining()` method and use `InputStream.available()` and `ByteBuffer.hasRemaining()` to determine whether there are still bytes left. cc @ijuma
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
I really like this class.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
Relatedly, I think there might be some sort of checks in unit tests in maybe the producer or consumer that validate metrics are unregistered, might be able to use a similar approach here.
`error` is unused
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
paused -> running
this state is missing from the KIP, it should be added
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
nit: extra line
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
What if that never happens? It's better to always have a timeout in tests.
Seems like we should do a single `close` with some timeout. cc @cmccabe
Stream instance "one" -> "two"
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
Worked fine when I tried it locally: ```java assertEquals(Collections.singleton(tp0), records.partitions()); ```
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
the method name changed to `windowedTable` and `windowSize` parameter is missing
nit: might be better to set end to another timestamp.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
Any reason why this block of code isn't in the `if(tasksByTopicGroup.get(topicGroupId) != null)` block? After the `for(..)`? If it is not null then it is going to have some tasks, right? So numPartitions will always be > -1
`a graph containing`, and correct space between `1. Build`
we could use `computeIfAbsent`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
This part and the line 525-529 below can be extracted out of if condition.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
Also, instead of adding an extra operator node, I'd suggest we just do the checking within the operators themselves to reduce virtual function call overheads, for example see `KStreamKTableLeftJoinProcessor.process`.
As mentioned above, we should defer all the checks of null keys to "repartition (before join or aggregation)", and the "join / aggregate" operators themselves. I think we are already doing this for most cases, but just double checking.
Minor typo "will is"
an -> a
Thanks for verifying @vvcephei!
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
`before or after`
Same as above mentioned, the validation didn't get handled in new API.
Well, we wouldn't want to just remove that clause -- in the case of `SerializationException` you want to maintain the `SerializationException`, but if there's some other `RuntimeException` (which there can easily be for serializers that aren't aggressively catching exceptions and converting to `SerializationException`) then you still need to convert it to a basic `KafkaException`. I think you could do this: ``` try { // parse record } catch (SerializationException e) { throw new SerializationExceptionException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } catch (RuntimeException e) { throw new KafkaException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } ``` as long as we're confident there aren't any other `KafkaExceptions` we'd want to handle differently (i.e. any other more specific types of `KafkaException` where we'd want to preserve the same type instead of generalizing to `KafkaException`).
A quick look shows that other code has access to e.g. `topic` and doesn't include it in the exception message. Seems like having the fields there could help with better exception messages.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
Aha! Thanks. Yeah, I'd be in favor of coding defensively here as well.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
Yes, does not hurt to leave it. Just for sure.
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
Input parameter `partitionTimes` should always contain the correct partition time, hence, we can just get it: ``` final long partitionTime = partitionTimes.get(partition); ```
Even without EOS and assuming KIP-211 is in place, we'd probably still to commit regularly as a pre-requisite to fix https://issues.apache.org/jira/browse/KAFKA-6502.
This is not a feedback: as we are changing the main loop of StreamThread, we may need to carefully benchmark if this change along with the main loop changes will have unexpected performance penalty: with low traffic input stream, we are effectively sending sync commit requests more frequently. cc @mjsax If it does become a problem for performance, we could consider making the commit request async, and consider a commit only completed after the commit response is returned. Of course it means more complicated logic.
`replicaing` -> `replicating`
I think the assertion on 219 would pass even if the 1st mocked interaction never happened. Do we need something to tighten up the expected behaviour? Maybe something like: ```java verify(kafkaBasedLog, times(2)).send(any(), any(), any()); ```
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
Not necessarily, we could pass in both processing mode and the expected output as parameters, if the test workflow looks essentially the same.
Also could we try to parameterize this part as well? Like passing processing mode flags to each test
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
Hmm. I feel the `final` would be worth capitalizing the var name.
I think we can.
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Good point, thanks for clarifying.
Why do we need this? Seems to be a wrapper for `NetworkClient`, but does not add too much value IMHO.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
nit: Please fix code style.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
Nice tidy up of this test class :-)
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Shouldn't need this line, it's handled by the superclass's constructor.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
an -> a
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
method name changes
This overload does not take `Materialized` parameter
I wonder if the id check is sufficient. If a broker was reprovisioned with a new IP address, for example, we'd probably want to update this collection with the new Node.
nit: Empty line could be removed.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
Nit: might be worth adding a simple assertion on the result just to make sure.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
@alexjg The code is actually looping in this line, before the `waitForCondition` call is actually triggered, I think that is why you are seeing the indefinite hanging.
typo: we want to test the **case** that poll() returns no records.
as above `final` and one parameter per line
nit: add `final` to parameters / reformat one parameter per line
nit: remove -- not used
It's worth including the broker here as it was originally intended.
Does it still make sense to have the if/else here? we log debug anyway and the exception should contain enough information to figure out the type of error.
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.
I'd go for being consistent with the other logging statements
is a topic node
`with TopicNode and TopicsInfo` looks weird to phrase like this when we are defining `TopicNode`, maybe something like `with topic information associated with each node`.
do a link: `method setRepartitionTopicMetadataNumberOfPartitions` -> `{@link #setRepartitionTopicMetadataNumberOfPartitions}`
`...build a graph to calculate partition number of repartition topic, and numOfRepartitions of underlying TopicsInfo is used for memoization.`
Is this punctuate function necessary? Since we do not `schedule` in the init functions I think this punctuation will never be triggered.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This is not necessary, since the for loop below would be a no-op.
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
Is this ok to do for both reasons this would be called? Do we actually want to request a commit when in the case of the connector throwing a RetriableException? That'll result in the connector being forced to flush all its data.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
typo: moreq -> more
For a nice example where caps make sense see right below, where two sentences are included.
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Nit: reword to avoid "log" being ambiguous as verb or noun: "Writes errors and their context to application logs."
nit: as in `position` below, `this` is not required
Maybe we could use a different value here.
+1 to this
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Oh good point, we definitely need the key. But I think separating them turned out well
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Ah got it, my bad :)
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
If using hamcrest matchers, I think it's better to use static imports to make them more concise.
Oh, I just noticed. Then `synchronized` is not needed anymore.
@dguy @hachikuji if it sounds good to you I can go ahead and make this change while merging.
Yeah, it seems to me like we should remove it.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
this line can be merged. for example: ```java FindCoordinatorRequestData data = new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.rebalanceConfig.groupId); ```
We don't use null entries in JSON, because it gets too confusing. You should check against empty string here.
There's a pattern for all of the Trogdor JSON code where we don't use null anywhere. The problem with null is it gets annoying to check each collection for empty vs. null, each string for empty vs. null, etc. etc. null is also handled kind of inconsistently in Jackson. Sometimes Jackson will serialize a field that is null as `"foo": null` whereas sometimes it will just omit the field. (I think that `"foo": null` is actually not conforming JSON, by the way...) There are probably ways to configure all this, but null doesn't really provide any value 99% of the time, so it's simpler to just treat empty as null.
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Right, sorry I misread that line.
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
`return stream(null, null, keySerde, valSerde, topics);` Do the call directly instead of the cast.
nit: can we break this line? (similar below)
Part of the JIRA is to remove KeyValuePrinter class after it was replaced with `ForeachAction`
I wonder if this could be generalised further? Probably a broader discussion, but `KStreamPeek` is really the same as `KStreamMapValues` (there may be others). They just don't share a common interface for the action. I personally feel this would be a better rationalization of classes etc than combining `KStreamForeach` and `KStreamPeek`
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
I think, it is better to keep the default initial capacity of an `ArrayList`. Otherwise, the first time a stream thread is added, we immediately run into a memory allocation. Since we do not know how many stream thread we might expect, let's use the default. We could also consider using a `LinkedList` since we never access by index in production code.
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
Please simplify to ```suggestion Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count())); ```
Add missing `<p>` tag
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
You are right. NVM.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
This seems like it's a kind of weird restriction. I guess it'd be odd to use the same name for an internal topic and another, though if you know its going to be prefixed it might not be great to not be able to use that same name.
Yeah if it exists elsewhere let's just leave it as is for now.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
I removed this as it wasn't being used.
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
`a graph containing`, and correct space between `1. Build`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
nit: add a space after "multiple". i.e. `despite being claimed by multiple[ ]`
nit: to be consistent, we can just add `consumer` to `membersWithOldGeneration` and then let them to be cleared at the end.
NVM, I realized it should never happen.
nit: unintentional? one more in `subscription`
Small remark: By throwing here an exception you are giving up. But I am wondering if you should not try to fix it. I do not know which consistency guarantees a kafka consumer group exactly gives. But if it is possible to get into a split brain situation (network split), where temporarily a consumer group is split in two and gets two leaders. You get in a situation where the `SitckyAssignor` will never recover. If it was me, I would write a big fat error, and drop one of the two assignments. The same for line#322.
Hmmm... Seems to be in issue... The actual final return type is `KTable<Window<K>, V>` and thus is window-type agnostic. So we already have such a "container". -- However, `windowedBy(SlidingWindow)` returns a `TimeWindowedKStream`... Return types are not easy to change... And I don't think we can just switch from `TimeWindow` to `SlidingWindow` as concrete type either for the sliding window case... Maybe we are stuck and cannot fix the bug without a breaking change? For this case, we would indeed need to carry on with the KIP (but we could only do it in 4.0...), but I am wondering if it's worth fixing given the impact? Also: we have a few issues with the current DSL that we cannot fix easily (eg KIP-300). Thus, a long term solution could be, to leave the current API as-is, and built a new DSL in parallel (we did this in the past when we introduced `StreamsBuilder`). This way, we can change the API in any way, but it would be a long-term solution only. It might also help with regard to the new PAPI that uses `Record` instead of `<K,V>` type, and that is not easily adopted for `transform()` (and siblings). We could change the whole DSL to `Record` (ie, `KStream<Record<K,V>` -- or course we don't need `Record` in the generic type -- it's just for illustrative purpose). It would also cover the "add headers" KIP, fix KIP-300, we could introduce a `PartitionedKStream` (cf current KIP-759 discussion) and a few other minor issue (like rename `KGroupedStream` to `GroupedKStream`) all at once... And we could cleanup the topology optimization step and operator naming rules (which are a big mess to understand which `Named` object overwrites others...) -- We can also get rid of the wrappers for `KeyValueStore` to `TimestampedKeyValueStore` and change the interface from `Materialized<XxxStore>` to `Materialized<TimestampXxxStore`) -- In the past it was never worth to start a new DSL, but it seem we collected enough individual cases to maybe justify this investment now? The only thing that we should consider is our investment into "versioned state stores / version KTables". If we build a new DSL it should be compatible to it -- if we cannot guarantee it, we might want to wait until we understand what API we need to versioned KTables in the DSL and make the cut afterwards? \cc @ableegoldman @guozhangwang @vvcephei @bbejeck @cadonna (also @inponomarev @jeqo @vcrfxia)
Are you saying the `CachingWindowStore` internally uses a `TimeWindow`? Or is the `TimeWindow` somewhere along the store supplier code path...? Either way, doesn't this mean there's still a hole in the API since you can't use a custom WindowStore for a sliding windowed aggregation with the windowSize set to 0? If the WindowStore is going to represent different kinds of constant-size windows, it should probably be agnostic to the specific type of constant-sized window.
But I wouldn't be afraid to just use a full if/else block, either. ```suggestion final WindowBytesStoreSupplier storeSupplier; if (inOrderIterator) { storeSupplier = new InOrderMemoryWindowStoreSupplier("InOrder", 50000L, 10L, false); } else { storeSupplier = Stores.inMemoryWindowStore("Reverse", ofMillis(50000), ofMillis(10), false); } ```
This is how I typically break up ternaries. ```suggestion final WindowBytesStoreSupplier storeSupplier = inOrderIterator ? new InOrderMemoryWindowStoreSupplier("InOrder", 50000L, 10L, false) : Stores.inMemoryWindowStore("Reverse", ofMillis(50000), ofMillis(10), false); ```
Thanks @vvcephei -- that is convincing.
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
Are we intentionally not logging the exception as the extra parameter? If the exception wraps a more useful exception, we won't see any information about the wrapped exception unless we can see the stack trace in the warning log message.
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
Nit: ```suggestion final long maxAttempts = maxRetries + 1; ```
```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a *. {@link org.apache.kafka.connect.errors.RetriableException} * before retrying again; must be 0 or more ```
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
The goal of the ticket is to actually remove this check.
we also want to remove this check
We can just use `ConsumerRecords#records(TopicPartition partition)` since it should only contain one topic-partition's records.
In that case could we just use the foreach loop after `ConsumerRecords#records` to get the filter list from the returned list? I just felt leveraging on `ConsumerRecords#iterator` is unnecessarily costly.
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
That sounds good to me 
Update return type to `L` (if we introduce `L`)
nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
If we swallow the exception here, and the test always throws an IO exception, we will never notice. I guess it would be better to use `fail()` with a message.
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
Ah, I see. Thanks for the explanation
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
nit: remove empty line
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
im not *100%* sure, but i think you want this to also hit the controller. i hit the controller in my personal client, and @cmccabe is doing it here https://github.com/apache/kafka/pull/2472/files#diff-7378a806dbf302c1e7a9098c4780d2a8R283
I personally don't think it makes sense to wait for the metadata to propagate to all brokers. A broker may be partitioned away from the controller, but not the clients, for example. I would prefer to make the consumer and producer smarter when they get stale metadata from a random broker.
Sounds fine then (what else could you do if you don't even know who the controller is :)).
The case where you should fetch the metadata from the controller is if you expect the metadata response to include the topic that was just created. For example, if you create a topic, it returns a success and _then_ you ask any broker for the metadata, it may be missing the topic that was just created (since metadata updates are asynchronous). I haven't checked the code to understand if this matters or not.
I'm hoping KIP-117 will do that eventually.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
We probably shouldn't change this default to true -- we should override it in the specific test we need by calling `self.mark_for_collect(consumer, 'verifiable_consumer_stdout')` where `consumer` is the `VerifiableConsumer` instance. stdout for verifiable consumer can result in _huge_ log files, so collecting them by default will result in very large archived data for some tests.
I don't think this is a problem yet, but we should start thinking about these `Service` classes as at least semi-public interfaces. I know we rely on them in muckrake (although perhaps not this particular one yet) and I know others are starting to/planning to/want to be able to build tests on top of the pieces included in Kafka. While these definitely aren't the same as our client APIs, I think we should make an effort to provide some degree of compatibility.
@hachikuji Actually I asked for a name which made it clearer that this object contains numerical indices, and not actual node objects... :)
Haha, fair enough. Another option would be to actually use the node objects in the set. Anyway, just a nitpick.
A docstring for this method would be good :)
You could just do `selector.poll(100)` instead of `poll(0) + sleep(100)`. `poll()` returns when an operation is ready, so we are not waiting unnecessarily.
Stream instance "one" -> "two"
we need to shutdown the executor service even if there is a test failure (either in a finally block or the teardown).
Nit: you can call `Thread.enumerate` directly. Also, it would be good to assert that `threadCount` is < than `threads.length`.
Shouldn't the workers discover that the coordinator is unavailable while it is down? I'm imagining this test going like this: 1. steady-state workers are running 2. brokers stop 3. workers discover the coordinator is unavailable 4. workers stop their tasks 5. brokers start 6. workers discover the next coordinator 7. workers start their tasks 8. workers are running unaffected
nit: seems you can use `new ArrayList<>`
Maybe it's worth adding a code snippet which shows how to use the sticky assignment with the rebalance listener? It's a little different than with "range" and "roundrobin" from memory.
It might be better to use a Kafkaesque schema definition.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
Should be final
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
We need to keep this public method and deprecate. Perhaps throw an exception if multiple group ids were specified and retain existing behaviour for single group id.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
nit: we could use mkMap helper here as well.
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
```suggestion /** * Metadata of a stream thread. */ ```
```suggestion * Metadata of a Kafka Streams client. ```
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
i know it was motivated by findbugs, but this was probably a good refactoring anyway :) `RestServer.httpRequest` to *make* an http request has always been a bit awkward
We should read the metadata inside the while loop since it could change.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
createMetadataTopic() is no longer used.
Typo -- or is that Italian? :-)
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
Might not a big problem, but I wonder if we should check for the authentication exception before the `wait` as well? It is possible that `awaitUpdate` returns before the authentication failure happens. A subsequent call may then begin with the `authenticationException` not null which would cause a needless `wait`. I think an easy solution is to move this line up to the beginning of the `while` block.
This message seems a little low level for something which will get propagated back to the user. An alternative to consider would be to let `awaitUpdate` return a boolean indicating whether the update happened or not. That would allow us to raise an exception with a producer-specific message from `send()`.
The `CachingKeyValueStore` doesn't throw an NPE here, rather `org.apache.kafka.common.utils.Bytes.LexicographicByteArrayComparator.compare` does. We probably should add `Objects.requireNonNull(...)` to `CachingKeyValueStore#put(..)` etc
Can we change `subject` to `rockDdStore` -- it's a weird name IMHO.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
nit: add `{ }` -- we use curly braces for all blocks nit: remove double space after `==`
nit: add space `configEntry :` -- wondering why this is not detected by checkstyle...
Similarly, `adminClient.describeConfigs` does not read from ZK but from broker cache, and hence maybe subject to race conditions.
nit: double space after `=`
Thinking about this a bit more, using `adminClient.describeConfigs` here would be okay since we never dynamically change the config, we just want to test that the configs specified when the topic was created is expected. So if the topic creation is not yet propagated, it will fail and re-try on admin client anyways. So nvm.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
Add the `@Overrride` annotation to this method.
I'd suggest keeping the format slightly closer to what we had before. Specifically: `Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted`
Hmm... I don't believe "cancelled" is a term we've used in public-facing surfaces in the past. For example, when a task takes too long to shut down now and we have to cancel it, we log the message that "Graceful stop... failed": https://github.com/apache/kafka/blob/5964401bf9aab611bd4a072941bd1c927e044258/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L866 Personally I think the additional code complexity is worth it; the original ticket mentions a case where these messages confuse users because they're generated for cancelled tasks, so I'd rather err on the side of making things as obvious as possible to them. It might be possible to keep things simple and eliminate branches by tweaking the message to make it clear that newer task instances won't be impacted by this failure, though. A possible downside to this is that it might be confusing if there are no newer instances that will be brought up on the worker (because the connector has been deleted, the number of tasks has been reduced, or the task has been reassigned to another worker). But with some careful wording we might be able to avoid misleading people into thinking that this message implies there's already another instance running.
Looks good to me!
Should we just write this as: ``` if (cancelled) { } else if(stopping) { } else { } ``` ? Should be equivalent but a bit easier to follow.
Shouldn't this result in a change to a failed status? I think we'd either need to do that here (if thread safe) or have some way to communicate it to the original thread? We can defer this to another patch if we already weren't handling this case properly.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
We should not duplicate code, but instead extract an internal `private` helper method `putInternal` that can be called by `put` and `putAll`
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
If not, we should move the exception capturing logic inside the dbAccessor as well.
original was better
nit: since we're not doing anything in the EAGER case, couldn't we simplify this: ```java if (protocol == COOPERATIVE) adjustAssignment(ownedPartitions, assignments) ``` Similarly in `onJoinPrepare`
original was better
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
maybe: `inputKeySerde` and `inputValSerde`
nit: ".. select the grouping key and the value to be aggregated".
records to it, and reading all records from it, such that
an -> a
To be consistent, we should say "into a new instance of [a windowed] {@link KTable}".
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
We should also mention somewhere that we do not support concurrent transactions.
It may also be useful to have a test that checks that IBM Kerberos classes are available if `Java.isIBMJdk` is true and `com.sun` Kerberos classes are available if false. In particular, you could check the classes `com.ibm.security.krb5.internal.Config` and `sun.security.krb5.Config` which are loaded in `SaslChannelBuilder`.
@mimaison It is true that the test would check only one class depending on the JRE. But it checks that the relationship between `java.vendor` and Kerberos classes matches the expectation in the code (for that JRE). The other unit test is checking if String comparison works, which is fine as a unit test, but it doesn't really test the actual System property based on the JRE.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
I fixed this one to use the constant before merging.
We should have a constant rather than using '262' directly
Should we preserve the error message? Otherwise the user can't actually tell why this happened, they'll see the same error message for both stale config and forwarding errors. It's pretty important to expose enough info to tell what's going on because in cluster mode the user making the request may not have easy access to the worker logs.
nit: due _to_ no known leader URL ... or similar
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
This is public API meant to be used by users. I don't mind if our tests are a bit more verbose but we should aim to have succinct public APIs
Does this need to be public? Making it private forces use of `of` method, which I think is good.
nit: Update the unit test to use this new method. Jose changed them to use C'tor.
That's correct. The implementation becomes a bit tricky, as we can't just use `Arrays.asList` and be done.
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
this will never be called if one of the assertions fails
nit: java style
Since this only returns tasks whose previous client was caught up, I think we can simplify the first half of `assignTasksWithCaughtUpClients` as just ``` tasksToPreviousClients.forEach((t, c) -> assignment.computeIfPresent(c, (k, v) -> { v.add(t); return v;})); unassignedTasksWithCaughtUpClients = new ArrayList<>(tasksToCaughtUpClients.keySet()); unassignedTasksWithCaughtUpClients.removeAll(tasksToPreviousClients.keySet()); ```
```suggestion * Assigns tasks for which one or more caught-up clients exist to one of the caught-up clients. ```
```suggestion final Map<TaskId, List<ID>> taskToCaughtUpClients = statefulTasksToRankedClients.entrySet().stream().collect(Collectors.toMap( Entry::getKey, t -> t.getValue().stream() .filter(c -> c.lag() == 0 || c.lag() == Task.LATEST_OFFSET) .map(ClientIdAndLag::clientId) .collect(Collectors.toList()))); ```
(just a suggestion based on what I personally find easier to read)
TBH, I'm a little skeptical of using this style too much. Nothing against functional programming; it's just that, having done quite a bit of FP-heavy programming and maintenance for quite a few years, I've settled into an opinion that it's most efficient when employed in simple contexts. When you get into nested transformations like this, it becomes harder to come back to the code in three years with a completely blank slate and read it. Plus, it has a tendency to steer you away from efficient code and you can wind up doing multiple iterations over the same collection when one would have done. So, I tend to use the FP APIs to do stuff like turn a list of Tasks into a list of TaskId, and I'm happier to see regular loops and conditionals for stuff like this. This is very much a matter of preference, though, and I'm only expressing mine.
Nitpick: `maxWaitMs` would be a better match for Kafka's naming convention.
I would say something like: ``` java /** * Wait for condition to be met for at most {@code maxWaitMs} and throw assertion failure otherwise. * This should be used instead of {@code Thread.sleep} whenever possible as it allows a longer timeout to be used * without unnecessarily increasing test time (as the condition is checked frequently). The longer timeout is needed to * avoid transient failures due to slow or overloaded machines. */ ```
What is the reasoning for not throwing an exception if the condition is not met after the timeout? That would make the tests more concise.
Maybe mention the advantage of using this over `Thread.sleep`.
Consider adding some small back-off time (say 100 millis) to avoid busy looping and possible log polluting.
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
Seems this duplicates `L733`. Might be good to extract into a small helper method.
s/assignments/oldAssignments, and name could be simplified as `toNewAssignment`
This name seems backwards.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
Also add `@params topics`
Not for this patch, but we should do a KIP to add support for batch topic creation.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
Thanks for the catch!
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
Maybe we should simply pass the `ProducerRecord` in the constructor? We could then also just use the `ProducerRecord.toString` in the error so that we don't have similar issues in the future.
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Nit: new line is unnecessary, and there's a misspelling: ```suggestion log.error("Encountered an error while awaiting an errant record future's completion."); ```
nit: initialization is not required
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
prop: make the value a `SortedSet` so we can just insert clients as we build the map and use a custom comparator to automatically sort the clients based on lag
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
It would be good to have constants instead of hardcoding the fields in many places.
This was probably discussed in the KIP, but obviously the downside is that users won't get any warning or hint that they should transition. But I guess we don't get a substantial benefit from removing `AdminClient`, so maybe we'll just never do it.
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
Also set the store name in this test.
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
Worked fine when I tried it locally: ```java assertEquals(Collections.singleton(tp0), records.partitions()); ```
We can use `assertThrows` for this kind of pattern: ```java RecordDeserializationException rde = assertThrows(RecordDeserializationException.class, () -> consumer.poll(Duration.ZERO)); assertEquals(invalidRecordOffset, rde.offset()); assertEquals(tp0, rde.partition()); ```
It may also be useful to assert that the current consumer position is equal to `rde.offset`.
Might be simpler to use the mock deserializer only for values.
With this approach, it seems we could drop this case? There is also the insertion of the null for the UNSUPPORTED_FOR_MESSAGE_FORMAT case in `handleListOffsetResponse` that we probably can drop.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
That sounds good to me 
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
Hmm, this doesn't need to block merging this, but we should think carefully about doing delay this way. The rest of Connect avoids trying to rely on Java's `interrupt` behavior because it's not really a reliable way to *actually* interrupt threads, and in a system where there are pluggable components that are allowed to block indefinitely, relying on functionality that most Java developers don't understand well probably isn't going to work all that well. It may not have actually gotten to a KIP, but there was at least some discussion on a JIRA somewhere about making connect perform interrupts in addition to the basic task `stop()` calls it already does, but it doesn't currently do this. For anything that can end up with pretty long sleep periods, we should try to make sure there's a good way of interrupting it and moving on (e.g. so rebalances wouldn't get delayed because there's a connector that's encountering errors). At a minimum, since we don't do interrupts currently, I think we wouldn't interrupt this code currently. The other approach we use elsewhere is to `wait` on a monitor so we can set a flag and interrupt with `notify` and have it bail out immediately.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
Setter methods again.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
@rajinisivaram makes sense. I was thinking if we can break the JAAS config file and made them into client config properties. But just passing JAAS config file will make it easier and extensible.
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
methods => `mechanisms`
Can you please elaborate why we no longer read the header during construction? It seems to me that `checkHC` could be a constructor parameter and then we could keep it as a private and final variable and less changes would be required. But maybe I am missing something. Note that public and mutable variables are generally avoided in Java.
Both `GZipInputStream` and `SnappyInputStream` read the header in the constructor, so it would make sense to me to remain consistent in that respect.
Also, 5ms seems a bit extreme. Maybe this could be 20ms or so and we could use the minimum of this and the configured retry backoff so that users can adjust it lower if they need to.
Yeah, I have no doubt the performance is better. It's just that it seems like a lot of excess traffic and is going to be amplified by the number of transactional producers. It may be fine in the common case if the write markers are pretty quick, but if there is any kind of delay, then I'd be concerned about the brokers being overwhelmed with these requests (though maybe it's not as bad with request throttling). I'd rather err on the safe side for now since users can manually adjust the backoff. For the 0.11.0.1 release, we can provide a better solution. Most users will probably hold off until then anyway.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
@edoardocomar `Prepared` doesn't convey much meaning in terms of an externally visible metric. I imagine you chose it rather than `authenticated` since you intended it to work for `PLAINTEXT`. But `PLAINTEXT` doesn't go through this if-block since `channel.ready()` returns `true`.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
I seem the right fix would be to add the missing `{@code }` annotation in L184? ``` <pre>{@code ``` The closing `}` is already in L190.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
This syntax is a bit hard to follow with the conditional at the end. Can you rewrite it to something like: ```python self.jmx_tool = None if jmx_object_names is not None: self.jmx_tool = ... ``` (and also check the attributes as mentioned above)
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
looks good, thanks
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
Nit: space missing before `timestamp_type`.
I think we can simplify these functions. Something like this: ```java private static byte readByte(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 1) readMore(buffer, input, bytesRemaining); return buffer.get(); } private static long readVarLong(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarlong(buffer); } private static int readVarInt(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarint(buffer); } ``` I think we shouldn't need more than one call to `readMore`.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
We could port this function when it is actually needed.
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
Nit: we don't need this tag before the parameter list.
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
nit: remove empty line
nit: remove empty line
Add a check to verify, whether the iterator has no more elements.
nit: remove empty line
Here it is better to use to distinct records, because if the code contains a bug that adds a record twice, you would not discover it.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
Not introduced in this patch: "is non" => "as non"
Can initialize to `new HashMap<>()` here as is done with `invalidExtensions` below.
Yes, we could add `ignoredExtensions` and include that in the log in the server.
@rajinisivaram @stanislavkozlovski LGTM with the possible exception of maybe adding support for retrieving/logging any ignored extensions? I'll defer to your preference on this.
extension name must not be empty
We also need a test to validate that some extensions can be ignored (neither valid nor error).
I think it'd be useful to verify the behavior of casting all of the logical types to strings, just so that we verify the formats (e.g., timestamps, times, and dates should use the ISO 8601 representation) and have some regression tests for the future to help ensure we maintain backward compatibility.
It would be great to have a few more test cases to cover more scenarios: 1. cast to types other than `int32` and `int64` (e.g., `float64` and `float32`), including where we lose precision 2. verify we can cast these to strings 3. verify the cast fails in expected ways 4. casting null values should not fail
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
Is there a reason not to include support for casting `DECIMAL` to other primitives? Yes, this might be lossy, but it also would help in cases where there would be no loss of precision.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
This loop is a little puzzling to me. Is it not the same as the following: ```python assert all_captured_preferred_read_replicas[non_leader_idx] > 0, ... ```
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
It is a shame we have to do it like this, but i don't see a viable alternative
Ah! I misread this as turning `logAll` *on* instead of *off*. Now I get it :)
Not sure whether this change is necessary. The value should be validated already.
We can drop the parenthesis here. Same below
Looks good. I like the additional checking that you're doing here.
Good catch, thanks for cleaning this up!
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
We should also verify the thrown cause
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
Could combine these into a single `testCycleCollection()` method if you put a `null` value in the list (e.g. `"A", null, "C"`) and for every one of the 4 positions (0-2 and cycling back to 0) you also check the `peek()` value. I think it would be clearer compared to what you have currently since the last 2 methods you have now are a bit haphazard.
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
nit: Could we move `new DeleteTopicsRequestData.DeleteTopicState().setName(topic1)` to a new line in order to align it with the following `new DeleteTopicsRequestData`? There are few other cases like this.
Use `StringBuilder`? Possibly also change the name of the method since it is not a getter.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
Nit: maybe `("Topic: " + topic)`
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
nit: double space
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
We just need to make sure the extracted generation from all three processors are the same.
If we're just testing broker compatibility I don't think we even need this part of the test.
Instead of pulling the value out with a regex, what do you think of `streamsString.contains("appId")`. Although what you have works as well.
Could just use `false`
could use `assertFalse`
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
Sounds fine to me.
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
How about this. First, we can augment the message above to something like this: ```scala log.trace("Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`", position, nextPosition, completedFetch.partition, partRecords.size()); ``` This gives us enough information in the logs to see which partitions caused the `poll()` to return and it tell us exactly where in the log the aborted transaction/control records exist. Second, maybe we can add back your previous log line, but make it a little more terse and put it at trace level: ```scala log.trace( "Returning empty records from `poll()` since the consumer's position has advanced " + "for at least one topic partition") ```
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
I'm inclined to either remove the log line entirely or move it back to its former location in `KafkaConsumer`. Will leave it up to you.
nit: could be useful to log the type of exception in the assertion message.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
typo: moreq -> more
Also: should only call onPartitionsLost on owned partitions that no longer exist
nit: move below the shortcut return below.
If stop throws we won't count down the latch. No harm will result except there will be an erroneous log messages about exceeding the stop timeout.
We should name the thread so that thread dumps are a bit more informative. I _think_ these should be daemon threads because if we're prepared to basically ignore the non-return of `task.stop()` during runtime I don't see why we'd block jvm exit for them.
`toString` is not required.
Is this line intentional? Unit tests normally don't need to print out to console.
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Just let the Exception flow, that will automatically fail the test
We can use `Collections.singletonMap()`
By catching Throwable, aren't we silencing the error and not immediately failing the test? Also we should not be catching Throwable, if the JVM throws an Error we want it to fail the test and never want to handle it. There're a bunch of them in these files
The `enable.idempotence` will be true by default in 3.0, so we should perhaps have the "up to 5" thing first.
Hello and thanks for reviewing! The callout about record ordering was originally requested within Confluent; the same is mentioned at this page: https://developer.confluent.io/tutorials/message-ordering/kafka.html. We think this would also benefit AK docs.
typo: with message ordering preserved for any allowable **vlaue** --> **value**
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
I meant to have "Note that enabling idempotence requires this config..." before "Allowing retries...". And break the two parts with a paragraph.
@cnZach Looks like the intention was to print out the exception with stack trace? You would want to use: ``` public void warn(String msg, Throwable t); ```
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Is this test needed? It seems that loginContextName can never be null.
Harsha did this.
It seems that we need to set the login time during the initial login as well.
nit: add a size? There are a few cases in here where we could do this.
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
We should read the metadata inside the while loop since it could change.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
You are right. Checking ```Error reading field 'field2'``` should be enough for this test case.
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
```suggestion // Only return a default value fallback instead of `null` when the field is actually required ```
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
Nit: let's not add an unnecessary extra line.
SGTM. We can keep it as is then.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
rewrite test as above using `assertThrows()`.
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
As for all tests, we should verify `topologyDescription`
This method is only used once -- we should inline the code and remove the method.
```suggestion assertThat(CLUSTER.getAllTopicsInCluster(), contains(changeLog)); ``` And we can remove line 426, `final Properties config = CLUSTER.getLogConfig(changeLog);`, since it would be unused.
Do we need to wrap with the LinkedHashMap? Could we just do `Collections.unmodifiableMap(metrics.metrics());`
This is not required as contained in the check next line.
@rodesai I see your point here. However, the downside of not throwing is that we will also not notice the bad behavior in our tests like the soak tests. I personally prefer to improve tests instead of downgrading the reaction to bad behavior. Assume in future somebody makes a change that breaks the assumption of the non-shared metrics registry, we would find this bug immediately during development instead of during production. Another option that comes to my mind is to classify exceptions that originate from the metrics framework differently in the uncaught exception handler, but that would probably need some more work.
nit: These two functions are not for testing only.
nit: Capitalize `p`.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
This is not required as contained in the check next line.
This causes an ArrayIndexOutOfBoundsException: `i <= threads.length`
Up to you I guess. No need to expand the scope even further for a tiny nit.
Yeah, that might be better.
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
Did we want to use `valueAndSchema.value()` for the last parameter? Same for the message below.
Sure, I'm just saying that `msg.value()` is an array, which does not have a useful `toString` anyway.
AK convention is to not use `set` setters or `get` getters.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Nitpick: I'd call this `getOrCreateFileChannel`.
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Let's move this helper into `ListOffsetRequest`
`new ArrayList<>` is suffice
As we don't have the list of TopicPartition available to filter the list of futures, we could actually directly complete the future within the loop instead of populating the HashSet. It avoids building the HashSet and having to traverse the futures.
This returns the resulting map, so we can avoid the `get` below.
We would want to check for `InvalidMetadataException` here as well. We need to go back to the Metadata call if we find a metadata error. This is similar to the cases when we call `rescheduleFindCoordinatorTask`.
We should use `aasertThrows` and verify the error message similar to the `TransformerSupplier` test
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
as above -- guess some more below
I think, we should remove the expected class here (note, that a `StreamsException` might be thrown not just from the last statement that we want to test -- thus, using `expected` the test might pass even if the exception is thrown from a different place what would be an actual failure
As above: use `assertThrows` and verify error message
nit: move to line above.
Some of these lines have extra 4 space. Other lines have extra 8 spaces.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
What's the purpose of this warning? It doesn't seem needed.
A bit unclear why we need this. In my mind, `readFully` should be as close to `FileChannel.read` as possible with the exception that it attempts to fill the buffer while end of file is not reached.
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I know this is just how the other tests are doing it, but it's not really an airtight way to validate the expected results...if nothing is returned then we never enter the `while` loop and the test passes, even if we did in fact expect there to be actual output. The important thing here was just to make sure it didn't throw an exception so it still does that, but it would be good to fix this up maybe in a followup PR
I think it is still possible. Here's one scenario: 1. last checkpoint at offset 100; all writes goes to old CF. 2. continue writes to old CF til offset 110, but no checkpoint written yet. 3. non-graceful shutdown happens, and upon restarting new CF is used. 4. we start restoring from offset 100 to log-end-offset 110, to the new CF. Now we ended with data of offsets 100-110 in both CFs.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
@spena just ping to make sure you get this on the follow-up PR.
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
We could detect if the processorTopology contains only `StaticTopicNameExtractors` and still throw in that case if the topic name isn't in the topology.
Ok, let's just keep it in our back pocket for now.
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
nit: `final` params
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
Why could we not fix this? The underlying issue seems to be the state transitions of `StreamThread` -- it allows to go from `CREATED -> RUNNING` -- if we change this, and we can only go to `CREATED -> PARTITION REVOKED` we should be able to tackle this issue? (Maybe a different PR to do this though.)
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Nit: fix line break
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
Nit: remove `this`
Hmm, if we performs an unclean leader election, the only replica in ISR should just be the new leader since the data in existing ISR is not guaranteed to match with the new leader.
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
If you use `assertEquals` it will compare the content of the arrays and print their value if they don't match.
with in => within
Maybe I am wrong, but shouldn't this be moved inside the `while` loop, so it is initialized every time a new assignment is selected? Because, now I think only the first consumer will be challenged against this previous partitions.
Detail: just to be sure, I would initialize it to `this.generation`, to make sure the generation always increments.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
nit: unintentional? one more in `subscription`
Cool, I was kind of hoping you would put this in a separate integration test class
Why set this? zero is the default anyway
Could we define it as `@BeforeClass`? I think we do not need to create input/output topics for each test case.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
the field "ERROR_CODE" is never used.
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
Let's use `Map` on the left side instead of `HashMap`
Add a reference to KIP-511 here
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
Minor: maybe move this to initialization? ``` java int newTaskCount = configs.size(); ```
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
I think we ditch the before/after methods as I previously recommended.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
ditto on removing before/after.
nit: remove empty line
Let's remove the brace changes please.
Kafka doesn't mandate braces in `if` statements.
Seems there's no need for this method to declare `ConcurrentLinkedQueue` as the return type. You can use a normal `Collection`, which would then allow you to use `Collections.emptyList()` instead of pointlessly creating an empty queue.
Re-reading, he did say that, but I think his main point is that it would be good to be consistent. :)
@hachikuji is suggesting that `requestIterator` should return an empty iterator instead of `null` to be consistent with the other methods.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
Might be better to use an `Exception` variable `firstException` and rethrow at the end if not `null` -- IIRC, behavior is undefined if we throw a second exception (ie, `finally` would executed after the first (outer) `catch` block.
I think this is the issue you reported in the Jira. The `RaftClient.Listener` should not use `RaftClient.leaderAndEpoch` to determine if it is the leader. It should instead use `RaftClient.Listener.handleLeaderChange`. For this state machine `ReplicatedCounter` we should look at the `claimedEpoch` variable. I am going to create an issue to remove this method. cc @hachikuji
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
Not really sure this has value if the test case expects the leader change correctly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
nit: braces unneeded
need to update `./gradlew uploadArchivesAll` at line no: 644
I meant `public_html` directory.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
We can use `==` to compare enums.
Let's keep the existing `trace` and `error` log lines in the `else` block. My suggestion is to add a line at the debug or trace level in the `if` block so users can know if an error is ignored.
you dont need the `String.format` here would need `%s`->`{}`
Writing one log message per record, seems excessive, even for `DEBUG` There's a trace level message above for the record. Maybe we want to append the topic name there.
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
nit: Provide a message to the IllegalStateException constructor
nit: do we still need this function? Could we just reference the `metricLock` object directly? Since it is private my understanding is that it was not intended to be used outside this class.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
Instead of doing this, I think we can have the following 3 methods in `SslFactory`. Thoughts? ```java public SSLEngine createSslEngine(Socket socket) { return createSslEngine(peerHost(socket), socket.getPort()); } /** * Prefer `createSslEngine(Socket)` if a `Socket` instance is available. If using this overload, * avoid reverse DNS resolution in the computation of `peerHost`. */ public SSLEngine createSslEngine(String peerHost, int peerPort) { if (sslEngineFactory == null) { throw new IllegalStateException("SslFactory has not been configured."); } if (mode == Mode.SERVER) { return sslEngineFactory.createServerSslEngine(peerHost, peerPort); } else { return sslEngineFactory.createClientSslEngine(peerHost, peerPort, endpointIdentification); } } /** * Returns host/IP address of remote host without reverse DNS lookup to be used as the host * for creating SSL engine. This is used as a hint for session reuse strategy and also for * hostname verification of server hostnames. * <p> * Scenarios: * <ul> * <li>Server-side * <ul> * <li>Server accepts connection from a client. Server knows only client IP * address. We want to avoid reverse DNS lookup of the client IP address since the server * does not verify or use client hostname. The IP address can be used directly.</li> * </ul> * </li> * <li>Client-side * <ul> * <li>Client connects to server using hostname. No lookup is necessary * and the hostname should be used to create the SSL engine. This hostname is validated * against the hostname in SubjectAltName (dns) or CommonName in the certificate if * hostname verification is enabled. Authentication fails if hostname does not match.</li> * <li>Client connects to server using IP address, but certificate contains only * SubjectAltName (dns). Use of reverse DNS lookup to determine hostname introduces * a security vulnerability since authentication would be reliant on a secure DNS. * Hence hostname verification should fail in this case.</li> * <li>Client connects to server using IP address and certificate contains * SubjectAltName (ipaddress). This could be used when Kafka is on a private network. * If reverse DNS lookup is used, authentication would succeed using IP address if lookup * fails and IP address is used, but authentication would fail if lookup succeeds and * dns name is used. For consistency and to avoid dependency on a potentially insecure * DNS, reverse DNS lookup should be avoided and the IP address specified by the client for * connection should be used to create the SSL engine.</li> * </ul></li> * </ul> */ private String peerHost(Socket socket) { return new InetSocketAddress(socket.getInetAddress(), 0).getHostString(); }
`while` seems to be missing
It was removed from the other versions of `group` but not from here.
I think we need to skip all the code in this else block if the partition is no longer assigned.
@enothereska Yes, I think that is a better solution. But I think @hachikuji was right that we don't need to cover both if/else branches. We just need to cover the `parseCompletedFetch`
@enothereska The trunk code does not need to access `subscription.position`, instead it uses `PartitionRecords.nextInlineOffsets` which should be the same as position because the position is updated to this value every time after a successful `fetchRecords()`. The big try/catch is to make sure the the exception from `fetchRecords` will also be caught and not result in loss of non-empty `fetched`.
@enothereska Looking at the previous patch, it seems like the scope of the try/catch is unnecessarily large. I think it only needs to cover the call to `parseCompletedFetch`. If we do that, then there should be no need to access `subscriptions.position` down this path.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
The number has changed and 5 is no longer relevant.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
Hmm.. In fact, there is not necessarily any relation between the partitions that are being consumed and those that are being written. Usually you would expect them not to overlap. I wonder if it actually makes more sense to track these offsets in a separate map.
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I think we should use the same name here; the metrics-scope would differentiate rocksdb from in-memory, which is sufficient.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: The sentence sounds slightly better if you remove `the`
`replicaing` -> `replicating`
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
this overwrite of mm2config should go in the setup method, IMHO
nit: add `final` (we use `final` whenever possible)
nit: `maybeDeleteInternalTopics` and `maybeReset...`
could we turn this into a loop? i.e., sth-like? ``` List<KeyValue> records = Arrays.asList(...); for(KeyValue record : records) { mockTime.sleep(10); IntegrationTestUtils.... } ```
nit: also add java doc for type `T, O` here
nit: add `final`
Do we lose anything if we use `Set` instead of `Collection`? Seems like set is the right semantics.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
nit: extra line
This is also related to the following PR https://github.com/apache/kafka/pull/1015 that uses sentinels in a number of places. It would be nice to be consistent on -1 versus null.
`long` -> `Long` is a binary incompatible change.
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
Pass this in as a ctor param rather than constructing it? The `stateRestoreCallback` is only used to create the `CompositeRestoreListener`
nit: line too long
Got it. For future readers the function is `maybeCompleteShutdown`.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
this is creative :)
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
nit: seems unnecessary? similar for `State.FAILED` below.
consumerThread -> rebalancingConsumerThread
Use log object if this is to be kept
Since condition is just a comparison, you can put the comparison here directly
nit: when records2 is empty, you can return immediately.
This can be moved to ConsumerRecords class
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Should we include the brokers instead of removing them? Same below.
same question around log level as above
same question around log level as above
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
Why adding a suffix? Is there any problem if the topic name is equal to the store name? This breaks ktable.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I am not sure we want to move all the tasks in a topology? Maybe we can do that by task or sub topology? maybe topology is best but I will need to think about it a bit
It would be much simpler but unfortunately its not as simple as we first thought. The producer has only one transaction, so the records of the good tasks are mixed in with the records of the failed task and there is no way to separate them. So we need to take the tasks that we know will fail and process all the other tasks without them. That way we continue making progress. We can take the failing tasks and backoff and retry again later.
It might be nice to keep the tasks/topologies that have failed in another list entirely. Then when reprocessing after an exception we can run all the good tasks first and commit them before running the failures. This will be important for EOS as we con't commit only part of a transaction. The larger part of that doesn't need to be done it this PR but keeping the groups separate would be nice in my mind
```suggestion final Map<TaskId, List<ID>> taskToCaughtUpClients = statefulTasksToRankedClients.entrySet().stream().collect(Collectors.toMap( Entry::getKey, t -> t.getValue().stream() .filter(c -> c.lag() == 0 || c.lag() == Task.LATEST_OFFSET) .map(ClientIdAndLag::clientId) .collect(Collectors.toList()))); ```
(just a suggestion based on what I personally find easier to read)
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
Nit: I think `0.0` is idiomatic in Java.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
That sounds good to me 
I hope we can get rid of those conversion in the future :)
I would call this one `topics()` as you did already in the request.
nit: we can use `map#compute` to replace getOrDefault + put.
Same here, we can cache the result of `Builder.getPartitions(data)` for re-use.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
Nice, thanks for the update. Looks good
```suggestion public void shouldInstantiateAssignor() { ```
The parameters should be the other way round (expected is `conns` and actual is the metric value).
We cannot guarantee that this poll will see all completed connections, so it would be better to poll in a loop until the total connections returned from`selector.connected()` after the poll equals `conns`.
You could just do `selector.poll(100)` instead of `poll(0) + sleep(100)`. `poll()` returns when an operation is ready, so we are not waiting unnecessarily.
`selector.connected()` is cleared after each `poll()`. So we should keep track of the total number and compare the total against `conns`.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
nit: use `{}` instead of string concat for `retries`
Not sure about this -- why not add two generics to the store, one for "wrapped" and one for "root" and keep this method that return the root type? I would also rename `inner()` -> `root()`
Thanks for identifying this issue. Piggybacking on OP_WRITE may not be the best way to fix this though. I was thinking, if connected is true, we can probably just call socketChannel.finishConnect() and register the socket channel with OP_READ, which is what PlaintextTransportLayer.finishConnect() and SslTransportLayer.finishConnect() will do. Then, we don't need to change the logic in poll.
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
Should be more specific about the type of error being caught -- catching all exceptions should be reserved for very special cases, like protecting the top stack frame of a thread to avoid uncleanly exiting the thread. I suspect that here you specifically want to capture `CalledProcessError`, which indicates an issue running the command on the remote host and/or `ValueError`.
Does mirror maker support multiple consumer configs? A quick glance at the code suggests it only supports one.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
We can use the `replace` overload that takes a `char`.
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
I think it was my suggestion to add it. I was going off the pattern for tagged node metrics in Selector.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
I don't think limiting the pool to 2 threads is really what you want, since you have 3 runnables that you are creating later on. Check out some of the other newXYZThreadPool functions. There is at least one that doesn't limit the number of threads.
Maybe something like "No command specified" would be more descriptive
Suggest using `Executors#newCachedThreadPool` rather than manually counting threads. We don't need a `ScheduleExecutorService` here either... just a plain `ExecutorService` is fine.
Should this be `num_lines=3` (cf. L116 and L126)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Not for this patch, but we should do a KIP to add support for batch topic creation.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
Like said earlier, I think we could just return `return new StreamsResetter().run(parameters, cleanUpConfig) == 0`
I looked at the test and at least one failed test namely `testReprocessingFromDateTimeAfterResetWithoutIntermediateUserTopic` does not have any internal topics to create, and hence none to delete. So it's not clear if bumping up the timeout would help here. I'd suggest we first augment the error messages to include the expected topics and the actual topics
I am always a little "concerned" about bumping timeouts if we don't understand why it actually fails. 30 seconds seems like quite some time. How long is deleting topics supposed to take? As far as I understand, we send a single request to delete all internal topics via AdminClient to the brokers. Is there a relationship between the expected completion time to delete all topic the the number of topics in the request? \cc @cmccabe
Fair enough. Let's leave it as-is.
nit: insert space `String... expected`
`validateStoreOpen()` can be outside of lock block.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
Can `rebalancing()` throw a `InvalidStateStoreException` ? If yes, we need to split this an apply try-catch-fail pattern instead of using `@expected`
You can remove this `assertThat` and the loop below as you've already proven this is true in the test above. So no need to assert it again.
nit: if you want a new paragraph you need to add `<p>`
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
Ok. then LGTM
I noticed that `queryableName` is a different parameter than the one we actually build the parent processors with (`storeBuilder.name()`). It wouldn't surprise me if there's a subtle difference between them.
Yes, that sounds like the right thing to do. Thanks!
Can remove the `W extends Window` generic
It seems like your indentation is set to 8 spaces instead of 4.
> this logic Seems only needed because we have the check in 309 (?) No, I don't think so. It should be for line 279: ```java // to handle the case that when there are still unassignedPartition left, but no more members to be assigned. if (unfilledMembersWithUnderMinQuotaPartitions.isEmpty() && unfilledMembersWithExactlyMinQuotaPartitions.isEmpty()) { throw new IllegalStateException("No more unfilled consumers to be assigned."); ``` In line 309, it is just an early error detect and log for it. Not related to `potentiallyUnfilledMembersAtMinQuota` (or now `unfilledMembersWithExactlyMinQuotaPartitions` members)
Honestly it took me quite a while to understand the fix :P After understanding that I think maybe it's better to rename these two collections more explicitly: 1) `unfilledMembers` -> `MembersWithLessThanMinQuotaPartitions`. 2) `potentiallyUnfilledMembersAtMinQuota` -> `MembersWithExactMinQuotaPartitions`. And also (since the maxQuota is always either == minQuota or minQuota + 1): 3) `expectedNumMembersAssignedOverMinQuota` -> `expectedNumMembersWithMaxQuota` 4) `numMembersAssignedOverMinQuota` -> `numMembersWithMaxQuota`
@ableegoldman , I reviewed it again, and found we forgot to sort the `unfilledMembersWithExactlyMinQuotaPartitions` list here, to have deterministic result.
nit: add a space after "multiple". i.e. `despite being claimed by multiple[ ]`
I'm thinking we can have a test for package scope `partitionsTransferringOwnership` in `AbstractStickyAssignorTest`. I found we didn't test it before. We can verify the doubly assigned partitions and other revoked partitions are put into `partitionsTransferringOwnership` correctly.
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
Nit: go with single parameter per line.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
We can use `List<Class<? extends Connector>` to avoid the warning
This is unused too
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
It looks like this is not used anywhere
That's a good point.
This catch is a bit weird to me. Could you create a true `CompletableFuture` carrying the exception `new TimeoutException()` instead of mocking object? For example: ```java CompletableFuture<RecordMetadata> future = new CompletableFuture<>(); if (success) future.complete(new RecordMetadata(new TopicPartition("tp", 0), 0, 0, 0, 0, 0)); else future.completeExceptionally(new TimeoutException()); ```
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).
req: typo unknown Pid
@xiaodongdu, I was not suggesting getting rid of the field. It's fine to have a new field, but we should call the field `kafkaClusterId` rather than `clusterId` since the latter could be misinterpreted to mean the _Connect_ cluster ID.
I wonder if you also considered moving startup and shutdown into the `Worker`? The advantage is that we already have an executor there and then we'd get the parallel implementation for `StandaloneHerder` as well, though admittedly that may not matter as much.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
empty line needed
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
Maybe doesn't matter, but seems a little more intuitive to check that the error is not NONE.
Given usage, this could probably be a Set.
nit: usually we drop the `get` prefix on getters.
If we follow the same pattern as `ConsumerGroupOperationContext`, then this could be a static method which takes the response as a parameter.
Not something we have to do here, but one way we could improve this in the future is by taking into account leader epoch information from individual partitions. We can ensure that epochs increase monotonically in order to prevent using stale information during retry. Another thing we could do is reduce the topics we are fetching metadata for as the ListOffsets requests complete. Ideally we'd only be refetching metadata for topics with metadata errors.
@hachikuji is correct. We can't do blocking operations in the admin client service thread. We certainly can't do blocking operations that wait for the service thread itself. This will deadlock. I think it's a good idea to have a coordinator node provider, but we need to build out a little more infrastructure to make it possible. I have a change which should help with that, at https://github.com/apache/kafka/pull/4295
The ideal would be to use the `CoordinatorNodeProvider` here. There is not much benefit in having it if we just invoke it inline. The problem is that the `provide()` method is called by the send thread, so we cannot have it block on an operation which itself depends on the send thread. To make it work nicely in this way, we probably need an asynchronous `NodeProvider` API which effectively lets us chain the `DescribeGroup` request on to its completion. For example, maybe something like this could work: ```java interface AsyncNodeProvider { KafkaFuture<Node> provide(); } ``` cc @cmccabe (who may have some ideas as well)
nit: also add java doc for type `T, O` here
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
As above: need to keep default value.
Are we sure we want to make them required config for `KerberosLogin`? I noticed that we have actually defined default value for these configs in `SaslConfigs.addClientSaslSupport()`. These default values are used by producer and consumer. And if these configs should be explicitly provided by user, should we have a class that extends `AbstractConfig`, define these configs as required, and use this class in KerberosLogin to handle user-provided properties, in the same way that ConsumerConfig is used? This would allow us to throw exception in case of missing config using a unified mechanism.
I was also wondering how you ran into these NPEs. This is an internal class and the configs should never be null as there are default values.
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
super nit: not part of this PR proper, but maybe set `RuntimeException` returned from the `suspendTask()` call to variable then use it in the `assert` statement? Makes things a little easier to understand.
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
We should also verify the thrown cause
Q: Why do you use a mock here? In the ticket you said you just want to replace `MockStreamsMetrics` with `StreamsMetricsImpl`. req: If there is no specific reason, I would propose to either create a common mock that can be used everywhere as I proposed or to consistently replace `MockStreamsMetrics` with `StreamsMetricsImpl`.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
nit: toString not necessary
nit: `--topic` and `--partition` could be extracted as helper static functions
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
nit: we could just build the expected result as a whole set and compare
typo: byteArrray -> byteArray
And nitpick: there should be a space after the comma.
super nit: taskCreationLock --> taskDirCreationLock
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
these overrides don't seem to add much.
never mind then. I'll leave this to AI.
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
ditto here, should be moved to RocksDBStore#close
configSetter.close() will clean up any resources constructed in configSetter.setConfig, and should only be called in RocksDBStore#close()
Just following on my other idea about collapsing into a single class here: maybe instead of naming it as keyValueWithTimestamp, we just name it as: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 to now. And the flag can just be indicating if it is 1) or 2) above; in the future if we need to do this again we can then have: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 - 2.5 (just made that up). 3) "3.0" -> version 3.0 - now. etc.
I cannot find in rocksDB docs whether newIterator.seekToFirst().isValid() is a safe / efficient way to check emptiness, could we had a unit test for this specific case? 1) starting the DB with empty cf at the beginning. 2) starting the DB with non-empty cf at the beginning. 3) same as 2), but delete all keys and restart the DB and check again.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
We could refactor out a helper function here.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
This doesn't need to be declared outside the loop (it can be final at the assignment).
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
In line 58 above, in `recordLatency()`, we need to mention that `If the passed sensor includes throughput metrics (e.g. it is create via the XX function), the throughput metrics will also be recorded as a single occurrence of this event.`
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
why do we make lines longer? harder to read now
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
To keep this logic same as one in `Call::fail` method, lets set the new time as: > nextAllowedTryMs = now + retryBackoffMs
@hachikuji Yup, you're right -- I missed the fact that the overridden `needRejoin` takes into account the data in `subscriptions` and was only accounting for the base class's `rejoinNeeded` variable. Makes sense now.
Does this always work if you're the leader during a rejoin? The flag gets reset by `JoinGroupResponseHandler` but the leader will still need to complete the sync group phase. These two steps are combined, but using `ConsumerNetworkClient.poll(RequestFuture)` to wait for the entire operation to run can invoke the internal `ConsumerNetworkClient.poll(timeout)` multiple times, which in turn runs delayed tasks. So with bad luck with timing I think you may execute the delayed task between the two requests and this `needsRejoin()` check won't work.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Although the checkstyle rules currently do not enforce curly brackets in if/else blocks that contain a single statement, because this statement here spans multiple lines, I feel it'd be best to enclose it within `{ }`, even if that's optional.
nit: `,` (comma) doesn't seem required in the sentence.
This change is a good suggestion.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
This seems to always enforce a materialization, but I think we should materialize only if we need to.
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
nit: avoid unnecessary `this.` prefix
We should improve this test by moving this line outside/before `try`
I think we need to insert a `fail` here to fix this test
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
This seems like it's a kind of weird restriction. I guess it'd be odd to use the same name for an internal topic and another, though if you know its going to be prefixed it might not be great to not be able to use that same name.
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
`setNeedsCommit` -> `{@link #setNeedsCommit}`
`needCommit` -> `needsCommit`
visibility could be restricted to be package-level (i.e remove keyword `protected`)
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
Would `isUnknown` be clearer? I find that boolean methods without any prefix feel a bit ambiguous when reading them.
It seems that we can just use one level of if/else.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
nit: braces unneeded
Is the `myCommittedToken == null` check unnecessary here since it can never be the case when there are extensions? I think we make sure of this since we only call `identifyExtensions()` when there is a token.
nit: style seems to be to not include braces when there is only one if or else statement
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
typo: moreq -> more
An error is intended to shut down the worker. So we should call `doneFuture#complete` here.
It seems like the race conditions for `process` won't turn into any errors. It maybe is better to be explicit about the race conditions and have locks as to ease future maintainability of this task (especially by other contributors)
There is a race condition here. `ExternalCommandWorker#process` is used by the `ProcessMonitor` thread, and also here in a different thread. Similarly, `controlChannel` is accessed by multiple threads and could be be used before it is initialized. I think `process` and `controlChannel` need to be initialized and accessed under a lock. I would suggest something like: 1. take lock 2. check if process is null. if so then EXIT 3. create ControlCommand 4. release lock 5. call executor#awaitTermination for 1 minute 6. if the executor did not finish all tasks, then take the lock again, invoke process#destroy, release lock, call awaitTermination again
Originally this was just intended as a sanity check. However, thinking about it more, it might be better to change it to a much longer period, or even get rid of it entirely. It should not be needed since there is a task `durationMs`, of course. That might make sense as a separate PR, since we should change all the workloads.
@cmccabe is right about the race condition I think. we should probably check that `controlChannel` is initialized here
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: missing `<p>` for new paragraph
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
We can use the `replace` overload that takes a `char`.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
I think it was my suggestion to add it. I was going off the pattern for tagged node metrics in Selector.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Yeah I think that makes sense here
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
nit: I would throw it as an error since I do not see any difference in severity compared with the execution exception. I think if you add the java timeout exception as the cause of the Kafka timeout exception, it will print also the stack trace of the Kafka timeout exception with `Caused by`. However, it will print the stack trace later in the logs. But that is OK, IMO, and we do that also in other places I guess. ```suggestion log.error("Could not remove static member {} from consumer group {} due to a timeout: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new TimeoutException(e.getMessage(), e); ```
nit: Just added some more information to the messages. ```suggestion log.error("Could not remove static member {} from consumer group {} due to: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new StreamsException("Could not remove static member {} from consumer group {} the following reason: ", e.getCause()); ```
Shouldn't this be `timeoutMs - (time.milliseconds() - startTimeMs)`? Also, it's not too big of a deal, but the checks for `Long.MAX_VALUE` seem like overkill.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
I considered this but I think that it is clearer when kept separated.
I checked again, and I think it's OK. Thanks.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
Let's capitalize the log statements for consistency. There are a few of these.
Since we have this logic of checking what the type of `t` is in multiple places, we could consolidate this logic within `call.retry()`, since we are already passing the throwable instance. We could then also rename the method to be a bit more encompassing, like `attemptToRetry` or something.
Right, if retries are exhausted and it's a retriable exception, then it seems like it should be a TimeoutException.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
In addition to passing through the timeout to `createRequest`, we also need to pass it through in `newClientRequest`. Only some of the APIs support an explicit request timeout. This appears to have been a pre-existing bug.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
`before or after`
nit. I think there is `.` missing `since 3.0[.] Use`
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Right, if retries are exhausted and it's a retriable exception, then it seems like it should be a TimeoutException.
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
You could also use `MemoryRecords.EMPTY` here (I think).
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
Let's rename `headers1` and `headers2` here too
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
Thanks. I prefer to avoid using `Atomic*` classes when we don't need atomicity guarantees. I introduced a `LongRef` class in the broker for this reason. I think the right solution long-term is probably to introduce a class to complement `Time` that caches the last returned value so that one can choose when to refresh the value (for performance reasons). This was suggested some time ago in a different PR. For this PR, as per your suggestion, I think using a private static class with two public fields is probably the way forward (since Java doesn't have tuples).
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
I think you can just use `partition < partitionsCount` instead of using `compareTo`. Similar in line 558.
Ack. By bad.
apply `final` wherever possible (also within method)
Why do we need to augment this function here? I.e. by the time the stream task is created, we should have populated the `sourceByTopics` map with the pattern matched topics already, so I'm not sure if the additional computational logic is needed? cc @bbejeck .
key -> topic
`Topic not found` sounds like as-if the topic was not found in the cluster -- however, what actually happened is that we received a record but the record's topic is unknown in the sub-topology. Similar to above, "deterministic" is not really easy to understand. I would also not phrase it as a question, but as a statement: ``` ... This may happen if different KafkaStreams instances of the same application execute different Topologies. Note that Topologies are only identical if all operators are added in the same order. ``` Or similar.
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
Just FYI, for KIP-360 I'm doing this check for both idempotent and transactional, since it triggers an epoch bump instead of a producerId reset. I'll just pull this call out to a shared code path, the rest of this method shouldn't need to change.
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
By the way, do you think it would be safer to return `Optional<Integer>` and keep the use of the sentinel confined to this class. Same thing for `lastAckedOffset`.
Hmm, we're using a raw type here and a few other places. This is discouraged (type checking is disabled in these cases). If we don't want to propagate the generics when we use the superclass, we should probably drop them.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
We do not need to have a separate `KTableForeach`, since it does not generate any new KTable object and hence no need for `view` etc. Instead we can just reuse `KStreamForeach` inside `KTableImpl`.
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
Not sure about this -- why not add two generics to the store, one for "wrapped" and one for "root" and keep this method that return the root type? I would also rename `inner()` -> `root()`
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
Still not used
`tp` is not used anymore.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
Aha! Thanks. Yeah, I'd be in favor of coding defensively here as well.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
When I suggested it, I thought we could do a bit better, maybe something like `(id: 5, www.example.com:9123)`, but maybe that's actually worse.
I was mostly trying to get rid of the word `Node` because it's a bit redundant when you look at the log messages.
I think the intent was to remove generation in the original PR.
this line can be merged. for example: ```java FindCoordinatorRequestData data = new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.rebalanceConfig.groupId); ```
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
I don't think we really need this function any more... we can just submit to the executor from the other function.
Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage). I am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the `Fetch` and `BeginQuorum` protocol and not from the state machine (Kafka Controller) itself. It is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
> I had considered this previously and decided to leave the fetches in purgatory while the election was in progress to prevent unnecessary retries since that is all the client can do while waiting for the outcome. On the other hand, some of the fetches in purgatory might be from other voters. It might be better to respond more quickly so that there are not any unnecessary election delays. I'd suggest we open a separate issue to consider this. Sounds good to create a Jira for this.
Minor typo "will is"
nit: "name" => "joinThisName"
I see. Could we do something like this: first assign the partitions for all internal topics as the writing topology's number of tasks, i.e.: ``` // for all internal source topics, // first set the number of partitions to the maximum of the depending sub-topologies source topics for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) { Set<String> internalTopics = entry.getValue().interSourceTopics; for (String internalTopic : internalTopics) { Set<TaskId> tasks = internalSourceTopicToTaskIds.get(internalTopic); if (tasks == null) { int numPartitions = -1; for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> other : topicGroups.entrySet()) { Set<String> otherSinkTopics = other.getValue().sinkTopics; if (otherSinkTopics.contains(internalTopic)) { for (String topic : other.getValue().sourceTopics) { List<PartitionInfo> infos = metadata.partitionsForTopic(topic); if (infos != null && infos.size() > numPartitions) numPartitions = infos.size(); } } } internalSourceTopicToTaskIds.put(internalTopic, Collections.singleton(new TaskId(entry.getKey(), numPartitions))); } } } ``` And then update the Cluster metadata with `Cluster.withPartitions`, then in the `ensureCopartitioning` call, if after the first for-loop, `numPartitions` is still -1, it means all topics in this co-partition group are internal topics, and then in this case read from `metadata.partitionsForTopic` and took the maximum among all of them; and then later after calling `prepareTopic`, the metadata will be updated again with `metadata.withPartitions()`.
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
Also, instead of adding an extra operator node, I'd suggest we just do the checking within the operators themselves to reduce virtual function call overheads, for example see `KStreamKTableLeftJoinProcessor.process`.
It seems like this test would be more useful just as an assertion of the default behavior.
```suggestion public void should createChangelogByDefault() { ```
```suggestion assertThat(CLUSTER.getAllTopicsInCluster(), contains(changeLog)); ``` And we can remove line 426, `final Properties config = CLUSTER.getLogConfig(changeLog);`, since it would be unused.
`WithOverlappingKeys` (or `WithSharedKeys`)
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
This doesn't need to be declared outside the loop (it can be final at the assignment).
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
nit: `,` (comma) doesn't seem required in the sentence.
Although the checkstyle rules currently do not enforce curly brackets in if/else blocks that contain a single statement, because this statement here spans multiple lines, I feel it'd be best to enclose it within `{ }`, even if that's optional.
This change is a good suggestion.
I'd rather not use exception handling for branching. I think I'd prefer to address this case with an earlier check for `leaderUrl() == null` and instantiate a more informative exception to pass to `cb.onCompletion`
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
I don't know what has changed here.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
nit: This block is repeated in many tests. I wonder if we could define an helper for it.
nit: We could also define a helper method for this one to avoid the code repetition.
nit: we could just build the expected result as a whole set and compare
nit: add `final`
This test is highly overlapping with `testOldBrokerAbortTransaction`, could be good to refactor out a common flow.
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
I believe this is fixed in my next PR.
Nit: add `final`
nit: `maybeDeleteInternalTopics` and `maybeReset...`
This might be instable in Jenkins.
We could refactor out a helper function here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Not sure what has changed here.
Nit: we should probably include a little more detail. Maybe something like: ```"Found invalid wrapper offset in compressed v1 message set, wrapper offset '" + wrapper offset + "' is less than last inner record offset '" +lastOffsetFromWrapper + "'and it is not zero."```
Should be `post 0.10.0 Java clients`, I guess.
Nit: instead of `older`, maybe we should say `certain versions of librdkafka`. The Java client never does this for `v1` as far as I know.
> Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right Yes. > I think we can skip the call if consumedOffsetsAndMetadataPerTask is empty. For non-eos, yes, because for non-eos `commitOffsetsOrTransaction()` would only commit offsets via the consumer (this can be skipped if empty). However, for eos (alpha and beta), we might have a pending transaction that we need to commit on the producer, too.
Yeah I think if the actual `consumer.commit` call failed, then we should not trigger postCommit for any one. As for `postCommit`, I think it should never fail (we swallow the IO exception happened, because for non-EOS it is just fine, for EOS we would bootstrap from scratch).
Sounds good, in that case the nested try-catch would be necessary.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
Similarly here, this state check could be internalized.
A better way is to first call `this.mockTime.milliseconds()`, then `this.mockTime.sleep(1000)` then call the `milliseconds` again with the second batch.
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
Ah got it, my bad :)
Stream instance "one" -> "two"
Could you please add some line breaks? This and some of the other verifications are too long.
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
Ack, I get it now. Thanks for clarifying.
nit: might be better to name it windowedKTable
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
This should not have the `sink` suffix
Calling `maybe_start_jmx_tool` after each line that gets read from the producer process doesn't seem quite right. I think we want the behavior to be: - start producer "asynchronously" - start jmx tool asynchronously - wait for producer to finish - process each line of producer output I think it would look something like: ``` cmd = "same_as_before &" # now the cmd is "async" node.account.ssh(cmd) wait_until(producer is alive) self.start_jmx_tool(node) wait_until(producer is finished) for line in node.account.ssh_capture("cat /mnt/producer-performance.log"): # same as the previous for loop ```
Maybe this line and the one below could be moved into `start_jmx_tool`? If we go that far, we could do the locking in there as well.
A docstring for this method would be good :)
These two statements don't have the same semantics. Previously we would block until the first line of output, which here probably also guarantees metrics are being reported via JMX. From what I can tell here, it looks like you could easily be running `start_jmx_tool` before the metrics are ready. I don't think the previous version is great, but I think it at least provides the guarantee since we're piping logs elsewhere -- one line of output should indicate consumption has started.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
It does seem like we could pass the node id to `RequestSend` without much issue.
This is not required as contained in the check next line.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
nit: line 365 above can be moved down now since it is only needed before line 379.
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
Another tab here that should be replaced.
Also we can just pass in the `StringBuilder` as an argument rather than create a new one here
perhaps a better name is `getConfigKeyRst()`
these 2 lines shouldn't be here
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
@spena just ping to make sure you get this on the follow-up PR.
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
cc @mjsax as well, LMK WDYT.
nit: add `final`
nit: add `final`
nit: add `final`
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
This is still not used
No longer used.
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
Gotcha. Okay, two points: 1) It may be overkill, but you may want to consider separating the input (source, resource, etc.) files from the output (compiled class, JAR) files. It'd more closely mirror the build setup that's commonly used for Java projects and would probably make this code easier to modify in the future. 2) This bug initially surfaced with use of the `ServiceLoader` mechanism; it'd be great if we could have a test that verifies that the changes here fix how that works in isolated plugins.
Is it guaranteed that the contents of `errors` will only actually be compilation errors and not, e.g., warnings? Might be worth just using the return value of [call()](https://docs.oracle.com/javase/8/docs/api/javax/tools/JavaCompiler.CompilationTask.html#call--) to determine if compilation succeeded.
If these aren't expected to change at runtime, I'd suggest initializing the `HashMap` in the static block as a temporary variable , then wrapping it to create an unmodifiable map which is then used for the constant here: ```java Map<String, File> pluginJars = new HashMap<>(); try { pluginJars.put(..., ...); // More of the same } finally { PLUGIN_JARS = Collections.unmodifiableMap(pluginJars); } ```
Thanks. Not a blocker.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
Thanks! Will push this shortly.
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
paused -> running
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Yes, we can open a JIRA to do it later.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
nit: We should use `groupId.idValue` here and in the others.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
Sorry, you're right. The thing being tested should be second.
generally in assertEquals, the thing being tested comes second. If there is an error message, the first thing appears as the "expected value"
Good question. I can't think of any right now (maybe someone else can?) so I would lean towards keeping it package-private for now.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
a standby task is never in
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nevermind, I see that's the pattern we follow everywhere else
nit: `log.debug("Deleted check point file upon resuming with EOS enabled");`
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
`expectedTimestamp` parameter missing (insert before `expectedHeaders` to align with method name.
nit: missing comma `headers[,]`
`timestamp` missing (twice)
`timestamp` missing (three times)
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Yeah I think that makes sense here
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
nit: I would throw it as an error since I do not see any difference in severity compared with the execution exception. I think if you add the java timeout exception as the cause of the Kafka timeout exception, it will print also the stack trace of the Kafka timeout exception with `Caused by`. However, it will print the stack trace later in the logs. But that is OK, IMO, and we do that also in other places I guess. ```suggestion log.error("Could not remove static member {} from consumer group {} due to a timeout: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new TimeoutException(e.getMessage(), e); ```
I'm not sure how `removeMembersFromConsumerGroup` would behave if you passed in `""` as the `group.instance.id`, do you know? If not then let's just be safe and check what `streamThread.getGroupInstanceID()` returns, and skip this call if there is no group.instance.id (ie if not static)
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
i.e., add `fail` after this line
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
Thanks for the explanation. A bit subtle as you had said. :)
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Use log object if this is to be kept
consumerThread -> rebalancingConsumerThread
Since condition is just a comparison, you can put the comparison here directly
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
+1, we should just throw in this case.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
```suggestion "restoring by partitions map contained {}, and the restored partitions set contained {}", restoringByPartition, restoredPartitions); ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
nit: log an error and include the `inputRecordTimestamp`
nit: `fall between 0 < inputRecordTimestamp` -> `fall between 0 <= inputRecordTimestamp`
nit: log an error and include the relevant info (eg `windowStart` and `inputRecordTimestamp` at least). Same for the IllegalStateException in `processEarly`
Oh good point, we definitely need the key. But I think separating them turned out well
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
Also discussed offline with Becket, but this test can probably be simplified significantly
Still not used
`tp` is not used anymore.
Do you mean it should NOT be included...
This is still not used
nit: I think there's no need to reference the JIRA. The explanation seems clear enough without any additional context.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Same here and below
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Why don't we need this check anymore? It's still done for `globalThread`.
This was originally to try and prevent a dead-lock, i.e, the `UncaughtExceptionHandler` is triggered by Thread-1. The user calls close (still on `Thread-1`). Thread-1 join will never return as we are executing on `Thread-1`, but we already know it isn't running. I think we still need this check
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
add check for restore-consumer and admitclient
as above -- add check for two missing clients
I think, we should use three different values to make sure that the different prefixes overwrite the configs for the corresponding clients. Looking into the test below, they seem to be redundant with this one? We can also remove this test and keep the other three (that would avoid redundancy, too)
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
rewrite test as above using `assertThrows()`.
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
We could get away with a single `*`
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
can you please also check that the partition id gets set to -1
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
Updated this when merging.
req: This is unnecessary
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
add `final` twice
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
It's a little unclear as to how we are removing things from the cache here. If I understand correctly, we stick an empty Optional in the cache to represent removal. And here by _not_ handling the empty optional case, we exclude the broker from the new image. But if that's the case, why do we need the Optional in the first place? Can we directly remove the broker from "changedBrokers" when handling UnregisterBrokerRecord? This question applies to some other Delta classes that are using Optional as well
Also, are we fine with the config logging being bumped up to `info` (which is what `logAll` does) vs `debug` (which is what it was here).
It's probably worth while to mention that this method starts the task for a source connector with older behavior (without exactly once support).
if we keep ending up with this pattern, it might be clearer to create a `Listener` implementation that delegates to a list of listeners instead of chaining them manually this way
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
this state is missing from the KIP, it should be added
Why is `false` (inexact decimals) the default for the no-arg constructor? If this is an attempt to maintain backward compatibility, we should consider whether this bug, when fixed, compatible. Seems like it would be, since having the deserializer use the trailing zeros would be fine/better than not using them.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
I recommend to add special handling for JsonParseException - just log it instead of rethrowing. If such an exception is not handled properly, consuming may be blocked with any non-json message - just text, for example. I got this while playing with Kafka locally: just one simple "dummy" message from console client brought tons of exceptions to my log.
Consider renaming to `safeToDropTombstones`: ```suggestion boolean safeToDropTombstones() { ```
can we change the type definition of these 2 to be `Admin`? Then we don't need the cast
nit: indentation is 2 tabs (total 8 spaces) and its best to match the similar method below.
nit: extra space, ditto below.
nit: needs a comma after the `{@link ...}`
nit: fix indention (same below in other constructor)
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
same here. let's make all method params as `final`
nit: line too long
nit: final on params here and methods below.
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Sensor names don't appear in JMX.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
req: The names of this method and the previous method should be switched.
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
Make all params `final`
And again with `final` if you don't mind
remove this line -- not required.
as above nit: double space `to Kafka`
Should not have an implementation but call overloaded method.
Nit: should the method be named `testOptionsDoesNotIncludeWadlOutput()` instead? The point of this PR is to prevent including WADL in the OPTIONS output, but the existing method name makes it seem like we're testing the content of the WADL output.
is there a reason for using protected here? seems like it could be package private or private
Could also use `Collections.singletonList`, which would also make this immutable
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
Seems this duplicates `L733`. Might be good to extract into a small helper method.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
IMO, `createStreamThread()` would describe the behavior better.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Thinking a bit more. This is a bit tricky since we probably can't just continue here. For channels like SSL, we need to do the handshake after the socket is connected. Currently, the handshake will be triggered immediately after the connection is established through channel.prepare() and this has to be done in the same poll(). Otherwise, the selector may not be able to select the key again for initiating the SSL handshake. This applies to all those immediately connected keys not coming from the selector. So, to get around this issue. We can probably create a new KeyIterator that iterates both keys in this.nioSelector.selectedKeys() and those in connectableChannels. The iterator can return a <key, alreadyConnected> tuple. For keys coming from selectedKeys(), alreadyConnected will be false. For keys from connectableChannels, alreadyConnected will be true. Then, we just need to change line 291 to check if (key.isConnectable()) || alreadyConnected) and leave the rest of the code as it is. This way, keys in connectableChannels will be handled in the same as way as those from selectedKeys().
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Not sure about this. `SslTransportLayer#hasBytesBuffered` returns true if there is any data in `netReadBuffer`. If more data is needed to unwrap and no data arrives from the client, I think the handling of `keysWithBytesBuffered` results in a tight polling loop with timeout=0.
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
nit: no need newline of 104 below.
This method is not `synchronized`. So there could be a race condition here. I think it should be: ``` final NamedCache cache = getOrCreateCache(namespace); final LRUCacheEntry result = cache.putIfAbsent(Bytes.wrap(key), value); maybeEvict(namespace); return result; ```
This line is failing checkstyle.
nit: Provide a message to the `IllegalStateException` constructor
nit: Provide a message to the IllegalStateException constructor
We can use `computeIfAbsent(...)` to eliminate the prior newly-added line: ```suggestion allAddedPlugins.computeIfAbsent(pluginClassName, n -> new ArrayList<>()).add(plugin); ```
Mmmm, I'm not sure we should be making decisions here based on dynamic plugin loading for two reasons: 1. This change can be backported to older versions of Connect, which will never have that feature. 2. It's unclear exactly what the mechanism for dynamic plugin loading will be, and it's possible that a re-scan of all known plugins after loading has taken place (either the initial start load or a subsequent dynamic load at runtime) could still be beneficial Also, it's actually not that uncommon for 3+ copies of the same plugin to appear on the plugin path for a worker. For example, some connectors come packaged directly with converters; all it takes is at least two such connectors and a separately-installed copy of that converter to lead to that number of copies, without any error or misconfiguration on the part of the user.
Oh, gotcha--in that case, should we do a check somewhere else, since this will be triggered potentially multiple times for a single plugin? For example, if there are three copies of a connector, the warning will be logged twice right now, with different values for `inner.keySet()` each time. Also, it may also help to log exactly which one we're going to use either instead of or in addition to the complete set of discovered versions of the duplicated plugin.
Offline you suggested that we change this to warn, based on @C0urante's earlier observation that sometimes the same converter might be included by multiple plugins. This latter isn't an issue if it's the same converter version in all of them. So +1 to change this to warn: ```suggestion log.warn("Detected multiple plugins contain '{}'; using plugin {} and ignoring {} plugins ({}). " ```
Since this is an error, it probably would be good to clarify the error message and provide an action. WDYT about: ```suggestion log.error("Detected multiple plugins contain '{}'; using plugin {} and ignoring {} plugins ({}). " + "Check the installation and remove duplicate plugins from all workers.", pluginClassName, usedPluginDesc, ignoredPlugins.size(), ignoredPlugins); ```
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
Ah got it, my bad :)
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
same question around log level as above
same question around log level as above
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
This shouldn't be possible, right? It wouldn't make much sense to put a topic in the result if it didn't have a corresponding `TopicListing`.
All the `null` checks at each layer of the call stack make me think that particular issue might be better handled with an exception. Not critical since this is all internal code, but seems like then we'd only need to check version compatibility in one or two places.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
add `final` twice
same here. let's make all method params as `final`
This data construction seems better to be put in the `AlterConfigsResponse` constructor.
Yea, my suggestion would be to reuse the existing constructor as the construction of the `AlterConfigsResponseData` seems non trivial for a caller to do, compared with passing a map of errors.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Let's keep this as a private method.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
I think we might want to consider dropping some of these `log.debug`s to `log.trace`. Some of the logs in error conditions make sense at `debug`, but logging every fetch request and response at `debug` might make changing from `info` to `debug` a bit overwhelming.
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
Currently, for controller initiated ISR change (controlled shutdown or hard failure), we always bump up the leader epoch. Also, the name alwaysBumpLeaderEpoch is a bit weird since the code in handleNodeDeactivated() doesn't directly bump up leader epoch.
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
Nice, thanks for the update. Looks good
```suggestion public void shouldInstantiateAssignor() { ```
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Not sure this is what people will generally mean by lag -- while the committed offset matters, normally if the consumer is in the process requesting the lag I think it'd mean the FetchRequest lag, i.e. how far behind *processing* the records is the consumer in comparison to what the broker indicates is the most recent offset. in other words, I think i'd update this at the end of each `put()` and change from `committedOffsets` to `processedOffsets`.
Hmm, good question. I may have actually been wrong about which values should be involved. I think @gwenshap and I had a long discussion about this awhile ago too and there are many ways you could define lag. I think the real problem here is that we may not be exposing enough information from the consumer to compute what I would really think of as lag -- FetchRequests include high watermark info so you know how many records are in the log but not yet returned to you, and the consumer creates metrics based on that. But we don't have access to that info. A connector that commits on every message would look like it has 0 lag, but it could be very far behind in the topic.
same question as other pr -- this is `sink-task-metrics` instead of `sink-tasks-metrics` in the KIP
sorry, that was unclear. i meant put the `Math.max` inside the loop and check `diff` instead of the aggregated `activeRecords`. you shouldn't get negative numbers, but if a connector does something wonky with the offset commits, it could happen that committed offsets are ahead of consumed offsets.
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
Where do you cleanup the childrenSensors object? Otherwise we will maintain a reference to the Sensor objects always.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
We usually avoid the get prefix in cases like this
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
I see @mjsax has already a PR for that, great!
This would unfortunately break the python script, but it's ok i can fix that. Thanks.
As i said above, we should `requireNonNull(mapper, ...)`
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
nit: ditto here, `cluster()` will reconstruct a new object on each call.
line/sentence formatting `{@code null}`.
nit: parameter/line formatting
nit: single parameter per line
+1 to @vvcephei's suggestion here.
Was the intention here to avoid the deprecation warning? If so, you can just call this method `name()` and do this: ```java @Override @SuppressWarnings("deprecation") // TODO remove this when {@link Joined#name} is removed public String name() { return name; } ``` Callers won't see the deprecation warning as long as they access the method via a `JoinedInternal` and not a `Joined`.
Not for this patch, but we should do a KIP to add support for batch topic creation.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a *. {@link org.apache.kafka.connect.errors.RetriableException} * before retrying again; must be 0 or more ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
req: Lag should be a long
Actually, do we even need this at all? It seems like we get everything we need from `statefulTasksToRankedClients` -- it should have all tasks (and clients), and either a) the previous client was caught-up, in which case it should be the first rank and we can determine it was the previous host from a lag of `Task.LATEST_OFFSET`, or b) the previous client was not caught-up, in which case we don't really care what the previous host was for that task
Hmm...I'm wondering if `Map<String, List<TaskId>> previousAssignment` is sufficient to pass in, won't we lose all the tasks that were assigned to a client that no longer exists for whatever reason? Maybe we should just pass in `Map<TaskId, String> tasksToPreviousClients` (aka `tasksToHostClients`) directly. We can build that map up during other steps in `assign`
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
nit: we're missing a period and space following the partition. I can fix this when merging.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
`KeyValueStore` -> `TimestampedKeyValueStore`
nit: remove empty link
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Let's be consistent to name it throwable
it might be a bit cleaner to invert this if statement to write it like this: ```java if (this.retryContext.tries() <= maxRetries) { log.debug("{} failed: {}. Beginning retry #{}", this, prettyPrintException(throwable), retryContext.getTries()); runnable.call(this, now); } else { failWithTimeout(now, throwable); } ```
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
It's probably more of an issue now, but I think we may have already not been entirely thread safe with the methods we can call when starting connectors/tasks. In particular, `reconfigureConnectorTasksWithRetry` can invoke `addRequest`, and `addRequest` expects to already be locked but `reconfigureConnectorTasksWithRetry` doesn't guarantee that. I'm guessing we've missed that issue until now because that happens rarely and I think the only thing that could conflict with it would be external HTTP requests. I think the only other piece that needs to be protected as fallout from this parallelization is the step where we call `configBackingStore.putTaskConfigs` in `reconfigureConnector`. The backing store is not thread safe (and neither is its underlying `KafkaBasedLog`).
They are using the same underlying enum, but `TaskStatus.State.DESTROYED` would probably be clearer here.
Aha, good point.
nit: This check seems superfluous (and is inconsistent with the collection of connector callables).
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
Nit `.` at the end
nit: both lines missing . at end
nit: single parameter per line
`keySerde` -> `valueSerde`
nit: parameter/line formatting
@junrao We are relying on fall-through for the second case, so it's not straightforward to make that change.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
requestHeader.apiKey() can just be apiKey.
`apiKey` is of type `ApiKeys` while `requestHeader.apiKey()` returns a `short`.
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
We lack unit test coverage for this case
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Thanks, looks good. Yes, it's O(1), but a lot less efficient than returning a local variable. Take a look. :) ```java public V get(Object key) { Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1) & h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } else if (eh < 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) return e.val; } } return null; } ```
You could remove this `if(..)` block as `putIfAbsent` below will cover it
I did not find any `toString` in `RocksDBStore` so I added one in `Segment` (which can then expose the id, which is not something that `RocksDBStore` knows about).
Seems like it would be better to add a `toString` to the segment or superclass.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Docstring doesn't match the class
I think the split between the two classes is fine, just wanted to understand better what the division is. Re: varying single parameters, this is currently not possible in ducktape and unfortunately hard to work around, which is kind of annoying since it would be nice for cluster scheduling purposes to know the full set of services when you first instantiate the class but the need to vary parameters per-test requires deferring that process until you invoke the test method.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
nit: "which provide underlying producer and consumer clients for this {@code KafkaStream} instance".
`keySerde` -> `valueSerde`
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
Could store `entry.getKey()` in a local variable since it is used several times
Is there a case when updateQuota() can return false? Ditto for removeQuota().
Can this be a `byte`.
I was checking the code in the protobuf repository and it turns out that the optimised version of this code is pretty complex: https://github.com/google/protobuf/blob/master/java/core/src/main/java/com/google/protobuf/CodedInputStream.java#L2473 I think it makes sense to start with the simple implementation, but we should file a JIRA for considering the optimisations in their implementation.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
the message format => that message format
We could make this field access `public`
nit: we can use `map#compute` to replace getOrDefault + put.
Let's use `Map` on the left side instead of `HashMap`
We could remove this function
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Fine with me to keep the guard. Was just double checking.
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
As we expect a specific `groupId`, I would check the provided `groupIds`.
`mkProperties` could compactify this code, but it's not necessary; your call.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
We can use `ApiResult.completed()`
nit: We could revert this change as it does not bring much and re-align like it was before.
ditto about the log level (also for the below uses of `debug`)
In this case I think we should include some error details here. In particular, the last seen error for each topic. I'm worried about cases where we try to create but the create times out but is eventually successful. We'd return an error back, but the user would have no way to know that setup failed because an internal topic already exists.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
Hmm, normally `IllegalArgumentException` indicates that the argument to a function is bogus, right? That's not really the case here-- the function argument was fine, but the topic wasn't set up correctly. This can probably just be a generic `RuntimeException`, since we don't have a need to make it something fancier.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
We don't use LeaderNotAvailableException in listTopics
request "got" re-sent to the control
We don't use ReplicaNotAvailableException in listTopics
Good point, but it could be clearer. This implementation can be used in production, but the `PropertyFileLoginModule` that also ships with this reference implementation should NOT be used in production.
Incase => In the case
Use `File.separator` instead of `/`
Just some more nits. Can you add `final` wherever possible: `ClassLoader`, `String filename`, `BufferedReader`, `for(final String...)`, `Exception`
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
There is a race condition here. `ExternalCommandWorker#process` is used by the `ProcessMonitor` thread, and also here in a different thread. Similarly, `controlChannel` is accessed by multiple threads and could be be used before it is initialized. I think `process` and `controlChannel` need to be initialized and accessed under a lock. I would suggest something like: 1. take lock 2. check if process is null. if so then EXIT 3. create ControlCommand 4. release lock 5. call executor#awaitTermination for 1 minute 6. if the executor did not finish all tasks, then take the lock again, invoke process#destroy, release lock, call awaitTermination again
this overwrite of mm2config should go in the setup method, IMHO
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
Isn't MM2/Connect using at least once by default? ie, the producer in the runtime can cause duplicates.
As this does not change, I wonder if we could direct initialize `consumerProps` when it's declared
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
Thinking about this some more, not sure there's a lot of value in forcing users to set `idempotence=false` in cases where they're setting `acks=1|0` or `retries=0`. So, I'd change the warning to info for these cases. `max.in.flight.requests.per.connection` is different since it's an implementation constraint that `idempotence` doesn't work when it's > 5 vs inherent to the configuration. For this one, I'd have a warning and mention that it will become an error in Kafka 4.0.
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
prop: Could we pass into `getMovements()` the number of warm-up replicas and only compute as many movements as needed instead of computing all movements and then using just the first couple of movements.
Q: IIUC, we do not check if the movement is for free. That is, if the destination is a caught-up client. If it were we would not need to assign a warm-up replica and could consider one more movement. I am also fine with post-poning that to a follow-up PR.
This is a bit suspicious... If we're polling the queue, we should just loop until the queue is empty, not iterate over another another collection we happen to know has the same number of elements. More specifically, `poll` might return `null`, but `offer` throws an NPE if `client` is `null`.
```suggestion public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates, final Set<TaskId> allTasks, final Set<TaskId> statefulTasks, final AssignmentConfigs configs) { ```
Please avoid raw types, even when you don't need the type bound. ```suggestion for (final RankedClient<ID> rankedClient : rankedClients) { ```
nit: add `final`
I am just wondering: it seems that we don't have a check in place that `storeBuilder.name()` does not return `null` -- this would be bad and should not be allowed. Also, we should never put `null` as name, but generate a name. This check `isQueryable()` should be done in `GlobalKTableImpl#queryableStoreName()`.
Ack, I get it now. Thanks for clarifying.
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
Sure, but that led to the opposite problem, in which the enum was inconsistent with the state. In regard to position, I think we should handle this at transition time as mentioned below. If we ensure that position is not null in the fetching and validating states, then I don't see a problem changing `hasPosition` to check it directly.
Thanks for the explanation. A bit subtle as you had said. :)
Should we also check that ``` final long start = SessionKeySerde.extractStart(bytes.get()); final long end = SessionKeySerde.extractEnd(bytes.get()); return end >= from && start <= to; ``` Although the current impl of RocksDBWIndowStore naturally checked that for us, it does not guarantee all underlying store impls guarantee that.
@guozhangwang @dguy we cannot guarantee that all the entries for one key will necessarily precede the entries for the next key. The following code still fails with this patch, and only returns `0001` and `0003`, since the key for `("a", "0005")` will come after the key for `("aa", "0004")` ``` final RocksDBWindowStoreSupplier<String, String> supplier = new RocksDBWindowStoreSupplier<>( "window", 0x7a00000000000000L, 2, true, Serdes.String(), Serdes.String(), 0x7a00000000000000L, true, Collections.<String, String>emptyMap(), false); windowStore = supplier.get(); windowStore.init(context, windowStore); windowStore.put("a", "0001", 0); windowStore.put("aa", "0002", 0); windowStore.put("a", "0003", 1); windowStore.put("aa", "0004", 1); windowStore.put("a", "0005", 0x7a00000000000000L - 1); final List expected = Utils.mkList("0001", "0003", "0005"); assertThat(toList(windowStore.fetch("a", 0, Long.MAX_VALUE)), equalTo(expected)); ```
Note that `WindowStore.fetch()` should return `WindowStoreIterator` where the key is `Long` indicating timestamp and `value` is the value.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
Should this ever happen? If it does happen should we consider it a bug? Ditto for the other `hasNextCondition`.
Nit: param alignment.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
We lack unit test coverage for this case
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Thanks for the follow-up.
Ditto here for different exception types.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
only one parameter should be `null` -- otherwise it's unclear what this test actually does
If we call `getWindowStore`, it will give us a null because of type mismatch (expected window store, actually timestamped key-value store)
Is there any extra benefit in the large input tests? If not, maybe drop the large input tests and rename the first two tests.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
The map is not used.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
I asked you exactly that a few months ago :) You referenced some old PR but basically the takeaway was, a restoring task hasn't initialized anything but its state, therefore needs to close only the state manager
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
I think we need to insert a `fail` here to fix this test
req: This is unnecessary
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
Seems this `fail` did not work as expected? Otherwise the test would have failed all the time? Maybe we should rather set a boolean flag that we evaluate outside of the callback to let the test fail? Also, we have one run with zero exceptions and one run with 2 exception (one exception type each) -- not 4. Thus, we need to handle this differently for the error-injection and the "clean run" differently depending on the boolean test flag.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: it is naming a source node, not a processor node. -> `"source"`
nit: move `topology.globalStateStores(),` to next line.
nit: insert space `String... expected`
This method is not `synchronized`. So there could be a race condition here. I think it should be: ``` final NamedCache cache = getOrCreateCache(namespace); final LRUCacheEntry result = cache.putIfAbsent(Bytes.wrap(key), value); maybeEvict(namespace); return result; ```
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
If not, we should move the exception capturing logic inside the dbAccessor as well.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
Overall LGTM. But can we format it differently? We should start a new line for each sentence. If we update the docs later, it make the diff simpler to read.
extra space after `*` needs to be removed
nit: needs a comma after the `{@link ...}`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: missing `<p>` for new paragraph
nit: should we have a newline for each partition? Otherwise that ling maybe too long.
I wonder if we want to print the actual list of partitions here, which might be long. And do it twice. I see the same pattern is applied elsewhere. I understand the value of explicit listing.
I was thinking it was odd that this wasn't reusing `flushAndCommitOffsets` since they are basically the same. But I see the exception handling is different, so that makes sense. But then I noticed there's no call to `onCommitCompleted` if there's an exception during the flush, which we do for every other path during commits. _Then_ I realized that this is actually for the special case of closing, and that the callbacks that are invoked by `commitOffsets` in this special case are weird since the seqno will never match and it is probably always getting logged as an "error" at debug level. This isn't critical since it's just at debug level, but should we have `onCommitCompleted` check for the sentinel value and ignore the callback in that case? But also, I think the seqno handling is kind of unnecessary now. I think this is a holdover from possibly 2 separate things. First, we previously handled offset commit differently with different threads. Second, the semantics of offset commit in the new consumer were very unclear at the time this code was original developed (that was back when it wasn't even fully implemented and I was trying to sort out what semantics we wanted, so ended up being somewhat defensive in this code). I believe it is the case now that you cannot have callbacks for offset commit come back out of order (despite the fact that they are async) and that since we guard offset commits with a check on whether we are currently committing, we can only end up with multiple because we explicitly allow commits to expire to allow a newer one to be submitted. But will this ever actually help? Because of the way the protocol works, won't the offset commits just get queued up anyway? Would it make sense to adjust this so we have to wait for the commit to finish regardless, but that we might do something like pause processing if it takes too long? Basically, since we don't do it synchronously, we allow processing to continue since it's nice to not have to stall during the commit, but at some point if we couldn't commit, we shouldn't just cancel that commit, we should wait until it actually completes. (And, of course, we also need better handling if the commits repeatedly fail.)
It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this: ```java try { producer.beginTransaction() producer.send(...) producer.sendOffsetsToTransaction(...) producer.commitTransaction() } catch (Exception ) { producer.abortTransaction() } ```
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
In the ZK based code, we also take live brokers into consideration when selecting a new leader.
We need to choose at least a live replica.
Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case.
Yes, the alterIsr doesn't change leader, but generates a PartitionChangeRecord. On replaying this record, the code following code bumps on leaderEpoch? ` PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);`
This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: remove (was tested already)
not used: can be removed
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Similar to below. Maybe `testManualAssignmentChangeWithAutoOffsetCommitEnabled` is a more descriptive name.
request "got" re-sent to the control
Should be larger
Typo: should be "or larger than the number of available brokers"
Please include TopicDeletionDisabledException here.
This exception can't be thrown by DeleteTopics.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
nit: add `final`
nit: add `final`
req: Since we do not need to validate `valueTransformer`, could you please remove it from the `verify()`.
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
We can use `new ProducerRecord<>(topic, "value");` to simplify it a tiny bit
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
`orderInGroup` param is duplicated for key & value converter
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
Might be overkill if this is the only use case, but we could also add a composite validator.
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
Don't insist -- would prefer though (for this case) -- in general, this pattern can be useful -- just don't see it for this particular case.
This should not have the `sink` suffix
This part and the line 525-529 below can be extracted out of if condition.
nit: avoid unnecessary `this.` prefix
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
Ah got it, my bad :)
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
Currently in this file the indentation style used is: ```java protected boolean maybeAddConfigErrors(ConfigInfos config, Callback<Created<ConnectorInfo>> callback) { ``` Still, once we move to single arguments per line it should be: ```java protected boolean maybeAddConfigErrors( ConfigInfos config, Callback<Created<ConnectorInfo>> callback ) { ``` I'd pick one of these. (First I confused `callback` for a local variable)
Method should not be `final`. Additionally the `final` keyword for method arguments and local variables is not required and does not improve readability of the code here. Indeed Java does not distinguish between readonly and read-write variables. But unless an anonymous class is declared (this requirement is removed after Java 8) or the variable is used further down in the code (improved readability) marking every single readonly variable as final does not make things better IMHO.
nit: config is an overloaded term in the code, you might prefer to name this argument configInfos for instance.
No `final` needed. Also using names `configInfo` and `configInfos` respectively removes the need for abbreviations (here `cfg`).
nit: `err` is not used elsewhere. You may choose to iterate directly over the collection that is returned.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
placeholder may not be required for exception
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
The extra parameter seems a little more annoying than the effort it saves (which is really just checking a couple flags). If we really wanted to avoid the redundant checks, maybe a better way is to add a private `doAutoCommitAsync` or something like that, which simply sends the request assuming the coordinator is known and autocommit is enabled. Then the two `maybeAutoCommit` calls could delegate to `doAutoCommitAsync` after doing the checks themselves.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
nit: final and also below
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
Should we call close in the `finally` block? Here and elsewhere
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
Ah got it, my bad :)
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
Same for all such calls.
We lack unit test coverage for this case
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
It was removed from the other versions of `group` but not from here.
unnecessary type in constructor, can use `new HashMap<>()`
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I don't think we need this `null` check either.
Nit: "The file channel position..."
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
I don't think we need the `null` checks.
We should include `startPosition` in the message.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
We should mention somewhere that users should prefer this new assignor for newer clusters.
We should limit this suppression to the method for which we really need it instead of the whole class
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
this line can be merged. for example: ```java FindCoordinatorRequestData data = new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.rebalanceConfig.groupId); ```
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
We can do it separately if you like.
Yes, we can open a JIRA to do it later.
Well I was thinking two threads could be two threads, one calling `consumer.groupMetadata` and one calling `consumer.poll` which is creating a new object, but on a second thought this race condition maybe fine anyways since we cannot guarantee time-ordering if it happens.
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
Thanks for doing this. I also noticed it was missing and fixed it in this PR: https://github.com/apache/kafka/pull/10085/files#diff-1da15c51e641ea46ea5c86201ab8f21cfee9e7c575102a39c7bae0d5ffd7de39R134-R137 Maybe reconcile the two changes and we can merge this one.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
I think `will go` should simply be `go`.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
Should not have an implementation but call overloaded method.
an -> a
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
Please remove empty lines here and in the other test methods.
You usually do not want to mock the class under test, because you want to test it. Also partial mocks should only be used if absolutely necessary. A rule of thumb is if a partial mock is needed then most probably the design has a flaw. In this specific case, you should mock RocksDB's iterator. For the class under test, you should test `hasNext()`, `next()`, `peekNextKey()` and `close()`, because those are the one exposed (`makeNext()` should actually de declared as `protected`, IMO). ```suggestion final String key1 = "a"; final String key2 = "b"; final String key3 = "c"; final String key4 = "d"; final String value = "value"; final Bytes key1Bytes = Bytes.wrap(key1.getBytes()); final Bytes key2Bytes = Bytes.wrap(key2.getBytes()); final Bytes key3Bytes = Bytes.wrap(key3.getBytes()); final Bytes key4Bytes = Bytes.wrap(key4.getBytes()); final byte[] valueBytes = value.getBytes(); final RocksIterator rocksIterator = mock(RocksIterator.class); rocksIterator.seek(key1Bytes.get()); expect(rocksIterator.isValid()) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(false); expect(rocksIterator.key()) .andReturn(key1Bytes.get()) .andReturn(key2Bytes.get()) .andReturn(key3Bytes.get()) .andReturn(key4Bytes.get()); expect(rocksIterator.value()).andReturn(valueBytes).times(4); rocksIterator.next(); expectLastCall().times(4); replay(rocksIterator); final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( STORE_NAME, rocksIterator, Collections.emptySet(), key1Bytes, key4Bytes, true ); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key1Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key2Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key3Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key4Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(false)); verify(rocksIterator); ```
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
same here -- sounds like CachingKeyValue with TimestampStore
I'd suggest rename `ProcessorName` to `GraphName` to be consistent with the base `StreamGraphNode`, also to distinguish with the physical topology's `XXProcessorNode`. Ditto else classes.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
We can use the `replace` overload that takes a `char`.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
nit: `e` -> `fatal`
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Do we really need to print `super.toString`? Ditto above.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
nit: Provide a message to the `IllegalStateException` constructor
nit: Provide a message to the IllegalStateException constructor
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
nit: one parameter per line -> move `retryBackOffMs` to its own line
nit formatting. `cause` is indented correctly, it's weird that it align to the arguments above that are not parameters of `StreamsException` (like `cause`). Better: ``` throw new StreamsException( String.format( "Could not create topic %s, because brokers don't support configuration " + "replication.factor=-1. You can change the replication.factor config or " + "upgrade your brokers to version 2.4 or newer to avoid this error.", topicName ), cause ); ```
Should we still log it, perhaps as a warning? If I understand the background, this case is unexpected except with 0.10 brokers, so it seems like swallowing it could mask an important condition.
`seems existing already but it doesn't` -- this might be confusion. What about: ``` Could not create topic {}. Topic is properly marked for deletion (number of partitions is unknown). Will retry to create this topic in {} ms (to let broker finish async delete operation first). Error message was: {} ```
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
I wonder if we can just get a Map with the default size. I don't expect this code path to be very hot
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
+1 for consistency
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
nit: `--topic` and `--partition` could be extracted as helper static functions
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Since you've noticed that these don't depend on any instance state, do you think it makes sense to move them to a separate file (e.g. `ConnectorUtils`)? The `instantiate` function below would be another candidate.
I believe that including the name of the property in the error message is redundant as that information will be available already in the REST response. I also think we may want to be clearer about the error message here. Users can't supply null values, but developers (by specifying `null` as the default value for a property in a `ConfigDef`, for example) definitely can, and we may want to make it clear which variety we're prohibiting. What do you think about this? ```suggestion .map(configEntry -> new ConfigValueInfo(configEntry.getKey(), "The JSON literal `null` may not be used in connector configurations")) ```
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
Same thought w/r/t performing assertions on the complete set of returned plugins: ```suggestion Set<Class<?>> excludes = Stream.of( ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES, ConnectorPluginsResource.TRANSFORM_EXCLUDES ).flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of( SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS, CONVERTER_PLUGINS, HEADER_CONVERTER_PLUGINS, TRANSFORMATION_PLUGINS, PREDICATE_PLUGINS ).flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(false)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ```
```suggestion /** * Task ID of the task. * * @return task ID consisting of subtopology and partition ID */ ```
```suggestion /** * Source topic partitions of the task. * * @return source topic partitions */ ```
```suggestion * Metadata of a Kafka Streams client. ```
```suggestion /** * Metadata of a stream thread. */ ```
```suggestion /** * Offsets of the source topic partitions committed so far by the task. * * @return map from source topic partitions to committed offsets */ ```
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
I think it would be better to make this a static factory method and keep the constructor for the case where we receive `FetchResponseData`.
We usually avoid writing to standard out in test cases. A few more of these.
Please remove empty lines here and in the other test methods.
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
nit: add `{ }`
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
NVM, I realized it should never happen.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
`deteremined` => `determined`
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
nit: 'else' can be dropped
We may as well use the more specific type `TimeoutException` given the name of the method.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
return type is `void` -- remove this line
So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly: ``` log.error("Could not initialize partition time. Committed metadata is corrupted.", e); ```
Nevermind, I was thinking of `position += n`, but `limit` is actually the result of that (bounded by records.size).
Thinking about it, this may be the root of the problem. We are removing from the head of the ArrayList, one at a time. For each removal, we cause all the elements to be shifted in the underlying array, which is very inefficient if n is not small.
Also, `n >= records.size()`? Seems we could clear `this.records` and use this fast path if they are equal. Would also remove need for checking `iterator.hasNext()` below.
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
Sounds fine to me.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
```suggestion * Disable the changelog for this suppression's internal buffer. ```
nit: might be better to set end to another timestamp.
`windowSize` should be `Duration`
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
As above: need to keep default value.
`orderInGroup` param is duplicated for key & value converter
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
Might be overkill if this is the only use case, but we could also add a composite validator.
as above mentioned, the `listStore.all()` is not closed here.
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
nit: `final` params
Hmm, I'd just generate the randoms during set-up and add them to an array.
We don't want to be converting from int to string in the benchmark code.
Probably want to make this a `ConcurrentHashMap` or `synchronize` access to it. It can be modified etc from multiple threads
The state that is being updated is private data of `KafkaStreams`. It should be responsible for synchronizing access to its data, not external classes.
This is the callback from the `StreamThread`s so it will be called from multiple threads, i believe. See the inner class `StreamStateListener`
I had a similar thought first. But `StateListener.onChange()` is only called within `KafkaStreams#setState()` which is synchronized.
Hmmm... Even if we use `ConcurrentHashMap` I guess this would not be enough. In `StreamStateListener#onChange()` we check the state of each thread to compute the new state for KS -- this most be atomic. Using `ConcurrentHashMap` would not prevent a race condition there. So maybe `StreamStateListener#onChange()` should have `synchronized` keyword -- it a single instance we hand into into each thread.
This is added in #986 as well. The later patch to go in will have to rebase, or we can extract out the common code as a separate PR. I am fine with either way. FYI @becketqin.
Is this constructor used at all? If not, I'd not include it one should generally provide a message explaining more.
while this is fine, perhaps we should follow the same pattern for getters in these 3 new classes.
I'd also consider removing this one too if we are not using it.
typo `to to` (also missing `.` at the end of the sentence)
placeholder may not be required for exception
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
The extra parameter seems a little more annoying than the effort it saves (which is really just checking a couple flags). If we really wanted to avoid the redundant checks, maybe a better way is to add a private `doAutoCommitAsync` or something like that, which simply sends the request assuming the coordinator is known and autocommit is enabled. Then the two `maybeAutoCommit` calls could delegate to `doAutoCommitAsync` after doing the checks themselves.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
nit: we can use `map#compute` to replace getOrDefault + put.
nit: add a space before the `:`.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
Yeah, we're still feeling out the best patterns for handling older versions.
nit: extra line
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
That's right, changed it locally.
It doesn't seem that the client needs principalBuilder.
Checked with Jun and this is fine.
original was better
original was better
original was better
original was better
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: empty line.
We can remove the code block line 82-85 above since it will be called here.
nit: align parameters.
For compatibility, we cannot change any message format of old versions, so I think we need to bump up the version to 5 and only do compression at 5 while no compression at version 4. Think about the case: when a streams application with multiple instances are rolling bounce to be upgraded, maybe some instance are already on version 5 and hence sends the assignment back compressed while some other instance do not recognize this version at all. We need to distinguish the cases when to compress and when to not compress. @mjsax has done the version probing protocol and he can provide more.
I think the versions of subscriptionInfor / assignmentInfo are coupled, so if we bump up on one side we should do the same on the other.
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
Done now, thanks @ijuma
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
This should disappear with my suggestion.
Fair enough, let's just leave it as is then. Thanks for the explanation.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
Maybe: Include in the log the Connect key, value, and other details of records that resulted in errors and failures.
Cleaner to just check if `tasks.isEmpty` after the loop is over.
`TaskManager#tasks` has to be accessed through the state change thread. It can't be accessed here by incoming requests since there is no lock or synchronization. Probably the easiest thing to do is to have a CreateTasks runnable which does what you want.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
The next 2 methods aren't used in this PR. I guess they are used in one of the others...
nit: should be `named` can't be null
This should be new `ValueMapperWithKey`
nit: add `final`
as mentioned above: remove this overlaod
nit: add `final` (2x)
nit: remove empty line
nit: `child` -> `toChild`
nit: remove (was tested already)
nit: remove -- not used
Personally I think it makes sense to just disallow calling `ofTimeDifferenceAndGrace(...).grace(...)` entirely, this seems like abusing the API
Missing `.` after `since 3.0`
But grace and retention are two different things. In fact, I just had another conversation about this issue, and it seem we need to fix this by allowing people to specify a retention time IMHO. Not sure if we need to add a `Materialized` parameter or add `Joined#withRetention()` that we use to specify serdes etc.
We should call out explicitly that this is setting the grace period to 0, which means that out of order records arriving after the window end will be dropped. Otherwise it's too easy to just use this method without thinking any further about the grace period and what it means/whether you want it
Same as above mentioned, the validation didn't get handled in new API.
`This conversation was marked as resolved by wcarlson5` :)
Removed this, as it describes `CogroupedKStream` what is not appropritate here.
Nit: we don't need this tag before the parameter list.
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
There is no `CoGroupedStream#reduce()` -- we can remove this
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
I've been trying to follow this. Looking into `ConsumerCoordinator`, I think I see what you're talking about, there are several opportunities to return early if the timer expires. But is `1ms` enough to guarantee that we'll actually get through to where we fetch the data? Is it possible to just modify the conditionals farther down to make sure that we can always make progress even when the timer is set to 0ms (i.e., the request is fully asynchronous). To me, a timeout of 0ms doesn't mean that the operation should take no time at all, just that it shouldn't block. It seems "within bounds" to still perform any operation we need in order to guarantee we make progress.
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
There is code duplication between these 3 methods. Let's figure out a way to remove this duplicate code.
Let's check that `needsDrain` returns true after this point.
Hmm I think a better test would be to skip the linger wait time. This would show that calling `forceDrain` causes us to forego the linger: ```java assertFalse(acc.needsDrain(time.milliseconds())); acc.forceDrain(); assertTrue(acc.needsDrain(time.milliseconds())); assertEquals(0, acc.timeUntilDrain(time.milliseconds())); ```
Add the `@Overrride` annotation to this method.
s/assignments/oldAssignments, and name could be simplified as `toNewAssignment`
This name seems backwards.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: extra line
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
I seem the right fix would be to add the missing `{@code }` annotation in L184? ``` <pre>{@code ``` The closing `}` is already in L190.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
This seems to change the behaviour...
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
This is added in #986 as well. The later patch to go in will have to rebase, or we can extract out the common code as a separate PR. I am fine with either way. FYI @becketqin.
Seems that this class is a bit redundant, i.e, we could just construct an `AssignedTasks` with the `logContext` and `"standby task"`
Maybe it's worth not including this constructor. It's only used in tests and it's generally a good idea to provide a message with the exception.
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
I am wondering, if we might want to remove this method? Do use it only at one place and can easily pass the record timestamp there instead? Or is it useful for some special cases? Of course, this would require a KIP and separate PR -- it's might also not be high priority and just creating a JIRA for it might be fine. WDYT? \cc @guozhangwang @bbejeck
I created https://issues.apache.org/jira/browse/KAFKA-7245
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
and -> a
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
You probably need to do the same in `toEnrichedRst()` and `toRst()`
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
Should we use this check as a condition to wait? Sleeping 10 secs feels pretty brittle
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
Could the cluster aliases be constant as these are used all over the place
We can use `StringDeserializer.class.getName()`
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
That sounds good to me 
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
We tend to use the steam api for such small transformations but I don't feel strong about this.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
This statement is always false.
Maybe put a log statement, indicating that we are putting some data aside in the `pausedNextInLineRecordsPerTopicPartition` plus its size.
as above: we need to remove adminPrefix configs
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
Not introduced in this patch: "is non" => "as non"
As i said above, we should `requireNonNull(mapper, ...)`
Is our coding style suggesting to always make multi-line function call with more than one parameters? My preference is only use multi-line if there are 3+ parameters, AND if we put them into a single line it would be too long.
As mentioned above, we should defer all the checks of null keys to "repartition (before join or aggregation)", and the "join / aggregate" operators themselves. I think we are already doing this for most cases, but just double checking.
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
If the goal is preventing generic "BufferUnderflowExceptions" I think its worth adding. Doesn't need to be this patch if you don't want to though.
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
Is`atexit` is the right approach here? The registered function will only run when the entire ducktape process finishes (i.e. _after_ all ~215 tests finish). Use of the `tempfile.mkdtemp` will still avoid path collisions from concurrent processes, so maybe it's good enough. Another possibility that at least results in immediate cleanup: put directory removal into `clean_node` (but check the containing directory is present before removing to avoid errors)
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
Shouldn't need this line, it's handled by the superclass's constructor.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
nit: add `final`
nit: add `final`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
I think the name of the function is better defined as `interleaveTasksByConsumers`
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
Could we rename this to something like "remainingPartitions"
Won't that result in a dead coordinator and be handled by the first `if` block.
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Oh good point, we definitely need the key. But I think separating them turned out well
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
If you want to check that records for non-pause partitions are cleared, we should have a separate test.
```suggestion if (!batch.records().isEmpty()) { return Optional.of(batch); } ```
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
nit: remove empty line
as above. endOffset should be `12` and passed offset limit in next line should be 6.
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
Oh yeah, duh. Nevermind this 
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
I think we can move this logic into ValueOrOtherValue as another static constructor.
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
these overrides don't seem to add much.
never mind then. I'll leave this to AI.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
This is the same code as in `KTableFilter` -- we should refactor and share code.
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
Not introduced in this patch: "is non" => "as non"
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
what's a requested topic partition? Also, above we mention just `partition`
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
While we're at it, we may as well use another `{}` substitution for the parameter and remove the unneeded `toString()`.
I know you just moved these lines around, but while you're doing that it probably would be worthwhile to combine these 2 statements into one. If the log is busy, these might not appear next to each other.
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Consider naming the topic "topic2" since there are only two topics in the test
why do we put "unknownTopic" here? Should we subscribe to "topic1" and "topic3"? Or we can actually pass in an empty list? I guess this is copied from `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks` -- there we need to pass in `unknownTopic` as this topic does not exist in cluster metadata.
as above. (please fix in `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks`, too)
as above: flip arguments
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
Cheating the compiler, woohoo!
Nit: could just throw the exception directly here; doesn't appear to be much benefit to putting that in a separate `setup` method.
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
This can be final.
Seems to fit in one line
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: if you want a new paragraph you need to add `<p>`
`deteremined` => `determined`
Nit: space missing after `for`.
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
seems like it should at least be info, if not warn
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
Thanks for the explanation. Make sense.
Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
nit: you could just as well use a ternary operator here: ``` return (parts == null) ? Collections.<List<PartitionInfo>>emptyList() : parts; ```
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
nit: it would improve readability to factor out some functions for some of the work here. Here we can have a separate function with a nice name for building the assignments
nit: `Short.MAX_VALUE` is 32767
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
@ijuma Sorry, I don't know of a standard way of doing this,
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
nit: maybe iterate over `entrySet()` instead.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
Seems this could be a function as well. For example: ``` java Map<TopicPartition, PartitionInfo> partitions(Map<String, InternalTopicMetadata>); ``` (I'm looking for small independent chunks of code that can be taken out of this function.)
Seems you can replace this with this: ``` java topicPartitions.addAll(partitionsForTask.get(id)); ``` Same below.
Similarly here, this state check could be internalized.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
prop: You can remove this line. You only need `expectLastCall()` if you need to do some further expectation settings on a call that returns `void`, e.g., `expectLastCall().times(3)`
This should say `AdminClient`, not `Consumer`.
Personally, yes, I prefer one call, but I leave it up to you.
Thanks! This should have been done quite some time ago.
@kkonstantine, thanks for the correction -- the admin properties for the sink worker task (and specifically the DLQ reporter for the sink task) should use a combination of the `producer.*` and `consumer.*` properties that are connector-specific.
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
Let's use try with resources here and the other test so that the file is closed after it's used.
Maybe we should add one case where `position > 0`.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Ouch! Sorry about that!
It would be good to verify that the buffer contents are correct as well.
Ack, I get it now. Thanks for clarifying.
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
nit: might be better to name it windowedKTable
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
This first argument (`1L`) can be missed, given that that's the only line in which we pass 2 arguments. I'd add one argument per line.
I would move this assertion out to the assertion on line 351. With this move you save one parameter and you have both assertion next to each other.
This is not required as contained in the check next line.
If that was the case wouldn't we get an exception due to the topics not being co-partitioned? We should have a test for this case.
When some expected external topics do not exist or do not have metadata updatable, current strategy is that Streams will not create any tasks of its "affected" sub-topologies, including the changelog topic and repartition topic as well, and will expect another rebalance to trigger once these topics shows up. If the source topics are input ones for the "head sub-topologies" then no tasks will be created and no internal topics will be created.
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
How about using `MessageDigest.isEqual` method here also: ``` MessageDigest.isEqual(new String(password).getBytes(StandardCharsets.UTF_8), expectedPassword.getBytes(StandardCharsets.UTF_8)` ``` cc @rajinisivaram
Good point. Btw, this particular class (the default SASL/PLAIN server-side callback handler) is not recommended for production use, so perhaps not that critical.
Would it help to actually list the method that was used, in case somebody thought they were using basic? ```suggestion log.trace("Request credentials used {} authentication, but only {} supported; ignoring", BASIC, method); ```
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Style nit for "note" and whitespace between `Bytes` and `byte[]`: > (note, state stores always have key/value types {@code <Bytes,byte[]>} should be > (note: state stores always have key/value types {@code <Bytes, byte[]>}
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
Nit: `.` missing at end of sentence
nit: parameter descriptions are no sentences, thus no `.` at the end (on many other places, too). If we say they are sentences, they it should start with upper case `[T]he TopicPartition`
nit: remove `which is`
I understand that. My question is whether there is some thinking on which configs make sense to be set by users. Ideally, we'd do that instead of being reactive.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
```suggestion ConfigDef.Importance.HIGH, ```
Oh you're totally right, sorry for letting my paranoia start spreading conspiracy theories here  Given all this I'd still claim that the FSM is in need to being cleaned up a bit (or a lot), but if you'd prefer to hold off on that until the add thread work then I'm all good here. Thanks for humoring me and explaining the state of things. I just wanted/want to make sure we don't overlook anything, since there's a lot going on. For example in the current code, if the global thread dies with the old handler still in use then we'll transition to ERROR. However the user still has to be responsible for closing the client themselves, and it will ultimately transition from ERROR to NOT_RUNNING. Whereas if we transition to ERROR as the result of a SHUTDOWN_APPLICATION error code, the user should NOT try to invoke close themselves, and the ERROR state will be terminal. That's pretty confusing eg for users who use a state listener and wait for the transition to ERROR to call close(). We should make sure that ERROR has the same semantics across the board by the end of all this work. Anyways I'm just thinking out loud here, to reiterate I'm perfectly happy to merge this as-is. But for reasons like the above, I think it's important to tackle the FSM in the next PR and make sure it all gets sorted out by the next AK release
Hm ok this might be a problem. Since this is thrown from another catch block and not from the try block, it won't be caught by the catch block below and will slip through the exception handler.
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
```suggestion "Updating global state failed due to inconsistent local state. Will attempt to clean up the local state. You can restart KafkaStreams to recover from this error.", ``` Just a thought to indicate why just restarting would recover anything.
Do we _know_ that it will resolve the problem? Maybe better: ``` Changing the location of state.dir may resolve the problem ```
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: break line
nit: line too long
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
No worries. I was only recommending to change the name if you stopped using it to gate the processing and just relied on producing messages to control when they could be processed, if you did want to do that. Either way is fine with me so you can just leave it as is
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
Let's avoid making whitespace changes like this, since they tend to make the change harder to read
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
Ditto above. I would recommend having consistent explanations here.
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
Can you elaborate? Seems to be orthogonal to the timestamp fix.
If not, we should move the exception capturing logic inside the dbAccessor as well.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
I think the expectation is that these 2 would be atomic (i.e. would be bad if one thread executed 615, then another thread executed 615 again and got the same sequence number, before the first thread got a chance to execute 616). Also I think the expectation is that batches that are ordered one after another in the queue would get the sequence numbers in the same order (i.e. that batch that is later in the queue would get higher sequence number). Previously these expectations were protected by the queue lock so "poll", "get sequence", "update sequence" would execute as atomic block, with this change the operations could interleave.
Moving `close` outside of locked scope LGTM
No, not a blocker.
Looks like it won't happen since we only lock on deque object, but just want to confirm, to make sure it won't break anything.
Make sense, and that's also what I've seen. Thanks for confirmation!
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
```suggestion WORKER_SETUP_DURATION_MS, "Initial group of workers did not start in time."); ```
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
`hop` vs `advance` is subjective :) I am fine with `hop`, too; it's called a `HoppingWindow` after all. `advance` is just a term that is quite common in literature so I am somewhat used to it.
I would prefer `advanceBy(long advance)`
Remove double blank.
Remove double blank.
This will probably be subjective, but I'm ok with "hop" for now.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
recommended; ditto below.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
as above: we need to remove adminPrefix configs
could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok
That makes sense, I think keeping it as-is is better.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
Let's piggy-back on this PR: it should not be a big fix.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
I think we can just add these configs as part of the PR.
I think this could just be if(!SecurityProtocol.values().contains(securityProtocol))
That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
We don't use star imports in Java classes
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
I think you missed this one.
nit: since this is indicating entry into a method, I think it can be trace level.
I wonder if it would be worth improving this log message slightly, to something like: > Timed out waiting to flush offsets to storage; will try again on next flush interval with new offsets Strictly speaking, it's unrelated to the changes made in this PR. But for users seeing this in the log it would be helpful to know that despite it being an error that should be looked into, the next flush interval will attempt to commit all (potentially-updated) offsets.
What do you mean by "we still waited for the data in the buffer to flush"? The `beginFlush()` method doesn't actually do any flushing; it merely performs the snapshot of the offset writer's data.
I don't have a good place to put this... on line 208, if commitSync was succesfull, we still need to call workerThread.onCommit().
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
nit: unneeded parenthesis
Thanks for the clarification, makes sense.
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
We would want to check for `InvalidMetadataException` here as well. We need to go back to the Metadata call if we find a metadata error. This is similar to the cases when we call `rescheduleFindCoordinatorTask`.
`timestamp` missing (three times)
nit: missing comma `headers[,]`
`timestamp` missing (twice)
`expectedTimestamp` parameter missing (insert before `expectedHeaders` to align with method name.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
NPE: ![image](https://user-images.githubusercontent.com/925755/55269396-d55f3480-5292-11e9-9c29-78c524d63c65.png) I'm not using a topic pattern, equality should still work.
Mentioned offline, we can probably move move the conversion to empty string into the `OffsetAndMetadata` constructor so that we always handle this consistently.
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
Sorry for the dumb question, but I am curious if ``` if (partitionsPerTopic == null) return that.partitionsPerTopic == null; ``` Wouldn't be ``` if ((partitionsPerTopic == null && that.partitionsPerTopic != null) || (partitionsPerTopic != null && that.partitionsPerTopic == null)) return true; ```
Are you implying cache efficiency? I'd imagine the order to matter if one of the fields had more chances to return `false` than the others. But I don't think it's worth thinking about this here. Feel free to leave this as you have it. When I saw it, I just thought your editor performed the reordering.
For standby tasks, I think cleanup the state directory is fine; adding it into the rebalance protocol needs bump up the metadata serialization version for upgrade and hence more complicated.
I think it is less of an issue to be a separate JIRA, but either is fine to me.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
That's right, changed it locally.
It doesn't seem that the client needs principalBuilder.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Nit: we should probably include a little more detail. Maybe something like: ```"Found invalid wrapper offset in compressed v1 message set, wrapper offset '" + wrapper offset + "' is less than last inner record offset '" +lastOffsetFromWrapper + "'and it is not zero."```
Should be `post 0.10.0 Java clients`, I guess.
Nit: instead of `older`, maybe we should say `certain versions of librdkafka`. The Java client never does this for `v1` as far as I know.
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
I think this should be used in the define method below too.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
Nice tidy up of this test class :-)
Nit: add `final`
We could iterate through v1 to v5 here to test every case.
```suggestion public void shouldInstantiateAssignor() { ```
nit: add a size? There are a few cases in here where we could do this.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
Nit: you can remove `value =`
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
WDYT about this: ```suggestion long millisRemaining = Math.max(0, end - System.currentTimeMillis()); if (millisRemaining > 0) { Utils.sleep(millisRemaining) } ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
Nit: including "execute" here is completely unnecessary. ```suggestion throw new ConnectException("Fail to " + descriptionStr + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
nit: move `Serde.String()` into next line (also, I would prefer to not have static import `Serde` but prefix. I was initially confuse why this compiles and why it's not `new String()`... (because the method name starts with capital `S`...)
nit: remove blank line
Can you elaborate? What do you mean by > otherwise the state won't proceed
nit: remove blank line
nit: remove blank line
nit: should be `named` can't be null
nit: add `final`
nit: missing `<p>` for new paragraph
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
@spena just ping to make sure you get this on the follow-up PR.
Side improvement: I think we should skip late record directly and also record it in `TaskMetrics.droppedRecordsSensorOrExpiredWindowRecordDropSensor`
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
Yes, I think it's worthwhile to check the result of `Connector.config()` just in case `Connector.validate()` is overridden and the default check there is no longer used.
I was referring to the call to `connector.validate` two lines above. That is where the other null check in this patch in `Connector` would be applied, unless the user has overridden `validate`.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
A bit confusing that a second assignment follows if the `if` statement is true. I'd also call the variable `taskState` (as opposed to `connectorState` above) Ternary can be used here as well: ```suggestion AbstractStatus.State state = request.shouldRestartTask(taskStatus) ? AbstractStatus.State.RESTARTING : taskStatus.state(); ``` (as with any suggestion from github, please check it compiles and conforms to the style)
the drawback of renaming here is that the name `start_cmd` is used with `start`/`start_node` pretty standardly across the rest of this code base
Unless I'm wrong, we move to this directory and that's where we execute all the rest of the commands (such as the echos in output files below). Just want to make sure this is what we want (which looks like it is)
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
This debug message seems like it would appear _before_ we actually attempt to do any checking. It's probably worth keeping the old message (or something similar) _after_ the checking has been completed.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
@vahidhashemian, yes, that's what I mean.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
empty line needed
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
nit: move .collect to new line
Nit: remove unnecessary `this`.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
Where do you cleanup the childrenSensors object? Otherwise we will maintain a reference to the Sensor objects always.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
What kinds of failures are we trying to mask here? Frequently retries also require delays between retries.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
I think it would be good to include a message giving context before we start listing unresolved issues.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
I don't think it makes a big difference either way. The intent of the offset commit interval is just to make sure committed offsets don't fall too far behind. It does not need to be a strict schedule. It seemed more intuitive and simpler to me to reset the interval. In any case, we should get rid of this relative tracking of the next commit. If we use an absolute time, then we will not have problems like this in the future.
Maybe this is a little simpler? ```java nextCommit = now + offsetCommitIntervalMs; ```
Is there any reason not to accept this suggestion? I will go ahead and push an update to this PR next week if there are no further responses so that we can get this fix into the next release.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
This seems to always enforce a materialization, but I think we should materialize only if we need to.
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
nit: top of class
```suggestion for (final Map.Entry<TaskId, SortedSet<ClientIdAndLag<ID>>> taskToRankedClient : statefulTasksToRankedClients.entrySet()) { ``` Just to resolve a warning about referencing the subclass instead of the interface.
TBH, I'm a little skeptical of using this style too much. Nothing against functional programming; it's just that, having done quite a bit of FP-heavy programming and maintenance for quite a few years, I've settled into an opinion that it's most efficient when employed in simple contexts. When you get into nested transformations like this, it becomes harder to come back to the code in three years with a completely blank slate and read it. Plus, it has a tendency to steer you away from efficient code and you can wind up doing multiple iterations over the same collection when one would have done. So, I tend to use the FP APIs to do stuff like turn a list of Tasks into a list of TaskId, and I'm happier to see regular loops and conditionals for stuff like this. This is very much a matter of preference, though, and I'm only expressing mine.
Fine with me -- it's just a "prop" after all. FWIW I generally find these functional-style methods harder to read, but for whatever reason in this case I was finding the original a bit hard to understand and thought this suggestion helped to "get to the point" faster. But of course it's always easier to read your own code than someone else's 
(just a suggestion based on what I personally find easier to read)
```suggestion final Map<TaskId, List<ID>> taskToCaughtUpClients = statefulTasksToRankedClients.entrySet().stream().collect(Collectors.toMap( Entry::getKey, t -> t.getValue().stream() .filter(c -> c.lag() == 0 || c.lag() == Task.LATEST_OFFSET) .map(ClientIdAndLag::clientId) .collect(Collectors.toList()))); ```
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Your understanding is correct @mjsax .
nit: avoid unnecessary `this.` prefix
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
What do we do if there's an exception? If it's expected, let's make it clear
Just let the Exception flow, that will automatically fail the test
typo: byteArrray -> byteArray
`< Callback >` this explicit type is not necessary.
nit: we could split this lone line by different key, value by new line to make it clear. ex: ``` String[] args = new String[] { "--topic", "Hello-Kafka", "--num-records", "5", .... }; ``` Same as below.
nit: remove the redundant line. Same as below.
redundant type arguments `<ProducerRecord<byte[], byte[]`
`replicaing` -> `replicating`
I think the assertion on 219 would pass even if the 1st mocked interaction never happened. Do we need something to tighten up the expected behaviour? Maybe something like: ```java verify(kafkaBasedLog, times(2)).send(any(), any(), any()); ```
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
Not necessarily, we could pass in both processing mode and the expected output as parameters, if the test workflow looks essentially the same.
Also could we try to parameterize this part as well? Like passing processing mode flags to each test
ditto here and others below
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
rewrite test as above using `assertThrows()`.
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
remove try-catch and replace with: ``` final StreamsException s = assertThrows(StreamsException.class, () -> testDriver.pipeInput(consumerRecord)); ``` assert afterwards and don't re-throw.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
nit: add `final`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
nit: add `final`
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
We'll need to fix this in a follow-up so that followers send the right replicaId.
I hope we can get rid of those conversion in the future :)
nit: we can use `map#compute` to replace getOrDefault + put.
The current implementation is problematic in the sense that all cached data will be removed if users subscribe to the existing set of topics and `topicPartitionsToClear` is empty. Can we replace `clearBufferedDataForTopicPartitions(...)` with `updateCompletedFetches(Set<TopicPartition> assignedPartitions)`? updateCompletedFetches will keep only those data whose partition is still assigned. This will fix the problem above and also makes the logic in the caller simpler. Instead of having to determine the partitions that have been removed, the caller only needs to provide the latest assigned partitions as `subscriptions.assignedPartitions()`.
Do we need to also clear data from `nextInLineRecords`? Also, it seems that `assignedTopicPartitions` is never null, right? Maybe we can skip this check.
It is public API for a class that we are using internally. Thus we are able to enforce rule via code review that `assignedTopics` is never null. And in general it is preferred not to call a function with null parameter, since otherwise we will have a lot of null check in the code which will make the code unnecessarily verbose. So it seems simpler not to check whether it is null. Note that a few other public methods do not check whether the input parameter is null. For example `Fetcher.getTopicMetadata` does not check whether request is empty. So it is probably consistent not having to check it. And for public APIs that are exposed to user, e.g. `Consumer.assign(partitions)`, because we can not ensure via code review that `partitions` is not null, we currently throw `IllegalArgumentException` if the value is null, instead of allowing user to call the method with null value.
Can we simplify the patch by not having this method? We pause a partition after putting it in `recentlyUnPausedTopicPartitions`, we can still use `fetcher.subscriptions` to check whether the partition is unpaused while going over the partition in `recentlyUnPausedTopicPartitions`.
nit: move below the shortcut return below.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
`KStream` => `{@link KStream}`
Typo: "_of_ key-value pairs"
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok
You need to pass in a `Pattern` but not a `String` here. This actually exposes a "bug" in the test-driver -- it should check if the topic name is valid -- and a pattern is not a valid topic name. Not sure if we should have a different PR for a fix or piggy back on this PR. \cc @guozhangwang @bbejeck @vvcephei
Let's piggy-back on this PR: it should not be a big fix.
Can't you use `createClient`? This call confused me... (same below)
It's not about capacity, is it? It's about having not task that does not have the task assigned (as active or standby). -> `shouldNotAssignStandbyTaskReplicasWhenNoClientAvailableWithoutHavingTheTaskAssigned`
nit: `completeRefresh` to be more concrete in what it completes? Complete can be understood to mean a total completion of the connection, whereas this completes a single refresh (something finer-grained and more often)
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
We should also close `targetAdminClient`
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
This is unused too
Why do we have SSL specific methods here? Could we move all the SSL bits into the SSL class? We have fields for the configurations. So we could set them accordingly (without or without SSL) in each concrete class. Then in the base class, we just use the fields to create the clusters without having to know if it's SSL or not.
We can use `List<Class<? extends Connector>` to avoid the warning
Just let the Exception flow, that will automatically fail the test
nit: formatting -> should be in the line above.
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
We actually don't need to name the store. This could be `.count()` plus updating the name for the repartitioning and changelog topic.
I understand your point -- however, naming must be deterministic and should not change from release to release (otherwise, upgrading hard harder and we need to mention it in the upgrade docs). Thus, if we use internal names in tests, we have some implicit testing that naming does not change. That's why I prefer to not name the operator, too, if not required.
nit: move `table1` to next line, or fix indention below to align with `table1`
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
may be use Objects.requireNonNull
We should test that delete twice in a row fails with `IllegalStateException`
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
nit: `addMetadata` -> `put`
The goal of the ticket is to actually remove this check.
we also want to remove this check
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
You could do `Assert.fail(...)` here rather than tracking it in a boolean etc
nit: Empty line could be removed.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
same question around log level as above
same question around log level as above
Yeah, that works, too, and is more align with the current code.
You can also do this more concisely as `topicsByName.keySet().removeAll(existingTopicNames)`.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
This is a change in semantics, right? Before, we would never expire a connection that has been processed during the `poll` because we use `currentTimeNanos`. After this change, if something in `poll` takes long enough, we could end up expiring a connection in `completedReceived` for example.
this problem also affect replication node fetch process? our server occur a error: [2019-04-05 23:59:46,084] WARN [ReplicaFetcherThread-1-1], Error in fetch kafka.server.ReplicaFetcherThread$FetchRequest@b22a64c (kafka.server.ReplicaFetcherThread) java.io.IOException: Connection to 1 was disconnected before the response was read but return to normal after a reboot kafka server.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
nit: seems these could be final? Same in `ConsumerUserData`.
nit: we can use `map#compute` to replace getOrDefault + put.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
nit: It might be a little clearer to use `Optional<Integer>` for the type here.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
Also, 5ms seems a bit extreme. Maybe this could be 20ms or so and we could use the minimum of this and the configured retry backoff so that users can adjust it lower if they need to.
Yeah, I have no doubt the performance is better. It's just that it seems like a lot of excess traffic and is going to be amplified by the number of transactional producers. It may be fine in the common case if the write markers are pretty quick, but if there is any kind of delay, then I'd be concerned about the brokers being overwhelmed with these requests (though maybe it's not as bad with request throttling). I'd rather err on the safe side for now since users can manually adjust the backoff. For the 0.11.0.1 release, we can provide a better solution. Most users will probably hold off until then anyway.
ditto (code style)
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
nit: also add java doc for type `T, O` here
Same (should be threads_per_worker)
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
Maybe just check that `minikdc` is not None here
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Huh. Bummer that you have to make it queriable in order to use the in-memory store. I never noticed that before.
Yes, this would be better. Not sure if it helps, but for reference, this is what we did in `org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinMaterializationIntegrationTest#getTopology`
Instead of specifying the whole thing for both cases, you could just create a ``` final WindowBytesStoreSupplier supplier = inOrderIterator ? new InOrderMemoryWindowStoreSupplier(...) : Stores.InMemoryWindowStore(...) ``` and then pass that into the `Materialized` without having to list the whole topology out twice.
I knew John would know what's up with the weird type nonsense 
It should ultimately be the same for both iterators, but there might be some weird type nonsense going on. These problems should go away if you go with the approach of just setting a `StoreSupplier` based on `inOrderIterator` and then only specifying the topology once
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Kafka doesn't mandate braces in `if` statements.
Let's remove the brace changes please.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
Better be `cooperative-sticky`? `cooperative` is too general I think.
We should mention somewhere that users should prefer this new assignor for newer clusters.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: The mention of join group comes out of nowhere
Also add `@params topics`
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
I wonder if we should move all of this into a new private method that takes the interceptor callback and intercepted record. The reason is that it's a bit easy to make a mistake and use `record` and `callback` instead of the intercepted ones (with the current approach).
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Q: I might have missed the discussion. Why does an unknown offset result in `1` and not in `Long.MAX_VALUE`? Sorry if you have already answered this question elsewhere.
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
This should call `configState.rawConnectorConfig(connector)`, which returns the _user-supplied_ configuration _with variables not resolved_. The `connectorConfig(connector)` call returns the configuration _with variables already replaced_, which means we might be leaking passwords and other secrets specified using variables.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Actually, it's the new method that needs a `default` implementation if we choose to add it to this interface.
Suggestion: ```suggestion // Set up the offset backing store for this connector instance ```
This should not have the `sink` suffix
Sounds good. We can consider this resolved.
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
Is our coding style suggesting to always make multi-line function call with more than one parameters? My preference is only use multi-line if there are 3+ parameters, AND if we put them into a single line it would be too long.
Ack, I get it now. Thanks for clarifying.
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
There is code duplication between these 3 methods. Let's figure out a way to remove this duplicate code.
Let's check that `needsDrain` returns true after this point.
Hmm I think a better test would be to skip the linger wait time. This would show that calling `forceDrain` causes us to forego the linger: ```java assertFalse(acc.needsDrain(time.milliseconds())); acc.forceDrain(); assertTrue(acc.needsDrain(time.milliseconds())); assertEquals(0, acc.timeUntilDrain(time.milliseconds())); ```
Add the `@Overrride` annotation to this method.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
I think we put args on the same line unless the list is too big
Maybe `Producer epoch...`. Also, not sure the exception message adds anything given what's already logged. Maybe we should remove that.
This was probably unintentionally reduced to debug level while fixing conflicts.
the method name changed to `windowedTable` and `windowSize` parameter is missing
and -> a
and -> a
`windowSize` should be `Duration`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Nit: "..and producer's {@link DefaultPartitioner}".
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
records to it, and reading all records from it, such that
To be consistent, we should say "into a new instance of [a windowed] {@link KTable}".
Thanks for the catch!
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
nit: ditto here, `cluster()` will reconstruct a new object on each call.
nit: `equals` => `equal`
SGTM. We can keep it as is then.
remove var -- only used once.
nit: move `"topic2"` to next line and fix indention (should only indent by 4 spaced) -- this will reduce the line length and make code better readable.
I see the value in setting this precedent, but unfortunately, we can't do it, due to the need to continue supporting older brokers (we support older versions that don't allow record headers). Instead, if `!consistencyEnabled`, we should just not add headers at all (i.e., we should continue to pass `null` as the headers).
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
nit: I don't spot any, but safer to avoid typos by just having constants for these
super nit: extra blank line
```suggestion put(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, "/tmp/foo"); ```
Could we get rid of the backoffOnFailure variable and just use: ``` java if (isLeader() && needsRejoin) ``` It might make the code a little easier to follow if that's really the only case where we want to backoff.
Please include TopicDeletionDisabledException here.
We don't use LeaderNotAvailableException in listTopics
This was probably discussed in the KIP, but obviously the downside is that users won't get any warning or hint that they should transition. But I guess we don't get a substantial benefit from removing `AdminClient`, so maybe we'll just never do it.
This is a breaking change, right? Same for the other `create` method in this class.
We don't use UnknownTopicOrPartitionException in listTopics
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
Is this just to prevent it from processing anything until you're ready to proceed? It seems like you can/are doing that just by controlling when to produce input messages and doing so one at a time (if that's accurate, then WDYT about renaming `process` to `processed` and flipping the boolean so it more clearly serves the purpose of indicating whether a record has yet been processed)
No worries. I was only recommending to change the name if you stopped using it to gate the processing and just relied on producing messages to control when they could be processed, if you did want to do that. Either way is fine with me so you can just leave it as is
Can we also assert that the state gets to `RUNNING` after the new thread has joined
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
 fair enough
How about returning a Set instead of List? ``` return topics .stream() .filter(topic -> !topic.isEmpty()) .collect(Collectors.toSet()); ```
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
Guess we can keep it, but this helper doesn't seem to be doing much for us anymore.
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
I think a better test scenario is to move the logic in `close()` call, i.e. when the stream thread is being shutdown, and topology is closing, we call `processorNode.close()` in which we wait for a while and then tries to access the global store. It mimics the case where in closing the store cache is flushed and hence tries to access the global store again.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
Good point, thanks for clarifying.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Ditto on the properties and the driver.
ditto on the properties and the driver.
Ditto on the properties and the driver.
ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
Just a question: Why is this not `6L` ? (it should be `5L` after you applied the fix you want to do in a follow up PR).
nit: should we merge this into existing test? If not, rename to `shouldChooseNextRecordBasedOnHeadTimestampe`
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
The overhead is minimal. The benefit is clarity: if someone looks at the code it is immediately obvious that null is not allowed.
nit: should we have a newline for each partition? Otherwise that ling maybe too long.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
nit: we could just use `Map` for `startOffsets` and `endOffsets`
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
Is this test needed? It seems that loginContextName can never be null.
Harsha did this.
It seems that we need to set the login time during the initial login as well.
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
It seems that we should setLastLogin() in setLogin() instead of here.
building a topology is done on the main thread when calling `StreamBuilder.build()` so I think it's safe to remove `synchronized`.
How can we be sure that `partNums` is not empty? If empty, `next()` would throw.
Since we are only going to verify number of partitions, I think we could just set value as integer
Let's try to be consistent to use `copartition` instead of `coPartition`
nit: missing space between logPrefix and 'found'
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
validateStoreOpen() can be outside of lock block.
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
Why are we changing this? The constructor with a single argument takes a message and not the groupId
typo: drop "the" before "whether"
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
What you had is fine.
Sorry for not catching this earlier, shall we try to be consistent when it comes to `toString` output? Here's an example of a recent Java class: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClientResponse.java#L67 What do you think? Also, it would be good to include an example of the logging output.
There's no benefit in using `StringBuilder` for something like this. Using `+` will have the same effect (the case where `StringBuilder` helps is when there's a loop.
This doesn't seem to be used.
I wonder if it is worth refactoring this to remove duplication, i.e, add a Functional interface, then implement it three times to just perform the op on `globalStateRestoreListener` then have a method sth like: ``` performOnGlobalListener(GlobalListenerAction action, String storeName, TopicPartition partition) { if (globalStateRestoreListener != null) { try{ action.perform(); } catch (final Exception e) { /// streams exception stuff } } } ``` Other methods delegate to this new method. The guard and error handling are encapsulated in one place. When we eventually get to java 8 we can just do a lambda call
There is similar code forming StreamsException. Consider refactoring to reduce code dup (can be done in another issue).
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
I was talking about the delayed interrupt time. Ideally, the shorter it is, the faster the test terminates. So I thought we could use 1s instead of 2s.
Seems to fit in one line
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
`final` and initialize in constructor instead? Doesn't seem to depend on the config at all.
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
Map `statefulTasksWithClients` is only used to iterate over its entries. I think it would be better to use the following nested loops and remove `statefulTasksWithClients`: ```suggestion for (final TaskId statefulTaskId : statefulTaskIds) { for (final Map.Entry<UUID, ClientState> entry : clients.entrySet()) { final UUID clientId = entry.getKey(); final ClientState clientState = entry.getValue(); if (clientState.activeTasks().contains(statefulTaskId)) { assignStandbyTasksForActiveTask( numStandbyReplicas, statefulTaskId, clientId, rackAwareAssignmentTags, clients, tasksToRemainingStandbys, tagKeyToTagValues, tagValueToClients ); } } } ```
Currently the code iterates over the active tasks and assigns all standby tasks for each active task. If the standby tasks cannot all be assigned, we might end up with all standby tasks assigned for some active task but none for others. What do you think about to assign one standby task for all active task and then assign the second standby task for all active task, and so on. In this way, it is more likely that at all active tasks have at least one standby task assigned. I am aware that the default standby assignor has the same drawback.
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
The code is correct, but confusing to read for me as a human...
should be `apply(oldAgg, value);`
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
Since I was fixing stuff anyway, I went ahead and fixed a bunch of formatting issues that I didn't bother mentioning before.
nit: insert space `String... expected`
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
nit: move .collect to new line
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks @vvcephei -- that is convincing.
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
given that the previous messages say "Reading to ..." maybe it would make sense to say: ```suggestion log.trace("Read to end offset {} for {}", endOffset, topicPartition); ```
unboxing will happen in the comparison in the `if` branch anyways, so probably better to do it early declaring the type `long` here.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
This check `records.size() < offset` seems a bit sketchy to me. Basically we are assuming that the input topic `data`'s starting offset is always 0, and there is no "holes" in the topic partitions so that the `offset` indicates the number of records we can read from the input topic. Maybe a more robust way to do that would be 1) read the input topics `data` and optionally `repartition` based on `withRepartitioning`, stop when the current record's offset is equal to or larger than the committed offset, and remember the number of records; 2) read the output topics (again optionally `repartition`) from the beginning to the end (use `seekTo`), and check that the number of records are the same as the number of records read from the input. Then we do not need to truncate, and also in verification we do not need to check list size again since they are already checked here.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
nit: use class `import` to avoid long name here
It's better to keep the parameters aligned (having same indentation)
spelling -> recrord -> record
This might be instable in Jenkins.
This method could be replaced with the use of a Junit Rule and TemporaryFolder, i.e., `@Rule public final TemporaryFolder folder = new TemporaryFolder() ... logDir = folder.newFolder() `
Typo -- or is that Italian? :-)
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Compiler still gives some warning here, as you are erasing here the java generic type. Adding the `<>` infects the rest of your code with the `? super T` you have to add at the correct places.
Fixed this in my latest update of #4033.
For this to be useful to me in my work on KIP-183 I also need `addWaiter()` to be made `public` on `KafkaFuture`.
nit: I would rather use the full name instead of using acronyms.
typo: CompleteableFuture -> CompletableFuture
I wonder if it is worth refactoring this to remove duplication, i.e, add a Functional interface, then implement it three times to just perform the op on `globalStateRestoreListener` then have a method sth like: ``` performOnGlobalListener(GlobalListenerAction action, String storeName, TopicPartition partition) { if (globalStateRestoreListener != null) { try{ action.perform(); } catch (final Exception e) { /// streams exception stuff } } } ``` Other methods delegate to this new method. The guard and error handling are encapsulated in one place. When we eventually get to java 8 we can just do a lambda call
There is similar code forming StreamsException. Consider refactoring to reduce code dup (can be done in another issue).
nit: "another thread wrote to ..."
I think we should probably do a null check here and throw. Setting the listener to null doesn't seem valid to me
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
We probably have to keep the `size() == 0` behavior for compatibility.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
I think we'd want to use `updateLastSeenEpochIfNewer`. The provided epoch just gives us a lower bound on an acceptable leader epoch.
nit: move below the shortcut return below.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
nit: parameter/line formatting
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
Nit. use `{ }` for all code blocks
Maybe add an `addSubtopology` method? I don't really like the idea of a) exposing, and b) mutating an internal collection.
nit: move below the shortcut return below.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
I was looking at the `append` code and it seems a bit brittle. It assumes that: - The collection returned from `PartitionRecords.take` is safe to mutate even though the latter returns `Collections.emptyList()` if `records == null` - That `part.take` will always return at least one element This is fine today, but it may be worth making it a bit more robust to refactorings.
I personally support Damian idea here. Don't think it's overkill but actually more elegant OO code. If you feel strong to keep as it, also fine with me though.
I don't think it is overkill either, it is just good OO design. Anyway, i'm not going to block this PR because of it.
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
`of` -> `or`
`out-of-order` `window closed`
`before or after`
Same as above mentioned, the validation didn't get handled in new API.
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Shouldn't need this line, it's handled by the superclass's constructor.
Actually, it seems more than one figure needs updating. Makes me wonder if we should actually remove them altogether and let people read the code (which can't go stale) instead.
is it important to have 3 brokers for this test? I'm wondering if the tests would be more resilient and faster with just one broker and replica of each topic.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
Sounds legit. Thanks.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
nit: These two functions are not for testing only.
style nit: normally we'd use braces around blocks unless they're a single line
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
nit: add a space before the `:`.
What you had is fine.
strictly speaking, you don't need this yet. Still needed when we evolve I suppose
I guess in the end we will find these classes a better place (currently they are a bit scattered, e.g. `KeyValueStoreFacade` is in the materializer class).
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
Btw: all those test only set differn window size and grace and thus it seems we can share more code (ie, setting up the topology). Only the window definition is different.
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
Shouldn't we be using `SASL_PLAINTEXT` as this point? Also, it may be worth including a test with `SASL_SSL` as well for this case.
It isn't about blocking vs non-blocking. It is about how a Kafka broker behaves to a non-authenticated request on SASL ports... we need to test it (since it is the requirement of this JIRA). We hope that SASL behaves exactly the same as PLAINTEXT, but we don't know that it does without a test.
I wonder if this could be generalised further? Probably a broader discussion, but `KStreamPeek` is really the same as `KStreamMapValues` (there may be others). They just don't share a common interface for the action. I personally feel this would be a better rationalization of classes etc than combining `KStreamForeach` and `KStreamPeek`
Also as stated in the JIRA, it is worth exploring whether it is cleaner to collapse the implementation of `KStreamForeach` and `KStreamPeek` into a single class, with an flag indicating whether to `context().forward(key, value)` after `action.apply`.
Part of the JIRA is to remove KeyValuePrinter class after it was replaced with `ForeachAction`
I'd keep `FileNotFoundException` in the signature as it indicates a specific error as opposed to the more general IOException
Why was `FileNotFoundException` removed from the signature? It can be useful to list specific subclasses that may be worth handling separately from the general `IOException`.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Nit: I know this was inherited from the existing code, but it would be nice to do something like `ByteArrayDeserializer.class.getName()` for serializers and deserializers.
```suggestion public void shouldInstantiateAssignor() { ```
```suggestion protected final List<Future<Void>> futures; ```
Is that for tests? Anyway, we can revisit in a cleanup in the future.
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
The first exception will be the "cause"
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
nit: final on params here and methods below.
We could port this function when it is actually needed.
This can be initialized here and be `final`
`skipBytes` doesn't avoid decompression though, the `readBlock` call below decompresses the buffer: ```java class: KafkaLZ4BlockInputStream public long skip(long n) throws IOException { if (finished) { return 0; } if (available() == 0) { readBlock(); } if (finished) { return 0; } int skipped = (int) Math.min(n, available()); decompressedBuffer.position(decompressedBuffer.position() + skipped); return skipped; } ``` We do avoid GC pressure though.
I pasted the code from `skip` which is called from `skipBytes`. I know we don't call `readFully`.
nit: extra space between `structure` and comma.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
Maybe we should say `a partition (which consists of log segments) can grow to...`
I think this should be used in the define method below too.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
I tweaked this a little before merging.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
Seems this duplicates `L733`. Might be good to extract into a small helper method.
IMO, `createStreamThread()` would describe the behavior better.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
Maybe we can just a better name for `path` since it makes this code look suspicious.
Let's keep this as a private method.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
I see. I do not have a strong preference actually. But I remember we use singulars not plurals in many other classes and just wanted to be consistent. If it is actually the opposite case I'm happy to have them all to be plural.
I meant to have all "singulars" for consistency, i.e * Creates a * Starts the * Shuts down this * Does a clean up I'm OK with imperative style.
`.. of this instance`.
I feel there are still some room for improvements. @theduderog's feedbacks would be very helpful, while here are my two cents: 1. Start a new line with different levels of indent as we go through the streams granularities, streams->threads->tasks->topology, etc. 2. For the topology specifically, the parent-child relationship seems incorrect from the `-->` hierarchy, so I'd suggest reducing the verboseness a bit by removing `sourceNode[..., sourceTopic,` etc, and also just displays the child name instead of using `-->`, for example take the above topology: ``` KSTREAM-SOURCE-0000000000: topics [streams-file-input], children [KSTREAM-FLATMAPVALUES-0000000001] KSTREAM-FLATMAPVALUES-0000000001: children [KSTREAM-MAP-0000000002] KSTREAM-MAP-0000000002: children [KSTREAM-AGGREGATE-0000000003] KSTREAM-AGGREGATE-0000000003: stateStores [Counts], children [name=KSTREAM-FILTER-0000000005] KSTREAM-FILTER-0000000005: children [KSTREAM-SINK-0000000004] KSTREAM-SINK-0000000004: topic [X-KSTREAM-MAP-0000000002-repartition], children [KSTREAM-FILTER-0000000005] KSTREAM-SOURCE-0000000006: topics [X-KSTREAM-MAP-0000000002-repartition], children [KTABLE-TOSTREAM-0000000007] KTABLE-TOSTREAM-0000000007: children [KSTREAM-SINK-0000000008] KSTREAM-SINK-0000000008: topics [streams-wordcount-output] ```
I tried to print the topology with the current code in WordCountDemo, and here is the output: ``` ProcessorTopology[sourceNode=ProcessorNode[name=KSTREAM-SOURCE-0000000006,sourceTopic=X-KSTREAM-MAP-0000000002-repartition-->node=ProcessorNode[name=KSTREAM-AGGREGATE-0000000003,StateStores[Counts,],--> sourceNode=ProcessorNode[name=KSTREAM-SOURCE-0000000000,sourceTopic=streams-file-input-->node=ProcessorNode[name=KSTREAM-FLATMAPVALUES-0000000001,--> node=ProcessorNode[name=KTABLE-TOSTREAM-0000000007,--> node=ProcessorNode[name=KSTREAM-MAP-0000000002,--> node=ProcessorNode[name=KSTREAM-SINK-0000000008,sinkTopic=streams-wordcount-output,--> node=ProcessorNode[name=KSTREAM-FILTER-0000000005,--> node=ProcessorNode[name=KSTREAM-SINK-0000000004,sinkTopic=X-KSTREAM-MAP-0000000002-repartition,--> ] ```
Hmm, why did we do this? I thought we'd have a try/catch block.
There are a couple of checkstyle failures in these two lines because of missing spaces
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
Ah, I see. Thanks for the explanation
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
Maybe add an `addSubtopology` method? I don't really like the idea of a) exposing, and b) mutating an internal collection.
Maybe we can create a JIRA for tracking, but it's not that important for now. Since the common parts that can be consolidated may be not much: in the actual topology building process we need to set the internal topic names, set copartition topics etc which are not needed for the topology description building at all. What I was originally thinking is the the topology building process may be extending from the topology description building process with its additional functionalities like I mentioned above, but I am all hand-wavy on the devil details now.
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Since we have three threads for this test, there can be multiple rebalances before the streams instance stabilize and start processing...
Couldn't we simply wait for the current state to become `RUNNING`? ```suggestion private void waitForRunning() throws Exception { waitForCondition( () -> kafkaStreams.state() == KafkaStreams.State.RUNNING, DEFAULT_DURATION.toMillis(), () -> String.format("Client did not transit to state %s in %d seconds", expected, DEFAULT_DURATION.toMillis() / 1000) ); } ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
Why do you remove `transform` ? We only add a new `flatTransform` but `transform` is not removed.
Unless @mjsax objects, I vote to just delete these lists. At this point, it almost looks like it's directing you to all the other methods in this interface, which seems redundant. I'm not sure I follow your last question. The list exists to direct readers to other relevant methods. I'm not sure why adding `flatTransform` renders `transform` irrelevant...
Yeah, I'm fine with defining and applying a coherent strategy. In lieu of that, I guess the default thing to do here would be to just add the new method to the list without removing any other items.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: missing `<p>` for new paragraph
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
You probably want to also change this to `Throwable` in the vein of https://github.com/apache/kafka/pull/1788/commits/6c6f6761a3e08ad04b2659386eaf6e288d36e546
This could probably also be an exception since this shouldn't be called with invalid target states.
This is assuming that `totalTopics` is always a multiple of `MAX_BATCH_SIZE. Is that always true? Perhaps it is better not to make that assumption in any case.
@cmccabe Since `flush` blocks until all records are sent, wouldn't it be better to compute the delay time after flush completes? Ideally, an async flush would be better to avoid more delay than required, but that would need another thread. Alternative is not to flush at all, since only the records in the last incomplete batch would be delayed when `linger.ms > 0`.
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
No need to pass the spec in the constructor here, as all the connections have access to the internal spec.
Maybe something like "No command specified" would be more descriptive
Ok, I was thinking more along the lines that we have just one requestCommit instance while we would have multiple punctuator instances all using it. It looked like a bug, but it seems like it'll be fine.
Yeah, that's what I was thinking
Good catch! I would prefer to remove it then.
req: Could you please remove `context` from the `reset()`? Since we did not specify any behavior for it, we also do not need to reset it.
req: Since we do not need to validate `valueTransformer`, could you please remove it from the `verify()`.
@rajinisivaram makes sense. I was thinking if we can break the JAAS config file and made them into client config properties. But just passing JAAS config file will make it easier and extensible.
by no means a blocker (especially given public interfaces relying on this same problem), but i would not be opposed to making internal APIs not rely on these booleans -- clear enums, with properly handled default cases (presumably throwing exceptions) would be better. we tend to let things like this leak into internal (and subsequently external) APIs, but I think it is probably always a mistake -- unclear from the caller standpoint and not extensible.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
This still doesn't seem correct. We're only invoking configuration if the plugin implements `Configurable` afaict. This does not work for `Converter`s implemented against the new API and assuming the "forward" configuration. We *must* always invoke the "old" `configure(Map, boolean)`, and only invoke the `Configurable` version as a backup. Possibly it would make sense to indicate on the `HeaderConverter` that the `Configurable` methods should be idempotent if we need to be able to implement both. Not sure if we can test this easily with unit tests, but I think we might want a plain old `Converter` (that does not implement `HeaderConverter`) in tests to validate compatibility... but it's possible we'd need either integration or system tests to properly validate.
this is the kind of assertion that could become flaky given incremental population of `connectors`.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
What do we do if there's an exception? If it's expected, let's make it clear
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
and -> a
and -> a
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
If the goal is preventing generic "BufferUnderflowExceptions" I think its worth adding. Doesn't need to be this patch if you don't want to though.
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
Also, it might be better to use `synchronized (AbstractCoordinator.this) { }` to mutate both `rejoinReason` and `rejoinNeeded` in order to ensure that they are consistent with each others.
Is ```JoinGroupResponseHandler``` a better place to log error? For example, the error ```UNKNOWN_MEMBER_ID``` is log twice. (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L605)
Your reasoning makes sense to me. From a first read, the PR looks pretty good. I will make a second pass on Monday to ensure that I cover all the cases.
Can we still specify that we failed during the JoinGroup? eg `Rebalance failed on JoinGroup with {}` or something
Different question: can we report the exception as the "cause", rather than just getting the toString of it? ```suggestion log.info("Rebalance failed.", exception); ```
This block can be moved outside of the `try-catch-block`
Why not do this unconditionally? If it's not `clear` we won't commit anyway. It's seems cleaner to avoid to many branches and it's not on the hot code path so the overhead of updating `partitionTime` is not relevant.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
A `if (clean)` should be sufficient now
Similarly here, this state check could be internalized.
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
Maybe we can create a JIRA for tracking, but it's not that important for now. Since the common parts that can be consolidated may be not much: in the actual topology building process we need to set the internal topic names, set copartition topics etc which are not needed for the topology description building at all. What I was originally thinking is the the topology building process may be extending from the topology description building process with its additional functionalities like I mentioned above, but I am all hand-wavy on the devil details now.
Maybe add an `addSubtopology` method? I don't really like the idea of a) exposing, and b) mutating an internal collection.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
Also, same as above (and elsewhere), mixture of `final` and not `final` locals. They could all be `final` - i don't really care either way, but consistency would be good
nit: move parameter to next line
nit: single parameter per line
Ack, I get it now. Thanks for clarifying.
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
The test itself is great -- but the name does not really match -- there is no `GlobalKTable` in this test. Might also be good to pipe some data using the TTD in this test.
It's a little nice for future reference when we also say when it became deprecated, such as "since 2.6".
`use {@link #toStream()} followed by {@link KStream#to(String)} and {@link StreamsBuilder#table(String)} to read back as a {@code KTable}` ?? same below
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
That is fine, just curious if we have ever thought about how users would leverage the APIs to determine which stores to query. We can discuss this in a follow-up JIRA.
an -> a
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say `assertDescriptions`, that verifies a descriptions map contains the information about a single log dir/topic partition? Something like `assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture)`.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
This can be static
nit: This empty line is not necessary.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
typo: moreq -> more
Also: should only call onPartitionsLost on owned partitions that no longer exist
Can you do something like: ```java static final int tableSizeFor(int cap) { int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1); return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } ```
Or https://github.com/google/guava/blob/master/guava/src/com/google/common/math/IntMath.java#L56-L72 It is safe to look as it is Apache License 2.0.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
This is not completely compatible with the behavior of older Streams apps. See #10953 for a fix and more details.
Right, sorry I misread that line.
we also want to remove this check
nit: "another thread wrote to ..."
nit: would be nice to be consistent on the pattern we use here
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
This may be useful, since the log messages in TransformationChain do not print the schemas in `ConnectRecord`
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
The order is not really that important here, either way works
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Please remove empty line.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
nit: this can be final
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
I suggest passing all of the variables we access in this constructor via`SustainedConnectionWorker.this.spec` to be switched to parameters we pass upon instantiation
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
nit: `sooner` -> `as soon as possible`
Here I'd suggest doing the opposite: `poll(0)` since it is during the normal processing, not during restoration; so we can afford to not having some time in a few iterations. Instead, we want to proceed to the next iteration to call the normal-consumer.poll sooner to not be kicked out of the group.
`replicaing` -> `replicating`
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Perhaps something like "Represents a pattern that is used by ACLs to match zero or more Resources"
Is it worth having this class? Not sure there is much overlap apart from storing the fields.
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Can `rebalancing()` throw a `InvalidStateStoreException` ? If yes, we need to split this an apply try-catch-fail pattern instead of using `@expected`
`rebalancing()` should never throw an `InvalidStateStoreException` as it is just constructing the `CompositeReadOnlyKeyValueStore` wrapper. The underlying stores should not be accessed until `get`, `range`, or `all` are called. So, i think this is safe to leave it as it is
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
typo: Woth -> With
nit: use "{}.x." vs. string concatenation
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
Better be `cooperative-sticky`? `cooperative` is too general I think.
I don't have all the context, but isn't `3` pretty low? We don't do exponential back-offs, so the recommendation for no data loss is typically higher.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
nit: same formatting would have been nice for all places with `parser`.
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Hmm.. This is an interesting case. As far as I can tell, the state would only get reset if we are changing the producerId in `resetProducerId`. So the question is whether it is valid to associate the last acked offset of the old producerId with the new one? I suspect the answer is no. The last acked offset is used in order to try and detect spurious UNKNOWN_PRODUCER_ID errors, so I think our assumption is that this offset is associated with the producerId at the time of the request.
nit: seems we don't need this method anymore since `addPartitions` is idempotent.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
We could refactor out a helper function here.
The verb following 'should' would be in active tense. consider naming this test: userSerdesShouldBeInitialized
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
I think we're testing `testDir` Occupied here, not `AppDir`.
I can't make out the difference between this test and the previous one. looks identical from the code.
Not clear why we want to use a separate thread to call `joinGroupIfNeeded`? In unit test we would try to avoid any unnecessary multi-threading as it can easily cause flaky tests.
`assertNull`s shouldn't be here but few lines bellow.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say `assertDescriptions`, that verifies a descriptions map contains the information about a single log dir/topic partition? Something like `assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture)`.
You can use `EnumMap`.
You can use `EnumMap`, which is a lot more efficient.
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
It should be something like "Sent by an (admin) client to get data about consumers groups managed by a broker. To get a list of all consumers groups in the cluster, it needs to be sent to all brokers."
It should be something like "Sent by an (admin) client to get data about a specific consumers group like main information about members in such group."
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
Does it make sense to do this check for all types on read? INT64, INT32, etc
If the goal is preventing generic "BufferUnderflowExceptions" I think its worth adding. Doesn't need to be this patch if you don't want to though.
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
nit: should we merge this into existing test? If not, rename to `shouldChooseNextRecordBasedOnHeadTimestampe`
Just a question: Why is this not `6L` ? (it should be `5L` after you applied the fix you want to do in a follow up PR).
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
nit `stays at 2` seems to be correct -- it's `equalTo(2)` below.
nit: `final` (also next line)
The call to `closeGracefully` can result in some responses being returned from the brokers. Is it intentional that we do not invoke the completion handlers? I think this probably makes sense since anything awaiting the responses has probably shutdown by now, but wanted to check.
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
You should use `assertThrows` instead. Otherwise we need a `fail` after `close()` to make sure we actually raise an exception.
Maybe we can just a better name for `path` since it makes this code look suspicious.
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
may be use Objects.requireNonNull
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
nit: line too long
Don't we have to do something with the futures returned by `send`? How do we know if the requests completed? Also, cc @hachikuji.
The consumer close code needs to block until the fetch sessions are closed. `KafkaConsumer` has `close()` and `close(long timeout, TimeUnit timeUnit)`. In the case of the latter one, we don't want to wait longer than the specified time.
More specifically, there's no point in having this if we close the network client before we even get to the point where the request has reached the network layer.
I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably.
I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required.
This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
Still not used
Nit: instead of `older`, maybe we should say `certain versions of librdkafka`. The Java client never does this for `v1` as far as I know.
```suggestion ConfigDef.Type.LIST, ```
```suggestion ConfigDef.Importance.HIGH, ```
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
nit: move first parameter to next line, too
either move `this` to next line, or fix indention.
nit: no need newline of 104 below.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
nit: Provide a message to the IllegalStateException constructor
> most recent max timestamp Huh? I think I know what you're trying to say here but it seems like two different phrases got a bit mixed up here
Oh good point, we definitely need the key. But I think separating them turned out well
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
+1 to this
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Should have a comma after "for example"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Typo: should be "or larger than the number of available brokers"
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
Nit: including "execute" here is completely unnecessary. ```suggestion throw new ConnectException("Fail to " + descriptionStr + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
Yeah I think that makes sense here
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
nit: I would throw it as an error since I do not see any difference in severity compared with the execution exception. I think if you add the java timeout exception as the cause of the Kafka timeout exception, it will print also the stack trace of the Kafka timeout exception with `Caused by`. However, it will print the stack trace later in the logs. But that is OK, IMO, and we do that also in other places I guess. ```suggestion log.error("Could not remove static member {} from consumer group {} due to a timeout: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new TimeoutException(e.getMessage(), e); ```
nit: Just added some more information to the messages. ```suggestion log.error("Could not remove static member {} from consumer group {} due to: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new StreamsException("Could not remove static member {} from consumer group {} the following reason: ", e.getCause()); ```
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
This name seems backwards.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Maybe: Include in the log the Connect key, value, and other details of records that resulted in errors and failures.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
Please remove empty lines here and in the other test methods.
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
You usually do not want to mock the class under test, because you want to test it. Also partial mocks should only be used if absolutely necessary. A rule of thumb is if a partial mock is needed then most probably the design has a flaw. In this specific case, you should mock RocksDB's iterator. For the class under test, you should test `hasNext()`, `next()`, `peekNextKey()` and `close()`, because those are the one exposed (`makeNext()` should actually de declared as `protected`, IMO). ```suggestion final String key1 = "a"; final String key2 = "b"; final String key3 = "c"; final String key4 = "d"; final String value = "value"; final Bytes key1Bytes = Bytes.wrap(key1.getBytes()); final Bytes key2Bytes = Bytes.wrap(key2.getBytes()); final Bytes key3Bytes = Bytes.wrap(key3.getBytes()); final Bytes key4Bytes = Bytes.wrap(key4.getBytes()); final byte[] valueBytes = value.getBytes(); final RocksIterator rocksIterator = mock(RocksIterator.class); rocksIterator.seek(key1Bytes.get()); expect(rocksIterator.isValid()) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(false); expect(rocksIterator.key()) .andReturn(key1Bytes.get()) .andReturn(key2Bytes.get()) .andReturn(key3Bytes.get()) .andReturn(key4Bytes.get()); expect(rocksIterator.value()).andReturn(valueBytes).times(4); rocksIterator.next(); expectLastCall().times(4); replay(rocksIterator); final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( STORE_NAME, rocksIterator, Collections.emptySet(), key1Bytes, key4Bytes, true ); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key1Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key2Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key3Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key4Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(false)); verify(rocksIterator); ```
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
And again with `final` if you don't mind
Make all params `final`
We do not need to mention "partition" here since it is supposed to be abstracted from users, ditto below.
as above nit: double space `to Kafka`
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
This is a breaking change, right? Same for the other `create` method in this class.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
This is not completely compatible with the behavior of older Streams apps. See #10953 for a fix and more details.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
nit: could make access private and get an accessor.
This will probably be subjective, but I'm ok with "hop" for now.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
I really like this class.
same here -- sounds like CachingKeyValue with TimestampStore
I'd suggest rename `ProcessorName` to `GraphName` to be consistent with the base `StreamGraphNode`, also to distinguish with the physical topology's `XXProcessorNode`. Ditto else classes.
Maybe: Include in the log the Connect key, value, and other details of records that resulted in errors and failures.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Do we need to prefix these with `msg.`? Also, it might be nice to control the message format when message keys and values are to be included. I know that's more code, but it could be made to be similar to the other option.
Is there any way to put this into methods on the `ProcessingContext`, like `toString(boolean includeRecord)`? That way, its `toString()` method could really just be `toString(false)`. Not only is it a bit more testable, the logic of this reporter is more readable and focused on writing to the log.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
This field can be final as well.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
this pattern of ` if (shouldRecord) measureLatency(X) else X` is not very DRY. You have this same condition (shouldRecord) in several places, and the code for measureLatency(X) vs X is essentially copy-pasted. Instead, add maybeMeasureLatency(f, sensor) with `if (sensor.shouldRecord()) ... `.
If it is no more an integration test, this should be removed.
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
nit: I don't spot any, but safer to avoid typos by just having constants for these
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage). I am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the `Fetch` and `BeginQuorum` protocol and not from the state machine (Kafka Controller) itself. It is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
nit: remove the `<` and `>` here and next line. Just to keep it consistent with everywhere else we do this
nit: here and the next line remove the `<` and `>` to be consistent
as above. `requireNotNull` not necessary any longer
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
this line is still a bit long... You could try a static import for `singletonList`.
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
this is creative :)
`function` -> `method` ? `{@code null}`
As above. Simplify to: ``` Enable querying of stale state stores, i.e., allow to query active tasks during restore as well as standby tasks. ```
as above -- add check for two missing clients
add check for restore-consumer and admitclient
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Do we actually have to mock `generation()` and `rawConfig()` for this test? Looking at `connector()`, it looks like it only relies on the snapshot.
I don't think we need to mock `generation()` in this test.
Do we need to do this `close` and `open` here? We do it also on lines 283 & 286
If not, we should move the exception capturing logic inside the dbAccessor as well.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
if (comparator.compare(nextNoTimestamp.key.get(), nextWithTimestamp.key.get()) == 0) we need to advance on both ends while only returning the one from with-timestamp iterator, otherwise we may get duplicates returned.
nit: add a size? There are a few cases in here where we could do this.
You might consider using `OptionalDouble`.
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
Seems to fit in one line
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: 4-space indention plus move `builder` down one line
nit: avoid unnecessary `this.` prefix
nit: move this `if` statement below/above version check on line 62
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
Since log.error(.. ex) will print the stack trace already, may be we can save re-throwing the exception again. EDIT: if we want to stop the whole process by throwing the exception, we can then save log.error().
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
nit: how about just put these two in one line? We usually only use multi-lines if there are 3+ parameters.
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: `child` -> `toChild`
Nit: both parameters can be `final`
Nit: can be `final`
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
`deteremined` => `determined`
would be nice if this was a method in the config
Also, topic can never be `null` if it's coming from a parsed config value that doesn't have `null` as its default value. (another way to think of that is that you can't pass a `null` value from properties)
Nit formatting: ```suggestion return WorkerErrantRecordReporter.createAndSetup(adminProps, producerProps, connConfig, keyConverter, valueConverter, headerConverter); ```
Nit formatting: ```suggestion WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter( id, sinkConfig, connectorClass, keyConverter, valueConverter, headerConverter); ```
At a higher level, why are we not reusing the `RetryWithToleranceOperator` here? I thought that was kind of the intent of the KIP, that this new `report(...)` method is just more way to capture problematic records using the existing DLQ functionality. I understand that might require other refactoring of that class (like returning a Future from the produce-like methods), but it seems like it would simplify things substantially by avoiding having to create our own producer and reuse a lot more of the functionality, such as metrics, retry count, logging, using the same producers, etc.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
createMetadataTopic() is no longer used.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
This is assuming that `totalTopics` is always a multiple of `MAX_BATCH_SIZE. Is that always true? Perhaps it is better not to make that assumption in any case.
Fair enough, let's keep it inside StreamThread for now. In a longer term refactoring, maybe we could have an StreamsUtil class where such static functions / fields can be stuffed in.
`Not for this PR`: I think a better place for these static methods is StreamsConfig.
nit: put this in the `Tasks` class with the other metadata
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
This is part of KIP-516
req: This is unnecessary
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
I'm fine as well, will make a reference to 10055 of this PR
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
I see your point, but I do also not see the need for an internal state for which we need to avoid invalidation. Variables `numStandbyReplicas` and `numStandbyReplicas` are configs that can be stored as member fields of `ClientTagAwareStandbyTaskAssignor` or passed along to the methods that need them. Variables `tagKeyToTagValuesMapping`, `clientsPerTagValue`, `standbyTaskClientsByTaskLoad`, and `clientStates` can also be passed to the methods that need them. Avoiding state makes reasoning about code simpler and here it seems possible to avoid state. See `HighAvailabilityTaskAssignor`, it does not have any state.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
nit: missing space between logPrefix and 'found'
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
nit: could you add a logPrefix indicating the threadID
nit: use string interpolation. Also probably good to add the groupId to these messages.
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
Nit: `.` full stop missing.
nit: static is redundant
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Also add `@params topics`
Better be `cooperative-sticky`? `cooperative` is too general I think.
Why replace `null` with `"null"` ? (similar for value)
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
Oh yeah, duh. Nevermind this 
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
No, what I was suggesting is to add synchronization to the methods inside `Heartbeat` itself. For example: ``` public synchronized long timeToNextHeartbeat(long now); ```
I think the original code was unintentional. We changed it in #8702 to the following: ```scala int joinGroupTimeoutMs = Math.max(client.defaultRequestTimeoutMs(), rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE); ```
The previous code handles overflow.
nit: the typical thing to do is invoke the other constructor. e.g. `this(props, true)`
This looks unintentional.
I would remove `will`: `This tool reset[s] offsets` same below
`[because] the tool`
Can you please format this differently. No line should be longer than 120 chars and please start a new line after each sentence (otherwise reviewing is quite cumbersome). Thx
typo: `will does not`
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
Not really related to this line. Could you verify that the state store is closed in the unit test that tests line 148? The name of the test is `shouldThrowStreamsExceptionForOldTopicPartitions()`.
Ah I see, thanks!
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
Should not have an implementation but call overloaded method.
an -> a
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
I think it would be good to include a message giving context before we start listing unresolved issues.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
nit: add `final`
nit: add `final`
Semantically really bad to forward `0x3`, but well, it is what it is.
You need to pass in a `Pattern` but not a `String` here. This actually exposes a "bug" in the test-driver -- it should check if the topic name is valid -- and a pattern is not a valid topic name. Not sure if we should have a different PR for a fix or piggy back on this PR. \cc @guozhangwang @bbejeck @vvcephei
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
@ijuma Sorry, I don't know of a standard way of doing this,
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
The code isn't huge, so by no means a blocker, but @kkonstantine pointed out that the entire block before configuration (and most of config modulo the conditional) is identical between header and "normal" converters. The main difference in the header blocks are just the class referenced (`Converter.class` vs `HeaderConverter.class`). Consolidation would be nice if it's easy to do, but at the same time I'd rather get a fix to the immediate problem in, so definitely wouldn't block on saving 10 lines of duplicated code.
This still doesn't seem correct. We're only invoking configuration if the plugin implements `Configurable` afaict. This does not work for `Converter`s implemented against the new API and assuming the "forward" configuration. We *must* always invoke the "old" `configure(Map, boolean)`, and only invoke the `Configurable` version as a backup. Possibly it would make sense to indicate on the `HeaderConverter` that the `Configurable` methods should be idempotent if we need to be able to implement both. Not sure if we can test this easily with unit tests, but I think we might want a plain old `Converter` (that does not implement `HeaderConverter`) in tests to validate compatibility... but it's possible we'd need either integration or system tests to properly validate.
We don't use ReplicaNotAvailableException in listTopics
We don't use LeaderNotAvailableException in listTopics
We don't use UnknownTopicOrPartitionException in listTopics
Typo: should be "or larger than the number of available brokers"
not used here (InvalidTopicException is used instead)
I think it'd be useful to verify the behavior of casting all of the logical types to strings, just so that we verify the formats (e.g., timestamps, times, and dates should use the ISO 8601 representation) and have some regression tests for the future to help ensure we maintain backward compatibility.
It would be great to have a few more test cases to cover more scenarios: 1. cast to types other than `int32` and `int64` (e.g., `float64` and `float32`), including where we lose precision 2. verify we can cast these to strings 3. verify the cast fails in expected ways 4. casting null values should not fail
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
Is there a reason not to include support for casting `DECIMAL` to other primitives? Yes, this might be lossy, but it also would help in cases where there would be no loss of precision.
Other enums in Kafka tend to use id for this.
I know, but you can save it in a static final.
Other enums in Kafka tend to use id for this.
Another reason for having these classes in common (i.e. KAFKA-5265) is that they can potentially be used by the Authorizer interface when we move it to Java.
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
use `return CODE_TO_VALUE.getOrDefault(code, UNKNOWN)`
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
i think we should just stick with `joiner` for the name of this param. here and elsewhere
@rhauch I don't believe that's the effect the code here has. In the method call: ```java assertEquals( connectorName != null, connectorName != null ? context.startsWith("[" + connectorName) : false ); ``` if `connectorName` is null, then both arguments are guaranteed to evaluate to `false`. I think your intent may have been something like this: ```java assertEquals( connectorName != null, context.startsWith("[" + connectorName) ); ``` which would probably be acceptable, but may also benefit from a `message` that clarifies the expected behavior, possible something like `"Context should begin with connector name if and only if connector is non-null"`
Line too long (also some lines above and further below)
fyi, the pr for non-double stats got merged, so we could switch this back to the original design / fill in the missing string metric
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
only one parameter should be `null` -- otherwise it's unclear what this test actually does
Yes, does not hurt to leave it. Just for sure.
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
I would like to propose the following change to take care of the source consumer group changes ```suggestion for (Map.Entry<TopicPartition, OffsetAndMetadata> convertedEntry : convertedUpstreamOffset.entrySet()) { TopicPartition topicPartition = convertedEntry.getKey(); for (Entry<TopicPartition, OffsetAndMetadata> idleEntry : group.getValue()) { if (idleEntry.getKey() == topicPartition) { long latestDownstreamOffset = idleEntry.getValue().offset(); // if translated offset from upstream is smaller than the current consumer offset // in the target, skip updating the offset for that partition long convertedOffset = convertedUpstreamOffset.get(topicPartition).offset(); if (latestDownstreamOffset >= convertedOffset) { log.trace("latestDownstreamOffset {} is larger than convertedUpstreamOffset {} for " + "TopicPartition {}", latestDownstreamOffset, convertedOffset, topicPartition); continue; } } } offsetToSync.put(convertedEntry.getKey(), convertedUpstreamOffset.get(topicPartition)); ```
Nit: space missing after `for`.
Use diamond (`<>`).
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
What do you mean by "we still waited for the data in the buffer to flush"? The `beginFlush()` method doesn't actually do any flushing; it merely performs the snapshot of the offset writer's data.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
nit: move below the shortcut return below.
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
request "got" re-sent to the control
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
these 2 lines shouldn't be here
nit: I would rather use the full name instead of using acronyms.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
Let's be consistent and just use string concatenation for both fields.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
Yeah that is fine. My bad.
Similarly here, this state check could be internalized.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
The intent was to remove the input partitions from the map any time we remove a task from `tasks`. It looks like your code maintains this (in a clearer and cleaner way).
Ah, then it was my mistake before! Good catch.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
As mentioned above, we can make this constructor default access.
nit: usually we write this like this: ```java this.groupInstanceId = requireNonNull(groupInstanceId, "group.instance.id can't be null"); ```
nit: extra line
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
If you pass the new one, then you can probably get rid of `changedTopicId`
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
nit: missing . at end
nit: missing . at end
maybe: `inputKeySerde` and `inputValSerde`
records to it, and reading all records from it, such that
`KeyValueStore` -> `TimestampedKeyValueStore`
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
nit: I would rather use the full name instead of using acronyms.
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
Nit: var should be named `deserializeValue`
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
may be use Objects.requireNonNull
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Just a note. This may need consideration together with #1707 where the metadata age may subject to change during producer startup.
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
`tp` is not used anymore.
Should this be `num_lines=3` (cf. L116 and L126)
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Not for this patch, but we should do a KIP to add support for batch topic creation.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
Cool, I was kind of hoping you would put this in a separate integration test class
Why set this? zero is the default anyway
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
nit: unintentional? one more in `subscription`
Effectively what we are doing is sorting the assignments for each partition using the generation and picking the latest two for the current and previous assignments. Is that right? Could we simplify the logic by building a `SortedMap<Integer, String>` for each partition where the key is the generation and the value is the memberID? Then the current assignment would be the last entry and the previous assignment would be the one prior.
Effectively what we are doing is sorting the assignments for each partition using the generation and picking the latest two for the current and previous assignments. Is that right? Could we simplify the logic by building a `SortedMap<Integer, String>` for each partition where the key is the generation and the value is the memberID? Then the current assignment would be the last entry and the previous assignment would be the one prior.
Small remark: By throwing here an exception you are giving up. But I am wondering if you should not try to fix it. I do not know which consistency guarantees a kafka consumer group exactly gives. But if it is possible to get into a split brain situation (network split), where temporarily a consumer group is split in two and gets two leaders. You get in a situation where the `SitckyAssignor` will never recover. If it was me, I would write a big fat error, and drop one of the two assignments. The same for line#322.
You can probably simplify this using `computeIfAbsent`.
This seems unnecessary since we're throwing away the collection anyway.
nit: unintentional? one more in `subscription`
This class is public API, so we cannot remove `setTimestamp` but can only deprecate it. We also need to update the KIP to mention the deprecation and the newly added methods.
nit. I think there is `.` missing `since 3.0[.] Use`
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
nit: break line
req: `subscription` -> `subscription info`
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
nit: remove newlines
nit: I'd suggest "Ignoring the fetched committed offset"
This was probably unintentionally reduced to debug level while fixing conflicts.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
nit: move below the shortcut return below.
nit: it's a small thing, but the assertion failure message is more useful if we use the `Errors` type. ```java assertEquals(Errors.NONE, Errors.forCode(createTopicsResponseData.topics().find("foo").errorCode())); ```
Perhaps we can use `Uuid.randomUuid`? It's a little weird for all brokers to have the same incarnationId.
Is there a point to setting `min.insync.replicas` in this test? I am wondering why we don't just reuse the original create request.
You can call `fail` directly: `fail("Fencing of brokers did not process within expected time");`
Maybe you can replace this `while` loop with a util function `TestUtils.waitForCondition`. Ex: `TestUtils.waitForCondition(() -> fenceBrokers(), 3000, "Fencing of brokers did not process within expected time");` Just that it only accepted the `timeout` value, not the `waitIterations` like you did. FYI
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
You could probably create the `StreamsConfig` in an `@Before`
nit: i don't mind if you use `final` or not in the methods, but it would be good to be consistent. Same in other tests
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
this will never be called if one of the assertions fails
```suggestion final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore("aggregated", ofMillis(1200L), ofMillis(100L), false); ```
nit: as in `position` below, `this` is not required
nit: as in `position` above, `this` is not required
For a nice example where caps make sense see right below, where two sentences are included.
nit: as in `position` above, `this` is not required
nit: in kafka we usually don't use `get`/`set` prefixes
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
I now saw that in the consumer tests you use `Duration.ofSeconds(1).toMillis()` and `Duration.ofMillis(999).toNanos()`. This makes it already clearer. I think a variable with a meaningful name for the lower bound would make it even clearer.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
I'm not sure this is a good idea. If we're unlucky, the partition we're interested in may not be listed. Since this is an exceptional case anyway, I would suggest using the more verbose message.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
nit: drop reference to consumer groups
Not something we have to do here, but one way we could improve this in the future is by taking into account leader epoch information from individual partitions. We can ensure that epochs increase monotonically in order to prevent using stale information during retry. Another thing we could do is reduce the topics we are fetching metadata for as the ListOffsets requests complete. Ideally we'd only be refetching metadata for topics with metadata errors.
There are some reason to move from varargs to Collections? It's breaking a lot of projects.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
We need to keep this public method and deprecate. Perhaps throw an exception if multiple group ids were specified and retain existing behaviour for single group id.
nice fix! this has been bothering me.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
No worries, let's keep the scope small for now. Just wanted to raise the question
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
Yeah, it seems to me like we should remove it.
nit: could be useful to log the type of exception in the assertion message.
I think this is the issue you reported in the Jira. The `RaftClient.Listener` should not use `RaftClient.leaderAndEpoch` to determine if it is the leader. It should instead use `RaftClient.Listener.handleLeaderChange`. For this state machine `ReplicatedCounter` we should look at the `claimedEpoch` variable. I am going to create an issue to remove this method. cc @hachikuji
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
Not really sure this has value if the test case expects the leader change correctly.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
nit: braces unneeded
For some test cases we may want to use the same `MockTime` object on both server and client, for example in window store changelog truncation (cc @dguy ). So instead of creating the object internally we may want to pass it through parameters.
We should update the Scala `TestUtils` to call this method.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
I think it was my suggestion to add it. I was going off the pattern for tagged node metrics in Selector.
We can use the `replace` overload that takes a `char`.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Is this used anywhere? I see we have changed client code to use the other C'tor.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
Out of curiosity, why do we do this here? In the normal case, we only update the position of the duplicate buffer, it seems.
Ah, yes, the magic is hardcoded here.
However, the ```@RecordBuilderSource(haveInvalidCompress = true)``` is a inscrutable to me :(
What we have here is not exactly what I had intended. I have to take a closer look, but just wanted to share that.
My point was ```haveInvalidCompress``` is hard to understand which compression is invalid. In current test cases, only zstd is possible to be excluded, right? If so, the name like "NotZstd" is more readable to me.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Yeah good catch, see above
Very good catch. Thanks, @abbccdda .
nit: newline for better IDE debugging.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
> most recent max timestamp Huh? I think I know what you're trying to say here but it seems like two different phrases got a bit mixed up here
Oh good point, we definitely need the key. But I think separating them turned out well
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
How about `return admin.endOffsets(assignment);`
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
given that the previous messages say "Reading to ..." maybe it would make sense to say: ```suggestion log.trace("Read to end offset {} for {}", endOffset, topicPartition); ```
When I suggested it, I thought we could do a bit better, maybe something like `(id: 5, www.example.com:9123)`, but maybe that's actually worse.
I was mostly trying to get rid of the word `Node` because it's a bit redundant when you look at the log messages.
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
I think `nanoseconds` should be `milliseconds`. It's a little more idiomatic to do it like this: ```java time.sleep(autoCommitIntervalMs); coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); ```
nit: could be useful to log the type of exception in the assertion message.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
nit: move to line above.
req: typo unknown Pid
nit: remove empty line
nit: add `final` (2x)
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
nit: move below the shortcut return below.
nit: remove -- not used
not used: can be removed
nit: remove (was tested already)
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
Ditto here, we can use AssertionError
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
Double-brace initialization is an anti-pattern. It would be preferable to use `mkProperties`.
rewrite test as above using `assertThrows()`.
There's one extra thing to do. We should set `minTime = MAX` before opening the `store.all()` so it resets the minimum in case there are no records available in the iterator. This is an example I run: I have a few records in the shared state store (1,5,7). Then a new record arrives that expire all the 3 records. Record 50 for instance. For each record, the minTime will be set to 1, then 5, then 7. Now for every new record after 50 that is still part of the window, the condition at the beginning of this method `minTime > maxStreamTime - ...` will be false, thus opening the iterator again and again. If we reset the minTime to MAX, then the next time, the iterator will be opened, but no records will be available, so minTime will stay in MAX. And the future records that do not expire will not open the iterator because `minTime (MAX) >= maxObservedTime - ...`
Why adding both joinAfterMs and joinBeforeMs? The records expire when `window().start() + joinAfterMs + joinGraceMs` are lower than maxStreamTime. For instance, say we have a record in the shared state store with time = 1. Now a new record arrives with time = 17. ``` inputRecordTime = 17 maxObservedTime = 17 minTime = 1 window = 10 (beforeMs = 10, afterMs = 10) grace = 5 ``` Isn't the record 1 suppose to expire and be emitted because 1 + 10 (afterMs) + 5 (grace) = 16? which is lower than maxStreamTime? With the condition you have, the minTime registered is 1, so `(1 >= 17 - 10 - 10 - 5)` is true, and thus it returns and do not emit the record 1 until another 10 ms has passed.
I think we should use the same name here; the metrics-scope would differentiate rocksdb from in-memory, which is sufficient.
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
The other constructor calls the parameter `sampledStat`. We should be consistent.
Nit: should we call this `rateUnit`? Same for the other constructor.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
Not for this patch, but we should do a KIP to add support for batch topic creation.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
Checked with Jun and this is fine.
Do we need to set OP_READ? It seems it's always on.
I believe this is fixed in my next PR.
Why do we need this? This is logged anyway.
`setNeedsCommit` -> `{@link #setNeedsCommit}`
`needCommit` -> `needsCommit`
nit: `addMetadata` -> `put`
While you are there, can we fix this one? it is " Running at : " + miniKdc.getHost() + ":" + - miniKdc.getPort()
`than` -> `that`
nit: than -> that
nit: add `a {@link Named} config`
`windowSize` should be `Duration`
and -> a
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
this test doesn't seem to throw `InterruptedException` as well
This type is not parameterized. It's generally better to list the parameters when you reference a parameterized type.
Oh right, forgot that it doesn't have the window times either. Nevermind then
@mjsax I think I'm sold on your arguments, let's keep them as WARN then :)
Good point, thanks!
nit: additional new line
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
How about setting initial size of `records`? `new ArrayList<>(ids.size())`
I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.
In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.
Currently, the follower never removes the leader out of ISR. So, perhaps we should just throw an exception if this is not the case.
We probably should name this sth like removeFromIsrAndMaybeChooseLeader.
We should check `stream` parameter is not null here.
could move line 368 above this and then declare this as: `String [] parentNames = {this.name, streamImpl.name}`
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: remove unnecessary `this`.
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Use diamond (`<>`).
Nit: space missing after `for`.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
We should also mention somewhere that we do not support concurrent transactions.
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Here too it seems like we can use the generic version of `prepareOffsetCommitResponse`.
Ideally, we'd always use brackets on control flow operators.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
Since this is basically just a pass-through method anyway, there isn't that much to test here -- you could simplify this test just to use a simple set of connectors you create yourself. There's a lot of code in this test when all you really want to see is that the set makes it back out of the call to ConnectorPluginsResource
It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in `RebalanceSourceConnectorsIntegrationTest` or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
We can drop the parenthesis here. Same below
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Ok. Thanks for clarifying.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
Nit: let's avoid unrelated line additions.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
I guess we could pull this into the `partitionsAutoAssigned` block.
We have three `remainingTimeAtLeastZero` functions, in AbstractCoordinator, ConsumerCoordinator and KafkaConsumer. Is it intentional? If not we could leave just one to avoid unintentional code divergence in the future. cc @vvcephei
Thanks for the explanation. A bit subtle as you had said. :)
Forgive the bikeshedding, but I was curious why this was located here instead of instead `KafkaConsumer.pollOnce`.
The extra parameter seems a little more annoying than the effort it saves (which is really just checking a couple flags). If we really wanted to avoid the redundant checks, maybe a better way is to add a private `doAutoCommitAsync` or something like that, which simply sends the request assuming the coordinator is known and autocommit is enabled. Then the two `maybeAutoCommit` calls could delegate to `doAutoCommitAsync` after doing the checks themselves.
nit: should be `named` can't be null
nit: add `final`
nit: 4-space indention plus move `builder` down one line
nit: missing `<p>` for new paragraph
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
Thanks for the explanation. A bit subtle as you had said. :)
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Would it be more intuitive to put this logic in AdminClientRunnable.makeMetadataCall.handleResponse()? This allows us to reorder calls in `callsToSend` only when the MetadataResponse has no error. And we don't have to check timestamp to know whether the metadata has been updated.
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
Similar to above we should rename this.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
nit: ".. select the grouping key and the value to be aggregated".
remove "on a window basis"
remove "on a window basis"
This looks unintentional.
nit: extra line
This only works if the parent dir of generationDir exists. Try using generationDir.mkdirs().
nit: move `logContext` to its own line
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
It is probably cleaner to have an explicit `EXPIRED` state.
nit: 'else' can be dropped
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
How about `return admin.endOffsets(assignment);`
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
can we move this down such that it is the last line? Again, just making sure that the store has been initialized before we put it into the map.
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
nit: maybe iterate over `entrySet()` instead.
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
nit: add `final
prop: Would it make sense to also have an overload with just a flat list, i.e., ``` void runTestWithDriver(final List<TestRecord<Long, String>> expectedResult) ``` Maybe it would simplify the code of some of the tests. Hopefully, you can share some of the code in the overloads.
nit: Add `.` at the end of the sentence.
nit: I would move this one next to `consumeInBatches` as they are used together.
nit: As we don't reuse `batch`, we could directly pass `list.subList(batchStartIndex, batchEndIndex)` to `accept`.
nit: This empty line could be removed.
nit: `--topic` and `--partition` could be extracted as helper static functions
Similarly `Consumed#toString` is not implemented either, we can remove this line.
Just for reference: fixed via https://github.com/apache/kafka/pull/5588
Good catch. I think you're right.
oh, yeah, it's not a bug, just a little confusing. I neglected to say that.
Do we really need to print `super.toString`? Ditto above.
We can return `false` on the first mismatch no need to check the rest of the arrays.
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
I was checking the code in the protobuf repository and it turns out that the optimised version of this code is pretty complex: https://github.com/google/protobuf/blob/master/java/core/src/main/java/com/google/protobuf/CodedInputStream.java#L2473 I think it makes sense to start with the simple implementation, but we should file a JIRA for considering the optimisations in their implementation.
I could not find any unit tests for this method.
with in => within
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
nit: extra spaces after the `->`
```suggestion public void shouldDropWindowsOutsideOfRetention() { ```
```suggestion final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore("aggregated", ofMillis(1200L), ofMillis(100L), false); ```
Can we insert one that's like right on the border of the retention period? So if the streamtime at the end is 2,000 then the window cut off is 800 (or start time of 700), and verify that anything starting before 699 is gone and everything after that is there.
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
Actually a more general question is that for assign(), is checking subscription.isEmpty() sufficient or not. Today we allow subscribe(empty_list) whose semantics is different from unsubscribe(), but they will leave the same state in subscription map.
I think the semantics of subscribe(empty-list) should be similar to pause(all), but leave the consumer still registered as member of the group with coordinator; for unsubscribe() the consumer means "do not talk to coordinator anymore" and moving forward we may add a leave-group request (there is already a ticket I think). As for now let's keep the original approach to check the assignment upon each commit call; thoughts? @hachikuji @onurkaraman
nit: remove newlines
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Nit: too many blank lines.
It's intentional to avoid build warnings about importing deprecated classes.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
Hi, may I ask why do you do `@link` instead of `@see` annotations? :)
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
nit: missing space between logPrefix and 'found'
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
The additional validation doesn't hurt, but it might be more natural to move this check into the `if` below since we don't actually cast unless the condition is true.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
an -> a
records to it, and reading all records from it, such that
To get rid of the test failure, you need to change this: ```suggestion final KafkaMetric metric = metrics.metric(new MetricName("prefix-scan-rate", STORE_LEVEL_GROUP, "", tags)); ``` Sorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.
This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.
nit: Please use 4 instead of 8 spaces indentation.
This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.
I do not think you need to put an entry if you use mocks.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
This may be useful, since the log messages in TransformationChain do not print the schemas in `ConnectRecord`
Thanks @ewencp! Glad I'm still up-to-date on that. Happy to adjust per project and yes, unnecessary diffs are better to be skipped in non-cleanup/non-refactoring PRs. Huge fan of that.
the changes to the optimizer code LGTM
@ijuma I looked into this in more detail. We don't need a new type `R` -- instead, we should update to ``` public class KTableKTableJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K, Change<V1>, Change<V2>, Change<VR>> ``` and use `private final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal;` (ie, `VR` instead of `R`. (Note, the you always pass `<..., Change<X>, X>` in the current PR, what is redundant and can be avoided.)
as mentioned above: remove this overlaod
This part and the line 525-529 below can be extracted out of if condition.
either move `this` to next line, or fix indention.
I see what you intended. Thanks for the response.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
I think people want to use a global store in DSL, too. And forcing people to call `build()` to add one, is not a good idea IMHO. (cf option (2)). Also, if you argue like this, we could remove `addStore`, too, because people can also add the store via `builder.builder().addStore()` -- however, in KIP-120 discussion, it was explicitly requested to add both methods to `StreamsBuilder` to avoid this pattern. Also note, people who are new and want to use a stateful process() will always ask: how can I add a store? There is not API on `StreamsBuilder`.
For option 2 seems a little awkward IMHO, I second what @mjsax says.
I think in DSL, users may still wants to access a global state store in `process/transform`, that was the motivation for adding this API. Personally I'd vote for option 2) above, to keep the APIs succinct without semi-duplicated calls. But seems you all prefer option 1) in terms of user convenience, so I'm fine with that as well.
as above nit: double space `to Kafka`
Typo: "you can create [a] windowed ..."
same question as above about moving this above the call to `configure`
`HeaderConverter` and this method don't exist prior to AK 1.1.
yeah, that's just about internal code being readable and understandable, not critical to current issue.
In fact, now I am wondering if we should rename the enums to be clearer. We can follow up on that separately, but I realize these branches aren't really very clear currently.
Shouldn't this be in the contract of `Utils.newInstance` to not return some other class that doesn't match? I think this is pulled from `AbstractConfig` which makes sense for consistency, but I don't get why `Utils.newInstance` would ever return a value with an invalid type.
Maybe use a semicolon instead: "task failure; 'all' changes..."
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
I think the risk of introducing `options()` is that some developers might accidentally use `values()`. The pattern used in `ConnectorType` is far better, as it overrides the `toString()` method. That doesn't handle the case-independence for parsing, though `ConverterType` is a better pattern to follow if that's required. Let's be consistent with the new enums, and have each follow one of those two patterns depending upon whether parsing case-independently is required.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
Java doc for lifecycleListener.
nit: seems unnecessary? similar for `State.FAILED` below.
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
add `final` twice
nit: `final` params
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
Actually, WDYT about adding this class in the "add configs" PR and then rebasing this PR on top of that? Then I could do the same (since I need this class in my next PR as well)
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
It looks like the primary purpose of this logic is to compute the list of clients, given the `statefulTasksToRankedClients`. Let's instead just make the set of clients an input to the assignment method instead. I realize it's not in the method signature I specified in the KIP, but then again, that was just algorithm pseudocode. This method isn't used anywhere (yet), so let's take advantage and just assume we'll be passed everything we need in the most convenient format for us.
```suggestion public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates, final Set<TaskId> allTasks, final Set<TaskId> statefulTasks, final AssignmentConfigs configs) { ```
This is an interesting question. One low-fi solution would be to think about using `equals()` (I think to pull this off, we'd need to introduce a requirement that serde/serializer/deserializers implement equals in a way that would be semantically sound for us. This would not be a back-ward compatible change. On the other hand, since callers actually subclass `Serde<T>` with a fixed type like `Serde<String>`, it actually should be available at runtime. I don't remember the hoops you have to jump through to get it right now, but I'll revisit it tomorrow.
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
Nit: remove unnecessary `this`.
nit: is it necessary for triggering a warning? My personal preference is that if a single line is not too long, then it is okay to do so.
nit: 4-space indention plus move `builder` down one line
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
nit: add `{ }`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit: use static imports to get rid of `Assert.`
Please remove empty lines here and in the other test methods.
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
Atm we cannot say for sure, but it's likely that what's observed on KAFKA-9701 is a broker-side issue; we can either 1) add the check on broker-side across all members to make sure the selected protocol is consistent for everyone, so if the broker already made a wrong choice itself would log an ERROR, or 2) let it check on the client side. I think for trouble-shooting purposes option 2) is fine, and if we later discovered that this bug is actually on the client side I'm happy to revert this change after fixing it.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
nit: Could just to `new ArrayList<>();`
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
Yeah might as well change it I think, it results in shorter code
No need to pass the spec in the constructor here, as all the connections have access to the internal spec.
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
Changing old version encoding would break our upgrade path, ie, all existing `encodeVersionX()` methods cannot be modified.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
nit: should be removed (similar below)
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
I felt this refactoring is a bit messy compared to its benefits, so how about: ``` OutputStream baos = new ByteArrayOutputStream(); // first write the version as byte array directly before switch baos.write(version); // then wrap with compression if it is newer version if (version >= 5) { baos = new GZIPOutputStream(baos) } try (final DataOutputStream out = new DataOutputStream(baos)) { switch (usedVersion) { // ... } } ```
nit: in kafka we usually don't use `get`/`set` prefixes
For a nice example where caps make sense see right below, where two sentences are included.
nit: as in `position` below, `this` is not required
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
doesn't look a great name for its behavior. perhaps something like currentContext
I'd err on the side of not adding configs for now and only add them if we find a need, so this seems fine.
I wonder if you also considered moving startup and shutdown into the `Worker`? The advantage is that we already have an executor there and then we'd get the parallel implementation for `StandaloneHerder` as well, though admittedly that may not matter as much.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
This method could be replaced with the use of a Junit Rule and TemporaryFolder, i.e., `@Rule public final TemporaryFolder folder = new TemporaryFolder() ... logDir = folder.newFolder() `
This is unused too
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
Not sure if it makes sense to convert a `TimeoutException` into a `TaskMigrated` exception... Also note that we created https://issues.apache.org/jira/browse/KAFKA-7932, so it might be best to tackle all Streams related changes there? \cc @vvcephei
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
The error message of `RecoverableClientException` does not match any longer... (I would recommend to _not_ add the error name to begin with, as the root cause exception is pass into the constructor anyway)
Moving forward if we managed to make producer logic inside RecordCollector only then we do not need to capture those "raw" exceptions on the stream thread but only task-migrated exception.
The fact that we're ignoring it suggests that it's not an `error`. For something that indicates something unexpected has occurred, but that we can handle it and continue correct execution, we should use `warn` level. Also, the type of exception will be captured in the stacktrace, so I would just say something more descriptive like, "Channel closed unexpectedly before lock release."
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
Updated this when merging.
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L149 can reference to this field
I tweaked this a little before merging.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
Don't need `Vin` here
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Don't need `Vin` and `W extends Window` here
Can we call this something like `ensureCopartitioning` or `processRepartitions` or something? My take is that the copartitioning is the main point of this method so that's probably good to include in the name
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
nit: the 'else' can be omitted.
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
Nit: `describeTopics(Collection<String> topicNames, ...)`
not used here (InvalidTopicException is used instead)
We should emphasize that this partitioner should be stateless as it can be shared across topics / sink nodes.
You can also do this more concisely as `topicsByName.keySet().removeAll(existingTopicNames)`.
All the `null` checks at each layer of the call stack make me think that particular issue might be better handled with an exception. Not critical since this is all internal code, but seems like then we'd only need to check version compatibility in one or two places.
Nit: might be worth adding a simple assertion on the result just to make sure.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
nit: Empty line could be removed.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
Yes, this seems fine then.
Shouldn't need this line, it's handled by the superclass's constructor.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
nit: fill in `@return` docs
nit: parameters on a separate line
either move `this` to next line, or fix indention.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
i think we should just stick with `joiner` for the name of this param. here and elsewhere
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
exception can be improved a bit - "failed to flush within X ms, successfully completed Y/Z batches". wuold help distinguish between slow connection and no connection.
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
typo: CompleteableFuture -> CompletableFuture
We should spell out that if the string is empty, it's because the remote end didn't send this information
Would `isUnknown` be clearer? I find that boolean methods without any prefix feel a bit ambiguous when reading them.
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
Do we need quotaType? It seems that it's unrelated to metric tags.
Hmm, we should probably specify that we can change the output of this String (i.e. parsing may break). I would have preferred if we didn't expose this as a public method and had a utility for it.
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
Hmm, this doesn't need to block merging this, but we should think carefully about doing delay this way. The rest of Connect avoids trying to rely on Java's `interrupt` behavior because it's not really a reliable way to *actually* interrupt threads, and in a system where there are pluggable components that are allowed to block indefinitely, relying on functionality that most Java developers don't understand well probably isn't going to work all that well. It may not have actually gotten to a KIP, but there was at least some discussion on a JIRA somewhere about making connect perform interrupts in addition to the basic task `stop()` calls it already does, but it doesn't currently do this. For anything that can end up with pretty long sleep periods, we should try to make sure there's a good way of interrupting it and moving on (e.g. so rebalances wouldn't get delayed because there's a connector that's encountering errors). At a minimum, since we don't do interrupts currently, I think we wouldn't interrupt this code currently. The other approach we use elsewhere is to `wait` on a monitor so we can set a flag and interrupt with `notify` and have it bail out immediately.
nit: we could define this transition list in a variable to be reused.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
We lack unit test coverage for this case
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
remove "on a window basis"
"top-k records" -> "top-k values"
remove "on a window basis"
nit: ".. select the grouping key and the value to be aggregated".
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
We can make this clearer if it helps. Maybe something like FullFetch? Not sure if there is a better option
These should be non-empty checks. Thanks for catching.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Thanks for verifying @vvcephei!
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
I think we can move this logic into ValueOrOtherValue as another static constructor.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
@spena just ping to make sure you get this on the follow-up PR.
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
It's a bit awkward to modify `onJoinPrepareAsyncCommitFuture` inside the `maybeAutoCommitOffsetsAsync` function since the function name itself indicate a general purpose, but specifically for join-prepare --- though I understand today it is indeed only used for that caller. How about letting the `maybeAutoCommitOffsetsAsync` to return the future instead of the boolean, and then let the caller a.k.a. the `onJoinPrepare` today to check if the future is completed or not.
I cannot understand this logic clearly.. my original thought is that, we do not reference the `onJoinPrepareAsyncCommitFuture` here at all, just create a future and return to the caller.
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
nit: additional new line
Do you mean it should NOT be included...
This message should say "Consumers earlier than 0.10.1.0..." since KIP-74 was part of 0.10.1.0.0.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
We can use `assertThrows` for this kind of pattern: ```java RecordDeserializationException rde = assertThrows(RecordDeserializationException.class, () -> consumer.poll(Duration.ZERO)); assertEquals(invalidRecordOffset, rde.offset()); assertEquals(tp0, rde.partition()); ```
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
No worries, let's keep the scope small for now. Just wanted to raise the question
The intent was to remove the input partitions from the map any time we remove a task from `tasks`. It looks like your code maintains this (in a clearer and cleaner way).
Ah, then it was my mistake before! Good catch.
Similarly here, this state check could be internalized.
Nice one on projecting the value!
It is wall clock indeed.
I'd suggest we put the time span as 4 days to be on the safer side, the 7 day log retention may kicks in a bit earlier.
`counts` is not used
nit: java style
Double.valueOf not required, auto-boxing can be used. Here and in the lines below.
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
Harsha did this.
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
nit: add `final`
We still need to add a default implementation which calls `onAssignment` without the generation. Then we can drop the override in `AbstractPartitionAssignor`.
This name seems backwards.
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This could be more concise: ``` topicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment); ```
Oh, indeed, I missed that. And in this case you'll need to have an extra line with put.
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
recommended; ditto below.
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
an -> a
`while` seems to be missing
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
The other constructor calls the parameter `sampledStat`. We should be consistent.
That's a good idea. Note: Kafka does not use this JUnit functionality yet (i.e. no use of ExternalResource, ClassRule, Rule as far as I can tell). @ijuma: Would it ok for us to introduce this? There's no additional dependency etc., it's just using a new JUnit feature that was introduced in 4.7 (we're on 4.12).
Seems that this class is a bit redundant, i.e, we could just construct an `AssignedTasks` with the `logContext` and `"standby task"`
nit: an object -> an object has associated partition offsets that can be ...
There is a built-in for this `Function.identity()`
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
do we need this invalid config step here
```suggestion waitForCondition( this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each." ); ```
`final` and initialize in constructor instead? Doesn't seem to depend on the config at all.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
This is an internal class -- no need to mark as deprecated.
nit: add `final`
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
Not sure if it makes a big difference, but we could use EnumSet for these.
Can just return `name.startsWith(acl.resourceName())`
nit: need to update this since resource name is before pattern type now
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
nit: `final` is redundant for private static method
Nit: param alignment.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
nit: `java.util.` can be removed (note, we only specify the whole package for `Topology`, because otherwise we would need to add an `import` statement and get a warning about "unused imports" and a failing build.
It's intentional to avoid build warnings about importing deprecated classes.
recommended; ditto below.
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
nit: remove empty link
Again, `ConfigProviders` doesn't make sense here. The `config.providers` property is listing the _names_ of the ConfigProviders, whereas the other `config.providers.<providerName>.class` (for example) specifies the name of the `ConfigProvider` implementation class for the named provider. IMO, we should use the term `ConfigProvider` to mean one of two things: 1. The name of the `ConfigProvider` interface 2. The instances of the `ConfigProvider` implementations that are instantiated by this class. Always using "instances" in these cases will help disambiguate the use of `ConfigProvider`. Also, it's probably worthwhile to begin this paragraph with: > The "{@code config.providers}" configuration property and all configuration properties that begin with the "{@code config.providers.}" prefix are reserved. The "{@code config.providers}" configuration property specifies the names of the config providers, and properties that begin with the "{@code config.providers.<providerName>.}" prefix correspond to the properties for that named provider. For example, the "{@code config.providers.<providerName>.class}" property specifies the name of the {@link ConfigProvider} implementation class that should be used for the provider.
We should specify whether the keys in this map have the `config.providers.` prefix, like they would in the `originals`.
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
We pass this map to `resolveConfigVariables`, which does: ``` providerConfigString = (Map<String, String>) configProviderProps; ``` That is unsafe if we are supporting props that are not actually a map of strings.
unnecessary type in constructor, can use `new HashMap<>()`
Thanks for clarifying this.
nit: I would delete these two line
Let's tweak this API to Throwable: ```suggestion StreamsUncaughtExceptionHandler.StreamsUncaughtExceptionHandlerResponse handle(final Throwable exception); ``` Here's a good explanation of why: https://stackoverflow.com/questions/2274102/difference-between-using-throwable-and-exception-in-a-try-catch The benefit is that we could handle `Error`s as well as `Exception`s. However, this comes with the obligation that we should not continue to use the thread after an Error occurs. I think we can deal with this restriction reasonably.
nit `Production exception handler` -> `{@code ProductionExceptionHandler}`
nit: move parameter to it's own line (same below)
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
nit: we could use mkMap helper here as well.
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
nit: move below the shortcut return below.
You should use try with resources here too.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
This and the other deserialize method have a lot of duplicated code. We could have a method that takes an InputStream with the shared logic.
Nitpick: I'd call this `deserialize`.
We should read the metadata inside the while loop since it could change.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
> Unfortunately I don't think we can shutdown a thread until we have started it. Have a look at https://github.com/apache/kafka/blob/aeeb7b2f9a9abe8f49543a2278757722e5974cb3/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L976-L983
Ditto on removing these before/after methods.
Ditto on removing before/after
It's intentional to avoid build warnings about importing deprecated classes.
To close out the earlier thread.. This test is okay, since `NOT_RUNNING` will make that instance go to DEAD state (or some non functional state like that) where the store cannot be obtained.. the lines below check that we can stil retrieve the keys from the other replica
nit: remove empty line
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
please add `final` for parameter and wherever else possible (i.e., also for vars within this method, and all other classes you add in this PR)
Nit: remove `this` (we try to avoid `this` wherever possible)
nit: is it necessary for triggering a warning? My personal preference is that if a single line is not too long, then it is okay to do so.
`no longer treated as an updated record` -> c&p error; it was a "fact/event" and is re-interpreted as update now.
For transition to `NOT_RUNNING`: the instance will only shutdown if the user uncaught exception handler decides to shutdown the whole instance, by calling `close()`, in this case it will still go through the `PENDING_SHUTDOWN` transition first? For `REBALANCE -> REBALANCE`, this is related to the thread-level `partition revoked -> partition revoked`, which I'm still wondering if we can avoid. Let's sync a bit on that.
Did we mean to swap `REBALANCING` and `RUNNING` around? If people were depending on the `ordinal` then this will break them
Again, could you describe: 1) Running -> Running 2) Partition Revoked -> Partition Revoked 3) Partition Revoked -> Dead 4) Assigning Partitions -> Dead
I'd probably pass this in via the ctor. `setStateListener` is always immediately invoked after construction so might as well just add the param to the ctor and do away with this method
Answering here the question about name changes. This is probably the best example, it has been renamed a couple of times: `ConsumerCoordinatorRequest` -> `GroupCoordinatorRequest` -> `FindCoordinatorRequest`. Also, I'd like to rename `Produce` to `ProduceRecords` and `Fetch` to `FetchRecords` so that all protocol APIs are consistent.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
As we mentioned in the ticket, the only test case that requires the underlying `metrics` object is `testMetrics`. For this purpose we should just rewrite this test function, to not reuse the class field `task` which is relying on the `MockStreamsMetrics`, but create another task that does pass in a real `StreamsMetricsImpl`.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
You mean `now`? :) If yes please feel free to resolve the ticket when you merge this.
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
@ijuma Sorry, I don't know of a standard way of doing this,
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Need to check if `group` is `null` in both `k1` and `k2`. Using this on, e.g., `DistributedConfig` from Kafka Connect doesn't currently work.
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
```suggestion public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler uncaughtExceptionHandler) { ``` We prefer to resist the urge to abbreviate, especially in the public-facing APIs.
Oh, sure. Now I know why you picked this name :)
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
I do not think we need to emulate the behavior of the java uncaught exception handler here. I do not see why a user should try to reset the uncaught exception handler. If we receive this requirement, we can still add it. I have the impression the "reset" makes this code unnecessarily complex.
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nit: fix line break
The typo is still there.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
This looks unintentional.
nit: I don't feel strong about the style here, but maybe we should consider align with other functions which has the first parameter on the same line with function name.
The previous approach was intended to avoid having this logic in many places.
Yeah, it's a bit error prone to have that logic in every constructor. We could move the `propsToMap` method to a utility class and use it on the consumer too.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Make sense, and that's also what I've seen. Thanks for confirmation!
Looks like it won't happen since we only lock on deque object, but just want to confirm, to make sure it won't break anything.
Moving `close` outside of locked scope LGTM
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nit: fix line break
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
Again, `ConfigProviders` doesn't make sense here. The `config.providers` property is listing the _names_ of the ConfigProviders, whereas the other `config.providers.<providerName>.class` (for example) specifies the name of the `ConfigProvider` implementation class for the named provider. IMO, we should use the term `ConfigProvider` to mean one of two things: 1. The name of the `ConfigProvider` interface 2. The instances of the `ConfigProvider` implementations that are instantiated by this class. Always using "instances" in these cases will help disambiguate the use of `ConfigProvider`. Also, it's probably worthwhile to begin this paragraph with: > The "{@code config.providers}" configuration property and all configuration properties that begin with the "{@code config.providers.}" prefix are reserved. The "{@code config.providers}" configuration property specifies the names of the config providers, and properties that begin with the "{@code config.providers.<providerName>.}" prefix correspond to the properties for that named provider. For example, the "{@code config.providers.<providerName>.class}" property specifies the name of the {@link ConfigProvider} implementation class that should be used for the provider.
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
We should specify whether the keys in this map have the `config.providers.` prefix, like they would in the `originals`.
We pass this map to `resolveConfigVariables`, which does: ``` providerConfigString = (Map<String, String>) configProviderProps; ``` That is unsafe if we are supporting props that are not actually a map of strings.
AK convention is to not use `set` setters or `get` getters.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
```suggestion return futures.stream().allMatch(Future::isDone); ```
Yeah, that works, too, and is more align with the current code.
Testing against log message is error-prone and hard to maintain, I think just making sure the thrown exception type is expected should be sufficient.
If you decide to still log the condition, we could rehabilitate this test to check for the log.
this doesn't seem to be used
this test doesn't seem to throw `InterruptedException` as well
You could do `Assert.fail(...)` here rather than tracking it in a boolean etc
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
nit: use "{}.x." vs. string concatenation
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
nit: A better general pattern is to use `assertEquals` comparing against empty list. Then if the assertion fails, the message will show what was in the collection.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
as above `final` and one parameter per line
nit: add `final` to parameters / reformat one parameter per line
nit: remove (was tested already)
nit: `child` -> `toChild`
`needCommit` -> `needsCommit`
nit: empty line.
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
The logic here is a bit over-complicated to me, can we simplify to sth like standard sort-merge here? We have e.g. `AbstractMergedSortedCacheStoreIterator` for a similar pattern.
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
rewrite test as above using `assertThrows()`.
It seems to be added on line 703.
Oh you mean `UnsupportedForMessageFormatException`? That doesn't seem to be added.
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
returned => will be returned
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
We could port this function when it is actually needed.
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
Thanks for verifying @vvcephei!
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
May as well add `topicName` to the message so that the user knows which case they've hit? Same for the others.
Another way to write this, that reduces a couple lines of code would be: ```java if (allTopics.remove(topicName) == null) { future.completeExceptionally(new UnknownTopicOrPartitionException(String.format("Topic %s does not exist.", topicName))); } else { future.complete(null); } deleteTopicsResult.put(topicName, future); ```
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
Discussed offline. This can instead be the producer's retry count. If the retry count is zero, then we will have to allow for at least one metadata request past its max age. So the staleness threshold will be: `retryCount * (backoff + requestTimeout) + maxAge`
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Similar to above we should rename this.
I meant to have "Note that enabling idempotence requires this config..." before "Allowing retries...". And break the two parts with a paragraph.
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
> I ignore 1 out of every 2 punctuations Uh. That's kinda painful... I think we need to discuss this in more details, ie, what semantics we want to provide. \cc @bbejeck @dguy @guozhangwang @miguno
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
nit: why double space? (similar below and further below)
nit: preserve empty line after `checkAndClearProcessResult`
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
store not used
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
Should we use "NO_CURRENT_THREAD" to replace "null"? It seems to me "NO_CURRENT_THREAD" is more readable.
`threadId` is no longer used.
@hachikuji @tedyu @ijuma OK, I am going to revert and commit for now. We could improve on it later if we make updates to the code again.
I think the enum approach was better. I'd go with `DISCARD`, `GRACEFUL` and `NOTIFY_ONLY`.
Yeah, no rush.
Since only two booleans affect decision making, I think reducing the number of enum to 3 would be more readable (there is no true false combination).
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
I see what you mean, and yea that is a fair point 
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
nit: `equals` => `equal`
We tend to use the steam api for such small transformations but I don't feel strong about this.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
Nit: fix line break
`return` is not necessary
Updated this when merging.
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
nit: why double space? (similar below and further below)
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
Could use `equalsIgnoreCase` directly.
validateStoreOpen() can be outside of lock block.
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
There is https://github.com/ben-manes/concurrentlinkedhashmap. Guava and Caffeine (Java 8 required) also have Cache implementations with the same underlying behaviour.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
That's a good point.
Maybe say something similar to the PR about this method being commonly used to set up ssl clients even though not public API so we're temporarily keeping it for backwards compatibility as of 2.3 (that will give a sense of timing when people see this code a year or two from now).
Can we call this `toHtml` to go along with the generically named `toRst`? If we want to change the output in the future, we won't need to add a new API.
Can replace the three lines with: ``` assertEquals(Utils.mkSet("TLSv1.2"), Utils.mkSet(engine.getEnabledProtocols())); ```
`SSL context` => `SslEngineBuilder`
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
We should also create a client connection with one of the newly disabled protocols like TLSv1.1 and verify that the client connection fails.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
Sorry, missed this earlier: We are creating a new `selector` in `checkAuthentiationFailed`, so we should ensure that the previous selector is closed. You could call `selector.close()` just before calling `checkAuthenticationFailed` here and also a couple of lines below.
We tend to use different `node` value when multiple connections are created by a test. You could just replace `node` here with "1" and a couple of lines below with "2".
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
nit: Empty line could be removed.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
nit: remove `this` here and line below
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
Do we really need to print `super.toString`? Ditto above.
Nit: ```suggestion * @return list of {@link ConfigValue} instances that describe each client configuration in the request and includes an {@link ConfigValue#errorMessages error} if the configuration is not allowed by the policy; never null ```
@rajinisivaram, hmm, I'd rather us specify the details or link to a config that specifies them. With security, people often struggle so the more information we can provide, the better.
methods => `mechanisms`
Better be `cooperative-sticky`? `cooperative` is too general I think.
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
getters should not use get. i.e. use `networkDevice` here, etc.
we don't typically use "get" in our getters, right? so this should just be `latencyMs`
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
How about "runs an external command for the worker."
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
It's a fair point that `Cluster` is public and we should be careful about what we add there.
Do we lose anything if we use `Set` instead of `Collection`? Seems like set is the right semantics.
Do we really need this method? It seems like we could pass the relevant information in the `update` method. Not yet sure which way is better, but I'd like to consider the option.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
add `final` twice
initialize the `KStream` here and make it `final`
This can be initialized here and be `final`
you don't need this. Junit gives you a new instance of the test class for every test method
You should also compare `expectedValues`.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)
as per previous `assertThat(..., instanceOf(...))` would be better
nit: it is naming a source node, not a processor node. -> `"source"`
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
This was the checkstyle error that was failing your build.
More explanatory error, as discussed.
Dropped this unnecessary duplicate code, as we discussed.
We shouldn't return `null`, but instead return a "unknown query" result.
If not, we should move the exception capturing logic inside the dbAccessor as well.
We should test settings with the `source.cluster.` prefix too. Same in the test below
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Don't we need to keep the existing methods for backward compatibility? We could perhaps deprecate them.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We need to keep this public method and deprecate. Perhaps throw an exception if multiple group ids were specified and retain existing behaviour for single group id.
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
nit: use `logContext
It's useful if the simulation test is deterministic. That way failures are easy to reproduce. Perhaps we can use a shared `Random` instance (between this class and the coordinator) with a defined seed.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
Ah, you're right.
nit: this indentation looks kind of funky
This should probably just return a boolean
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
I think you will need a space between `kafka_principals` and `self.extra_principals`
i don't think this is what you want here. `broker_ids` is evaluated once. the `wait_until` implies you want to wait until the `broker_ids` appear in the the log files for the broker. but that doesn't seem like the target you want.
i don't think there's a good way to do this directly with `monitor.wait_until` because it was written originally to look for a fixed pattern (we've mostly used it to monitor startup where we know a specific message gets logged once the service is actually serving requests). you could either look for the fixed pattern (e.g. if this message only gets logged once and before the broker ids it has a known msg) or do a `wait_until` yourself. `wait_until` takes a function, so you can just wrap up the check in a local function that returns a boolean and pass that into `wait_until`.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
this test doesn't seem to throw `InterruptedException` as well
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
nit: move this `if` statement below/above version check on line 62
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
Personally I'd prefer to have decodeTasksData in which we hard-code the logic of doing both prevTasks and standbyTasks, we do not code-share for these two task sets but we share code of constructing the set for version two and version three. I guess we cannot get both code sharing, and since it is really a nit I'm fine either way :)
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
Could you try to factor out some setup code? For example, in each test you create and initialize the names of the topics, member IDs and the consumer IDs. You could initialize a bunch of topic names and IDs globally and reuse them in the tests. Another example is the creation of `partitionPerTopic`, which is really similar in all tests, only the number of partitions vary. Factoring out setup code, would make the tests more readable IMO.
I do not say, you have to use the setup method for `partitionPerTopic` in all tests. You could implement a method `setupPartitionsPerTopicWithTwoTopics(long numberOfPartitions1, long numberOfPartitions2)` or similar and use it in those tests that need two topics.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
Consider naming the topic "topic2" since there are only two topics in the test
We need to be able to remove just a few specific metrics without disrupting the others, while also making sure to actually close/cleanup during an actual shutdown
I think it should be safe to call remove here multiple times, the important thing is just to make sure it's actually *at least* once
After closer look, I don't think there's a conflict between 429 and how the metrics are currently closed 
instead of creating a new set, thoughts on just returning an empty collection? (`Collections.emptyNavigableSet()`)
nit: we do this same thing in the other `#resize` for thread count changes, can you factor it out into a helper method? Then I think we can narrow the scope and make only that helper synchronized (should double check that though)
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
Nit: remove `this` (we try to avoid `this` wherever possible)
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
same for the store
nit: add `final`
`while` seems to be missing
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think `will go` should simply be `go`.
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
`deteremined` => `determined`
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
ditto for the rest of the test
This was not addressed yet. the whole sentence can be removed
There is no `CoGroupedStream#reduce()` -- we can remove this
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
`KeyValueStore` -> `TimestampedKeyValueStore`
nit: remove empty link
Note this correction
nit: "User rebalance callback **threw** an error"
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Nope, up to you. Just thought this might be more readable: ```java maybeInvokePartitionsAssigned(addedPartitions, firstException); ```
We seem to have lost the `info` message from the original.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
I don't think we really need this function any more... we can just submit to the executor from the other function.
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
Incase => In the case
Should we remove references to implementation from the interface? We can keep this doc in the implementation class.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
nit: add one whitespace at the end after "...state"
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
To keep this logic same as one in `Call::fail` method, lets set the new time as: > nextAllowedTryMs = now + retryBackoffMs
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
nit: I think it's better to just print the e.message in a single line.
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I think the only issue is that this message in particular might get a little spammy when we are discovering a new coordinator since it can take a little time for the cluster to converge. Seems fine to increase verbosity for the other errors though. By the way, it looks like we're missing the word "failed" in the message
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I had a look at this and your are right. It seems that keeping `TopicPartition` is better and difficult to change. In this case, have you considered pushing the conversion to the `Builder` by providing an overload of `setTargetTimes` which accepts a `Map<TopicPartition, ListOffsetPartition>`? That could make the code in the `Fetcher` a bit cleaner.
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
Sounds like a good idea
I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,
Another idea might be to pass `partitionsToRetry` into `groupListOffsetRequests`.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
I see your point, but I do also not see the need for an internal state for which we need to avoid invalidation. Variables `numStandbyReplicas` and `numStandbyReplicas` are configs that can be stored as member fields of `ClientTagAwareStandbyTaskAssignor` or passed along to the methods that need them. Variables `tagKeyToTagValuesMapping`, `clientsPerTagValue`, `standbyTaskClientsByTaskLoad`, and `clientStates` can also be passed to the methods that need them. Avoiding state makes reasoning about code simpler and here it seems possible to avoid state. See `HighAvailabilityTaskAssignor`, it does not have any state.
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
I think the risk of introducing `options()` is that some developers might accidentally use `values()`. The pattern used in `ConnectorType` is far better, as it overrides the `toString()` method. That doesn't handle the case-independence for parsing, though `ConverterType` is a better pattern to follow if that's required. Let's be consistent with the new enums, and have each follow one of those two patterns depending upon whether parsing case-independently is required.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
nit: we could define this transition list in a variable to be reused.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
We could refactor out a helper function here.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Ditto here for different exception types.
Nit: move into the `if` block where it is used. (also add `final`)
nit: final int `activeTaskAssignmentLength`.
nit: add `final` (same line below)
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: we can use `threadAssignment.size()` to replace the `threadTaskCount` variable. Same as below.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Fine with me to keep the guard. Was just double checking.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
nit: add a blank line before this one to make it easier to read
See above: we shouldn't rely on `previousRightWindow` here. Actually I don't think we need it at all? (assuming we move the check in it to the condition above where we use the combined window agg)
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
nit: missing comma `headers[,]`
`timestamp` missing (twice)
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
I don't think we need all this, if we delay adding the store to `writeToTopology()`
nit: avoid unnecessary `this.` prefix
nit: remove `this` here and line below
This part and the line 525-529 below can be extracted out of if condition.
Is our coding style suggesting to always make multi-line function call with more than one parameters? My preference is only use multi-line if there are 3+ parameters, AND if we put them into a single line it would be too long.
Should have thought of this before, but `CollectionUtils.groupDataByTopic` already does this.
Does this really need to be `Serializable`? Same for the other comparators above.
Wow, that's annoying. No harm making the Serializable I guess, but perhaps we can just add the `serialVersionUID` so that we don't need the suppressions.
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
Needs to be updated.
nit: need to update this since resource name is before pattern type now
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
Nit: Please use `{ }` (even for one line blocks)
Nit: can be `final`
Nitpick: Is there a better term than "join-tuples"? Perhaps we should highlight a bit more that that we return tuples of values (but e.g. not of keys).
Nit: we sometimes use "that contains", and sometimes use "containing". Either should be fine, we can just make it consistent. Also: "joined records computed by the given {@link ValueJoiner}, one for each matched record-pairs with the same key and within the joining window intervals." Similar below.
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
maybe: `inputKeySerde` and `inputValSerde`
Do we really want to always set like this? What's if a user want to provide a custom Long-Serde that just works fine? Maybe we should only overwrite if value serde is `null` ? It's just a thought -- not sure about it.
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
nit: move first parameter to next line, too
Thanks for this reformatting to make code readable on Github!
either move `this` to next line, or fix indention.
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
`a graph containing`, and correct space between `1. Build`
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
nit: remove empty line
ditto on removing before/after.
Ditto on removing these before/after methods.
Ditto on removing before/after
I think we ditch the before/after methods as I previously recommended.
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
I think we can move this logic into ValueOrOtherValue as another static constructor.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
We need to call `transactionManager#failIfNotReadyForSend` here, so that we don't try to append to the batch when we are not ready to send. Also, we should remove `failIfNotReadyForSend` from `TransactionManager#maybeAddPartitionToTransaction`
OK, makes sense. I was missing that context.
It would be nice to have information about what the "previous" partition was here in this log message. Also "retrying because of a new batch..." might sound nicer.
Maybe "attempting to append " would be better than "sending", since we might fail to append.
record.toString could be expensive, so it would be best to surround this in an if statement, like ``` if (log.isTraceEnabled()) { log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition); } ``` Of course, it's a bit unclear that we should be logging `record.toString` at all here, since we don't log it in the other cases. it would probably be enough to just log (at trace level) that we are retrying the partitioning for the particular topic because there was a new batch.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
Nit: keep first two parameters in their own lines (we either put all parameters in one line, or use one parameter per line -- hybrid formatting makes it harder to read the code).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
As above, I think we should create both tables using `toTable()` operator
Nit: line too long
Thinking about this, I guess we could improve `StickyTaskAssinger`. If I am not off, load balancing on stream basis is not optimal -- but I am also not sure if the effort to improve it is worth it... If we extend this test to assign more tasks, let's say 12, client `p2` will get 7 tasks assigned and `p1` get 5 tasks assigned (while it would be better to assign 8 tasks to `p2` such that all 3 thread get 4 tasks each). The problem is, that the capacity factors are not considered: `p2` should get twice as many tasks assigned as `p1` -- but the algorithm says only "more" -- and this more is determined be the diff of the capacity (ie. in this case `p2` will get at most 2 more tasks assigned than `p1`. Or maybe my analysis is wrong (I did not run the code and step through it.)
Seems to be covered by `shouldAssignMultipleReplicasOfStandbyTask()` already
It's not about capacity, is it? It's about having not task that does not have the task assigned (as active or standby). -> `shouldNotAssignStandbyTaskReplicasWhenNoClientAvailableWithoutHavingTheTaskAssigned`
Thanks for clarification. Does make sense now -- I did not consider the "task pairs" heuristic.
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
Line too long (also some lines above and further below)
This seems to be the same program as in the test above -- if yes, I think we should merge both tests into one.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
line too long
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
No longer used.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
Is this necessary? The leader epoch is -1 by default.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
Is this name correct? It seems like the buffer is never filled in this test.
It would be good to verify that the buffer contents are correct as well.
Maybe we should add one case where `position > 0`.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
We should also mention somewhere that we do not support concurrent transactions.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
i don't think this is what you want here. `broker_ids` is evaluated once. the `wait_until` implies you want to wait until the `broker_ids` appear in the the log files for the broker. but that doesn't seem like the target you want.
i don't think there's a good way to do this directly with `monitor.wait_until` because it was written originally to look for a fixed pattern (we've mostly used it to monitor startup where we know a specific message gets logged once the service is actually serving requests). you could either look for the fixed pattern (e.g. if this message only gets logged once and before the broker ids it has a known msg) or do a `wait_until` yourself. `wait_until` takes a function, so you can just wrap up the check in a local function that returns a boolean and pass that into `wait_until`.
Would this be better as an `nc -z` test? Grepping logs was always kind of a half-assed solution, and the real test we care about seems to be whether anything is actually listening on the port. We have this now for at least jmx (thanks to yours truly) and ZK (thanks to @kkonstantine) and I'd like to continue the trend elsewhere as it is far less brittle than grepping logs.
Safer to check `is not None`
Does it make sense to set `self.kdc` in the constructor? And then having non-None self.kdc would be part of the logic in `has_sasl_kerberos`
Yes, this seems fine then.
nit: two spaces between code and `#`
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
I wonder if more of this code is generic and should be pushed somewhere else.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
It seems that we can just use one level of if/else.
This was changed to`INVALID_PRODUCER_EPOCH` in the kip. Let's change it here too.
The exception can still be `ProducerFencedException`. Just the name of the error code should change.
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
nit: all the other collections are initialized in the constructor.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
+1 -- also note that this sort of only makes sense when using the named topology feature, as otherwise you don't have multiple topologies and not really any reason to set any of these configs differently in the TopologyConfigs vs StreamsConfig passed in to the KafkaStreams constructor. That said, it will definitely happen, so I guess we should check for overrides whether the topology is named or not. Maybe you can use the `isTopologyOverride` method, and remove the check for `namedTopology ~= null`? ie, you can/should use `isTopologyOverride`
Sounds good! There's no rush, but I'll make sure we have your new PRs reviewed and merged quickly whenever they are ready, since you've worked so hard on this already. I'm sorry I wasn't able to make another pass on your original PR, but hopefully this won't be too much of a bother.
Ah, I see the confusion. The `#isTopologyOverride` method checks whether the config has been overridden for the specific topology, ie has been set in the Properties passed in to `StreamsBuilder#build` -- it's not looking at what we call the `globalAppConfigs` which are the actual application configs: ie those passed in to the `KafkaStreams` constructor. So basically there are two sets of configs. The value should be taken as the first of these to be set by the user, in the following order: 1) `statestore.cache.max.bytes` in `topologyOverrides` 2) `cache.max.bytes.buffering` in `topologyOverrides` 3)`statestore.cache.max.bytes` in `globalAppConfigs` 4) `cache.max.bytes.buffering` in `globalAppConfigs` Essentially, using `#getTotalCacheSize` on the `topologyOverrides` if either of them is set (which this PR is doing) and on the `globalAppConfigs` if they are not (which is the regression here). On that note -- we also need to move `##getTotalCacheSize` out of StreamsConfig, because it's a public class and wasn't listed as a public API in the KIP (nor should it be, imo). I recommend creating a new static utility class for things like this, eg `StreamsConfigUtils` in the `org.apache.kafka.streams.internals` package. There are some other methods that would belong there, for example the `StreamThread` methods `#processingMode` and `#eosEnabled` should be moved as well Hope that all makes sense -- and lmk if you don't think you'll have the time to put out a full patch, and I or another Streams dev can help out 
I did have a closer look into the code. You are right. I also double checked and `Serde` does actually not implement `Configurable` (so there will also not be two calls what would be bad). Sorry for the confusion -- and thanks a lot for pointing out that it is correct as is!!
rewrite test as above using `assertThrows()`.
Should this be `num_lines=3` (cf. L116 and L126)
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Not for this patch, but we should do a KIP to add support for batch topic creation.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
```suggestion * A byte array comparator based on lexicographic ordering. ```
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
nit: add `final`
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
As above: use `assertThrows` and verify error message
nit: add `final`
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
req: This is unnecessary
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
Could we skip some of the checks below by setting the initial value? ``` boolean needsReadToEnd = configState.offset() < assignment.offset(); ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
Map.Entry<String, String> to avoid the check below
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
nit: the previous location seemed a little better since it was listed next to `autoCommitIntervalMs`.
Currently our configs are still going to pass-in a single partitioner which is then be used as a singleton list, hence we use `getConfiguredInstance` here.
Is this just temporary until we add better support in the configs for multiple assignors? I'd imagine we need to think through the exact semantics, if ordering matters at all, etc. Is the plan to eventually just switch this to a comma-separated list of class names? One thing I found with Copycat was that the more things that needed to be configured via the same config dictionary, the more problematic Kafka's standard approach to configuration became because you could easily hit cases where there were conflicting settings. Not sure if a) that'll be an issue here or b) if we even want to support assignors that have _that_ much config, but something worth thinking about before committing to this specific approach to specifying assignors.
Nitpick: double space after `private`
This looks unintentional.
Sounds good. I think this is convincing :)
```suggestion + " for the following reason: ", ```
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
nit: add `final`
typo: CompleteableFuture -> CompletableFuture
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.
req: I assume you do not want to test `handleAssignment()` here, so you should not specify behaviour verification on the mock. You could simply write ``` expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment))) .andStubReturn(singletonList(task00)); ``` `.andStubReturn()` is behavior that is not verified in the `verify()` method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test. Same applies to other similar locations in this test.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
nit: empty line.
I think it is still possible. Here's one scenario: 1. last checkpoint at offset 100; all writes goes to old CF. 2. continue writes to old CF til offset 110, but no checkpoint written yet. 3. non-graceful shutdown happens, and upon restarting new CF is used. 4. we start restoring from offset 100 to log-end-offset 110, to the new CF. Now we ended with data of offsets 100-110 in both CFs.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
validateStoreOpen() can be outside of lock block.
`validateStoreOpen()` can be outside of lock block.
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
same question here and below about locking around a `volatile` variable. Is this the only reason to lock here? One would think so based on previous usage.
I don't think locking buys you anything w/r/t to that. The producer#send in the status backing store is asynchronous. So what would describe above can happen anyways, regardless of whether you lock this object. Of course, if it wasn't asynchronous things would be much worse. A bottleneck would be created by the lock, waiting for the `send` to finish in every locked block, so that's not an option. Wdyt? That's what I see at the high level without spending to much time on it, but see if you can check this assumption and we can return to this question.
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
nit: can we make this debug level? Otherwise it will make this test a little spammy.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
`{@link TimestampedWindowStoreBuilder}` should probably be `{@link TimestampedWindowStore}`.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
Nit: capitalization isn't quite right, see `reauthenticationLatencyMs` for example.
Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
@edoardocomar `Prepared` doesn't convey much meaning in terms of an externally visible metric. I imagine you chose it rather than `authenticated` since you intended it to work for `PLAINTEXT`. But `PLAINTEXT` doesn't go through this if-block since `channel.ready()` returns `true`.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
nit: `toString` can be used implicitly
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
nit: would be nice to be consistent on the pattern we use here
Also: should only call onPartitionsLost on owned partitions that no longer exist
typo: moreq -> more
nit: add a size? There are a few cases in here where we could do this.
nit: maybe worth adding `this == o` to these `equals` implementations. This applies to all of the similar classes in this PR.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
I think this could be done with `computeIfAbsent` like in "finishSnapshot" above
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
nit: remove `this` (not required)
Use `KafkaException` instead of `RuntimeException`
nit: could be useful to log the type of exception in the assertion message.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
nit: remove extra newlines
nit: Maybe we can consolidate this logic into a function as well? E.g. "respondSentRequest" etc.
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Compiler still gives some warning here, as you are erasing here the java generic type. Adding the `<>` infects the rest of your code with the `? super T` you have to add at the correct places.
Fixed this in my latest update of #4033.
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Not sure how many mirror maker tests we'll end up having, but would it make sense to have a `MirrorMakerTest` utility like the `KafkaTest` one, or does that end up being too minimal to be worth it (looking back at the `KafkaTest` one now, it looks like it's now just a few lines of code...)
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
nit: use class `import` to avoid long name here
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
Please remove empty lines here and in the other test methods.
You usually do not want to mock the class under test, because you want to test it. Also partial mocks should only be used if absolutely necessary. A rule of thumb is if a partial mock is needed then most probably the design has a flaw. In this specific case, you should mock RocksDB's iterator. For the class under test, you should test `hasNext()`, `next()`, `peekNextKey()` and `close()`, because those are the one exposed (`makeNext()` should actually de declared as `protected`, IMO). ```suggestion final String key1 = "a"; final String key2 = "b"; final String key3 = "c"; final String key4 = "d"; final String value = "value"; final Bytes key1Bytes = Bytes.wrap(key1.getBytes()); final Bytes key2Bytes = Bytes.wrap(key2.getBytes()); final Bytes key3Bytes = Bytes.wrap(key3.getBytes()); final Bytes key4Bytes = Bytes.wrap(key4.getBytes()); final byte[] valueBytes = value.getBytes(); final RocksIterator rocksIterator = mock(RocksIterator.class); rocksIterator.seek(key1Bytes.get()); expect(rocksIterator.isValid()) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(false); expect(rocksIterator.key()) .andReturn(key1Bytes.get()) .andReturn(key2Bytes.get()) .andReturn(key3Bytes.get()) .andReturn(key4Bytes.get()); expect(rocksIterator.value()).andReturn(valueBytes).times(4); rocksIterator.next(); expectLastCall().times(4); replay(rocksIterator); final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( STORE_NAME, rocksIterator, Collections.emptySet(), key1Bytes, key4Bytes, true ); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key1Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key2Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key3Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key4Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(false)); verify(rocksIterator); ```
This does a system call every time. We can use `InputStream`'s `readAllBytes`: https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/io/InputStream.java#L389
I think this can only catch `IOException`s. We can be more explicit.
Hmm, why did we do this? I thought we'd have a try/catch block.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
Can this be a `byte`.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
Maybe it's worth adding a code snippet which shows how to use the sticky assignment with the rebalance listener? It's a little different than with "range" and "roundrobin" from memory.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
nit: would be nice to be consistent on the pattern we use here
Why this and other 4 tests below cannot reuse the `runGenericBenchmark` common function? It seems it is not necessary to use a separate thread for these tests to distinguish them from others that can leverage this common function.
This line (314) could be renamed to `expected Key` as well.
Does this have to be a different variable name than line 131 above? Same below.
Would it be better using a separate topic in order to keep a partition without any records? By changing this topic it affects existing checks in all tests
As this does not change, I wonder if we could direct initialize `consumerProps` when it's declared
You can use the default constructor `ReplicationControlTestContext ctx = new ReplicationControlTestContext();`
nit: `Arrays.asList` a bit more concise.
I can't make out the difference between this test and the previous one. looks identical from the code.
We cannot guarantee that this poll will see all completed connections, so it would be better to poll in a loop until the total connections returned from`selector.connected()` after the poll equals `conns`.
The parameters should be the other way round (expected is `conns` and actual is the metric value).
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Nit: remove `this`
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
Why don't we need this check anymore? It's still done for `globalThread`.
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
nit: add `final`
nit: you could just specify one `String` variable at the top of the method and set it accordingly if `topics` is `null` or not then have a single `return` statement. Just a personal preference though.
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
I've logged https://issues.apache.org/jira/browse/KAFKA-12380 for shutting down the worker's executor. Again, it's not an issue in runtime, but a *potential* issue in our tests.
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
Also set the store name in this test.
That makes sense. I did not think about the reconfiguration case.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
Map.Entry<String, String> to avoid the check below
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
This should disappear with my suggestion.
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
Done now, thanks @ijuma
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
This seems to always enforce a materialization, but I think we should materialize only if we need to.
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
cc @mjsax as well, LMK WDYT.
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
fair enough :)
Could we separate this assert by ```assertEquals```? It can produce more meaningful error message.
Could we address this issue in separate PR? It seem to me this issue does not belong to "trivial cleanups" :)
Semantically really bad to forward `0x3`, but well, it is what it is.
Same here. We should use `builder.stream().toTable()`
nit: `COGROUPKSTREAM-AGGREGATE -`
The output type is only `KTable<KR, VOut>` is we don't use windowing, right? Otherwise, it should be `KTable<KR, Windowed<VOut>>` ? Do we actually need two different `build()` methods for both cases? Maybe we can clean this up in the follow up PR though. Just something to think about
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
I think we should have this in this PR already.
It will result in the same list of versions -- both equally good IMHO.
Nit: space missing before `timestamp_type`.
Actually from `0.10.0` to `0.10.1` requires a full stop, from `0.10.1` to newer version does not. But I think it is still fine to leave it out.
I would say leave it out, I believe the change from `0.10.1` to a higher version requires a full stop and can't be done from a rolling restart.
Both should be fine -- not sure if the change improves anything -- but would not make it worth either.
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks @vvcephei -- that is convincing.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Changed it locally.
It doesn't seem that the client needs principalBuilder.
That's right, changed it locally.
Let's also add "socketChannel.socket().getInetAddress().getHostName() must match the hostname in principal/hostname@realm"
It seems that we can just use one level of if/else.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
I don't think we need to mock `generation()` in this test.
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
Looks good. I like the additional checking that you're doing here.
above (some some more times below)
nit: both lines missing . at end
Nit `.` at the end
Should this be protected? Compare the discussion in https://github.com/apache/kafka/pull/3807#discussion_r137603218
nit: is it necessary for triggering a warning? My personal preference is that if a single line is not too long, then it is okay to do so.
store not used
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
store not used
nit: add `final`
nit: add `final`
Same here. We should use `builder.stream().toTable()`
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
As above, I think we should create both tables using `toTable()` operator
I am frankly not sure, if we need to test left-table-table join explicitly (the table-table join test from above should be sufficient)
As for all tests, we should verify `topologyDescription`
```suggestion log.debug("New average number of connectors per worker rounded down (floor) {} and rounded up (ceil) {}", floorConnectors, ceilConnectors); ```
```suggestion log.debug("New average number of tasks per worker rounded down (floor) {} and rounded up (ceil) {}", floorTasks, ceilTasks); ```
We can use the fact that these are non-negative integers. ```suggestion int ceilTasks = floorTasks + ((totalActiveTasksNum % totalWorkersNum == 0) ? 0 : 1); ```
We can use the fact that these are non-negative integers. ```suggestion int ceilConnectors = floorConnectors + ((totalActiveConnectorsNum % totalWorkersNum == 0) ? 0 : 1); ```
```suggestion log.debug("Tasks on worker {} is higher than ceiling, so revoking {} tasks", existing, numToRevoke); ```
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
You can click on the below links named "Details" to see the failure message. Also try using `./gradlew checkstyleMain checkstyleTest` to check locally.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
Ah ok fair enough -- thanks!
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
req: no longer used
nit: add `final`
IMO, using reflection should be a last resort. It is pretty horrible and makes the tests harder to comprehend. I'm not a big fan of making methods visible just for testing, either, but I prefer this to having hacky test code using reflection. What would be better is if there was an easy way of testing this without either of the approaches.... That would require a fair amount of refactoring and smaller classes.
@dguy If we need to access an inner function for lots of unit tests it usually indicates that our class design patterns are not good since unit test should be testing a class's out-facing behavior only; in other words cases that you need to trigger an inner function in unit tests should be rare, and I think reflection is fine as long as it is rarely used.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
Can we actually just get rid of "test only" constructors? It couldn't be used by _that_ many different tests...
Maybe something like "No command specified" would be more descriptive
If there are multiple FatalExitError thrown, do we want call `System.exit()` only once instead of creating a thread for each of them? Maybe we can just log the error and return.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
That was what I meant.
No need to mention the version -- it's also not clear how long we keep it -- it's at least up to the next major release -- but might be longer. I would rather point to the new method to be use: `@deprecated use {@link #topicSet()} or {@link #topicPattern()} instead` This explains users how to rewrite their code and is more helpful.
nit: I don't feel strong about the style here, but maybe we should consider align with other functions which has the first parameter on the same line with function name.
spelling -> recrord -> record
I'm not sure it actually matters since users are unlikely to construct this object manually, but it seems like we should use `ConfigSource.DEFAULT_CONFIG` if `isDefault` is true and `ConfigSource.UNKNOWN` otherwise. Then `isDefault()` will continue to work with this constructor.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
@kkonstantine, thanks for the correction -- the admin properties for the sink worker task (and specifically the DLQ reporter for the sink task) should use a combination of the `producer.*` and `consumer.*` properties that are connector-specific.
as above: we need to remove adminPrefix configs
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
This syntax is a bit hard to follow with the conditional at the end. Can you rewrite it to something like: ```python self.jmx_tool = None if jmx_object_names is not None: self.jmx_tool = ... ``` (and also check the attributes as mentioned above)
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
looks good, thanks
Nit: space missing before `timestamp_type`.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
It's intentional to avoid build warnings about importing deprecated classes.
Make all params `final`
And again with `final` if you don't mind
as above nit: double space `to Kafka`
As this does not change, I wonder if we could direct initialize `consumerProps` when it's declared
Would it be better using a separate topic in order to keep a partition without any records? By changing this topic it affects existing checks in all tests
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
this overwrite of mm2config should go in the setup method, IMHO
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
Sorry, you are right! My bad!
Add a check to verify, whether the iterator has no more elements.
nit: remove empty line
nit: remove empty line
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
Seems like a no-op
`final` and initialize in constructor instead? Doesn't seem to depend on the config at all.
AK convention is to not use `set` setters or `get` getters.
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: use `{}` instead of string concat for `retries`
nit: log.info("Suspended {}", state());
nit: we could just use `Map` for `startOffsets` and `endOffsets`
How about `return admin.endOffsets(assignment);`
Nit: we don't need this tag before the parameter list.
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
`KeyValueStore` -> `TimestampedKeyValueStore`
This was not addressed yet. the whole sentence can be removed
There is no `CoGroupedStream#reduce()` -- we can remove this
Let's use `Map` on the left side instead of `HashMap`
Similar here, we can cache the result in case to be reused.
nit: we can use `map#compute` to replace getOrDefault + put.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
We could make this field access `public`
```suggestion ``` Do we need both of these debug messages? After all, `worker.assign(...)` is just adding a `ConnectorTaskId` to a collection. How about keeping the first one since this is at this point an on-going process and we've not actually assigned anything to the actual worker node.
```suggestion log.debug("Assigning lost tasks to {} candidate workers: {}", candidateWorkerLoad.size(), candidateWorkerLoad.stream().map(WorkerLoad::worker).collect(Collectors.joining(","))); ```
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
@cmccabe Looks like the log entry hasn't been updated.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
nit: move `logContext` to its own line
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
if you have an unsigned ~~8-bit~~ 16-bit data source
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
nit: remove empty link
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
A couple other potentially interesting test cases: 1. After starting in a resigned state, verify that the node will become a candidate after the election timer expires. 2. Verify that we can vote for new candidates when in the resigned state after beginning shutdown.
nit: add `final` -- applies to all other vars below too.
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
Personally I think it makes sense to just disallow calling `ofTimeDifferenceAndGrace(...).grace(...)` entirely, this seems like abusing the API
nit. I think there is `.` missing `since 3.0[.] Use`
`of` -> `or`
`out-of-order` `window closed`
Same as above mentioned, the validation didn't get handled in new API.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
nit: Indentation of those lines seems to be off here.
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
nit: We could use `TestUtils.assertFutureThrows` here.
Nitpick: I'd call this `deserialize`.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
I know this is a bit picky, but we (mostly @mjsax and i) are trying to encourage everyone to use `final` where possible
You should use try with resources here too.
Do we need to check if it is null here? I think it is probably ok if it doesn't throw any exceptions? Obviously it would be better if we could check that `loginManger.release()` was only called on the first invocation, but i appreciate that involves further refactoring
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
original was better
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
`Note when the windowed serde class is used, one needs...`
please fix this: `use {@link } instead`
I think we should stick with host:port for the near term unless there is specific requests in the KIP feedback.
My inclination is to provide more flexibility to the user, but add some guidance on what a "typical" setting would look like (think: `host:port`).
You can also remove the `@SuppressWarnings("deprecation")` at the top of the method.
I think that the driving condition of the test is the number of samples, not the time. But I thought I'd point out that time won't advance by default, but you can make it by using the `autoTickMs` constructor.
Do we need to wrap with the LinkedHashMap? Could we just do `Collections.unmodifiableMap(metrics.metrics());`
nit ```suggestion expectedSensor, StreamsMetricsImpl.CACHE_LEVEL_GROUP, tagMap, hitRatio, HIT_RATIO_AVG_DESCRIPTION, HIT_RATIO_MIN_DESCRIPTION, HIT_RATIO_MAX_DESCRIPTION ); ```
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
The change makes sense to me. I don't think anything would stop the auto-commits from going through. Even if there was such a mechanism, it seems better to explicitly disable it.
nit: extra blank line
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
nit: remove empty line
Another way to write this, that reduces a couple lines of code would be: ```java if (allTopics.remove(topicName) == null) { future.completeExceptionally(new UnknownTopicOrPartitionException(String.format("Topic %s does not exist.", topicName))); } else { future.complete(null); } deleteTopicsResult.put(topicName, future); ```
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
We probably should name this sth like removeFromIsrAndMaybeChooseLeader.
Currently, the follower never removes the leader out of ISR. So, perhaps we should just throw an exception if this is not the case.
In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.
This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception.
key -> topic
It would be nice if you made these `final` while you are doing this change.
add line: `TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();` and change to `RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
I am still confused, about `source` being `null`. In the original code (L121) `source` is handed to `createRecrodQueue` and must not be `null` -- because this was never an issues before, I am still puzzled. why it is now.
check `source != null` not necessary. In doubt add an assertion.
Can just return `name.startsWith(acl.resourceName())`
Not sure if it makes a big difference, but we could use EnumSet for these.
nit: need to update this since resource name is before pattern type now
nit: `final` is redundant for private static method
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
Still not used
`tp` is not used anymore.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Use diamond (`<>`).
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
We should also mention somewhere that we do not support concurrent transactions.
nit: newline for better IDE debugging.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Very good catch. Thanks, @abbccdda .
Yeah good catch, see above
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
Actually on a second thought.. if users configures `-1` it means they probably do not care about enforced processing, while on the other side the INFO entry may flood the logs here. So NVM.
Should we log INFO if we are indeed enforcing processing? I.e. there are some empty partitions.
It doesn't seem like we need this variable. Why not assign the result of `createFetchRequests` and call `size` on it? Less mutable variables that need to be tracked.
nit: `sooner` -> `as soon as possible`
While you're here, I suggest fixing `fetchablePartitions` so that it doesn't do two `remove` calls on an `ArrayList` (in the worst case, it has to shift all the elements in the underlying array twice). We could pass a predicate to `subscriptions.fetchablePartitions()` and make it more efficient.
NPE: ![image](https://user-images.githubusercontent.com/925755/55269396-d55f3480-5292-11e9-9c29-78c524d63c65.png) I'm not using a topic pattern, equality should still work.
Schema? It's Field class...
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
This will end with a comma after the last transformation.
@cyrusv Your example should print something like: `<SimpleClassName: blah blah blah>` But I don't see `<` or `>` in your example. So either the code example is wrong, or you didn't mean to use those "quotes" in the joiner. BTW probably not adding this decoration would be better.
nit: add `final`
nit: add `final`
nit: `final` params
req: Since we do not need to validate `valueTransformer`, could you please remove it from the `verify()`.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
as above mentioned, the `listStore.all()` is not closed here.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
@SupermanScott This change looks ok, but this is failing checkstyle: ``` :connect:runtime:checkstyleMain [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:280:1: File contains tab characters (this is the first instance). [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:282:21: '}' should be on the same line. [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:293:21: '}' should be on the same line. :connect:runtime:checkstyleMain FAILED ``` Looks like simple indentation cleanup. You can easily check that these are fixed by running `./gradlew :connect:runtime:checkstyleMain`.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Should probably be `createIoThreadRatio` since we mention `I/O thread` in the messages.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
It should always be called from the same thread, though we've taken a simple approach to locking in this class where everything is protected as far as I can tell. This does technically introduce a behavioral change though, since `heartbeatThread` is being reset to `null` whereas it was not before, but this doesn't seem like a problem.
Since we acquire the same lock in `heartbeatThread.close()`, maybe we should just call `hb.close` within the lock to make the locking clearer. It's also consistent with `startHeartbeatThreadIfNeeded` and `disableHeartbeatThread`. We just leave the `join` outside the lock.
you are right :P
identical strings in log and exception
Sounds good. I just want to avoid someone trying to simplify the tests in the future without understanding that this test is verifying both features work together.
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
There is https://github.com/ben-manes/concurrentlinkedhashmap. Guava and Caffeine (Java 8 required) also have Cache implementations with the same underlying behaviour.
Think you might have forgotten to remove some debugging here
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
This is not a suggestion: we are sending `null` with the guarantee that we should have never forward for this key before. I think a good test case coverage would be to have a windowed aggregation emit final, followed by a join. The join results would need both the old/new values to be able to correct, and if emit final we should only emit once, with old value setting to null.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
nit: add `final` (2x)
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.
typo: `consume` -> `restore`.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
req: This is unnecessary
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
I'd suggest we do not print the stack trace in either case; instead, we can just print the exception's `name()`, which is sufficient to tell it is the locking issue.
nit: `could not create task {} due to {}. Will retry.`
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
The order is not really that important here, either way works
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
Please remove empty line.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
@guozhangwang Yep, sounds good to me.
Here you should just need a queue as for `clientLevelMetrics`. We need a map for the other levels because there can be multiple objects for each level, e.g., there might be multiple stream thread and each one manages its sensors under a key in the map. However, there is only one client on client level.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
I don't think we need this. You can just inline the creation of the `List<KeyValue>` see above.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
nit: 4-space indention plus move `builder` down one line
This should not have the `sink` suffix
nit: avoid unnecessary `this.` prefix
No need to check null. We always have to forward oldAgg and newAgg, anyway.
add `final` twice
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
I think it might be ok for `windowSize` to be greater than `segmentInterval` as the window will comprise multiple segments, but I could be wrong about this. \cc @guozhangwang @mjsax
Is there a reason why we are using `no smaller` instead of `greater than or equal to`? I generally prefer to avoid logical negations in messages.
I would move line 328 and 329 to before this line.
I would move line 330 and 331 to before this line.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
ditto for the rest of the test
nit: "active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
We really need a bug fix release for this! \cc @guozhangwang
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
It might be nice to keep the tasks/topologies that have failed in another list entirely. Then when reprocessing after an exception we can run all the good tasks first and commit them before running the failures. This will be important for EOS as we con't commit only part of a transaction. The larger part of that doesn't need to be done it this PR but keeping the groups separate would be nice in my mind
It would be much simpler but unfortunately its not as simple as we first thought. The producer has only one transaction, so the records of the good tasks are mixed in with the records of the failed task and there is no way to separate them. So we need to take the tasks that we know will fail and process all the other tasks without them. That way we continue making progress. We can take the failing tasks and backoff and retry again later.
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
Maybe mention the advantage of using this over `Thread.sleep`.
What is the reasoning for not throwing an exception if the condition is not met after the timeout? That would make the tests more concise.
Consider adding some small back-off time (say 100 millis) to avoid busy looping and possible log polluting.
If we rewrite to use "absolute position" instead of delta, we can remove this.
Yeah that is fine. My bad.
ah, i missed that it was in the Validator and not ConfigDef itself.
same question about override here
Should probably add this as the first bit in this method: ```suggestion String strValue = (String) value; if (value == null || strValue.trim().isEmpty()) { return; } ```
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Yeah if it exists elsewhere let's just leave it as is for now.
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
I think we might want to consider dropping some of these `log.debug`s to `log.trace`. Some of the logs in error conditions make sense at `debug`, but logging every fetch request and response at `debug` might make changing from `info` to `debug` a bit overwhelming.
There's an interesting edge case where `record.isValid()` could throw a `IndexOutOfBoundsException` if the buffer size is smaller than 4 (i.e. we fail when we try to extract the checksum from the buffer). Obviously, this means that the record size field was itself corrupt (since it should never be that small), but it can happen. I'll file a separate PR for that.
There's a space missing after `offset`, I'll fix it before pushing.
Should it be "...consumption...is coupled"? Currently it is "...consumption...are coupled".
`deteremined` => `determined`
I think `will go` should simply be `go`.
`while` seems to be missing
yep, especially given the current producer behavior.
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
nit: `final` params
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
You should also compare `expectedValues`.
Definitely don't add an abstract class! Let's leave it as is for now, then.
I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular. So, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though 
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
same here have a `retrieve_generation_num` method for extracting the generation number from captured output
I'm +1 on removing this logic, we could not assume that the leader did not change even if it is not the one who get's bounced, since it may join the group late.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
And again with `final` if you don't mind
My minor concern is that KeyFactory and ValueFactory may be only specific to key-value stores, not not any general stores; for example for database stores you may have some functions like ``` withSchema() ``` that defines the data types for each column but not using "withKey / Value" any more. But since it is only for future improvements let's revisit this nested mechanism later.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
may be use Objects.requireNonNull
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
spelling -> recrord -> record
The config name was chosen for consistency with the producer, but it is not an ideal name when considering the consumer in isolation. The consumer can block longer than this if a longer timeout is passed to any blocking API that accepts a timeout. Can we explain that this config is only used as the default timeout for operations which do not have an explicit timeout option? Similarly for the changes in the upgrade docs.
Could use the parameter variable here. METADATA_FETCH_TIMEOUT_CONFIG
Could use the parameter variable here. MAX_BLOCK_MS_CONFIG
You don't get a warning if you use a deprecated field/method/class from a deprecated field/method/class.
The logic for a valid topic name is actually a little more complicated than this. The server side validation exists in `kafka.common.Topic.scala`. We also shouldn't duplicate the logic. So if we move it to the clients package, removing the Scala version and replacing its usages would be good too. Also #898 (KAFKA-3219) may change this logic a little bit too.
One issue is that the exception thrown by this method is `InvalidTopicException`. In this context, `IllegalArgumentException` seems like the right exception to throw to be consistent with the other things we check. Maybe we can catch the exception and rethrow it. Also, it would be good to mention it in a @throws clause.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Oh, indeed, I missed that. And in this case you'll need to have an extra line with put.
As before, it's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
It seems you can move this line after line422.
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
nit: maybe iterate over `entrySet()` instead.
@eliaslevy Since this part is covered in other unit test case, we want to remove redundant coverage to leave the unit test as succinct as possible.
nit: 'else' can be dropped
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Seems to fit in one line
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
Does this test ever encounter this exception? I don't think we will be able to backport this test to < 2.6 because the method won't exist at all, much less generate the exception that is being caught here. If anything, this generates a less informative NPE later in `put`, and hides the actual root cause.
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
typo: `per reach record`
as above nit: double space `to Kafka`
Make all params `final`
And again with `final` if you don't mind
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Also? ```suggestion assertEmptyRecords(); assertNoEmptyDeques(); ```
It seems like your indentation is set to 8 spaces instead of 4.
Can remove the `W extends Window` generic
We can remove the `<W extends Window>` now, right? Also it looks like the `initializer` input isn't needed anymore either
Don't need `Vin` here
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Yeah if it exists elsewhere let's just leave it as is for now.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
doesn't look a great name for its behavior. perhaps something like currentContext
nit: as in `position` below, `this` is not required
For a nice example where caps make sense see right below, where two sentences are included.
nit: as in `position` above, `this` is not required
I see that you are actually printing the sink nodes. I'm wondering if this if condition is necessary since in line 85 this function will be skipped anyway.
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
Another tab here that should be replaced.
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
The other constructor calls the parameter `sampledStat`. We should be consistent.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
nit: move to line above.
Thanks for confirming @guozhangwang !
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
For consistency: {@link KafkaStreams} instance
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
`The default "all" setting` -> `The default setting "all"`
nit: if you want a new paragraph you need to add `<p>`
nit: We should use `groupId.idValue` here and in the others.
Yes, we can open a JIRA to do it later.
We can use `ApiResult.completed()`
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
nit: We could revert this change as it does not bring much.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
the `null` is redundant.
add `final` twice
We could port this function when it is actually needed.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Looks good. I like the additional checking that you're doing here.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Thinking about this, I am wondering if we should just change the FSM to allow this transition and simplify the code here? \cc @guozhangwang
Nit: the method name an logic does not align 100%
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
This class is surprisingly similar to org.apache.zookeeper.Login, have we copied from the same source? ;-)
Nit: unnecessary new line.
nit: use the `SecurityManager` interface
Checked with Jun and this is fine.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
We can compute this once and pass it to `removeAttribute`.
Let's also verify that `containsMBean` returns `true` before the removal. Also, it wouldn't hurt to check that the second `removeMetric` call removes it from `JmxReporter`.
4th overload makes sense to me as well. I like the idea of placing the serdes in `Consumed` into `Materialized`, but I'm trying to think would there ever be a case where they need to be different? I can't ATM, so I it's a yes for me.
We can call the static function of KStreamImpl directly and get rid of the additional function in `InternalStreamsBuilder`.
method name changes
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
`windowSize` should be `Duration`
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too. Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.
AK convention is to not use `set` setters or `get` getters.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Nitpick: I'd call this `getOrCreateFileChannel`.
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
Can we also assert that the state gets to `RUNNING` after the new thread has joined
@parafiend Looking at this again, adding to `failedSends` in `poll` can result in multiple disconnect notiifcations for a channel (we have to guarantee exactly one). `failedSends` are processed on the following `poll()`. But for the write exception here, we process `close()` in the catch block and the channel is added to `disconnected` list. In the next poll, when `failedSends` are processed, the channel will be added again to `disconnected` list. It would be better to set a flag rather than update `failedSends` and change the close in the catch block to: ``` close(channel, !sendFailed); ``` This will close the channel in the current poll without waiting to process any outstanding requests.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
@spena just ping to make sure you get this on the follow-up PR.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Your words sound good to me, but they are directly in contradiction to the code here, which skips suspending and committing active tasks because it assumes they have already happened. In other words, there is an assumption here that the active tasks are in some kind of "committed" state, while the standbys are in either "created" or "running". If we're going to have a branch that explicitly assumes the task is already committed, then I'd like to verify it, otherwise experience says it will become false after refactoring, and we'd wind up trying to track down an IllegalStateException later on. On the other hand, if these transitions are idempotent, we can just include them in both branches (or rather move them outside the conditional block).
Hi all, thanks for the discussion. To start at the beginning, yes, I was advocating for throwing an IllegalStateException at the earliest possible moment when we can detect the illegal state. Right here in the code, we are making an invalid assumption. Namely, that if a task to be recycled is active, then it has already been suspended and committed, and if it is a standby, then it still needs to be suspended and committed. Why should this be true? Because some other code deep inside another set of nested conditionals thirty lines above looks like it does that right now? Experience says that this situation will not survive refactoring. We cannot test every branch, so we need to make assertions about the state being valid when it's in doubt. We could make an argument that if this assumption becomes incorrect, than we'll throw an exception later on before any state becomes corrupted, which would be good. We could make a stronger argument that the exception we throw later on will be perfectly crystal clear about the cause and therefore we won't spend weeks poking around a flaky test or a user bug report trying to figure out what happened. But both of those arguments would depend on even further assumptions about stuff that may or may not happen elsewhere in the code base. The best thing to do at all times is validate potentially dangerous assumptions. This looks very much like a potentially dangerous assumption. I'm asking that we validate it.
Much better name :)
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
These methods look like they are identical to those in the previous test class above
Definitely don't add an abstract class! Let's leave it as is for now, then.
initialize the `KStream` here and make it `final`
This can be initialized here and be `final`
add `final` twice
git: The sentence "So setting the strategy ... matching the given strategy name" reads a bit confusing here. I think we only need to state that when the change of the policy would take effects (the next time when compaction is triggered by the log cleaner thread), and emphasize that for "timestamp" and "header" we would always still retain the last record.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
remove var -- only used once.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
For compatibility, the mbean needs to be `kafka.controller:type=KafkaController,name=GlobalTopicCount`. That means you need to pass "KafkaController" here instead of "ReplicationControlManager".
I think it would be preferrable to call this "globalTopicCount" and call the gauge "globalTopicCountGauge" or something similar.
I think it would be preferrable to call this "globalPartitionCount" and call the gauge "globalPartitionCountGauge" or something similar.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Making sample age super high sounds better to me, comparing to having a ballpark check.
@granthenke Heh, I didn't even mean for the actual credit. But sometimes it's useful in tracking down where an issue was first introduced, the reasoning for the way a particular chunk of code is written, etc. It's an inconvenience, not a serious problem, especially for a set of changes this size.
@granthenke Ahh, great point! I guess that is both nice to enforce and also annoying in this particular case.... Seems like we did some significant reformatting under the initial checkstyle addition as well, which unfortunately messes up git-blame, but I guess it's a one-time cost.
Could we use ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG etc instead of hand-coded strings? It is less error-prone for possible future changes.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
Nit: fix line break
Would it make sense to set `position` explicitly to null if the `FetchState` does not expect to have it. For example, it seems currently when we reset the offset, we leave `position` at whatever value it had previously. If we were initializing, then it would be null. If we had an offset out of range, it would be non-null. It might be easier to reason about the logic if it is always null in the AWAIT_RESET state.
Nit: remove `this`
nit: extra newline here
I'd probably pass this in via the ctor. `setStateListener` is always immediately invoked after construction so might as well just add the param to the ctor and do away with this method
does always apply -> always applies
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
an -> a
the method name changed to `windowedTable` and `windowSize` parameter is missing
`windowSize` should be `Duration`
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
Hmm, maybe even better: ``` java long startWaitNs = time.nanoseconds(); long timeNs; boolean waitingTimeElapsed; try { waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS); } catch (InterruptedException e) { this.waiters.remove(moreMemory); throw e; } finally { long endWaitNs = time.nanoseconds(); timeNs = Math.max(0L, endWaitNs - startWaitNs); this.waitTime.record(timeNs, time.milliseconds()); } ```
We can probably remove this line `restoreAvailableMemoryOnFailure = false`.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
method should be static
Maybe consider: ```suggestion final CountDownLatch latch = new CountDownLatch(1); kafkaStreams.setStateListener((newState, oldState) -> { if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) { latch.countDown(); } }); kafkaStreams.start(); try { latch.await(IntegrationTestUtils.DEFAULT_TIMEOUT, TimeUnit.MILLISECONDS); } catch (final InterruptedException e) { throw new RuntimeException(e); } ``` Then, this method won't return until Streams is actually started, which we've seen can increase test stability.
Ah, yeah, there are some limitations to the linter. Thanks for taking care of it.
generics can be inferred here.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
Update return type to `L` (if we introduce `L`)
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
nit: remove `this` (not required)
Use `KafkaException` instead of `RuntimeException`
rewrite test as above using `assertThrows()`.
nit: `final` params
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
How about using lambda? `(groupInstanceId.map(s -> ", groupInstanceId=" + s).orElse(""))`
nit: usually we write this like this: ```java this.groupInstanceId = requireNonNull(groupInstanceId, "group.instance.id can't be null"); ```
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
We can do it separately if you like.
nit: A better general pattern is to use `assertEquals` comparing against empty list. Then if the assertion fails, the message will show what was in the collection.
What's the benefits of using a callback here than calling `openIterators` directly? I think adding to `openIterators` outside of the constructor makes sense, but cannot think of the rationale of doing this upon closure.
If the object is created with the no-argument constructor, this will throw an NPE.
Good point, thanks!
nit: move to line above.
This does work, but for a reason that is a bit obscure. When using an `ImplicitLinkedHashMultiCollection`, `remove` will remove the element b such that a == b, if it exists. This is necessary since if it just took the first element where `a.equals(b)`, it might be a different one than expected. It might be clearer to directly call `removeElementAtSlot`, since we already know the slot number.
Thanks for splitting out https://github.com/apache/kafka/pull/7076 @ableegoldman Please review the new PR, too, @pkleindl
Good point, thanks!
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
nit: line too long
I didn't think of that before, but now that you mention it, the change makes sense to me.
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Not sure this is what people will generally mean by lag -- while the committed offset matters, normally if the consumer is in the process requesting the lag I think it'd mean the FetchRequest lag, i.e. how far behind *processing* the records is the consumer in comparison to what the broker indicates is the most recent offset. in other words, I think i'd update this at the end of each `put()` and change from `committedOffsets` to `processedOffsets`.
Hmm, good question. I may have actually been wrong about which values should be involved. I think @gwenshap and I had a long discussion about this awhile ago too and there are many ways you could define lag. I think the real problem here is that we may not be exposing enough information from the consumer to compute what I would really think of as lag -- FetchRequests include high watermark info so you know how many records are in the log but not yet returned to you, and the consumer creates metrics based on that. But we don't have access to that info. A connector that commits on every message would look like it has 0 lag, but it could be very far behind in the topic.
What is the reason for having `assertDoesNotThrow` here and below? The test will fail if an exception is thrown, so seems like unnecessary noise.
rewrite test as above using `assertThrows()`.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
Ah, now I got it! Sorry! Makes sense! In that case we can reuse `REPLACE_THREAD` also for the global stream thread. Forgot about that!
I actually would be in favor of calling the enum value `REPLACE_STREAM_THREAD`. A stream thread is a stream thread and a global stream thread is a global stream thread. I am aware that the KIP calls the enum value differently, but we also have a config that is called 'NUM_STREAM_THREADS_CONFIG' and we have also 'addStreamThread()' and `removeStreamThread()`. So I guess, the name to the outside of this is stream thread and not thread. We have also other threads in Kafka Streams like the state directory cleaner thread and the RocksDB metrics recording thread.
nit: "can not" -> "cannot", same below
Thanks for clarifying this.
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
It was removed from the other versions of `group` but not from here.
Oh, I just noticed. Then `synchronized` is not needed anymore.
fyi, the pr for non-double stats got merged, so we could switch this back to the original design / fill in the missing string metric
Adding an extra "if" statement on every call to `read()` seems like a very heavy price to pay for this feature. We should look at where we're using the total length and check there, I think.
I think the right time to check this would be when we originally create the NetworkReceive object, not in the hot path for every read. Also, whatever we choose as the "upper limit" for a size to be interpreted as valid needs to become the new upper limit for `message.max.bytes` (which in turn, means that maybe this needs a KIP, since it changes a public config)
Are you referring to my fix? or my problem? I just would like my problem solved :) I'm not concerned about my fix - I was just mainly trying to demonstrate the problem.
@stanislavkozlovski understood. My proposed wording was aggressively making assumptions, but perhaps we could find a middle ground that still directs users to check their port? The issue with `Received a first response larger than 1MB (Is this a plaintext response?)` is that users won't know what to DO with that response. It would be great to reword it accurately to give some proposed action/suggestions to the user...
My understanding is that Jun is suggesting that we should set `checkHC` to true for the Java client if message format is 1.
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
Sorry for the dumb question, but I am curious if ``` if (partitionsPerTopic == null) return that.partitionsPerTopic == null; ``` Wouldn't be ``` if ((partitionsPerTopic == null && that.partitionsPerTopic != null) || (partitionsPerTopic != null && that.partitionsPerTopic == null)) return true; ```
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
Thanks for the explanation. Make sense.
This check `records.size() < offset` seems a bit sketchy to me. Basically we are assuming that the input topic `data`'s starting offset is always 0, and there is no "holes" in the topic partitions so that the `offset` indicates the number of records we can read from the input topic. Maybe a more robust way to do that would be 1) read the input topics `data` and optionally `repartition` based on `withRepartitioning`, stop when the current record's offset is equal to or larger than the committed offset, and remember the number of records; 2) read the output topics (again optionally `repartition`) from the beginning to the end (use `seekTo`), and check that the number of records are the same as the number of records read from the input. Then we do not need to truncate, and also in verification we do not need to check list size again since they are already checked here.
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
req: This is unnecessary
Not sure if we want to provide information on a topic on which user does not have permissions.
don't think we need to log here . If a topic is not matching to the pattern than why bother logging here.
This condition should probably be like a few lines below: ``` java if (subscriptions.getSubscribedPattern().matcher(topic).matches() && !(excludeInternalTopics && TopicConstants.INTERNAL_TOPICS.contains(topic))) ``` So we would want to extract that condition to a helper method perhaps.
Need to fix indentation below.
nitpick: we usually include a space before and after ":"
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Use diamond (`<>`).
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: blank line missing here
nit: fits in one line
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: extra blank line
nit: I'm sure these fit in a line shorter than the one below
This computation of `broker_ids` can be simpler with a set comprehension like `{node for node in self.nodes if self.is_registered(node)}`
normally in the tests we use `wait_until` instead of manually doing retries, and it generally makes the entire thing a one-liner. it's also generally better since you can just set time-based termination conditions instead of coupling # of retries with the timeout.
this looks cleaner as `"[" + ", ".join(self.idx(node) for node in self.nodes)` + "]"
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
i don't think this is what you want here. `broker_ids` is evaluated once. the `wait_until` implies you want to wait until the `broker_ids` appear in the the log files for the broker. but that doesn't seem like the target you want.
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
nit: we could set a final int for numRetries as: ``` put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), numRetries); ``` and use (numRetries + 1) here to clearly indicate we are trying to go beyond the retry limit.
exception not used.
Use 4 space format to align with other tests.
Even though the config is invalid, the passwords may be valid, so it seems safer not to include them. It would be nice if the sensitive entries in the JAAS config would be communicated in some way to improve debuggability (in the future). Btw, a nit: we seem to use inconsistent capitalisation of the word JAAS in our messages. It would be nice to make that consistent.
Nit: unnecessary new line.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
@ijuma Sorry, I don't know of a standard way of doing this,
You are right. Never mind.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
@satishd I re-read your code and I see that you are only de-allocating on exceptions instead of using a finally like in my suggestion. They both work (in my suggestion, we null the outer variable once we have used it to prevent it from being freed). One difference is that in your approach, we could end up calling `free.deallocate` twice if the first invocation throws an exception (which should not happen normally).
Seems like you want `try/finally` instead of `try/catch` here. It may be worth being conservative and having a wider `try/finally` and nulling out the outer variable if we have used the buffer successfully. Something like: ```java ByteBuffer allocatedBuffer = free.allocate(buffer); try { ... // after successfully using the buffer allocatedBuffer = null; } finally { if (allocatedBuffer != null) free.deallocate(allocatedBuffer) }
Can we not do: ```java if (appendResult != null) return appendResult; else { ... } ```
nit: extra line
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
nit: remove `which is`
Also add `@params topics`
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
Can we clarify that `resolve_canonical_bootstrap_servers_only` applies to the boostrap urls only. For advertised servers, both `use_all_dns_ips` and `resolve_canonical_bootstrap_servers` behave the same.
Each of these configs have an impact on bootstrap and advertized servers. So, we should be clear on what they do for each case.
same question around log level as above
same question around log level as above
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
request "got" re-sent to the control
All the `null` checks at each layer of the call stack make me think that particular issue might be better handled with an exception. Not critical since this is all internal code, but seems like then we'd only need to check version compatibility in one or two places.
nit: move to line above.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
nit: move below the shortcut return below.
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
nit: I'm sure these fit in a line shorter than the one below
nit: extra blank line
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
This can be initialized here and be `final`
Fails checkstyle, needs to be final
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
original was better
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
We would want to check for `InvalidMetadataException` here as well. We need to go back to the Metadata call if we find a metadata error. This is similar to the cases when we call `rescheduleFindCoordinatorTask`.
nit: add `final
nit: use `IntegrationTestUtils#safeUniqueTestName` for the name
Nice coverage with different num.partitions, thanks!
could be named as `processingMode`
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
Same here: we should leave this test here until we remove the deprecated API. (and just suppress the warnings for only this test method)
```suggestion assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD)).gracePeriodMs()); assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD + 1L)).gracePeriodMs()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
nit: add `final`
nit: missing empty line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
No need to check null. We always have to forward oldAgg and newAgg, anyway.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Overall, confusing indentions and line breaks...
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
This field can be final as well.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
Harsha address this, I believe.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
Do we need to set OP_READ? It seems it's always on.
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
I believe this is fixed in my next PR.
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
req: The names of this method and the previous method should be switched.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
Could you please add some line breaks? This and some of the other verifications are too long.
This is needed if you want to persist the limit across a reboot. But we are not rebooting here. Get rid of this.
These two statements don't have the same semantics. Previously we would block until the first line of output, which here probably also guarantees metrics are being reported via JMX. From what I can tell here, it looks like you could easily be running `start_jmx_tool` before the metrics are ready. I don't think the previous version is great, but I think it at least provides the guarantee since we're piping logs elsewhere -- one line of output should indicate consumption has started.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
This should probably just return a boolean
nit: unnecessary newline
This causes a checkstyle failure
Yeah, we're still feeling out the best patterns for handling older versions.
nit: add a space before the `:`.
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular. So, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though 
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
same here have a `retrieve_generation_num` method for extracting the generation number from captured output
I'm +1 on removing this logic, we could not assume that the leader did not change even if it is not the one who get's bounced, since it may join the group late.
Could we rename this to something like "remainingPartitions"
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
Hmm.. Do we know how this is getting propagated in all cases? Some of the responses are handled using a `RequestCompletionHandler`, but NetworkClient currently eats any exception raised from this callback.
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
Thanks for the clarification @hachikuji
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
nit: additional new line
nit: additional new line
nit: additional new line
I think you should create an interface rather than passing in the `AssignedTasks` class, it seems you only need a single method. The contract should be on an interface rather than the class.
Instead of passing the whole AssignedTasks object and hence introduce this penetrated dependency, could we instead throw the MigratedException as is, and only add the task information in `updateNewAndRestoringTasks` and re-throw? Then the exception's message function would depend on whether or not the task field is null.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
@mjsax Got it. Thanks for your response!
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
I think we should have this in this PR already.
The output type is only `KTable<KR, VOut>` is we don't use windowing, right? Otherwise, it should be `KTable<KR, Windowed<VOut>>` ? Do we actually need two different `build()` methods for both cases? Maybe we can clean this up in the follow up PR though. Just something to think about
nit: formatting -> only one parameter per line
Might be overkill if this is the only use case, but we could also add a composite validator.
As above: need to keep default value.
Got it. Thanks. The patch LGTM.
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
`orderInGroup` param is duplicated for key & value converter
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
nit: add a size? There are a few cases in here where we could do this.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
`timestamp` missing (three times)
nit: missing comma `headers[,]`
`timestamp` missing (twice)
`expectedTimestamp` parameter missing (insert before `expectedHeaders` to align with method name.
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
```suggestion import java.util.Collection; ```
```suggestion * Options for {@link Admin#electLeaders(ElectionType, Set, ElectLeadersOptions)}. ```
I don't think we want star imports.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
Yeah, builders are nice for the reasons you mention. However, since it wasn't discussed in KIP-222, I think keeping it package private for now might be better.
nit: add a newline here too.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
explain why `Integer`, `Long` is used instead of `int`, `long`
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
nit: top of class
just `name` should be fine
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
Was the intention here to avoid the deprecation warning? If so, you can just call this method `name()` and do this: ```java @Override @SuppressWarnings("deprecation") // TODO remove this when {@link Joined#name} is removed public String name() { return name; } ``` Callers won't see the deprecation warning as long as they access the method via a `JoinedInternal` and not a `Joined`.
+1 to @vvcephei's suggestion here.
super nit: ditto from above
It should be something like "Sent by an (admin) client to get data about a specific consumers group like main information about members in such group."
It should be something like "Sent by an (admin) client to get data about consumers groups managed by a broker. To get a list of all consumers groups in the cluster, it needs to be sent to all brokers."
I think similar to produce and fetch, LIST_OFFSETS should be sent to the leader of the partitions
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
No strong preference, but I think it's fine if the high-level description of the API is decoupled from the request/response objects, which are mostly just concerned with schemas. The location in `ApiKeys` makes the descriptions a bit easier to find.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
nit: this loop is a little unconventional. Maybe we could use `pollFirstEntry` instead of the iterator? Similarly in `setNumKip500BrokerNodes`.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
```suggestion private static final String ACCEPTABLE_RECOVERY_LAG_DOC = "The maximum acceptable lag (number of offsets to catch up) for a client to be considered caught-up enough" + ```
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
My approach is shared here: https://github.com/stepio/kafka-json/blob/master/src/main/java/org/stepio/kafka/common/serialization/JsonDeserializer.java
I recommend to add special handling for JsonParseException - just log it instead of rethrowing. If such an exception is not handled properly, consuming may be blocked with any non-json message - just text, for example. I got this while playing with Kafka locally: just one simple "dummy" message from console client brought tons of exceptions to my log.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
deliveryTimeoutMs should be mentioned
isFull is no longer used.
No longer used.
This is still not used
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
`windowSize` should be `Duration`
req: drop the `!caughtUpClients.isEmpty()` check here, if it's in the map it should have at least 1 caught-up client
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
req: we'll never hit this, as `taskToCaughtUpClients` only contains tasks _with_ caught-up clients IIUC. Can we just construct `unassignedTasksWithoutCaughtUpClients` as the set `totalTasks - taskToCaughtUpClients.keySet`? We can do that in `assignTasksWithoutCaughtUpClients` and remove `unassignedTasksWithoutCaughtUpClients` from `assignTasksWithCaughtUpClients` entirely
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
Not sure if this will actually be cleaner or end up more complicated, but you may be able to reuse some of the `StickyTaskAssignor` code here which does similar things
This might fix the issue, but don't you think it's a little weird that it's necessary? Wouldn't we have the same problem anywhere that we call `stop`? I'm wondering if we need to fix this in ducktape itself.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Delete this block - this was a specific check for ensuring log output in the test_console_consumer
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
Uggh, yeah, I forgot about this. We kind of inherit some annoying types from Kafka's config setup but I tried to ensure we're using String types where possible. It gets a bit hard to figure out what is valid where -- the JsonConverter actually gets passed a `Map<String, Object>` as that's what is returned by `AbstractConfig.originalsWithPrefix`, but in practice they are all `String` so type erasure allows this to work...
We are setting the default in the config already, we should not duplicate it here IMO. If the config is missing, we can probably throw an IllegalArgumentException.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks @vvcephei -- that is convincing.
Cool, yeah that addresses my concern 
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
There is a built-in for this `Function.identity()`
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
Nit: should we call this `rateUnit`? Same for the other constructor.
The other constructor calls the parameter `sampledStat`. We should be consistent.
nit: I would rather use the full name instead of using acronyms.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
one liner? partitionsInFlight = sendInOrder ? new HashSet : null;
Just a note. This may need consideration together with #1707 where the metadata age may subject to change during producer startup.
Discussed offline with Becket - rework this patch to avoid the null checks elsewhere; i.e., make the accumulator more explicitly aware of the `sendInOrder` requirement
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
Nice catch. Reminds me though, why the second rebalance may not be deterministic in migrating tasks back? I thought our algorithm should produce deterministic results? cc @ableegoldman
Yes, I think so
@ableegoldman is it related to the UUID randomness? If yes please ignore my other question above.
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
nit: we could define this transition list in a variable to be reused.
Would it be slightly simpler to use `private long nextAllowedRetryMs = 0`? In general `long` seems simpler than `Long`
nit: Normally for getters we have the convention of dropping the `get` from the method name.
Checkstyle failure: ``` Name 'JITTER_MAX' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName] ```
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Same here. We should use `builder.stream().toTable()`
As above, I think we should create both tables using `toTable()` operator
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
ditto on (what I think is) the impossibility of this condition being false.
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
```suggestion /** * The value of {@link StreamsConfig#APPLICATION_SERVER_CONFIG} configured for the Streams * client. * * @return {@link HostInfo} corresponding to the Streams client */ ```
```suggestion * Metadata of a Kafka Streams client. ```
aren't these just the defaults? if so, they can be omitted.
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
Yes, this seems fine then.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
I think we can use a utility method provided by the `ConfigDef` class here: ```suggestion List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST); if (topics.contains(dlqTopic)) { ```
Should we log the topic name for this exception? For example, ```has a topic name (xxx) which```
```suggestion Arrays.setAll(topics, i -> topics[i].trim()); ```
This could be `ConfigException` instead. With the `ConnectException` hierarchy we get a little bit stuck because we can't have `ConnectException` be the base of all our exceptions and not redefine some. There are some from core Kafka that are fine if triggered from within the framework, and `ConfigException` falls under that category -- the biggest value aside from standardization that asking for `ConnectException`/`RetriableException` from connectors/tasks is that it is a signal for how to handle the exception, but the runtime code doesn't require that everything inherits from those (and in fact we handle other exceptions from connectors as well, we just would prefer that the connector think about how it wants the framework to handle possible errors).
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
nit: "can not" -> "cannot", same below
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
Ok, this is going to be a little tricky...`removeMembersFromConsumerGroup` is async so we have two options. (1) just ignore the returned result and hope that it succeeded, or (2) check the returned `KafkaFuture` and wait/make sure that it succeeded. Probably we should go with (2) and just apply the remaining time of the timeout. If you haven't mucked around with the KafkaFuture class before, I believe `KafkaFuture#get(long timeout, TimeUnit unit)` is what you'd need here
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
nit: move below the shortcut return below.
Do we need `transactionInFlight` here? Previously we just considered if `eosEnabled` for committing a transaction.
Nit: or empty **if** this worker ....
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
I think we can avoid this alignment style. It leaves us with significantly less space to write lambdas, etc. (another indicator is that this style is not applied elsewhere in the file). Two tab stops in the line below should be fine, even if the declaration above is where it is now.
A bit confusing that a second assignment follows if the `if` statement is true. I'd also call the variable `taskState` (as opposed to `connectorState` above) Ternary can be used here as well: ```suggestion AbstractStatus.State state = request.shouldRestartTask(taskStatus) ? AbstractStatus.State.RESTARTING : taskStatus.state(); ``` (as with any suggestion from github, please check it compiles and conforms to the style)
WDYT about something like this: ``` /** * Signals whether the connector implementation is capable of defining the transaction boundaries for a * connector with the given configuration. This method is called before {@link #start(Map)}, only when the * runtime supports exactly-once and the connector configuration includes {@code transaction.boundary=connector}. * * <p>This method need not be implemented if the connector implementation does not support definiting * transaction boundaries. * * @param connectorConfig the configuration that will be used for the connector * @return {@link ConnectorTransactionBoundaries.SUPPORTED} if the connector will define its own transaction boundaries, * or {@link ConnectorTransactionBoundaries.UNSUPPORTED} otherwise. * @see TransactionContext */ ```
Similar to the mute in `poll()` - the mute could be delayed until a buffer needs to be allocated? It is possible that the channel already has a buffer allocated, in which case, we want it to complete read.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states: 1. Handshake 2. Authentication 3. Waiting to receive a message (receive == null) 4. Received partial message size (receive != null, buffer == null) 5. Received size and partial message body (receive != null, buffer != null) 6. Muted after receiving size due to OOM 7. Explicitly muted 8. Disconnect The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState` is unnecessary if muting is done on allocation failure and that makes the code simpler.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
Both of these cases are still testing the same thing. I think you are intending to set an invalid record count, but this is actually changing the size of the batch (i.e. the size in bytes). So whether it is 2 or 10, we're validating the same path.
as above -- guess some more below
As above: use `assertThrows` and verify error message
`ConsumerRecordFactory` is deprecated via KIP-470 (merge recently) -- please update the code to use `TestInputTopic` -- we cannot merge as-is, because the build would fail if we use deprecated classes.
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
nit: add `final
nit: add `final
prop: Would it make sense to also have an overload with just a flat list, i.e., ``` void runTestWithDriver(final List<TestRecord<Long, String>> expectedResult) ``` Maybe it would simplify the code of some of the tests. Hopefully, you can share some of the code in the overloads.
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
These can be final.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
```suggestion // Most of the records will be an internal sink record, but the task could potentially // report modified or new records, so handle both cases if (record instanceof InternalSinkRecord) { ```
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
`assertNull`s shouldn't be here but few lines bellow.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
`theStore.all()` should be outside -- chaining is not good for this test.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
This is the same code as in `KTableFilter` -- we should refactor and share code.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
No need to check null. We always have to forward oldAgg and newAgg, anyway.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
Is this necessary? The leader epoch is -1 by default.
This was a separate bug, I guess? Might be worth mentioning in the PR description.
I'm ok saving this for #7409.
We'll need to fix this in a follow-up so that followers send the right replicaId.
In the parsing logic, we still convert to struct first before calling `AbstractRequest.parseRequest`. I think we could bypass the `Struct` conversion by changing `AbstractRequest.parseRequest` to take the `ByteBuffer` instead of the `Struct`. ```java public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, ByteBuffer buffer) { ``` Then in the fetch case, we could just call this method.
nit: move `windows` to next line
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
nit: 4-space indention plus move `builder` down one line
Can remove the first two null checks as they are covered in the overloaded `reduce`
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
I see what you mean, and yea that is a fair point 
```suggestion // Emulate losing leadership in the middle of a non-atomic append by not writing ```
Thanks for cleaning up the code duplication.
If you pass the new one, then you can probably get rid of `changedTopicId`
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Sounds legit. Thanks.
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
prop: You can remove this line. You only need `expectLastCall()` if you need to do some further expectation settings on a call that returns `void`, e.g., `expectLastCall().times(3)`
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Could we fail the test right here? It doesn't seem like there is much benefit to returning the missing metrics from the method. That would let us simplify this a little. Instead of this: ```java Set<String> missingMetrics = getMissingMetricNames(expectedMetricNames, expectedGroup, expectedType); assertEquals(Collections.emptySet(), missingMetrics, "Expected metrics did not exist"); ``` we could have: ```java assertRegisteredMetrics(expectedMetricNames, expectedGroup, expectedType); ``` We could probably also drop `expectedGroup` since we only have `kafka.controller`.
nit: could use Utils.mkSet
nit: When this assert failed, we'll see the error messages: `Expected metrics did not exist` ==> expected: `emptySet`, but was: `missingMetrics` I think we should change the error messages, ex: `Expect no missing metrics` ==> expected: `emptySet`, but was: `missingMetrics`
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
Think you might have forgotten to remove some debugging here
Okay, sounds fine.
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
prop: Use `assertThat()` here and in the other overload of this method.
which method can throw ```InvalidStateStoreException``` in this case? It seems to me the potential methods are caught by ```assertThrows```
nit: ```suggestion private static final String storeName = "InMemorySessionStore"; ```
missing coverage on: * expired segments * retention time * fetchSession, which doesn't find a session * fetch
This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.
nit: add `{ }`
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
Should we guard against NPE? (same blow)
nit: add `final`
This could throw a NPE. I think we should guard against this, and throw `ParseException` if NPE happens `ts.split("T")` is redundant and should be extracted into a variable.
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all.
The map is not used.
typo in the test name.
request1 and request 2 are not used.
We may need to call sender.run() one more time to ensure the message is not reenqueued. The reqenqueued message won't be sent out again in the same sender.run().
```suggestion * @param timeoutDuration timeout duration; must not be null ```
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
And why is this test deprecated as well? More generally it seems the `context#getStateStore` function was being deprecated but it was not explained in the KIP wiki.
This is not introduced by this PR: the name has a typo: through => throw
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
If we're just testing broker compatibility I don't think we even need this part of the test.
Nit: space missing before `timestamp_type`.
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: it is naming a source node, not a processor node. -> `"source"`
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
nit: move `topology.globalStateStores(),` to next line.
nit: use `"table-source"` ? It's naming a source node, not a processor node.
why do we make lines longer? harder to read now
nit: empty line.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
Is this designed to have thread-level be the parent of the cache-level? I think originally we want to have task-level be the parent of cache-level (but there is a bug for that so it may not actually be the case).
this pattern of ` if (shouldRecord) measureLatency(X) else X` is not very DRY. You have this same condition (shouldRecord) in several places, and the code for measureLatency(X) vs X is essentially copy-pasted. Instead, add maybeMeasureLatency(f, sensor) with `if (sensor.shouldRecord()) ... `.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
nit: add `a {@link Named} config`
the method name changed to `windowedTable` and `windowSize` parameter is missing
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
`windowSize` should be `Duration`
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
Fails checkstyle, needs to be final
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
Ok, it looks better now. Let's leave it this way, with two lines.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
Yeah, `FetchResponse` will most likely be the last one to convert because we'll have to figure out how zero-copy will work.
I hope we can get rid of those conversion in the future :)
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
req: This is unnecessary
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
this will never be called if one of the assertions fails
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
Do we really want to do this? Might it be better to have a config for this? Or just run it with a fixed number of threads
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
```suggestion // Most of the records will be an internal sink record, but the task could potentially // report modified or new records, so handle both cases if (record instanceof InternalSinkRecord) { ```
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
I'm wondering if we also need to delay the call to `client.wakeup()`? For example, consider the following sequence: 1. disableWakeups() 2. wakeup() 3. poll(0) 4. enableWakeup() 5. poll(Long.MAX_VALUE) The underlying wakeup on Selector would effect the `poll(0)`, but we would suppress the exception. Then there would be nothing to wake us up from the `poll(Long.MAX_VALUE)`.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Yes, the sensors are created in `Sender/Fetcher` to avoid knowledge of the different names to the network layer. Recording is done in the network layer since `Sender/Fetcher` don't see all responses (now that any response may be throttled) and to use common logic.
Discussed offline. This can instead be the producer's retry count. If the retry count is zero, then we will have to allow for at least one metadata request past its max age. So the staleness threshold will be: `retryCount * (backoff + requestTimeout) + maxAge`
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
git: The sentence "So setting the strategy ... matching the given strategy name" reads a bit confusing here. I think we only need to state that when the change of the policy would take effects (the next time when compaction is triggered by the log cleaner thread), and emphasize that for "timestamp" and "header" we would always still retain the last record.
I tweaked this a little before merging.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L149 can reference to this field
We should check that `inputRecords.hasNext` is false after the for loop, or any manners to make sure that pairing lists have the same size.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
nit: `if (max = value) else (max = Math.max)`.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
Let's be consistent and just use string concatenation for both fields.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
This needs to be `totalAbortedThreads`
What do we want to achieve with this throttle? Do we just want to backoff for `THROTTLE_PERIOD_MS` whenever we can't find a connection you sent? I think we should simply use a `Thread.sleep` call. To be concrete, I recommend we instantiate a `org.apache.kafka.common.utils.SystemTime` class and use both its `sleep()` to sleep and `milliseconds()` to get the current time
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
nit: this can be final
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
nit: move this `if` statement below/above version check on line 62
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
Thinking a bit more. This is a bit tricky since we probably can't just continue here. For channels like SSL, we need to do the handshake after the socket is connected. Currently, the handshake will be triggered immediately after the connection is established through channel.prepare() and this has to be done in the same poll(). Otherwise, the selector may not be able to select the key again for initiating the SSL handshake. This applies to all those immediately connected keys not coming from the selector. So, to get around this issue. We can probably create a new KeyIterator that iterates both keys in this.nioSelector.selectedKeys() and those in connectableChannels. The iterator can return a <key, alreadyConnected> tuple. For keys coming from selectedKeys(), alreadyConnected will be false. For keys from connectableChannels, alreadyConnected will be true. Then, we just need to change line 291 to check if (key.isConnectable()) || alreadyConnected) and leave the rest of the code as it is. This way, keys in connectableChannels will be handled in the same as way as those from selectedKeys().
Maybe we should remove the `Collections.synchronizedList` wrapper(s).
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
nit - "once once"
Actually maybe we should wrap it in an `if hasPersistentStores` so that users won't get this warning if they don't have any persistent state
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
Same here, this exception message does not apply to the case this is trying to catch
I'd prefer this was left as it was. The `StateDirectory` doesn't need a `StreamsConfig`.
ditto here, please add a separate check and exception
How about: ```suggestion "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results.", ```
It might be simpler to just use `int transactionTimeout` -- Java will auto-cast to long in ``` if (transactionTimeout < commitInterval) { ```
That makes sense -- a `long -> int` cast is safe, but not the other way around.
We should use `long` as `commitInterval` is declared a LONG in `StreamsConfig`: ``` .define(COMMIT_INTERVAL_MS_CONFIG, Type.LONG, DEFAULT_COMMIT_INTERVAL_MS, atLeast(0), Importance.LOW, COMMIT_INTERVAL_MS_DOC) ```
Do we need to prefix these with `msg.`? Also, it might be nice to control the message format when message keys and values are to be included. I know that's more code, but it could be made to be similar to the other option.
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
Nit: let's not add an unnecessary extra line.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
Is it valid to have a blank string as the replacement value? If not, then the `replacement` config should have a validator that prevents using invalid values, and it probably would be good to succinctly describe the limitations in the doc string. And it may be better to only set `this.replacement` to a non-null string that should always be used. This would centralize the logic of determining whether it should be used in one place, and line 135 becomes a lot simpler and more efficient: ```suggestion if (replacement != null) { ```
I don't see how this is going to work as the callback is happening on the Producer's Send thread
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
Add the stream task id prefix here as well for both exception message and the warning log entry.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
You could call the class Offset (since the metadata is just an optional field).
The purpose of the `Map<TopicPartition, Long>` was to avoid adding a new object type. But since we're doing that anyway what about just making the call: ``` public void commit(List<OffsetMetadata> offsets, CommitType type) ``` where now the `OffsetMetadata` class includes the `TopicPartition`? This is arguably no more complex than the original call in the case where you aren't giving metadata. Also does `OffsetMetadata` imply metadata about the offset whereas in fact this is both the offset and metadata? Other options would be `OffsetCommit` or `OffsetInfo` or `PartitionOffset`. Don't have a strong feeling on this one.
@ewencp As you say, it does hinge on whether this `commit` overload is used often or not. If it is not, then having this _and_ having `commitWithMetadata` seems excessive. You guys have a better handle on this, so I'll leave it to you. :)
We could probably just bump up the number of parameter limit in the checkstyle file to 14.
@hachikuji asked you to change the name originally: ```text hachikuji 5 days ago Contributor nit: the name is a little awkward. How about hasRemaining to match ByteBuffer? ``` :)
Sorry for the confusion. I thought `hasRemaining` made sense initially, but then I realized that the name should be more suggestive of its usage. I'd prefer something like `ensureNoneRemaining`, but it's not a dealbreaker for me.
Nit: maybe this can call the newly introduced `withTransactionalRecords`.
We probably only want to set the DeleteHorizonTime if the batch contains tombstone.
> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later? It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
Currently, for controller initiated ISR change (controlled shutdown or hard failure), we always bump up the leader epoch. Also, the name alwaysBumpLeaderEpoch is a bit weird since the code in handleNodeDeactivated() doesn't directly bump up leader epoch.
+1 for 100 tasks. Thanks.
I think we're testing `testDir` Occupied here, not `AppDir`.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Note this is indeed fixed in trunk but not in older versions.
Thanks for clarification, @guozhangwang !
We've observed from the actual logs that it's not actually.. and the reason is this: https://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
you dont need the `String.format` here would need `%s`->`{}`
Hmm... Yeah, maybe the call in the metadata listener is sufficient. But we definitely don't want to fetch metadata for all topics. Also, it seems unnecessary to request a metadata update blindly. I think `metadata.setTopics` was previously requesting the update only if the topics are not already contained in the Metadata.
@vahidhashemian Are you concerned about the fact that we will have some unneeded metadata cached? That doesn't seem like a problem as long as we clean it up eventually (i.e. on the next scheduled refresh).
@vahidhashemian Feel free to ping me on Google chat if you want to talk about this. It would probably go a little faster that way.
Yes, it makes sense. Good catch.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
Nit: including "execute" here is completely unnecessary. ```suggestion throw new ConnectException("Fail to " + descriptionStr + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
I think we should clear `immediatelyConnectedKeys` at the end of each `poll()` as well.
Oops, that was actually my fault.
The need for this check is a bit unfortunate since it makes the api a bit unsafe (kind of tough to tell at a glance that all current uses are safe, though I think they are). Since we have only three valid options, I was wondering if we could replace the two booleans with an enum representing the disconnect state or something. Not too big of a deal since it's internal, but might be worth considering.
@hachikuji Thanks for the review. Updated to use an enum. Can you do another quick review? If it looks ok, I will merge and backport when the tests complete.
It was a good idea to remove the `processOutstanding` field from `CloseMode`, but I think this would be a little clearer if we kept the `notifyDisconnect` field.
Why is this needed? This is worse than the previous approach as it opens, closes and reopens the file.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
There is a file lock on this file that causes the issue, which might be hiding another issue even on other platforms.
Hmm, why did we do this? I thought we'd have a try/catch block.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
ditto on (what I think is) the impossibility of this condition being false.
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
nit: remove empty line
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
Yeah if it exists elsewhere let's just leave it as is for now.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
nit: should be `named` can't be null
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: add `final`
I guess if you are removing `this.` above, you could remove it here as well for consistency.
I guess if you are removing `this.` above, you could remove it here as well for consistency.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Should probably be `createIoThreadRatio` since we mention `I/O thread` in the messages.
one parameter per line (same below)
This is common enough that there's a util for that and is used extensively: ```suggestion Utils.closeQuietly(retryWithToleranceOperator, "retry operator"); ```
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
I'd suggest we do not print the stack trace in either case; instead, we can just print the exception's `name()`, which is sufficient to tell it is the locking issue.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
Cool, I will create a JIRA ticket for now to keep track of it.
nit: `ERR_MSG` -> `ERROR_MESSAGE` (avoid abbreviations to increase code readability)
nit: this can be package private (what is more restrictive)
I think, we should first check for this condition, because we should only check the most inner store -- if an wrapping store would (be mistake) implement `TimestampedBytesStore`, we would return `true` even if the most inner store does not -- this would be incorrect.
> Why does it do this? Maybe it's a translation layer for other stores? In which case, is it correct for Streams to second-guess the implementation and break its own contract by ignoring the marker interface and delivering non-timestamped binary data? I don't think that would work. Note, on restore, we always get the most inner store and would not call this "translation layer wrapper store" (and thus it would break as we would insert our converter and hand timestamped-bytes to the store that does not understand them). If one want to implement a translation wrapper like this, she need to "hide" it from Kafka Streams and not implement `WrappingStore` (ie, the translation wrapper must be the most inner store).
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
Also set the store name in this test.
Hmm, but if the value is null, we won't hit those points: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java#L638. With this patch, both of these tests fail: ```java @Test(expected = ConfigException.class) public void testEmptyTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, ""); connector.start(sourceProperties); } @Test(expected = ConfigException.class) public void testNullTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, null); connector.start(sourceProperties); } ```
 fair enough
How about returning a Set instead of List? ``` return topics .stream() .filter(topic -> !topic.isEmpty()) .collect(Collectors.toSet()); ```
rewrite test as above using `assertThrows()`.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
You only need to crate that instance once, right? It can be a member of the class
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
remove this line -- not required.
`PrintForEachAction<>` to remove warning. Also in the `print` method
Blank line can be removed.
That's right. Thanks for the explanation.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
nit: would be nice to be consistent on the pattern we use here
nit: add a space so it is "StreamsMetadata {...} topologyName=xyz"
nit: add `final`
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
rewrite test as above using `assertThrows()`.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
As before, we can use `assertThrows`.
We could get away with a single `*`
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
We can use `new ProducerRecord<>(topic, "value");` to simplify it a tiny bit
Sounds fine to me.
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
How about this. First, we can augment the message above to something like this: ```scala log.trace("Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`", position, nextPosition, completedFetch.partition, partRecords.size()); ``` This gives us enough information in the logs to see which partitions caused the `poll()` to return and it tell us exactly where in the log the aborted transaction/control records exist. Second, maybe we can add back your previous log line, but make it a little more terse and put it at trace level: ```scala log.trace( "Returning empty records from `poll()` since the consumer's position has advanced " + "for at least one topic partition") ```
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
I'm inclined to either remove the log line entirely or move it back to its former location in `KafkaConsumer`. Will leave it up to you.
`HashMap` can be replaced by `Map`.
nit: extra line
One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent. That would allow the "Skipping fetch for partition" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
From the default implementation and netty implementation, it looks like they would never be null. But I guess it is ok to do the null-check here since we may make SSLEngines pluggable.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
@guozhangwang Yes, keeping the reference is fine. I was concerned by the check itself because we were calling `generation()` twice in the previous implementation thus we could get two different instances. ``` generation() != Generation.NO_GENERATION && !protocolName.equals(generation().protocolName) ``` I haven't thought about the error-log but it is also a good point.
@dajac just to clarify, are you concerning that the `generation()` may change between the check and the error-log? If yes maybe we do not need to synchronize the whole function, instead we just get a reference of the returned `generation()` call and use that in the error-log, since the generation object is immutable.
i know it was motivated by findbugs, but this was probably a good refactoring anyway :) `RestServer.httpRequest` to *make* an http request has always been a bit awkward
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
Could also use `Collections.singletonList`, which would also make this immutable
nit: I don't spot any, but safer to avoid typos by just having constants for these
Looks good. I like the additional checking that you're doing here.
With further reading, I think the timer should still be 7 days for pid expiration.
Adding a bit more on my thoughts about initTxns call: in eos it is per-task so we can delay the initTxn call as much as possible to make sure the pid does not get expired; in eosBeta however, initTxn has to be triggered at the thread-level, and suppose after that has been triggered and then a rebalance happens which assigned more tasks to you, and restoring them takes quite long time during which we would not process any active tasks and hence not calling txn APIs causing the pid to be expired -- note that since we still call poll regularly we would not be fenced on the consumer side. It might be a big issue if we still have large restoration time practically, and in near future what I can think of for tackling it are in two folds: 1) try optimizing our restoration time so that we are confident such thing should be very rare. 2) allow active task processing at the same time while restoring others, hence keep the txns going; note that it is not always comprehensive since if all actives are restoring then it would not help.
prop: do you think we should name this as `threadProducer` for readability? cc @guozhangwang
@mjsax , just to clarify, are you asking whether https://github.com/guozhangwang/kafka/pull/6 would conflict in some way with restructuring the commit logic? I don't believe so. I'm only changing how the tasks' lifecycles are managed, so it ought to be encapsulated from the thread's perspective.
I think moving forward if we are always going to commit all tasks owned by a thread anyways, we should consider separating the logic of 1) flushing stores, writing checkpoint files 2) calling consumer / producer APIs for commit (or sendOffsets) For 1) it is per-task, while 2) is per-thread. In the refactoring atm we move all of 2) into the RecordCollector which is per-task, but maybe in the future we can make the RecordCollector per-thread shared by tasks to make step 2) "global".
nit: needs a comma after the `{@link ...}`
Overall LGTM. But can we format it differently? We should start a new line for each sentence. If we update the docs later, it make the diff simpler to read.
extra space after `*` needs to be removed
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: missing `<p>` for new paragraph
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
nit: seems you can use `new ArrayList<>`
`assertNull`s shouldn't be here but few lines bellow.
nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with `env.getCluster().nodeById(..)`.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
I don't think we really need this function any more... we can just submit to the executor from the other function.
I was thinking we need to do something about the `ProcessorContext`, too. The interface is quite broad. I think what you have suggested is ok, but I'd probably like to see `register` moved elsewhere, too. At the moment the `ProcessorContext` is accommodating initialization and processing, which means we have methods on it that aren't valid in all circumstances. IMO it would be nicer to have more focussed interfaces that make it less likely that you can do something that isn't allowed. The interfaces can all be implemented by the same object, of course.
About `ProcessorContext`: yeah definitely not for this PR, just throwing it out here for discussion. About `punctuate`: that is a good point, and it makes me thinking if we should just change the return type from `R` to `void` then (and I know we need a KIP for that..).
I'd probably extract this to an inner class. I just find it a bit unwieldy having an anonymous class of this size. I find it a bit distracting. But i'm not overly bothered either way. Just a suggestion
Should we throw an exception here? A ValueTransformer should never call this.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
As I understand it, handleResponse will always be called by AdminClientRunnable from the single 'network thread' (KafkaAdminClient.thread).
nit: format got unaligned. Please check the suggestion fixes it ```suggestion new ConcurrentHashMap<>()); ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
same question as other pr -- this is `sink-task-metrics` instead of `sink-tasks-metrics` in the KIP
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
We can compute this once and pass it to `removeAttribute`.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
We can remove only if mbean.metrics.isEmpty. This line should be added in metricRemoval() method after unregister call.
That does not sound right. If we throw a `StreamsException` the thread will die.
you dont need the `String.format` here would need `%s`->`{}`
Add the stream task id prefix here as well for both exception message and the warning log entry.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
as above, your "link" markup
This is my first time looking at this class, but I noticed that pretty much all of the "payload" of these description nodes are strings. Should we consider returning a string here instead? In fact, if we did that, we could consider calling `toString` on the extractor instead of returning the class name. This would allow authors of the extractors to provide more information about the extractor than just its name. This might be especially useful in the case of anonymous implementations.
Personally I think the default form of `someObjectClassname@hashcodenumber` is fine as to identify the extractor, other than a single topic name, is used, which is probably the most important illustration from `SinkNode` anyways.
`KeyValueMapper` need to be updated
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
Nit: remove `this`
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
Why don't we need this check anymore? It's still done for `globalThread`.
In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
return type is `void` -- remove this line
So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly: ``` log.error("Could not initialize partition time. Committed metadata is corrupted.", e); ```
Making sample age super high sounds better to me, comparing to having a ballpark check.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
True, will we ever want to have this ability? But the change seems fine to me.
Is this designed to have thread-level be the parent of the cache-level? I think originally we want to have task-level be the parent of cache-level (but there is a bug for that so it may not actually be the case).
req: I assume you do not want to test `handleAssignment()` here, so you should not specify behaviour verification on the mock. You could simply write ``` expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment))) .andStubReturn(singletonList(task00)); ``` `.andStubReturn()` is behavior that is not verified in the `verify()` method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test. Same applies to other similar locations in this test.
this global variable isn't great. Can't we hit some rest endpoint that can return this internal values out of the extension? probably makes for a better end to end test too.
another ideaa: add an assertion here? so that we are guaranteed that this method is called.
we can make method this public in `EmbeddedConnectCluster`.
It looks like this is not used anywhere
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
Should be final
nit: Could just to `new ArrayList<>();`
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
We need to keep this public method and deprecate. Perhaps throw an exception if multiple group ids were specified and retain existing behaviour for single group id.
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
Same here. We should use `builder.stream().toTable()`
As above, I think we should create both tables using `toTable()` operator
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
I am frankly not sure, if we need to test left-table-table join explicitly (the table-table join test from above should be sufficient)
The test itself is great -- but the name does not really match -- there is no `GlobalKTable` in this test. Might also be good to pipe some data using the TTD in this test.
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
Great. I will review and merge #1808 first and this PR then can be rebased.
Yeah that is fine. My bad.
Isn't this the same as: ``` tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue); ```
IMO, the code is easier readable if you name the variables consistently like `tagValueToClients` and `tagKeyToTagValues` or `clientsForTagValue` and `tagValuesForTagKey`. I prefer the former because it better visualises the mapping, but that is a matter of taste, I guess.
Yes, I think that makes sense. In this way you can also directly test the method. BTW, you can simply pass `statefulTaskIds` to this method instead of `statefulTasksWithClients`. The keys in `statefulTasksWithClients` should be the task IDs in `statefulTaskIds` and the values in `statefulTasksWithClients` are never used.
```suggestion final Set<UUID> clientsOnAlreadyUsedTagDimensions = findClientsOnUsedTagDimensions( usedClients, rackAwareAssignmentTags, clientStates, clientsPerTagValue, tagKeyToTagValuesMapping ); ```
```suggestion final Map<TaskId, Integer> tasksToRemainingStandbys = computeTasksToRemainingStandbys( numStandbyReplicas, statefulTasksWithClients ); ```
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
Is the following error code also retriable? 0x15 | KDC_ERR_CLIENT_NOTYET | Client not yet validtry again later
Would it be easier to understand if this handled all of the unwrap exceptions after the IOException? And then we could call this method `processUnwrapExceptionAfterIOException`.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
Probably a good idea to check if SASL_SSL for the second case and throw an exception otherwise (in case we ever add another SASL_\* protocol).
If stop throws we won't count down the latch. No harm will result except there will be an erroneous log messages about exceeding the stop timeout.
We should name the thread so that thread dumps are a bit more informative. I _think_ these should be daemon threads because if we're prepared to basically ignore the non-return of `task.stop()` during runtime I don't see why we'd block jvm exit for them.
`toString` is not required.
Is this line intentional? Unit tests normally don't need to print out to console.
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
This is needed if you want to persist the limit across a reboot. But we are not rebooting here. Get rid of this.
This should probably just return a boolean
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
A docstring for this method would be good :)
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Same as above: need to check `clientResponse.hasResponse()`
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
Thanks! Will push this shortly.
OK, makes sense. I was missing that context.
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
As before, we can use `assertThrows`.
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
Not really related to this line. Could you verify that the state store is closed in the unit test that tests line 148? The name of the test is `shouldThrowStreamsExceptionForOldTopicPartitions()`.
Ah I see, thanks!
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
This doesn't need to be declared outside the loop (it can be final at the assignment).
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
Thanks for cleaning up the code duplication.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
nit: it is a tad vexing to see all the `context` prefixes. I guess another option might be to define `RaftClientTestContext` as an abstract class so that the test method can define the test behavior within the scope of a subclass. For example: ```java new RaftClientTestContext(builder) { void run() { assertTrue(client.isShuttingDown()); ... } } ``` Not required, just an alternative to consider.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
```suggestion // Most of the records will be an internal sink record, but the task could potentially // report modified or new records, so handle both cases if (record instanceof InternalSinkRecord) { ```
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Should have a comma after "for example"
Typo: should be "or larger than the number of available brokers"
Please include TopicDeletionDisabledException here.
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
nit: remove empty link
nit add `a {@link Named} config`
`This conversation was marked as resolved by wcarlson5` :)
an -> a
modifiers should be in the order `private final`
nit: both lines missing . at end
Nit `.` at the end
`keySerde` -> `valueSerde`
nit: remove empty link
I'm not convinced we should allow this
It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in `RebalanceSourceConnectorsIntegrationTest` or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.
Why do we need to call `build()` here? (similar below)
nit: use `"table-source"` ? It's naming a source node, not a processor node.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Nit: fix line break
Nit: remove `this`
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
I'd probably pass this in via the ctor. `setStateListener` is always immediately invoked after construction so might as well just add the param to the ctor and do away with this method
nit: extra newline here
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
nit: `testAggregateRandomInput` to match up with other test names
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
Is there any extra benefit in the large input tests? If not, maybe drop the large input tests and rename the first two tests.
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
Is it really worth having this catch here? I think it's best to just let the exception propagate. Any method under test can throw an unknown exception after all.
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
I asked you exactly that a few months ago :) You referenced some old PR but basically the takeaway was, a restoring task hasn't initialized anything but its state, therefore needs to close only the state manager
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
We lack unit test coverage for this case
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
I think we're testing `testDir` Occupied here, not `AppDir`.
typo: we want to test the **case** that poll() returns no records.
```suggestion // This handles a tombstone message when schemas are enabled ```
Please double-check the logic, but I think if `enableSchemas` is false, then we would still effectively return `SchemaAndValue.NULL`. So maybe we can leave `enableSchemas` out of this check? One other bit of (optional) cleanup. The validation in `jsonToConnect` below doesn't make much sense to me. We are already ensured that `jsonValue` has the proper structure. Since this is the only use, I think we can get rid of `jsonToConnect` and pull the couple needed lines into this method.
Nitpick: `maxWaitMs` would be a better match for Kafka's naming convention.
"... is called previously... " without a subsequent call to `unsubscribe()`? Same below.
it treated => it is treated
it treated => it is treated
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
It is a shame we have to do it like this, but i don't see a viable alternative
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Add missing `<p>` tag
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
This is not used.
nit: "another thread wrote to ..."
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
Add missing `<p>` tag
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Overall, confusing indentions and line breaks...
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
nit: `COGROUPKSTREAM-AGGREGATE -`
The output type is only `KTable<KR, VOut>` is we don't use windowing, right? Otherwise, it should be `KTable<KR, Windowed<VOut>>` ? Do we actually need two different `build()` methods for both cases? Maybe we can clean this up in the follow up PR though. Just something to think about
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
Nit: let's not add an unnecessary extra line.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
(especially given that below you use the simple name)
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
Changed it locally.
Is this test needed? It seems that loginContextName can never be null.
Harsha did this.
It seems that we need to set the login time during the initial login as well.
nit: new Integer(1) => Interger.valueOf()
we can maintain `context`, `actions` as class variables initialized in setup().
resourceWildcard => resourceWildcard
AclAuthorizer is not a public class, so it may be ok to make this class public in AclAuthorizer instead of duplicating it here.
Maybe this should be called `fromBindings`.
nit: fix typo `store[s]` ;)
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
These methods look like they are identical to those in the previous test class above
Definitely don't add an abstract class! Let's leave it as is for now, then.
nit: Indentation of those lines seems to be off here.
nit: What about creating a small helper to create a `ListOffsetTopicResponse` for a given `TopicPartition` & co? That would reduce the boilerplate code.
`new ArrayList<>` is suffice
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
Here too it seems like we can use the generic version of `prepareOffsetCommitResponse`.
Yes, does not hurt to leave it. Just for sure.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
extra new line.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
What do we do if there's an exception? If it's expected, let's make it clear
I think this method can still block regardless of requestTimoutMs at least due to the following calls: ``` updateFetchPositions() -> coordinator.refreshCommittedOffsetsIfNeeded() -> fetchCommittedOffsets(missingFetchPositions) -> ensureCoordinatorReady() { // Using zero as current time since timeout is effectively infinite ensureCoordinatorReady(0, timeoutMs = Long.MAX_VALUE) }```
To clarify, there are two steps in `updateFetchPositions`. The first is fetching committed offsets; the second is lookup up missing positions. The latter is asynchronous now, but the former still blocks. We'll need to fix this once the KIP is approved, but it could turn out to be a little tricky.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
We probably have to keep the `size() == 0` behavior for compatibility.
Blank line can be removed.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
Please remove empty lines here and in the other test methods.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Nit: the same key.. ditto below.
Ditto above. I would recommend having consistent explanations here.
"of [a] windowed..."
"of [an] ever-updating ..."
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
In that case could we just use the foreach loop after `ConsumerRecords#records` to get the filter list from the returned list? I just felt leveraging on `ConsumerRecords#iterator` is unnecessarily costly.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
This check `records.size() < offset` seems a bit sketchy to me. Basically we are assuming that the input topic `data`'s starting offset is always 0, and there is no "holes" in the topic partitions so that the `offset` indicates the number of records we can read from the input topic. Maybe a more robust way to do that would be 1) read the input topics `data` and optionally `repartition` based on `withRepartitioning`, stop when the current record's offset is equal to or larger than the committed offset, and remember the number of records; 2) read the output topics (again optionally `repartition`) from the beginning to the end (use `seekTo`), and check that the number of records are the same as the number of records read from the input. Then we do not need to truncate, and also in verification we do not need to check list size again since they are already checked here.
If you use the `assertEquals` that takes a `double`, you can pass a `delta` value, which makes the code a lot more concise.
Seems like this should be a `long`.
We should have a constant rather than using '262' directly
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
It may also be useful to have a test that checks that IBM Kerberos classes are available if `Java.isIBMJdk` is true and `com.sun` Kerberos classes are available if false. In particular, you could check the classes `com.ibm.security.krb5.internal.Config` and `sun.security.krb5.Config` which are loaded in `SaslChannelBuilder`.
We do not throw `InvalidTopicException` "if [the topic] is not found"
"of [an] ever-updating ..."
"of [an] ever-updating ..."
records to it, and reading all records from it, such that
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
Seems like the indenting should be adjusted to the left, right? Applied to other changes in this file too.
This does not need to be a map, a list is good enough since we would not call `addList` with the same groupId, similar to `addError`.
original was better
We don't use UnknownTopicOrPartitionException in listTopics
@mjsax Got it. Thanks for your response!
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
nit: add `final`
We could port this function when it is actually needed.
nit: final on params here and methods below.
This is not related to change in this PR, but after second read, should we setup non-empty topology here. to prevent IllegalStateException to be thrown due to empty topology after some future change. ```suggestion testDriver = new TopologyTestDriver(setupSourceSinkTopology(), config); ```
nit: top of class
nit: I don't think the copier uses group instance ID (maybe we could add support for that separately?), so I don't think `FencedInstanceIdException` is possible at the moment.
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
req: This is unnecessary
Could we rename this to something like "remainingPartitions"
nit: fill in `@return` docs
nit: `planned in` -> `planned for` ? (similar below)
an -> a
an -> a
method name changes
I know this is a bit opinionated, but ... I think we should make an effort to make all locals `final` where possible. It is just a few extra keystrokes (that intellij can do for you!), and it helps to eliminate a class of bugs.
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
I wasn't aware that `finally` blocks had a large impact on performance, and haven't been able to find anything definitive on the subject that indicates they are. Can you provide a reference that backs up this claim? Fwiw, using a `finally` block would be more concise and readable here: ```suggestion try { return Utils.newInstance(klass); } finally { compareAndSwapLoaders(oldClassLoader); } ```
The other benefit is the way we are loading converters it could technically be loaded in a different plugin loader that what are keyed in the map since we're loading the ConnectConfig after swapping the class loader in `startTask`. If we prefer to keep it the way it is here, we need to revert that change.
This method is called from within `newConverter`, `newHeaderConverter`, and `newConfigProvider`, so mentioning "converters" and getting the available converter implementation classes is actually wrong when used to find the config provider implementation class. One way to address that would be to pass in additional parameters to the method, but since we want to backport this to branches before `2.0` we have to make that work without method references. So one simple option is to rename the method to `converterClassFromConfig` and add a new method for `configProviderClassFromConfig` that does essentially the same thing but is tailored for config providers. Perhaps a better alternative is to dynamically determine the name and the `pluginNames(...)` based upon whether `klass` is a subtype of `Converter` or `ConfigProvider`. This keeps a single method, but is a bit more dynamic.
Another way to put this is that maybe we should make sure our built-in converters can handle calls to both `configure(Map, boolean isKey)` followed by `configure(Map)` with the `TYPE_CONFIG`. then, only things that are `Configurable` see the second one. old implementations wouldn't see it as they would not have implemented `Configurable` (except in unusual circumstances). New implementations could be warned by docs in `HeaderConverer` that they should take care to handle that sequence of calls.
This still doesn't seem correct. We're only invoking configuration if the plugin implements `Configurable` afaict. This does not work for `Converter`s implemented against the new API and assuming the "forward" configuration. We *must* always invoke the "old" `configure(Map, boolean)`, and only invoke the `Configurable` version as a backup. Possibly it would make sense to indicate on the `HeaderConverter` that the `Configurable` methods should be idempotent if we need to be able to implement both. Not sure if we can test this easily with unit tests, but I think we might want a plain old `Converter` (that does not implement `HeaderConverter`) in tests to validate compatibility... but it's possible we'd need either integration or system tests to properly validate.
I don't think we need extra `toGiveUpTopicPartitions` to store the partitions to be deleted. We can log the warning message in L1103 here directly
OK, make sense.
nit: additional new line
Do you think this is better? //We might get`UnknownTopicOrPartitionException` after submitting their offsets due to topics been deleted. We should update the offsets list here. The worst effect is that we may keep retrying to commit the offsets for the topics not existed any more, before timeout reached.
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
> But this wasn't described in the KIP and wouldn't be a source compatible change (existing code with a catch (InterruptedException) As it can cause compatible change, we should deprecate it and then add a new method (for example: get(T value)) to replace it. This can be discussed in another issue :)
"do nothing" is probably the right thing here.
The variable name should be changed.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
"... as producer is closed" Same elsewhere
ok - same thing three times. Maybe extract it to a method `verifyTransactionInflight`
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
prop: abortTransaction can also throw ProducerFenced.
prop: abortTransaction can also throw ProducerFenced.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this: ```java try { producer.beginTransaction() producer.send(...) producer.sendOffsetsToTransaction(...) producer.commitTransaction() } catch (Exception ) { producer.abortTransaction() } ```
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Sensor names don't appear in JMX.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
req: The names of this method and the previous method should be switched.
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
nit: we can make this a `static` function and rename it to something like `convertToVoters`.
It seems you can move this line after line422.
We could change other callers of `isGlobalSource` and use the index instead? E.g. `describeGlobalStores()`, and then this `isGlobalSource` itself could be removed.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
I do not think we need to emulate the behavior of the java uncaught exception handler here. I do not see why a user should try to reset the uncaught exception handler. If we receive this requirement, we can still add it. I have the impression the "reset" makes this code unnecessarily complex.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
Where do you cleanup the childrenSensors object? Otherwise we will maintain a reference to the Sensor objects always.
Maybe extract lines 361-363 to a method named something like `createSinkNodeWithTopic` returning a `SinkNode` then this method body would just have single `if-else` statement. But not a big deal could stay as is.
Nit: param alignment.
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
Should this ever happen? If it does happen should we consider it a bug? Ditto for the other `hasNextCondition`.
This does not need to be a map, a list is good enough since we would not call `addList` with the same groupId, similar to `addError`.
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
Ok, I took a closer look and had a bit of a flash-back to how confusing these metrics are... Maybe https://github.com/apache/kafka/pull/7057 will help.
Thank you @vvcephei !
This is not part of the PR but I realized that `CumulativeCount` is in `streams.processor.internals.metrics` but not in `common.metrics.stats`. Is this intentional? @vvcephei
one parameter per line (same below)
I didn't think of that before, but now that you mention it, the change makes sense to me.
Good point. I think it is still worth keeping the optimization, although typically the the producer will only allocate poolable batch size, so the actual memory allocation should not happen very often.
Yeah, it's not pretty. However, reducing the concurrency of `BufferPool` in the common path is not desireable. We definitely need to handle OOMs correctly, but they are relatively rare and it's OK if that path is slower.
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
nit: line too long
So you want to advance `recordFactory` time too? (similar below)
We should initialize `TopologyTestDriver` with a fixed mock-timestamp and use this below instead of calling `System. currentTimeMillis()`
Is it intended that all records fall into a single window / segment? What was the root cause of the bug and how does this data/timestamp pattern cover it? (cannot remember)
Thanks for clarification. So it does not really matter what data we use in the test. Would there be a minimal setup that exposes the issue reliably? Just wonder if the test could be "more clear" from the code. If not, we can leave as is.
nit: insert empty line
For `shouldNotReturnDuplicatesInRanges` it seems best to use `processorContext.timestamp()` -- `processorContext` is passed in `init()` do you just need to add a member variable to the `Transformer` to store is so you can use it in `transform()`
old code sets `retainDuplicates=true` but new code sets `retainDuplicates=false`
It seems like we can migrate away from the deprecated method in this test.
Thanks for clarification, @guozhangwang !
We've observed from the actual logs that it's not actually.. and the reason is this: https://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception
nit: use `"table-source"` ? It's naming a source node, not a processor node.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: it is naming a source node, not a processor node. -> `"source"`
nit: insert space `String... expected`
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
nit. Add `{ }` to block (we always use them). Same below.
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: top of class
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time.
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
Tiny nitpick: `TopicPartition.toString` does what you are doing manually here, so you could simplify it by just saying: ``` java "Batch containing " + recordCount + " record(s) expired due to timeout while requesting metadata from brokers for " + topicPartition ```
Although, you simplified this code, you did not apply all simplifications I proposed in PR #7914. I think you can even simplify this code further as shown here: ``` return Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); }); ``` This simplification can be applied also to the other methods below. If you have any concerns about this simplifications please share your thoughts.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
The number of elements is not always 1. Each created thread-level sensor is added to this queue, e.g., `processLatencySensor`, `pollRecordsSensor`, etc. Check out the callers of `threadLevelSensor()`. Each queue contains all thread-level sensors for one single stream thread.
this is the same as above: we should extract to a method to avoid inconsistencies if one might get updated but the other one slips...
Although, we use this in other methods, I think the following is a bit simpler to read: ```suggestion final Sensor sensor = metrics.getSensor(fullSensorName); if (sensor == null) { clientLevelSensors.push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); } return sensor; ```
Yes, I think it's worthwhile to check the result of `Connector.config()` just in case `Connector.validate()` is overridden and the default check there is no longer used.
I was referring to the call to `connector.validate` two lines above. That is where the other null check in this patch in `Connector` would be applied, unless the user has overridden `validate`.
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
Since you've noticed that these don't depend on any instance state, do you think it makes sense to move them to a separate file (e.g. `ConnectorUtils`)? The `instantiate` function below would be another candidate.
And again - i'd say it is definitely worth having what i mentioned above
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
one parameter per line
nit: Capitalize `p`.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
`Integer.toString` is a slightly more concise way of doing this.
We also need a test to validate that some extensions can be ignored (neither valid nor error).
@mimaison It is true that the test would check only one class depending on the JRE. But it checks that the relationship between `java.vendor` and Kerberos classes matches the expectation in the code (for that JRE). The other unit test is checking if String comparison works, which is fine as a unit test, but it doesn't really test the actual System property based on the JRE.
It may also be useful to have a test that checks that IBM Kerberos classes are available if `Java.isIBMJdk` is true and `com.sun` Kerberos classes are available if false. In particular, you could check the classes `com.ibm.security.krb5.internal.Config` and `sun.security.krb5.Config` which are loaded in `SaslChannelBuilder`.
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Nit: dowrade -> downgrade.
Should this be `num_lines=3` (cf. L116 and L126)
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
After discussion on #4713 I think this idea should actually work. Nit: Can we rename the lock to `LOCK_FILE_NAME + " -" + taskId` though.
Maybe we should add one case where `position > 0`.
What's the purpose of this warning? It doesn't seem needed.
extra new line.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
`consumerProps` and `producerProps` are of type `Map`, therefore the `.toString()` is probably not readable. So you'd need to convert these into a comma-separated list sth like `K1=V1,K2=V2,...Kn=Vn`.
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
Fair enough :)
I think we should have this in this PR already.
nit: formatting -> only one parameter per line
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
Hmm... It seems a little inconsistent to use the offset of the first record. When the batch is empty, we use the base offset of the batch. Shouldn't we do the same here? Otherwise we will end up with some batches which have base offset smaller than the segment base offset. Note that the base offset is always preserved by compaction.
nit: why do we need to pass the base offset as a separate parameter? We are already passing `batch`.
We only need to write the DeleteHorizonTime if the batch contains a tombstone or a control record. This increases the chance for more optimized writeOriginalBatch case to happen.
We probably only want to set the DeleteHorizonTime if the batch contains tombstone.
Hmm, it seems that we can just get deleteHorizonMs from batch directly, instead of through filter.
nit: formatting -> should be in the line above.
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
nit: if unused, remove the variable instead of suppressing a warning.
ditto here and others below
rewrite test as above using `assertThrows()`.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Note that Kafka only supports kerberos as the SASL mechanism.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Yes, this seems fine then.
I do not think you need to put an entry if you use mocks.
This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.
To get rid of the test failure, you need to change this: ```suggestion final KafkaMetric metric = metrics.metric(new MetricName("prefix-scan-rate", STORE_LEVEL_GROUP, "", tags)); ``` Sorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
Ah, sorry to say, one more thing slipped by me before. We should `verify(inner)` at the end of both of these tests. It should actually fail for `shouldNotPutIfSameValuesAndGreaterTimestamp` because we should _not_ call `inner.put` in this case. To fix that, we would just delete L202, where we set up the mock for `inner.put`. Then, the mock would be initialized to expect no calls, and the verification would fail if we did call it.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Perhaps instead we could add this to a mixin type. Then if we find cases where getting accessing to the `ApiMessage` generally would be useful, we could just use `instanceof` checks. These would ultimately go away after the conversions are finished.
Is there an advantage to pulling this up? Seems like we still need to update a bunch more classes. Until we have all the protocols converted, it might be safer to find another approach.
I have a PR that does need. I really need to get that over the line.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
Are there ever situations where users would want the old behavior (to have access to the `ProcessorContext` for the record that triggered the lookup, rather than the context for the record that's being looked up)? For example, if the topic name is relevant for the transformer and all records (including the current one that triggered the lookup and the one being processed) are from the same topic, then the old behavior gives access to the topic name but this new behavior doesn't.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
req: Since we do not need to validate `valueTransformer`, could you please remove it from the `verify()`.
nit: add `final` (2x)
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
That makes sense. I did not think about the reconfiguration case.
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
I think this could just be if(!SecurityProtocol.values().contains(securityProtocol))
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
ditto to `KStreamImpl`
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
nit: ".. select the grouping key and the value to be aggregated".
nit: remove `this.`
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Add the stream task id prefix here as well for both exception message and the warning log entry.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
you don't need this. Junit gives you a new instance of the test class for every test method
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
You should also compare `expectedValues`.
this is not needed as every test method gets a new instance of the class
nit: also add java doc for type `T, O` here
nit: in spite of the getter convention, I still prefer setters be prefixed with `set`
Since we're only using a batch size of one, we should be able to just verify that we got a list of size 1 and it contained what we wanted, right? Also, we should also try to be robust against malformed responses that don't include the group we asked for (this will currently throw an NPE if we get one of those).
This is a bit subtle, but I think we want to raise the `InvalidMetadataException` rather than constructing a new `Call`. The problem is that we lose the retry bookkeeping which means these retries will not respect the backoff. By throwing the exception, we let the retry logic in `Call.fail` kick in. This would be consistent with the logic in `getFindCoordinatorCall`.
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
Nit: "..and producer's {@link DefaultPartitioner}".
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
records to it, and reading all records from it, such that
nit. I think there is `.` missing `since 3.0[.] Use`
Should we guard against NPE? (same blow)
nit: add `final`
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
should probably remove the `get` prefix and use `optionValue` since that is the convention followed in the Kafka codebase. Lots of other occurrences like this.
Thanks for the follow-up.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
Ditto here for different exception types.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
nit: `addMetadata` -> `put`
`setNeedsCommit` -> `{@link #setNeedsCommit}`
`needCommit` -> `needsCommit`
`encryptedString` cannot be `null` IIRC -- seems we should not check for this condition to no mask a potential consumer bug, but rather fail-fast.
(very minor) nit: inconsistent naming of these in this class
I'm not sure it actually matters since users are unlikely to construct this object manually, but it seems like we should use `ConfigSource.DEFAULT_CONFIG` if `isDefault` is true and `ConfigSource.UNKNOWN` otherwise. Then `isDefault()` will continue to work with this constructor.
I think you need `equals/hashCode` since `synonyms` is now included in the same methods for `ConfigEntry`.
nit: unneeded newline
And again with `final` if you don't mind
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Checked with Jun and this is fine.
It doesn't seem that the client needs principalBuilder.
That's right, changed it locally.
We probably shouldn't change this default to true -- we should override it in the specific test we need by calling `self.mark_for_collect(consumer, 'verifiable_consumer_stdout')` where `consumer` is the `VerifiableConsumer` instance. stdout for verifiable consumer can result in _huge_ log files, so collecting them by default will result in very large archived data for some tests.
I don't think this is a problem yet, but we should start thinking about these `Service` classes as at least semi-public interfaces. I know we rely on them in muckrake (although perhaps not this particular one yet) and I know others are starting to/planning to/want to be able to build tests on top of the pieces included in Kafka. While these definitely aren't the same as our client APIs, I think we should make an effort to provide some degree of compatibility.
A docstring for this method would be good :)
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
> I ignore 1 out of every 2 punctuations Uh. That's kinda painful... I think we need to discuss this in more details, ie, what semantics we want to provide. \cc @bbejeck @dguy @guozhangwang @miguno
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
Doesn't this introduce the possibility of conflict between two plugins (or I guess specifically connectors, since those are the only ones we strip suffixes from) which have different fully-qualified class names, but the same simple class name? Or where they would have the same simple class name, except that one ends with `Connector` and the other doesn't? In practice this is unlikely to come up but if we support it at the moment, probably best to take care here to avoid introducing a potential regression, especially if someone for some reason wants to run, e.g., two different `MySqlSink` connectors on their worker.
Also, it seems like using a default of `PluginType.UNKNOWN` here might be suboptimal. If someone wants to the view the config for a REST extension, for example, they'll end up seeing an error message later on (in `AbstractHerder::connectorPluginConfig`) that says something like "Invalid plugin type unknown. Valid types are..." I think it'd be clearer to users if we could differentiate between these two cases: 1. User requests config for a plugin that does exist on the worker, but which we don't expose config information via the REST API for (such as a REST extension or a config provider) 2. User requests config for a plugin that doesn't exist on the worker Status-wise, In the case of 1, a 400 response probably makes sense, but for 2, a 404 response might be more applicable.
Could we just add a toString function to ProcessorNode / SourceNode / SinkNode so that this code just becomes: ``` print(node) { // node.toString // also include listing children names foreach(child <- children) print(child). } ``` And also you do not to maintain the mapping as well.
I see that you are actually printing the sink nodes. I'm wondering if this if condition is necessary since in line 85 this function will be skipped anyway.
`instantiateConfigProviders` since this is potentially creating multiple providers
the message format => that message format
We could make this field access `public`
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
nit: we can use `map#compute` to replace getOrDefault + put.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
More specifically, there's no point in having this if we close the network client before we even get to the point where the request has reached the network layer.
Don't we have to do something with the futures returned by `send`? How do we know if the requests completed? Also, cc @hachikuji.
The consumer close code needs to block until the fetch sessions are closed. `KafkaConsumer` has `close()` and `close(long timeout, TimeUnit timeUnit)`. In the case of the latter one, we don't want to wait longer than the specified time.
I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably.
I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
We should pass in `record.topic()` instead of `""` into `deserialize`.
why use `topicName` here? Should it not be `failed: key=X actual=A expected=B...` instead of `failed: key=X <topicName>=A expected=B...`
`start` is not used.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
I think it is a bad idea to let callers concern about calling `maybeExpandBuffer`; previously the caller is abstracted from this away.
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
I guess "Necessary" still seems kind of open-ended/vague. By that you mean, "is not already created", right? But maybe we should wait until we see the final form of this method in case there are any further changes, and then we can go back and try to fish out a more specific name
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
`if (ignoreWhenShuttingDownOrNotRunning && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING))`
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
Hmm, it was moved outside of the `stateLock` for this reason: https://github.com/apache/kafka/pull/3622#discussion_r131438053
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Wonder if it might be simpler to initialize `partitionsToRetry` from the request key set.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
The slf4j `{}` placeholders will not work here since we are constructing the message ourselves.
We can use `ApiResult.completed()`
the fist argument is still "expected value" so we don't need to switch the args. There are many similar occurrence.
This change seems not needed.
please revert those changes as first argument should be "expected" value.
Not really sure this has value if the test case expects the leader change correctly.
Technically, this is `numDrainedRecords`.
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
There's a pattern for all of the Trogdor JSON code where we don't use null anywhere. The problem with null is it gets annoying to check each collection for empty vs. null, each string for empty vs. null, etc. etc. null is also handled kind of inconsistently in Jackson. Sometimes Jackson will serialize a field that is null as `"foo": null` whereas sometimes it will just omit the field. (I think that `"foo": null` is actually not conforming JSON, by the way...) There are probably ways to configure all this, but null doesn't really provide any value 99% of the time, so it's simpler to just treat empty as null.
We don't use null entries in JSON, because it gets too confusing. You should check against empty string here.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
records to it, and reading all records from it, such that
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
an -> a
It would be best if this test used a replication factor of 2. With a replication factor of 1 we will have no regular replication traffic occurring when the producer writes messages. It would be good to have both throttled replication and non throttled replication happening at the same time.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
same here. let's make all method params as `final`
nit: creating restoredPosition is not required if !constinceyEnabled
Good point! @vpapavas , can you handle stuff like this in a follow-on PR? I'm doing a final pass to try and get this one merged.
use `try-catch` instead of `expected` annotation -- not a single line test.
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
nit: add `final`
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
nit: can we make this debug level? Otherwise it will make this test a little spammy.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
NIT: I think we should keep the check consistent between subscribe(topic) and subscribe(TopicPartition). I am fine with either way of checking.
Hmm, but we're not actually calling the listener here. We do that separately.
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
We do not need to mention "partition" here since it is supposed to be abstracted from users, ditto below.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
Overall, confusing indentions and line breaks...
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
This field can be final as well.
nit: how about just put these two in one line? We usually only use multi-lines if there are 3+ parameters.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
Should be more specific about the type of error being caught -- catching all exceptions should be reserved for very special cases, like protecting the top stack frame of a thread to avoid uncleanly exiting the thread. I suspect that here you specifically want to capture `CalledProcessError`, which indicates an issue running the command on the remote host and/or `ValueError`.
Does mirror maker support multiple consumer configs? A quick glance at the code suggests it only supports one.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
It seems you can move this line after line422.
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
I have a small preference for `Future<Map<Integer, Future>>` because it seems more aligned to how we do it for other APIs but I don't feel strong about it.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
i don't think there's a good way to do this directly with `monitor.wait_until` because it was written originally to look for a fixed pattern (we've mostly used it to monitor startup where we know a specific message gets logged once the service is actually serving requests). you could either look for the fixed pattern (e.g. if this message only gets logged once and before the broker ids it has a known msg) or do a `wait_until` yourself. `wait_until` takes a function, so you can just wrap up the check in a local function that returns a boolean and pass that into `wait_until`.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
should this be `< 0`? Otherwise, we skip index 0
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
Do we need the if/else? Since this is a unit test, it seems OK to just assert that the first element is the rate and the second is the total.
Nit: should we call this `rateUnit`? Same for the other constructor.
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
Maybe add an `addSubtopology` method? I don't really like the idea of a) exposing, and b) mutating an internal collection.
Maybe we can create a JIRA for tracking, but it's not that important for now. Since the common parts that can be consolidated may be not much: in the actual topology building process we need to set the internal topic names, set copartition topics etc which are not needed for the topology description building at all. What I was originally thinking is the the topology building process may be extending from the topology description building process with its additional functionalities like I mentioned above, but I am all hand-wavy on the devil details now.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
nit: could make access private and get an accessor.
This will probably be subjective, but I'm ok with "hop" for now.
It seems better to say Producer.send() instead of send.
@sutambe I also think we should mention the scenario that a Record is added to a batch that is about to expire.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
Yeah, I don't think it's worth doing it for broker properties at the moment.
"consumer group management" => "group management" for generality? "When the session timeout expires..." => "when a consumer's heartbeat is not received within the session timeout, the broker will mark the member as failed and rebalance the group".
nit: needs a comma after the `{@link ...}`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: missing `<p>` for new paragraph
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
This doesn't really work as a general pattern. What I mean is that for `ElectPreferredReplicas` there is no analog of `NewTopic`: The `AdminClient` API is just a `Collection<TopicPartition>`, so there's no handy class in which to put the conversion code. It might be worth having a consistent place to put these conversion functions (for AdminClient protocol messages, at least); either a single common class (`AdminClientProtocolUtil`), or a `ElectPreferredReplicasProtocolUtil` etc. The latter would have one benefit: With the use of nested classes there's the possibility for imports like `import org.apache.kafka.common.message.ElectPreferredLeadersResponseData.Result;` to collide when another protocol uses a nested class named `Result`, and using outer class qualified type names makes the code ugly. Having a `ElectPreferredReplicasProtocolUtil` etc would avoid this.
This is unrelated to this PR but the check brings up a good point. Maybe instead of throwing an error on the pattern subscription finding an overlapping topic and then restarting the thread and trying again. Maybe we can pause a topology until the topics are not longer overlapping? EDIT: I can see this in the other PR
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
as above: we need to remove adminPrefix configs
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
add check for restore-consumer and admitclient
How about we get rid of the problem altogether -- define `WorkerConfig.rebalanceTimeout()` that returns null by default, and then override it in `DistributedConfig` to return `getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG)` since that will always be an integer. The latter's method is trivial, and the code in `RestServer` becomes simpler and easier to read. I think there's enough precedence in `StreamsConfig` and `ConverterConfig` to add getter methods (without `get` prefix). The fact that it cleans this up this significantly is also good justification.
Could also use `Collections.singletonList`, which would also make this immutable
static import maybe better
nit: I don't spot any, but safer to avoid typos by just having constants for these
this global variable isn't great. Can't we hit some rest endpoint that can return this internal values out of the extension? probably makes for a better end to end test too.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@mjsax What you suggested sounds right to me.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
the method `restorePartition` is no longer used and can be removed
The goal of the ticket is to actually remove this check.
Should not have an implementation but call overloaded method.
remove this line -- not required.
This should be the only method with actual code. All other overloads should call this one.
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Could it print the origin exception also? ```log.debug("xx", e)```
a tab is missing here for alignment too.
Nit: space before `:`.
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
nit: newline after if condition, also space before and after `!=`, and space after `if`.
Thanks for the explanation. Make sense.
rewrite test as above using `assertThrows()`.
Oh, right, I forgot that the metrics registry returns the same copy of the sensor when all the name, description, and tags are the same... Thanks.
nit: I slightly doubt whether these exact match tests are necessary
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
`error` is unused
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
Let's not create the `InternalSinkRecord` until *after* the transformation chain has been applied. That way we're not affected by any SMT that creates a new `SinkRecord` via a constructor (where we'd lose our `InternalSinkRecord`) rather than `newRecord(...)` (where we'd keep the `InternalSinkRecord`). ```suggestion ```
Sure, I'm just saying that `msg.value()` is an array, which does not have a useful `toString` anyway.
Could store `entry.getKey()` in a local variable since it is used several times
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Map.Entry<String, String> to avoid the check below
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
I think nicer to just `return requests.first()` here
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
Yes, that is the reason for the inconsistency. Obviously it was a mistake to let `OffsetAndMetadata` be serializable in the first place.
nit: I think parenthesis are a little more conventional.
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
`while` seems to be missing
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
I think `will go` should simply be `go`.
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
`deteremined` => `determined`
Should have a comma after "for example"
Should be larger
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Typo: should be "or larger than the number of available brokers"
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
This doesn't read well. Assuming it needs to be as long as `inactivity-gap` plus `grace-period` then the following reads better to me: ```suggestion * @param retentionPeriod length of time to retain data in the store (cannot be negative) * Note that the retention period must be at least as long as * inactivity-gap plus grace-period. ```
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
I would move line 328 and 329 to before this line.
I would move line 330 and 331 to before this line.
Same as above mentioned, the validation didn't get handled in new API.
ditto about the log level (also for the below uses of `debug`)
naming nit: topicConfigsWithRetention
In this case I think we should include some error details here. In particular, the last seen error for each topic. I'm worried about cases where we try to create but the create times out but is eventually successful. We'd return an error back, but the user would have no way to know that setup failed because an internal topic already exists.
nit: maybe iterate over `entrySet()` instead.
Ditto here for different exception types.
Nit: add `final`
as above: we need to remove adminPrefix configs
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
could we turn this into a loop? i.e., sth-like? ``` List<KeyValue> records = Arrays.asList(...); for(KeyValue record : records) { mockTime.sleep(10); IntegrationTestUtils.... } ```
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
This is public API meant to be used by users. I don't mind if our tests are a bit more verbose but we should aim to have succinct public APIs
Does this need to be public? Making it private forces use of `of` method, which I think is good.
nit: Update the unit test to use this new method. Jose changed them to use C'tor.
That's correct. The implementation becomes a bit tricky, as we can't just use `Arrays.asList` and be done.
@ableegoldman is it related to the UUID randomness? If yes please ignore my other question above.
Yes, I think so
Nice catch. Reminds me though, why the second rebalance may not be deterministic in migrating tasks back? I thought our algorithm should produce deterministic results? cc @ableegoldman
nit: extra space after second COMMIT
nit: why double space? (similar below and further below)
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
Thanks for the catch!
nit: ditto here, `cluster()` will reconstruct a new object on each call.
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
I am wondering if this can be lowered to `DEBUG` since it is handled internally.
How about we get rid of the problem altogether -- define `WorkerConfig.rebalanceTimeout()` that returns null by default, and then override it in `DistributedConfig` to return `getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG)` since that will always be an integer. The latter's method is trivial, and the code in `RestServer` becomes simpler and easier to read. I think there's enough precedence in `StreamsConfig` and `ConverterConfig` to add getter methods (without `get` prefix). The fact that it cleans this up this significantly is also good justification.
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
Yeah, the logic seems right to me.
We want the exception to be thrown in either case right? If requestTimeoutMs is greater than either sessionTimeOutMs or fetchMaxWaitMs an error should be thrown? I think this a difference between the english meaning of "and" and the programatic meaning of "&&".
This should say `AdminClient`, not `Consumer`.
nit: why double space? (similar below and further below)
Okay, sounds fine.
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
`long` -> `Long` is a binary incompatible change.
This is also related to the following PR https://github.com/apache/kafka/pull/1015 that uses sentinels in a number of places. It would be nice to be consistent on -1 versus null.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
nit: seems we don't need this method anymore since `addPartitions` is idempotent.
Unused import - this is causing checkstyle failure in the PR build.
This method is usually called `setUp()`.
Ah ok fair enough -- thanks!
nit: add `final` to cleanup code
Nit: rename to `shouldThrowOnInvalidTopicNames`
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
Private and call from the constructor? We only ever call this immediately after instantiating the class.
If it is no more an integration test, this should be removed.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
This reminds me that we should change the internal interfaces of `Segments` etc to use the new StateStoreContext, not the ProcessorContext anymore, will create a new JIRA.
This field can be final as well.
nit: could make access private and get an accessor.
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
nit: move .collect to new line
nit: Please fix code style.
nit: Please fix code style.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
There is no need to test this as it is calling the same method as above.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
+1 for consistency
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
nit: maybe `disconnect` is a better name given actual behavior.
Wondering if it would be more useful if we can control the faults more explicitly. For example, we could add a hook to make the coordinator temporarily unavailable and to restore it later.
If we follow the same pattern as `ConsumerGroupOperationContext`, then this could be a static method which takes the response as a parameter.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
Since this is basically just a pass-through method anyway, there isn't that much to test here -- you could simplify this test just to use a simple set of connectors you create yourself. There's a lot of code in this test when all you really want to see is that the set makes it back out of the call to ConnectorPluginsResource
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
Looks good. I like the additional checking that you're doing here.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
nit: use class `import` to avoid long name here
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
`apiKey` is of type `ApiKeys` while `requestHeader.apiKey()` returns a `short`.
requestHeader.apiKey() can just be apiKey.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Is this used anywhere? I see we have changed client code to use the other C'tor.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
We seem to have lost the `info` message from the original.
No worries, let's keep the scope small for now. Just wanted to raise the question
One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.
I'd go for being consistent with the other logging statements
Thanks. I will make another pass now.
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
This is also kind of unclear (what is a max right window?), but I get that we can't call it `previousRecordRightWindow` since we don't know that it is a previous record or not at this point. I think yet again, just keeping track of the previous record's timestamp as we iterate through the windows, will be the most clear; if `previousRecordTimestamp` is still null by this point, we know right away that we don't have to create a previous right window. And then we can actually drop the `rightWindowNecessaryAndPossible` check altogether, since we know the current record has to be in range of the right window of the previous record (since we're in `processEarly`). The one exception is if the previous record and current record are on the same timestamp, so we can actually skip the previous right window creation if `previousRecordTimestamp ` is `null` OR equal to `timestamp`
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
nit: we might want log different message if we're ignoring due to a fatal state instead of due to a bumped epoch or ID.
Discussed offline, but I guess the one case where it is not safe to adjust sequence numbers is when the number of retries are exhausted.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Looks like it won't happen since we only lock on deque object, but just want to confirm, to make sure it won't break anything.
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
as above (similar below)
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
Could use `equalsIgnoreCase` directly.
nit: why double space? (similar below and further below)
Just curious, could we possibly call this function for the same node more than once? It seems yes as you are checking `!keyChangingOperationsToOptimizableRepartitionNodes.containsKey(node)` here, but I cannot tell from the code...
Generally speaking we should not rely on the caller to pass in parameters that are guaranteed to no pass the check. What I suggested (below) is to have a slightly modified recursion pattern which do not rely that the first caller would never satisfy the predicate.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Fair enough. Let's leave it as-is.
nit: add `final
Could just use `false`
Hah, this was pretty janky. Good catch
nit: we should also filter for internal topics specifically, like what's done in `StreamsResetter#matchesInternalTopicFormat`. Actually you can probably just invoke that method directly for the filter here (it can be made static if necessary)
I understand that. My question is whether there is some thinking on which configs make sense to be set by users. Ideally, we'd do that instead of being reactive.
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L149 can reference to this field
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
Nit `.` at the end
Should not have an implementation but call overloaded method.
nit: `{@code null}` (same below)
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
No worries. All good. :)
Should this change not be more "radical" calling `KStream#peek()`? In order to resolve the `PEEK_NAME` vs `PRINT_NAME`, we can add a private `peek()` that take the name as parameter.
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
You only need to crate that instance once, right? It can be a member of the class
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
Hmmm... Good point. Let leave it as-is. It's also covered in integration tests that the right store is returned.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
not a huge deal, but technically, these should have brackets.
Think you might have forgotten to remove some debugging here
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
For consistency: {@link KafkaStreams} instance
State that `A timeout of 0 means to wait forever`. 1. Also, I feel it's better to use 0 as the default value in the above function to be aligned with Thread.join() semantics? 2. Could we return a boolean indicating if the instance has been successfully closed or it is timed out, besides the warning log? Users can then program around it instead of only doing operational actions based on logs.
Isn't MM2/Connect using at least once by default? ie, the producer in the runtime can cause duplicates.
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
I find it strange that this method closes the consumer it received.
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
Instead of doing: ``` if (condition) { return true; } else { return false; } ``` You can do: ``` return condition; ```
`brokerConfig` is a `Properties`, so we can call `getProperty()` to get back a `String` directly
nit: add `final`
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
@vahidhashemian, yes, that's what I mean.
I think we tend to use `forXxx` rather than `create` for mapping names to enums.
You might consider using `OptionalDouble`.
Nit: go with single parameter per line.
I think that the driving condition of the test is the number of samples, not the time. But I thought I'd point out that time won't advance by default, but you can make it by using the `autoTickMs` constructor.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
why do we make lines longer? harder to read now
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
nit: The callers of this iterate the committed offsets twice. Once in order to check partition ownership and then a second time to invoke `updateLastSeenEpochIfNewer`. I wonder if we could consolidate these two loops.
This exception message is not really clear to me. What about `"Cannot commit offset of topic partition " + tp + " since the partition was not dynamically assigned to this consumer"` or similar? In the code such messages usually start with an uppercase letter.
I think we'd want to use `updateLastSeenEpochIfNewer`. The provided epoch just gives us a lower bound on an acceptable leader epoch.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
Thanks @ewencp! Glad I'm still up-to-date on that. Happy to adjust per project and yes, unnecessary diffs are better to be skipped in non-cleanup/non-refactoring PRs. Huge fan of that.
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
(especially given that below you use the simple name)
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
Here's a nice blog re: exception costs and when they occur: https://shipilev.net/blog/2014/exceptional-performance
Is ```Exception``` in method signature necessary? the method signature of ```ErrorReporter#close``` don't include checked exception.
Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations: ```suggestion if (reporters.size() == 1) { return reporters.get(0).report(this); } List<Future<RecordMetadata>> futures = new LinkedList<>(); for (ErrorReporter reporter: reporters) { Future<RecordMetadata> future = reporter.report(this, callback); if (!future.isDone()) { futures.add(future); } } if (futures.isEmpty()) { return CompletableFuture.completedFuture(null); } return new ErrantRecordFuture(futures); ``` And since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a `LinkedList` instead to avoid excessive allocation when adding the first element to the `ArrayList`.
doesn't look a great name for its behavior. perhaps something like currentContext
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
I am not an expert on this API, but I would expect that even if we commit using the producer, the consumer should still be able to read the metadata. Did you verify that we cannot retrieve the metadata if EOS is enabled? If this is the case, I would claim it's a bug that need to be fix.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
return type is `void` -- remove this line
format: no need for curly braces
newuntil => newUntil
Changed it locally (and in one other similar place).
Changed it locally.
not be => not be able to
Harsha did this.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
nit: avoid inserting random blank lines.
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
nit: extra spaces after the `->`
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
Oh right, forgot that it doesn't have the window times either. Nevermind then
How about updating `nullableSeenMetadata` only if we don't return records? ```java if (longPollShouldReturn(records)) { ... } else { if (nullableSeenMetadata == null) { nullableSeenMetadata = new HashMap<>(records.metadata()); } else { nullableSeenMetadata.putAll(records.metadata()); } } ``` The benefit from above code is that we don't need to handle duplicate metadata which exists on both `FetchedRecords` and `nullableSeenMetadata` when it succeed to get records and metadata in first loop.
Also, `n >= records.size()`? Seems we could clear `this.records` and use this fast path if they are equal. Would also remove need for checking `iterator.hasNext()` below.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
Thinking about it, this may be the root of the problem. We are removing from the head of the ArrayList, one at a time. For each removal, we cause all the elements to be shifted in the underlying array, which is very inefficient if n is not small.
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
`no longer treated as an updated record` -> c&p error; it was a "fact/event" and is re-interpreted as update now.
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states: 1. Handshake 2. Authentication 3. Waiting to receive a message (receive == null) 4. Received partial message size (receive != null, buffer == null) 5. Received size and partial message body (receive != null, buffer != null) 6. Muted after receiving size due to OOM 7. Explicitly muted 8. Disconnect The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState` is unnecessary if muting is done on allocation failure and that makes the code simpler.
It's just more proactive, right? If you're out of memory, you're not going to be able to do anything (beyond the handshake) on any channel anyway. I think the value is that instead of going through a more polling unnecessarily only to end up muting all the channels, you can just do so immediately.
Would it be simpler to check channel.isMuted() instead of channel.isInMutableState()? Then, the latter can be a private method in KafkaChannel.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: if you want a new paragraph you need to add `<p>`
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
I wonder if we should move all of this into a new private method that takes the interceptor callback and intercepted record. The reason is that it's a bit easy to make a mistake and use `record` and `callback` instead of the intercepted ones (with the current approach).
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
ditto on the properties and the driver.
Ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
Ditto on the properties and the driver.
Ditto on the properties and the driver.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
Still not used
`tp` is not used anymore.
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
This is still not used
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
Please fix this, too.
nit: camel case
We are cycling forth and back here... This change indicate that our logic is still not correct -- and looking at the code it makes sense: Here https://github.com/apache/kafka/pull/4300/files#diff-46ed6d177221c8778965ecb1b6657be3R279 we set `nextPosition = record.offset()` what is `5` in this test case and equals to `endOffset` -- but we should return `10` here (and than this test would fail without the change). We are still mixing up the two cases using a regular changelog topic and the source topic as changelog...
old code sets `retainDuplicates=true` but new code sets `retainDuplicates=false`
You need to fix these other methods also. If the exception is raised, the call to `release` will lead to invalid state.
the condition is always false if you don't add brackets. ``` throw new IllegalArgumentException("Topic pattern to subscribe to cannot be " + (pattern == null ? "null" : "empty")); ```
Just throwing on the first is probably fine. Alternatively, if you want to list them all, I'd suggest iterating through them and collecting them into a collection rather than using suppressed exceptions.
Yes, it makes sense. Good catch.
@guozhangwang Yep, sounds good to me.
Might be better to add a proper POJO maybe `StreamsMetadata` or something that wraps the `streamTime` Long plus `ProcessorMetadata` instead of using `KeyValue` ? We might add new fields later on what is easier to do for a new POJO.
nit: is there a way to verify a function is never called? Like `consumer.commit()`
nit: we could use mkMap helper here as well.
looks like this isn't used
You mean `now`? :) If yes please feel free to resolve the ticket when you merge this.
Nit: how about "a new {@link KTable} ... as this {@link KTable}"? Ditto for all `through` calls of KTable and KStream
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
`use {@link #toStream()} followed by {@link KStream#to(String)} and {@link StreamsBuilder#table(String)} to read back as a {@code KTable}` ?? same below
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
Can remove the first 3 null checks as they are covered in the new overloaded `aggregate` method
Also need to change `keyValueStore` method to return `StateStoreSupplier<KeyValueStore>`
Can remove the first two null checks as they are covered in the overloaded `reduce`
var `initializer` seems to be redundant.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
maybe: `inputKeySerde` and `inputValSerde`
As i said above, we should `requireNonNull(mapper, ...)`
`PrintForEachAction<>` to remove warning. Also in the `print` method
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
Yeah if it exists elsewhere let's just leave it as is for now.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably.
I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required.
Same here, I think `computeIfAbsent` would simplify the logic
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
Let's use `Map` on the left side instead of `HashMap`
nit: we can use `map#compute` to replace getOrDefault + put.
Similar here, we can cache the result in case to be reused.
Same for generation and member.id. We could just call `request.data().generationId()`
We could remove this function
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
nit: add one whitespace at the end after "...state"
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
We can implement that when handling a response, invalid cluster id are fatal unless a previous response contained a valid cluster id.
What should we do if we see this error in a response? It looks like it would hit `handleUnexpectedError` currently which just logs an error. That might be ok for now. I think there is a window during startup when we could consider these errors to be fatal. This would be helpful detecting configuration problems. We probably do not want them to be fatal in all cases though because that might result in a misconfigured node killing a stable cluster.
The KIP talks about bootstrapping the topicId for the metadata topic. Is that part done already? I don't see it included in this PR.
I think a better way to do this is to modify `validateVoterOnlyRequest` and `validateLeaderOnlyRequest` so that we pass the clusterId. Then we can get rid of `getClusterId`.
One tricky aspect is that we now generate the string even if logging is disabled. Let me think about that a bit more.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Nit: param alignment.
If not, we should move the exception capturing logic inside the dbAccessor as well.
nit: empty line.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
recommended; ditto below.
as above: we need to remove adminPrefix configs
Use diamond (`<>`).
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
Nit: space missing after `for`.
Yeah, it seems to me like we should remove it.
Atm we cannot say for sure, but it's likely that what's observed on KAFKA-9701 is a broker-side issue; we can either 1) add the check on broker-side across all members to make sure the selected protocol is consistent for everyone, so if the broker already made a wrong choice itself would log an ERROR, or 2) let it check on the client side. I think for trouble-shooting purposes option 2) is fine, and if we later discovered that this bug is actually on the client side I'm happy to revert this change after fixing it.
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
Nice, thanks for the update. Looks good
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
```suggestion public void shouldInstantiateAssignor() { ```
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nit: fix line break
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
Thanks @vvcephei -- that is convincing.
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
I guess it doesn't harm.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
What we are getting here is not necessarily the bootstrap nodes that the user has passed as it depends on what the caller passes (the current implementation does that, but it could easily be changed). 3 options I can think of: - Leave the current implementation, but change the name of the field to something like `initialNodes` - Add an additional constructor parameter called `bootstrapNodes` - Leave as is
Passing the bootstrap cluster to Metadata sounds good. That's better than passing an additional constructor parameter called `bootstrapNodes` to `NetworkClient`.
Could this be completely fixed by ``` int startIdx = Utils.abs(this.randOffset.nextInt(Integer.MAX_VALUE)) % nodes.size(); ... for (int i = 0; i < nodes.size; i++) { int idx = (startIdx + i) % nodes.size(); } ```
You are right, they are actually the same.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
I'm against the sprinkling of `final` keyword everywhere. But more than that, I'm in favor of consistency w.r.t. the surrounding code. A few lines above, you may observe that `close` has a similar loop without `final`. Every project has its idiosyncrasies and Connect is not dogmatic w.r.t to `final` for local variables.
@cyrusv Your example should print something like: `<SimpleClassName: blah blah blah>` But I don't see `<` or `>` in your example. So either the code example is wrong, or you didn't mean to use those "quotes" in the joiner. BTW probably not adding this decoration would be better.
This will end with a comma after the last transformation.
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
nit: o2[%s] was **equal** to o1[%s]
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
nit: fix indention (same below in other constructor)
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
should this be null? perhaps throw an IllegalArgException
Another name might be `seek()`.
Style convention: this should be called `value()`.
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
Not really related to this line. Could you verify that the state store is closed in the unit test that tests line 148? The name of the test is `shouldThrowStreamsExceptionForOldTopicPartitions()`.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
+1 for 100 tasks. Thanks.
I think we're testing `testDir` Occupied here, not `AppDir`.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Should be final.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
@guozhangwang Yep, sounds good to me.
nit: use string interpolation. Also probably good to add the groupId to these messages.
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
I would say it's important _not_ to be able to create bogus requests. ;) We can introduce specific mechanisms for testing, but a public constructor for a request should do its own validation.
We could make this field access `public`
nit: we can use `map#compute` to replace getOrDefault + put.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
isFull is no longer used.
deliveryTimeoutMs should be mentioned
This is still not used
No longer used.
Still not used
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
This was the checkstyle error that was failing your build.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Should we clear the exception here? If not, then we'll have the exception thrown after the rebalance.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
variable exception unnecessary
Adding the exception is fine, but you can just throw it directly: ``` throw new OffsetOutOfRangeException(...)` ``` Not need to assign it to variable first :)
Ah got it, that makes sense.
We need to make sure the `fetch` bounds don't go into the negative. We only call `processEarly` if the record's timestamp is within the timeDifferenceMs, but here we search starting at timestamp - 2*timeDifferenceMs
I think this fetch might break if you go into the negatives, should just fetch starting from 0
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Maybe I'm missing something, but this doesn't appear to be used anywhere.
Yeah, the throw is what I was looking for.
nit: We should use `groupId.idValue` here and in the others.
nit: 'else' can be dropped
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
nit: we usually try to keep lines under 120 characters.
Yes, that's what I was thinking. Then we can remove the global references and use the same pattern consistently.
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
We should read the metadata inside the while loop since it could change.
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
nit: final on params here and methods below.
nit: remove empty line
nit: add `final` (2x)
We could port this function when it is actually needed.
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
nit: line too long
nit: single parameter per line
nit: static is redundant
nit: move parameter to next line
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
No need to check null. We always have to forward oldAgg and newAgg, anyway.
nit: remove empty line
nit: add `final` (2x)
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
I seem the right fix would be to add the missing `{@code }` annotation in L184? ``` <pre>{@code ``` The closing `}` is already in L190.
nit: add `a {@link Named} config`
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
same here. let's make all method params as `final`
nit: creating restoredPosition is not required if !constinceyEnabled
Good point! @vpapavas , can you handle stuff like this in a follow-on PR? I'm doing a final pass to try and get this one merged.
use `try-catch` instead of `expected` annotation -- not a single line test.
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
nit: unneeded parenthesis
Thanks for the clarification, makes sense.
The names are terrible. You don't suspendImpl something, you suspend it. You don't commitImpl, you commit.
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
This block can be moved outside of the `try-catch-block`
That should work.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
I think it would be better to make this a static factory method and keep the constructor for the case where we receive `FetchResponseData`.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
yes, it seems to be not what this test is checking on. I think we can drop it here.
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
Line too long (also some lines above and further below)
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
It looks like these are backwards. If I understand this method signature, even though iteration is reversed, the "from" key is still the lower bound, and the "to" key is the upper.
@vvcephei is correct. The `from` is still the lower bound and and `to` is the upper bound in the `reverseRange` method definition. We can confirm it by checking the parameter validation in the `CachingKeyValueStore#reverseRange` ``` public KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from, final Bytes to) { if (from.compareTo(to) > 0) { LOG.warn("Returning empty iterator for fetch with invalid key range: from > to. " + ... } ```
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
nit: reorder according to ordering of params.
Minor typo, `were` -> `where`? Also, `10 <= start time <= 20` might be slightly better notation.
nit: Please fix code style.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
I pasted the code from `skip` which is called from `skipBytes`. I know we don't call `readFully`.
Nit: too many blank lines.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
nitpick: we usually include a space before and after ":"
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
Not sure. PENDING_SHUTDOWN indicates a clean shutdown while this lets the thread fail.
One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent. That would allow the "Skipping fetch for partition" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.
`HashMap` can be replaced by `Map`.
nit: extra line
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
nit: past control records or aborted transactions when isolation.level=READ_COMMITTED
Nit: missing articles & punctuation in this description. "not available in buffer" -> "not available in the buffer". "available currently in buffer else return empty" -> "available currently in the buffer, else return empty"
Are we missing the ApiException here? I.e. error returned from servers that are not recoverable.
While we're here, may also want to mention as a second paragraph (?): > The task will be {@link #stop() stopped} on a separate thread, and when that happens this method is expected to unblock, quickly finish up any remaining processing, and return.
nit: an aync
Do you want `@DefaultValue` or whatever the appropriate annotation is? Same for rest of the `forward` flags.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
nit: Starting a message with lower case feels a little unusual.
OK. Sorry, I looked at this first before I saw the update to `TaskStateType.`
I think you need `Utils#join` here. Calling `toString` on an array will give you something like `[Values@3343c8b3`
It does seem like we could pass the node id to `RequestSend` without much issue.
We'll need to fix this in a follow-up so that followers send the right replicaId.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
Might help to have the partition in this message. Same thing in `add` below.
nit: the typical thing to do is invoke the other constructor. e.g. `this(props, true)`
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
and -> a
and -> a
If there are multiple FatalExitError thrown, do we want call `System.exit()` only once instead of creating a thread for each of them? Maybe we can just log the error and return.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
The original motivation is to maintain the information of which thread was hitting this error, but the current implementation does not maintain that any more.. I'm not sure if it was lost somewhere in previous commit or it is never the case. But this is what I've in mind originally: ``` final String logPrefix = String.format("stream-thread [%s] ", threadClientId); final LogContext logContext = new LogContext(logPrefix); final Logger log = logContext.logger(LogAndContinueExceptionHandler.class); ```
Would be helpful to understand the reason as well. The problem with a mismatch is that it's harder to track down the source of a log message and it messes up line numbers when printed.
Yes they are designed to be public.
`log` not used
You can either make them non-static or pass `Logger` as a parameter. Makes no difference to me, but `log` won't work as a static field when you have multiple instances.
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
These two cases don't seem to be different. I'd recommend just always wrapping the exception and throwing (currently the else block). If we just re-throw the first exception, reading the stack trace becomes very confusing. Especially since a lot of those exceptions don't even include the stack trace.
In newest trunk we always call `task.closeDirty` .
We should wrap `KafkaException` as `StreamsException` but rethrow all other `RuntimeException` unwrapped (at least this is the pattern we use everywhere else, and thus we should follow it here, too)
We lack unit test coverage for this case
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
We could perhaps add a `leaderId` in `PartitionMetadata`
nit: add a space before the `:`.
style nit: normally we'd use braces around blocks unless they're a single line
I would call this one `topics()` as you did already in the request.
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
+1, we should just throw in this case.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
```suggestion "restoring by partitions map contained {}, and the restored partitions set contained {}", restoringByPartition, restoredPartitions); ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
nit: both lines missing . at end
Nit `.` at the end
`keySerde` -> `valueSerde`
nit: single parameter per line
nit: parameter/line formatting
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
nit: "another thread wrote to ..."
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
This is unrelated to this PR but the check brings up a good point. Maybe instead of throwing an error on the pattern subscription finding an overlapping topic and then restarting the thread and trying again. Maybe we can pause a topology until the topics are not longer overlapping? EDIT: I can see this in the other PR
maybe "Restoration completed for partitions:"
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
Please remove empty line.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
Could you please add some line breaks? This and some of the other verifications are too long.
Can we also assert that the state gets to `RUNNING` after the new thread has joined
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
newuntil => newUntil
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
same here and below, we should assert false to `(config.unknown().contains("prefix.sasl.kerberos.kinit.cmd")` after using that config.
here, we should change to assert `unknown`, right? Same as below.
Should we add some more stuff to round this out (and make use of all the support for reporting more than one value...), e.g. some other `RuntimeMXBean` info? For example, classpath info seems like it'd be useful (probably more so before `plugin.path`, but still probably handy from time to time).
yeah, other stuff from `RuntimeMXBean` was just a suggestion, obviously i wouldn't limit to that. classpath was the main one i saw that is useful. i don't think that it's too confusing since it is also alongside a bunch of other general system info. also, we still support the classpath approach, we just won't fix conflicts. we've definitely had cases on the mailing list that ended up being classpath issues that we probably could have spotted the likely root cause more quickly if we had had access to the classpath info.
Should not have an implementation but call overloaded method.
Make all params `final`
remove this line -- not required.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
the method name changed to `windowedTable` and `windowSize` parameter is missing
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
Still not used
Tiny nitpick: `TopicPartition.toString` does what you are doing manually here, so you could simplify it by just saying: ``` java "Batch containing " + recordCount + " record(s) expired due to timeout while requesting metadata from brokers for " + topicPartition ```
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
We missed it in the first round probably because those metrics only had 1 tag, but the `tags` method should use something like a `LinkedHashMap`. Without this, the tags can get jumbled. I noticed this because jconsole's hierarchy was listing the task ID above the connector name.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
```If the thread is callback thread. Calling flush() from callback is not allowed since it makes deadlock.```
It seems to me ```IllegalStateException``` is more suitable for this case. Also, please update docs of ```flush()```
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
req: This is unnecessary
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
to me it seems like we can't possibly know what the constraints of all reporters would be and they don't provide an interface for validation, so it should be up to them to figure out how to substitute. but i've also asked some other folks to maybe chime in here who may have better context on how we've handled this elsewhere.
I know the naming thing has bit us in the past, is this same approach used elsewhere and/or how was it decided on? Specifically, metric name constraints really shouldn't be JMX specific if that is the case here, despite the fact that the metrics is so obviously JMX-inspired. I can easily find https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala#L46 but nothing else. Have we not had the same problems because metrics w/ topic names in them already have constraints on the naming? If I am remembering correctly, I think maybe both @gwenshap and @junrao were involved in some discussions, I think `-` vs `_` was a problem at some point? Maybe one of them could chime in here.
ah, right. nah, that's fine. just when reviewing I had the thought that if we guaranteed non-`null`/non-empty in the constructor, this wouldn't be necessary. i realized that it was actually intentional, but easy to miss when reviewing here and not getting the same highlighting as an IDE
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
This is part of KIP-516
nit: remove empty line
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
Another way to write this, that reduces a couple lines of code would be: ```java if (allTopics.remove(topicName) == null) { future.completeExceptionally(new UnknownTopicOrPartitionException(String.format("Topic %s does not exist.", topicName))); } else { future.complete(null); } deleteTopicsResult.put(topicName, future); ```
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
nit: extra line
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
Feel free to do a PR against https://github.com/apache/kafka-site
@mjsax Should we include indentation settings here: http://kafka.apache.org/coding-guide.html ? It's mentioned in the scala guidelines...
> Is there any official style guide or export of accepted settings? Not really :( Rule of thumb is, avoid reformatting if possible :)
Nit: avoid unnecessary reformatting. Also, we use 4 space indention, not 8 space.
remove try-catch and replace with: ``` final StreamsException s = assertThrows(StreamsException.class, () -> testDriver.pipeInput(consumerRecord)); ``` assert afterwards and don't re-throw.
To get rid of the warning, you can just copy the body of `close(long, TimeUnit)`. If you don't want duplication, you can make the deprecated method call the non-deprecated one.
Nit: It's probably a bit better to have this be near the `setPollException(...)` method.
Is this ok to do for both reasons this would be called? Do we actually want to request a commit when in the case of the connector throwing a RetriableException? That'll result in the connector being forced to flush all its data.
original was better
We probably have to keep the `size() == 0` behavior for compatibility.
With the type promotion changes I think you'll need to pass in both the original value and the projected value to this method and this assertion will have to change to something like `assertEquals(expectedProjected, projected)`.
Expected value here should be a list of longs.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
I am confused. If we change `store.range(hello, "zooom");` should `hasNext()` not return `true` (start and end are inclusive). Thus, to me it seems the test is wrong and there must be a bug in the iterator code.
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
I know this is just how the other tests are doing it, but it's not really an airtight way to validate the expected results...if nothing is returned then we never enter the `while` loop and the test passes, even if we did in fact expect there to be actual output. The important thing here was just to make sure it didn't throw an exception so it still does that, but it would be good to fix this up maybe in a followup PR
This line is failing checkstyle.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
If we do allow an `int` here, we should a) check for bounds and b) re-validate values after the cast. Otherwise you can end up with invalid results (which should never happen in practice given the range of sane values for replication factor, but down casts like this always warrant such checks). However, I suspect just shifting to using `short` all the way back to the configs is probably a better solution.
This shouldn't be possible, right? It wouldn't make much sense to put a topic in the result if it didn't have a corresponding `TopicListing`.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
All the `null` checks at each layer of the call stack make me think that particular issue might be better handled with an exception. Not critical since this is all internal code, but seems like then we'd only need to check version compatibility in one or two places.
You can also do this more concisely as `topicsByName.keySet().removeAll(existingTopicNames)`.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
the test passes without the second poll. the first poll finishes the sync ``` INFO Successfully synced group in generation ``` before the second poll is triggered. The second poll notifies the assignor and gets committed offsets which i don't think is necessary in this test
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
This looks unintentional.
This seems to change the behaviour...
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
I think nicer to just `return requests.first()` here
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
nit: could be useful to log the type of exception in the assertion message.
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
We may need to call sender.run() one more time to ensure the message is not reenqueued. The reqenqueued message won't be sent out again in the same sender.run().
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Ah, I see. Thanks for the explanation
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
If we swallow the exception here, and the test always throws an IO exception, we will never notice. I guess it would be better to use `fail()` with a message.
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
nit: newline for better IDE debugging.
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
Very good catch. Thanks, @abbccdda .
Yeah good catch, see above
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
We could perhaps add a `leaderId` in `PartitionMetadata`
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
It looks like this is not used anywhere
We can use `List<Class<? extends Connector>` to avoid the warning
We don't need this field, this could be a local in `startClusters()`
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
The number of elements is not always 1. Each created thread-level sensor is added to this queue, e.g., `processLatencySensor`, `pollRecordsSensor`, etc. Check out the callers of `threadLevelSensor()`. Each queue contains all thread-level sensors for one single stream thread.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
You can inline the value of variable `key` here and remove `key`. ```suggestion final String fullSensorName = CLIENT_LEVEL_GROUP + SENSOR_NAME_DELIMITER + sensorName; ```
Although, you simplified this code, you did not apply all simplifications I proposed in PR #7914. I think you can even simplify this code further as shown here: ``` return Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); }); ``` This simplification can be applied also to the other methods below. If you have any concerns about this simplifications please share your thoughts.
Although, we use this in other methods, I think the following is a bit simpler to read: ```suggestion final Sensor sensor = metrics.getSensor(fullSensorName); if (sensor == null) { clientLevelSensors.push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); } return sensor; ```
This is unused too
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
We can use `List<Class<? extends Connector>` to avoid the warning
Why do we have SSL specific methods here? Could we move all the SSL bits into the SSL class? We have fields for the configurations. So we could set them accordingly (without or without SSL) in each concrete class. Then in the base class, we just use the fields to create the clusters without having to know if it's SSL or not.
We can remove this
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
Nit: go with single parameter per line.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
Ok, it looks better now. Let's leave it this way, with two lines.
Nit: we typically just say `partition` in these cases. Same for the other `log.debug`.
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
nit: Indicate that this needs shallow iterations on the entries.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
@hachikuji OK, I am fine with that. @vahidhashemian Can you remove the `needUpdate` change here? Thanks.
Hmm.. In the consumer, we only fetch metadata if we know we need to, and that is sometimes conveyed only through the `needUpdate` flag (as in the case I mentioned). One additional note: this flag alone is not sufficient to prevent metadata retries if the metadata max age has expired (which will be the case on initialization).
It is to avoid unnecessary connections being made to update metadata e.g. from `Sender` thread which will continue to retry if `needUpdate` is set. The flag will be set again if the application performs another operation (e.g. another send), but we are avoiding automatic retry.
@rajinisivaram Yeah, I think we are handling the common (initialization) cases in this patch, just a bit discomforting to leave some of the other cases a little nebulously handled. In this particular case, it doesn't really seem worth trying to prevent additional metadata retries since the NetworkClient will still enforce a backoff and we expect the user to just close the client anyway. I would probably leave `needUpdate` untouched as we do currently on failure.
Might not a big problem, but I wonder if we should check for the authentication exception before the `wait` as well? It is possible that `awaitUpdate` returns before the authentication failure happens. A subsequent call may then begin with the `authenticationException` not null which would cause a needless `wait`. I think an easy solution is to move this line up to the beginning of the `while` block.
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
@mjsax Got it. Thanks for your response!
Also not clear why "numSegments - 1" here.
Actually, it seems more than one figure needs updating. Makes me wonder if we should actually remove them altogether and let people read the code (which can't go stale) instead.
It's fine to create a new file under the kafka system tests for now. In general though, it's OK to submit PRs against ducktape and advance the version - releasing new versions is a fairly lightweight process, and we pin the kafka system tests to a specific version of ducktape anyway.
I think `file_exists` is duplicated from `test_console_consumer.py` We should probably put this into something like `remoteaccount_utils.py` and reuse
Docstring doesn't match the class
It would be best if this test used a replication factor of 2. With a replication factor of 1 we will have no regular replication traffic occurring when the producer writes messages. It would be good to have both throttled replication and non throttled replication happening at the same time.
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
Could we use ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG etc instead of hand-coded strings? It is less error-prone for possible future changes.
This should disappear with my suggestion.
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
Done now, thanks @ijuma
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
It's internal. So should be fine.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
same here. let's make all method params as `final`
This field can be final as well.
This is the same code as in `KTableFilter` -- we should refactor and share code.
This debug message seems like it would appear _before_ we actually attempt to do any checking. It's probably worth keeping the old message (or something similar) _after_ the checking has been completed.
Hmmm. If that is the case then at least `kafka-leader-election.sh` won't exist in older versions of Kafka. I think we added it in 2.3.
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
nit: add a newline here too.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
I see what you intended. Thanks for the response.
@mjsax What you suggested sounds right to me.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
the method `restorePartition` is no longer used and can be removed
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
SGTM. If we find it flooding the logs and not helpful we can reconsider
@guozhangwang Yes, keeping the reference is fine. I was concerned by the check itself because we were calling `generation()` twice in the previous implementation thus we could get two different instances. ``` generation() != Generation.NO_GENERATION && !protocolName.equals(generation().protocolName) ``` I haven't thought about the error-log but it is also a good point.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
missing coverage on: * expired segments * retention time * fetchSession, which doesn't find a session * fetch
nit: ```suggestion private static final String storeName = "InMemorySessionStore"; ```
Can we "duplicate" this test and use default serdes from `StreamsConfig`? We recently discovered some bugs in other parts of the code with this regard and it would be good to verify that the code work correct upfront.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
I like this parity check. :+1:
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
This should also be synchronized
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
@hachikuji asked you to change the name originally: ```text hachikuji 5 days ago Contributor nit: the name is a little awkward. How about hasRemaining to match ByteBuffer? ``` :)
Sorry for the confusion. I thought `hasRemaining` made sense initially, but then I realized that the name should be more suggestive of its usage. I'd prefer something like `ensureNoneRemaining`, but it's not a dealbreaker for me.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
these 2 lines shouldn't be here
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
There is no need to test this as it is calling the same method as above.
This line is failing checkstyle.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
add `final` twice
You should also compare `expectedValues`.
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
Nice, thanks for the update. Looks good
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
Yea, my suggestion would be to reuse the existing constructor as the construction of the `AlterConfigsResponseData` seems non trivial for a caller to do, compared with passing a map of errors.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
If we're just testing broker compatibility I don't think we even need this part of the test.
`topics` defined here and in next test, maybe move up to `init`
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Nit: dowrade -> downgrade.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Could you please add some line breaks? This and some of the other verifications are too long.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
"this until it is" doesn't quite parse.
Nit: should we call this `rateUnit`? Same for the other constructor.
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
This seems to always enforce a materialization, but I think we should materialize only if we need to.
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
nit: add `final`
`windowSize` should be `Duration`
Adding this constructor is quite dangerous because it's almost the same as the one that takes `partition` (the only difference is that one is a `Long` and the other is an `Integer`)
`of` -> `or`
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
`out-of-order` `window closed`
`before or after`
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
As mentioned on the KIP discussion, `BytesDeserializer` should not be included.
nit: remove `this` (not required)
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
nit: log an error and include the relevant info (eg `windowStart` and `inputRecordTimestamp` at least). Same for the IllegalStateException in `processEarly`
Is this the same as in the non-early `process`? Maybe we can factor it out into its own `createRightWindowIfNeeded`(or whatever) method.
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
This is also kind of unclear (what is a max right window?), but I get that we can't call it `previousRecordRightWindow` since we don't know that it is a previous record or not at this point. I think yet again, just keeping track of the previous record's timestamp as we iterate through the windows, will be the most clear; if `previousRecordTimestamp` is still null by this point, we know right away that we don't have to create a previous right window. And then we can actually drop the `rightWindowNecessaryAndPossible` check altogether, since we know the current record has to be in range of the right window of the previous record (since we're in `processEarly`). The one exception is if the previous record and current record are on the same timestamp, so we can actually skip the previous right window creation if `previousRecordTimestamp ` is `null` OR equal to `timestamp`
Minor typo, `were` -> `where`? Also, `10 <= start time <= 20` might be slightly better notation.
Fixed via https://issues.apache.org/jira/browse/KAFKA-13699
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
and -> a
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: needs a comma after the `{@link ...}`
records to it, and reading all records from it, such that
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
Got it. Thanks. The patch LGTM.
Maybe we should move "This is also effectively a cap on ... which may be different from this." to after "This setting will limit ... sending huge requests.". It seems like the latter describes the purpose of the setting while the former is an additional implication.
I think this should be used in the define method below too.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
I miss @shikhar!
Seems like a no-op
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
Got it. Thanks. The patch LGTM.
I think this should be used in the define method below too.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
add `final` twice
This can be initialized here and be `final`
initialize the `KStream` here and make it `final`
you don't need this. Junit gives you a new instance of the test class for every test method
You should also compare `expectedValues`.
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
Updated this when merging.
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
"Should have thrown as producer is already initiailzed"
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Maybe get the value of this expression on the line above. IMHO it will make it easier to see what the `assertThat` is doing.
nit: might be better to set end to another timestamp.
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
and -> a
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
nit: add `a {@link Named} config`
I'm not sure why this needs to be a field. It looks like it is used to measure latency, but then why not just use a local variable around the code that is being timed?, i.e, ``` final long start = time.milliseconds() // do some computations computeLatency(start) ``` Also, the name doesn't seem correct as it isn't really the current time. It is more like `timerStartedMs`
seeing as you are restricting visibility (thanks!) this one could be private
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
This looks unintentional.
Since it looks like we expect numRecords to always be set, should we error out here if record is null? That would seem to indicate that our source topic was under-sized or some other issue (GC pause?) caused poll to timeout.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
I like the extra-detailed error messages, thank you.
ditto on removing before/after.
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
I think we need to insert a `fail` here to fix this test
We should improve this test by moving this line outside/before `try`
This seems like it's a kind of weird restriction. I guess it'd be odd to use the same name for an internal topic and another, though if you know its going to be prefixed it might not be great to not be able to use that same name.
rewrite test as above using `assertThrows()`.
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
Map.Entry<String, String> to avoid the check below
Yeah, that's fair. Just thought I'd mention it since it stood out while reviewing the patch.
It seems we never use the return value here? I also wonder if we should just remove the `connector` argument. This is only invoked from 2 places and I don't see a good reason to have used the list of connectors provided in the rebalance callback, which is the only place we don't use the `stopConnectors()` variant. We might want to validate the set in the callback and complain in a log if something looks wrong, but we always want to shutdown all connectors that are actively running. It would also mean we wouldn't need to check whether the connector exists in `stopConnector(String connName)`.
The return value is fine, I just noticed that it wasn't used, which made it feel unnecessary. Doesn't really matter either way as it can be added/removed easily. Re: `stopConnectors()`, rebalances will always revoke the entire assignment (at least as things stand today), so the two are equivalent. To be honest, this one is a bit up in the air anyway -- today we rely on the entire assignment being revoked, but in the future we may do a sort of incremental rebalancing. In that case, it isn't clear whether we would be able to use the callback to notify the listener exactly which ones are _actually_ being revoked or if that's too application-specific.
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
nit: double space
I'd suggest update the existing test directly in this PR.
`topics` defined here and in next test, maybe move up to `init`
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
nit: `final` (also next line)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Oh right, forgot that it doesn't have the window times either. Nevermind then
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
nit: Please fix code style.
nit: Please fix code style.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
There is no need to test this as it is calling the same method as above.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
We should verify the actual timestamp.
I think it is probably worth adding as even if it is deprecated it is still supported
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
nit: add `final` (2x)
nit: remove empty line
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
nit: not a big deal here, but for unit tests I think given the very low overhead it is better to separate out each of the cases into their own test as it can help make it more quickly obvious if issues are with a specific case or if it affects multiple cases.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Can we remove the `AndHighAvailabiltiyEnabled` suffix from the test name? And/or just generally shorten it if you have a better idea
Looks good. I like the additional checking that you're doing here.
You might consider using `OptionalDouble`.
There is a built-in for this `Function.identity()`
The first exception will be the "cause"
Nit: go with single parameter per line.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
`HeaderConverter` and this method don't exist prior to AK 1.1.
This method is called from within `newConverter`, `newHeaderConverter`, and `newConfigProvider`, so mentioning "converters" and getting the available converter implementation classes is actually wrong when used to find the config provider implementation class. One way to address that would be to pass in additional parameters to the method, but since we want to backport this to branches before `2.0` we have to make that work without method references. So one simple option is to rename the method to `converterClassFromConfig` and add a new method for `configProviderClassFromConfig` that does essentially the same thing but is tailored for config providers. Perhaps a better alternative is to dynamically determine the name and the `pluginNames(...)` based upon whether `klass` is a subtype of `Converter` or `ConfigProvider`. This keeps a single method, but is a bit more dynamic.
Map.Entry<String, String> to avoid the check below
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
This still doesn't seem correct. We're only invoking configuration if the plugin implements `Configurable` afaict. This does not work for `Converter`s implemented against the new API and assuming the "forward" configuration. We *must* always invoke the "old" `configure(Map, boolean)`, and only invoke the `Configurable` version as a backup. Possibly it would make sense to indicate on the `HeaderConverter` that the `Configurable` methods should be idempotent if we need to be able to implement both. Not sure if we can test this easily with unit tests, but I think we might want a plain old `Converter` (that does not implement `HeaderConverter`) in tests to validate compatibility... but it's possible we'd need either integration or system tests to properly validate.
nit: add `final` -- same next line
Ouch! Sorry about that!
```suggestion final OffsetCheckpoint checkpoint = new OffsetCheckpoint( new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME)); ```
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
Can we think of a better name for this? `pairs` doesn't really tell me anything.
nit: line too long
nit: single parameter per line
I think we should have this in this PR already.
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
`Count is a {@link SampledStat} that maintains a simple count of what it has seen.` So with this stat, its value will be increased for the window period, then suddenly drops to zero, then start rising again. So it's hard to alert on such a metric, on the other hand `Rate(Count())` will record the average rate per time unit (here second), so users in practice can easily set a threshold for alerting, and even if they want "zero tolerance", setting the threshold to be 0 can still satisfy their needs.
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
I didn't think of that before, but now that you mention it, the change makes sense to me.
This will probably be subjective, but I'm ok with "hop" for now.
Remove double blank.
Remove double blank.
Should be more precise. See discussion about `HoppingWindow` and `TumblindWindow` in #1250
I would prefer `advanceBy(long advance)`
either move `this` to next line, or fix indention.
This seems to always enforce a materialization, but I think we should materialize only if we need to.
nit: avoid unnecessary `this.` prefix
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
Is it ever possible that the `clientState.ownedPartitions().get(partition)` is not null but `allOwnedPartitions.contains(partition)` is false? It seems the second condition is redundant (and hence the `allOwnedPartitions` parameter throughout the `assign` function seems not needed).
My bad, mis read the lines :)
Nit: add `final`
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Though now I look at the message for `UINT16` I see it would be consistent with that. Still I think because there are two types involved here, the Java type and the network type, including both is clearest.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
even clearer: "Represents a signed integer"
"encoded on two bytes using network byte order" -> "encoded using two bytes in network byte order"? Similar for the other integer types if you like the suggestion.
nit: Values 0 and 1 *are* used..
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
Ditto here for different exception types.
Thanks for the follow-up.
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
nit: unneeded parenthesis
Thanks for the clarification, makes sense.
Technically, this is `numDrainedRecords`.
Also, maybe we should assert that `numExceptionReceivedInCallback.get() > 0` if we expect at least one to fail (in theory, if `numSentRecords == 100`, there would be no exceptionReceivedInCallback).
Maybe we can have a numRecords variable for the `100`.
Let's rename `headers1` and `headers2` here too
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
this problem also affect replication node fetch process? our server occur a error: [2019-04-05 23:59:46,084] WARN [ReplicaFetcherThread-1-1], Error in fetch kafka.server.ReplicaFetcherThread$FetchRequest@b22a64c (kafka.server.ReplicaFetcherThread) java.io.IOException: Connection to 1 was disconnected before the response was read but return to normal after a reboot kafka server.
This is a change in semantics, right? Before, we would never expire a connection that has been processed during the `poll` because we use `currentTimeNanos`. After this change, if something in `poll` takes long enough, we could end up expiring a connection in `completedReceived` for example.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
Please avoid raw types, even when you don't need the type bound. ```suggestion for (final RankedClient<ID> rankedClient : rankedClients) { ```
I see what you mean. I do not have any heart feelings here. Would be interesting to see in experiments how the two approaches differ.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
This is a bit suspicious... If we're polling the queue, we should just loop until the queue is empty, not iterate over another another collection we happen to know has the same number of elements. More specifically, `poll` might return `null`, but `offer` throws an NPE if `client` is `null`.
Again, a bit more information would be more useful: ```suggestion // Then if we delete the connector, it and each of its tasks should be stopped by the framework // even though the producer is blocked because there is no topic ```
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Shouldn't the workers discover that the coordinator is unavailable while it is down? I'm imagining this test going like this: 1. steady-state workers are running 2. brokers stop 3. workers discover the coordinator is unavailable 4. workers stop their tasks 5. brokers start 6. workers discover the next coordinator 7. workers start their tasks 8. workers are running unaffected
Nit: you can call `Thread.enumerate` directly. Also, it would be good to assert that `threadCount` is < than `threads.length`.
I think we can simplify this test. We only want to make sure the application directory gets deleted. Thus, we can create it manually, create the driver with corresponding config (passing in a empty Topology), call `close()` and check if the directory was deleted.
It reminds me that sometimes I did the same thing of piggy-backing re-orging in the code base with actual changes that makes the reviewers life harder :P All of us should be careful in the future. That being said, for this PR since Boyang and I have bite it already, I think we can save reverting the piggy-backed reordering and just merge as this.
Thanks. Should we instead make `referenceContainer` a field and access its members to get the references? The purpose of `AssignorConfiguration` is to parse the configs, not to be a general container for the configured values. Otherwise, we wouldn't need to assign any of the other fields here.
It would be good to have some assertions.
I stand by what i said, but I'll leave it up to you. It isn't a deal breaker for me!
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Thanks for cleaning up the code duplication.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
Hmm I think a better test would be to skip the linger wait time. This would show that calling `forceDrain` causes us to forego the linger: ```java assertFalse(acc.needsDrain(time.milliseconds())); acc.forceDrain(); assertTrue(acc.needsDrain(time.milliseconds())); assertEquals(0, acc.timeUntilDrain(time.milliseconds())); ```
I think the requirement should be stronger, we should check the subject has a KerberosPrincipal to skip the kerberos login. Else we can run into a situation that there is an alternate login context that has nothing to do with Kerberos and we'll fail.
we are not using getters and setters. I think this should be named as serviceName
We already have a `serviceName()` method that returns the `serviceName` variable. This method is different as it's getting the values from the config or JAAS file. I think it's useful to use a different name. I used the `get` prefix, but we could use `find` or some other prefix.
Could we just use configs.getString and avoid casting? There are a few other places like that.
This is just a `Map`, so we can't use `getString`. I wanted to change `configure` to take a `Config` type for this reason, but it would break API classes unfortunately.
This block can be moved outside of the `try-catch-block`
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
nit: add `final`
I don't have a better suggestion, but targetTopicPartitionsUpstream is a kinda confusing name.
Adding to `connectorProps` won't change the already instantiated `config`.
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
Nice, thanks for the update. Looks good
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
Worked fine when I tried it locally: ```java assertEquals(Collections.singleton(tp0), records.partitions()); ```
I think we may be able to get rid of some of the `bytesRead` bookkeeping. As far as I can tell, it is only used in the exception message below and it seems redundant there (i.e. sizeOfBodyInBytes - bytesRemaining = bytesRead).
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
It is a shame we have to do it like this, but i don't see a viable alternative
Ah! I misread this as turning `logAll` *on* instead of *off*. Now I get it :)
Not sure whether this change is necessary. The value should be validated already.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
Add missing `<p>` tag
> Unfortunately I don't think we can shutdown a thread until we have started it. Have a look at https://github.com/apache/kafka/blob/aeeb7b2f9a9abe8f49543a2278757722e5974cb3/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L976-L983
You've added a few empty lines in this file. We should remove these
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
Re: your concern, I don't think we can assume that a user's state store's `init` method is idempotent. AFAIK nothing should change that's relevant to the state store registration, but if something does (eg TaskCorrupted) we'd have to wipe out everything and start it all again anyways
+1, we can rely on `storeManager#getStore` inside `StateManagerUtil` to check if the store is already registered.
Nah, I think we should actually keep this (although `IllegalStateException` seems to make more sense, can we change it?) -- we should just make sure we don't reach it
+1 on disallowing the app to continue after an illegal exception. We need to reserve _some_ kind of exception for actual critical, fatal system errors that a user can't just ignore to spin up a new thread. And that has essentially been the meaning of these illegal exceptions in Streams thus far. As I mentioned in another thread, I've been very concerned about this in the new handler since we haven't been strict in properly cleaning up after an illegal exception
+1 on stopping the app after a deterministic illegal * exception. I am not sure if all illegal * exception we throw are deterministic, though. I guess most of them are. For now, we could just shutdown the app for all illegal * exception and then consider to use a different exception if we discover that a illegal * exception is transient.
Yes, I had misread the code originally.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
I fixed this one to use the constant before merging.
We should have a constant rather than using '262' directly
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
nit: `final` (also next line)
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
Would this be any clearer? ```java OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp); if (offsetRequestSpec == null) { future.completeExceptionally(error.exception()); } else if (shouldRefreshMetadata(error) { retryTopicPartitionOffsets.put(tp, offsetRequestSpec); } else { ... ``` Also, in the case of that we got back an unexpected partition, I think we can raise a new `KafkaException` and provide a clear message indicating what happened.
Currently in this case, we do the following: ``` future.completeExceptionally(error.exception()); ``` I am suggesting we do something like this: ``` future.completeExceptionally(new KafkaException("Unexpected partition in response...."); ```
The slf4j `{}` placeholders will not work here since we are constructing the message ourselves.
nit: conventionally, we put the `else` on the same level as the previous branch ```java } else if (... ```
This is not related to your PR at all. It seems that if `offsetRequestSpec` is `null` here, `future` will be `null` as well cause `futures` is initialised based on `topicPartitionOffsets`. If it turns out to be correct, it may be better to just log a warning here like we do in `createTopics()`.
The only place we need this duration is ``` duration.negate().addTo(now); ``` `java.time.Duration` has `negated` but its `addTo` is taking a `Temporal`, and all its extends like `LocalTime` etc do not have the right API of `getTime`. So I think we just keep it like this.
@mjsax LGTM Both `kafka/tools/StreamsResetter.java` and `kafka/admin/ConsumerGroupCommand.scala` should be migrated to remove usage of `javax.xml.datatype.*`
In line 126 above, maybe we can just call `e.toString` which will include the message string if it contains? otherwise LGTM
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
should we put `kafka` and `numThread` directly into `props` and reduce number of parameters here (would also simplify all the "passing through" code.
Nit: let's avoid unrelated line additions.
How about keeping this `private` and adding a protected `isCancelled()` method? Currently, the `cancelled` field is encapsulated entirely within the `WorkerTask` class, and modified only via the public `cancel()` method. We can just as easily keep the encapsulation. OTOH, if we were to make `cancelled` protected, we'd lose that encapsulation and make it a bit more complicated if a future developer did want to add logic upon cancellation.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
```suggestion WorkerSourceConnectorContext(OffsetStorageReader offsetStorageReader) { ```
`hop` vs `advance` is subjective :) I am fine with `hop`, too; it's called a `HoppingWindow` after all. `advance` is just a term that is quite common in literature so I am somewhat used to it.
I would prefer `advanceBy(long advance)`
Remove double blank.
Remove double blank.
This will probably be subjective, but I'm ok with "hop" for now.
Since we have three threads for this test, there can be multiple rebalances before the streams instance stabilize and start processing...
Why do we need this? This is logged anyway.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
none from what I can see, but I'm not sure it's worth holding up the PR for it.
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
This should not have the `sink` suffix
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
nit: 4-space indention plus move `builder` down one line
nit: avoid unnecessary `this.` prefix
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
We should not be using mockito internal classes.
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
nit: unneeded newline
This was probably unintentionally reduced to debug level while fixing conflicts.
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
@guozhangwang Yes, keeping the reference is fine. I was concerned by the check itself because we were calling `generation()` twice in the previous implementation thus we could get two different instances. ``` generation() != Generation.NO_GENERATION && !protocolName.equals(generation().protocolName) ``` I haven't thought about the error-log but it is also a good point.
@dajac just to clarify, are you concerning that the `generation()` may change between the check and the error-log? If yes maybe we do not need to synchronize the whole function, instead we just get a reference of the returned `generation()` call and use that in the error-log, since the generation object is immutable.
Hmm.. Are we safe to remove these given that this is a public API. It's _probably_ unlikely anyone is using them, but still..
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
Hmm, this doesn't need to block merging this, but we should think carefully about doing delay this way. The rest of Connect avoids trying to rely on Java's `interrupt` behavior because it's not really a reliable way to *actually* interrupt threads, and in a system where there are pluggable components that are allowed to block indefinitely, relying on functionality that most Java developers don't understand well probably isn't going to work all that well. It may not have actually gotten to a KIP, but there was at least some discussion on a JIRA somewhere about making connect perform interrupts in addition to the basic task `stop()` calls it already does, but it doesn't currently do this. For anything that can end up with pretty long sleep periods, we should try to make sure there's a good way of interrupting it and moving on (e.g. so rebalances wouldn't get delayed because there's a connector that's encountering errors). At a minimum, since we don't do interrupts currently, I think we wouldn't interrupt this code currently. The other approach we use elsewhere is to `wait` on a monitor so we can set a flag and interrupt with `notify` and have it bail out immediately.
`deteremined` => `determined`
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
Seems to fit in one line
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
Nit: can be `final`
Nit: `dir` only used once -- can be removed
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
We can compute this once and pass it to `removeAttribute`.
Let's also verify that `containsMBean` returns `true` before the removal. Also, it wouldn't hurt to check that the second `removeMetric` call removes it from `JmxReporter`.
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
We should limit this suppression to the method for which we really need it instead of the whole class
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
This doesn't need to be declared outside the loop (it can be final at the assignment).
I was thinking that this should be `Collections.unmodifiableMap(tempProducerDefaultOverrides)` to ensure that it is never mutated after it's assigned.
Nit: maybe we should call this `PRODUCER_DEFAULT_OVERRIDES` and the other one `CONSUMER_DEFAULT_OVERRIDES`.
Do we need to change these signatures from `ConnectorTaskId` to `String`? The `ConnectorTaskId` gives us the ability to define tasks-specific client configuration properties if necessary/desired. I'm afraid that switching to `String` will make it harder and more invasive to add that back in. Plus, if there's not a good reason to remove these, let's leave that for smaller PRs.
```suggestion WORKER_SETUP_DURATION_MS, "Initial group of workers did not start in time."); ```
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
An alternative might be to let verifiable producer accept just one compression type. Then in your test case, you could create separate instances, each with a different compression type. Seems a little more intuitive that way to me.
Probably not necessary to report consumer startup time in this test? Also `start_consumer` is a misleading name, since it doesn't actually start the consumer :) Also - on line 88, change `self.consumer.stop_node(node)` -> `self.consumer.stop()`
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Note that Kafka only supports kerberos as the SASL mechanism.
```suggestion log.warn( ```
How about: ```suggestion "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results.", ```
If all we're doing here is instantiating a map and then placing a single property into it, is there any reason to do so in a separate `beforeAll()` method instead of moving everything into the `setup()` method? Additionally, if that's the case, we can make `pluginProps` a local variable instead of an instance variable.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
It was removed from the other versions of `group` but not from here.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Oh, I just noticed. Then `synchronized` is not needed anymore.
empty line needed
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
nit: extra line
nit: if you want a new paragraph you need to add `<p>`
This name seems backwards.
Sounds good. I think this is convincing :)
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
could we turn this into a loop? i.e., sth-like? ``` List<KeyValue> records = Arrays.asList(...); for(KeyValue record : records) { mockTime.sleep(10); IntegrationTestUtils.... } ```
This was not addressed yet. the whole sentence can be removed
There is no `CoGroupedStream#reduce()` -- we can remove this
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
Nit: we don't need this tag before the parameter list.
`KeyValueStore` -> `TimestampedKeyValueStore`
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
`tp` is not used anymore.
Still not used
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
Yeah, I was thinking it was fine to remove it since users should not create an instance of this apart from tests and it's been deprecated for a few release cycles. Or if we think the method is useful, then it would make sense undeprecate it.
`long` -> `Long` is a binary incompatible change.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
Same here, this exception message does not apply to the case this is trying to catch
ditto here, please add a separate check and exception
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
Updated this when merging.
Nitpick: I'd call this `getOrCreateFileChannel`.
Why call it `now` and not `then` (or `previous`); `time.milliseconds()` give the current timestmap, ie, "now"
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
We should not use wildcard imports. Check your IDE setting to disable this auto-rewrite.
Nit: `.` full stop missing.
Nit: can you clean up the code further and add `{ }` (we prefer to use `{}` for all blocks. Thanks.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
super nit: extra blank line
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
Use `KafkaException` instead of `RuntimeException`
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
nit: maybe utility methods can be moved to the bottom of the class.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
nit: seems you can use `new ArrayList<>`
nit: add `final`
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
Also not clear why "numSegments - 1" here.
If not, we should move the exception capturing logic inside the dbAccessor as well.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
add `final` twice
How about ``` if (!cachingEnabled) { context.forward(key, new Change<>(newValue, oldValue)); } ```
nit: final on params here and methods below.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
I suggest only catching `KafkaException` and only close the client then. About other exceptions, we can probably try retrying or fail the thread, I don't have a strong preference. I don't think we should catch Throwable though, as that catches errors like `OutOfMemoryError`
I think that's fine, I don't think there's a way to recover (nor if it makes sense) from an OutOfMemoryError - https://stackoverflow.com/a/352842
Yes it is. I was asking whether we might want to alter it for the default case. I think it's fine to keep it as it is
Technically we may still have a connection. Do we need to close the client first before we decrement these? (ditto for others)
This won't log the error. We want to use the `public void error(String msg, Throwable t);` overload, so we need to change `e.getMessage()` -> `e`
Same here, I think `computeIfAbsent` would simplify the logic
nit: could use `computeIfAbsent` to simplify this
nit: ditto here, `cluster()` will reconstruct a new object on each call.
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
Should we consider including the error message of `e` in to the exception as well? also nit: capitalized `Fatal`.
Not sure what has changed here.
prop: I would use `taskId01.toString()` here, since you are not testing the `taskId01.toString()` method. Our assumption is that the folder has a name that is equal to the result of `taskId01.toString()` and not `0_1`.
req: Please also verify `stateDirectory.unlock("0_2")`. Only verifying `lockedTaskDirectories()` seems too weak to me.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
As i said above, we should `requireNonNull(mapper, ...)`
We need to choose at least a live replica.
In the ZK based code, we also take live brokers into consideration when selecting a new leader.
We are generating an UnregisterBrokerRecord.
As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too. Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.
Currently, for controller initiated ISR change (controlled shutdown or hard failure), we always bump up the leader epoch. Also, the name alwaysBumpLeaderEpoch is a bit weird since the code in handleNodeDeactivated() doesn't directly bump up leader epoch.
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
Actually, I now understand why the logic is where it is, and why the logical conversion doesn't need to be done. However, I still think the above logic using the schema's default and/or checking whether the schema is optional needs to only be performed when the `schema` is not null.
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
same... `@link` > `@close` for this case.
I would use the full qualified name of `Filter` and `Cache`. If anybody wants to look them up it is easier. Additionally, I would enclose it into `{@code ...}`.
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
And then here ``` Any object created with @{code new} in {@link #setConfig} and that inherits from {@code org.rocksdb.RocksObject} should have {@code close()} called on it here to ```
I would rather write the one line description in imperative. ``` Close any user-constructed objects that inherit from {@code org.rocksdb.RocksObject}. ```
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
This is not related to change in this PR, but after second read, should we setup non-empty topology here. to prevent IllegalStateException to be thrown due to empty topology after some future change. ```suggestion testDriver = new TopologyTestDriver(setupSourceSinkTopology(), config); ```
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
ditto here and others below
Nit: this is really primarily where the configuration properties go, plus any optional config provider properties. How about: ```suggestion * @param originals the configuration properties plus any optional config provider properties; may not be null ```
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
Could store `entry.getKey()` in a local variable since it is used several times
We pass this map to `resolveConfigVariables`, which does: ``` providerConfigString = (Map<String, String>) configProviderProps; ``` That is unsafe if we are supporting props that are not actually a map of strings.
It seems like we ought to just define `log` at the AbstractTask level and avoid having two almost identical `maybeInitTaskTimeoutOrThrow` method definitions.
This is the wrong format for this log message. The exception won't be logged. You have to format the string first: ```suggestion log.debug( String.format("Timeout exception. Remaining time to deadline %d; retrying.", deadlineMs - currentWallClockMs), timeoutException ); ```
nit: use `{}` instead of string concat for `retries`
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
nit: we could just use `Map` for `startOffsets` and `endOffsets`
\cc @lindong28 -- seem's you forgot to update this when dumping the version in `1.1` branch.
Why are these in a separate block when compared to `offsets-for-times-supported`, etc.
`topics` defined here and in next test, maybe move up to `init`
It will result in the same list of versions -- both equally good IMHO.
I'd suggest update the existing test directly in this PR.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
Are you going to update the other two to remove unnecessary operations? That's all that's left and then I can merge the PR. Thanks!
You made this stricter (+100 instead of +1000), I'd not do that given Jenkins variability.
Nitpick: `assertTrue` can be used instead of `assertEquals`.
Let's check that `previouslyAllocated`'s capacity is `batchSize` else `buffer.limit(sizeBytes)` is going to throw with a less useful stacktrace.
Sorry, my bad, but we should probably say that deallocation varies instead of "no deallocation". `8` implies that at least one of them happened.
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: missing `<p>` for new paragraph
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
we need to shutdown the executor service even if there is a test failure (either in a finally block or the teardown).
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Also, maybe we should assert that `numExceptionReceivedInCallback.get() > 0` if we expect at least one to fail (in theory, if `numSentRecords == 100`, there would be no exceptionReceivedInCallback).
Maybe we can have a numRecords variable for the `100`.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
Should be final.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
an -> a
method name changes
an -> a
nit: fill in `@return` docs
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
Sensor names don't appear in JMX.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
can you please also check that the partition id gets set to -1
As before, it's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
req: The names of this method and the previous method should be switched.
nit: I'm wondering if just using would suffice (IMHO slightly easier to immediately grok the meaning). ```java if (!cachingEnable) { context.forward(key, new Change<>(newValue, oldValue)); } ```
I think, we should first check for this condition, because we should only check the most inner store -- if an wrapping store would (be mistake) implement `TimestampedBytesStore`, we would return `true` even if the most inner store does not -- this would be incorrect.
> Why does it do this? Maybe it's a translation layer for other stores? In which case, is it correct for Streams to second-guess the implementation and break its own contract by ignoring the marker interface and delivering non-timestamped binary data? I don't think that would work. Note, on restore, we always get the most inner store and would not call this "translation layer wrapper store" (and thus it would break as we would insert our converter and hand timestamped-bytes to the store that does not understand them). If one want to implement a translation wrapper like this, she need to "hide" it from Kafka Streams and not implement `WrappingStore` (ie, the translation wrapper must be the most inner store).
`WrappedStore` should have a method `root()` (or similar name) that returns the most inner store.
`innerMost = ((WrappedStateStore) innerMost).wrappedStore();` Even if I think, that a `WrappedStateStore` should be able to return the `wrapped` and `root` (ie, recursively wrapped store directly -- ie, there should be methods `wrapped()` and `root()`)
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
Let's capitalize the log statements for consistency. There are a few of these.
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
Changed it locally.
It doesn't seem that the client needs principalBuilder.
That's right, changed it locally.
Let's also add "socketChannel.socket().getInetAddress().getHostName() must match the hostname in principal/hostname@realm"
It seems that we can just use one level of if/else.
nit: `child` -> `toChild`
nit: remove (was tested already)
not used: can be removed
nit: use static imports to get rid of `Assert.`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
an -> a
nit: missing `<p>` for new paragraph
records to it, and reading all records from it, such that
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
I think we could reduce the change of this PR by reverting the numbering change which seems unnecessary.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
You are right. Never mind.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
`tp` is not used anymore.
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
I see. Guess it's good as-is.
This is unnecessary as junit always create a new test class for each test case.
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
This line is failing checkstyle.
Heh, I actually just looked it up, because I was surprised all those other `<p>` elements were not closed... Apparently, even in HTML, you don't _have_ to close `<p>` elements: > Paragraphs are block-level elements, and notably will automatically close if another block-level element is parsed before the closing \</p\> tag. > https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p Sometimes, you just have to stop and marvel at HTML...
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
We lack unit test coverage for this case
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
Would it be a lot easier to do the following? ``` Map<String, Object> prefixedOriginals = connectorConfig.originalsWithPrefix(prefix); Map<String, Object> clientConfigs = configDef.parse(prefixedOriginals); ``` This does check a few more things (dependencies are set and validators are run), so maybe that's not really want we want to do here.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting.
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
Isn't this the same as: ``` tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue); ```
I do not understand why you re-add `clientsOnAlreadyUsedTagDimensions`. Those clients were not modified and not polled for sure due to line 140.
The variable name `polledClient` is unreadable. I think the variable is the client not having the same tag key/value, right? Could we give it a more meaningful name, ex: `clientUUIDNotOnUsedTagDimension`, or other better one if you have.
nit: `this method will....`
line/sentence formatting `{@code null}`.
nit: parameter/line formatting
All the callers seems already handles the `name == null` case so this seems unnecessary.
`keySerde` -> `valueSerde`
```suggestion * @throws ConnectException if the configuration fails to be serialized or if the request could not be sent ```
Java doc for lifecycleListener.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
we can make method this public in `EmbeddedConnectCluster`.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Thanks for cleaning up the code duplication.
nit: it is a tad vexing to see all the `context` prefixes. I guess another option might be to define `RaftClientTestContext` as an abstract class so that the test method can define the test behavior within the scope of a subclass. For example: ```java new RaftClientTestContext(builder) { void run() { assertTrue(client.isShuttingDown()); ... } } ``` Not required, just an alternative to consider.
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Hmm, but we're not actually calling the listener here. We do that separately.
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
the condition is always false if you don't add brackets. ``` throw new IllegalArgumentException("Topic pattern to subscribe to cannot be " + (pattern == null ? "null" : "empty")); ```
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Discussed offline with Becket - rework this patch to avoid the null checks elsewhere; i.e., make the accumulator more explicitly aware of the `sendInOrder` requirement
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
nit: There is an extra space before `=` here and below. I am not a huge fan of using `TestUtils.fieldValue`. Did you consider making both attributes package private or something like this instead? `ApiVersions` is passed to the constructor of `NetworkClient` so we could access it this way as well.
I think the gap in the current tests is that we are only testing with one partition. Indeed this test case also passes without the fix. To trigger the failure, we need one more partition in the test case.
nit: move `logContext` to its own line
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
nit: use `"table-source"` ? It's naming a source node, not a processor node.
nit: it is naming a source node, not a processor node. -> `"source"`
nit: move `topology.globalStateStores(),` to next line.
nit: I think it's better to just print the e.message in a single line.
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I think the only issue is that this message in particular might get a little spammy when we are discovering a new coordinator since it can take a little time for the cluster to converge. Seems fine to increase verbosity for the other errors though. By the way, it looks like we're missing the word "failed" in the message
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
Since we're specifying the key, we expect only to get back windows with the value for that key. The aggregation we specified is to sum all values for the key, and it comes out to `2` because we only write one value for each key; namely, the value is the same number as the key.
This is why the test was failing for you. The query is for a range of window start times, not record times. Since the window size is five minutes, the range `[now - 1 minute, now]` wasn't going to contain the actual window start time of `now - 5 minutes`. In other words, just a simple oversight :/ Sorry for the trouble.
It seemed like a good idea to check a few other query configurations, but none of them showed any problems.
Since I was fixing stuff anyway, I went ahead and fixed a bunch of formatting issues that I didn't bother mentioning before.
This isn't our fault. When we added the timestamped stores, we chose not to make SessionStores timestamped because the session bounds already have the end timestamp available, which is identical to the timestamp we would have stored in the value.
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
Ah, you're right.
nit: we could define this transition list in a variable to be reused.
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
@thspinto I think i am running into the same issue which you pointed here , so the source consumer group has different topic and is active and the target consumer group is idle but having different topic, but the code only checks for the topics and partitions matching the target site and adds them only when the target offset is less than source, it ignores other topics at the source
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
may be use Objects.requireNonNull
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
Also: should only call onPartitionsLost on owned partitions that no longer exist
typo: moreq -> more
nit: extract the call to this.interceptors.onConsume(new ConsumerRecords<>(records)) above line 1258 - its result would always be used
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Could use `equalsIgnoreCase` directly.
these 2 lines shouldn't be here
need a newline after the group before the underline, and a double newline after the underline
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
perhaps a better name is `getConfigKeyRst()`
Also we can just pass in the `StringBuilder` as an argument rather than create a new one here
I think you can do the zookeeper start in the `setUp` method
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
I'd suggest update the existing test directly in this PR.
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
Ditto as above, we could use any node to find coordinator.
Can `LogManager.getRootLogger().getLevel()` be `null`? With other loggers you return the effective level if that's the case.
Is this the format required by the mbean stuff? It might be nice to return something more structured here.
this _technically_ changes the public interface and would require a KIP if we're being pedantic about the process. I personally think we can go by without a KIP but we obviously need a committer to say what they think
@wicknicks would be useful to have some unit test for this class.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
nit: "Null value is encoded..." -> "A null value is encoded..."
"encoded on two bytes using network byte order" -> "encoded using two bytes in network byte order"? Similar for the other integer types if you like the suggestion.
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
nit: Values 0 and 1 *are* used..
even clearer: "Represents a signed integer"
We could get away with a single `*`
If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all.
Ditto here and elsewhere.
I now saw that in the consumer tests you use `Duration.ofSeconds(1).toMillis()` and `Duration.ofMillis(999).toNanos()`. This makes it already clearer. I think a variable with a meaningful name for the lower bound would make it even clearer.
typo in the test name.
I see. Could we do something like this: first assign the partitions for all internal topics as the writing topology's number of tasks, i.e.: ``` // for all internal source topics, // first set the number of partitions to the maximum of the depending sub-topologies source topics for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) { Set<String> internalTopics = entry.getValue().interSourceTopics; for (String internalTopic : internalTopics) { Set<TaskId> tasks = internalSourceTopicToTaskIds.get(internalTopic); if (tasks == null) { int numPartitions = -1; for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> other : topicGroups.entrySet()) { Set<String> otherSinkTopics = other.getValue().sinkTopics; if (otherSinkTopics.contains(internalTopic)) { for (String topic : other.getValue().sourceTopics) { List<PartitionInfo> infos = metadata.partitionsForTopic(topic); if (infos != null && infos.size() > numPartitions) numPartitions = infos.size(); } } } internalSourceTopicToTaskIds.put(internalTopic, Collections.singleton(new TaskId(entry.getKey(), numPartitions))); } } } ``` And then update the Cluster metadata with `Cluster.withPartitions`, then in the `ensureCopartitioning` call, if after the first for-loop, `numPartitions` is still -1, it means all topics in this co-partition group are internal topics, and then in this case read from `metadata.partitionsForTopic` and took the maximum among all of them; and then later after calling `prepareTopic`, the metadata will be updated again with `metadata.withPartitions()`.
Minor typo "will is"
nit: "name" => "joinThisName"
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
Also, instead of adding an extra operator node, I'd suggest we just do the checking within the operators themselves to reduce virtual function call overheads, for example see `KStreamKTableLeftJoinProcessor.process`.
Yeah, that works, too, and is more align with the current code.
AK convention is to not use `set` setters or `get` getters.
Right, so we should just reword Each connector gets their own dead letter queue topic to be more clear, so something like Each connectors dead letter queue is usually written to a different topic. Not super important, but Im just looking to eliminate potential usability issues.
Do we need to prefix these with `msg.`? Also, it might be nice to control the message format when message keys and values are to be included. I know that's more code, but it could be made to be similar to the other option.
Is there any way to put this into methods on the `ProcessingContext`, like `toString(boolean includeRecord)`? That way, its `toString()` method could really just be `toString(false)`. Not only is it a bit more testable, the logic of this reporter is more readable and focused on writing to the log.
This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.
Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
Not really sure this has value if the test case expects the leader change correctly.
nit: remove empty line
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
I think it is probably worth adding as even if it is deprecated it is still supported
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
Incase => In the case
Should we remove references to implementation from the interface? We can keep this doc in the implementation class.
Can this be a `byte`.
You can either make them non-static or pass `Logger` as a parameter. Makes no difference to me, but `log` won't work as a static field when you have multiple instances.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Code convention nitpick: there should be a space before the colon.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
ditto to `KStreamImpl`
nit: remove `this.`
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
I was looking at the `append` code and it seems a bit brittle. It assumes that: - The collection returned from `PartitionRecords.take` is safe to mutate even though the latter returns `Collections.emptyList()` if `records == null` - That `part.take` will always return at least one element This is fine today, but it may be worth making it a bit more robust to refactorings.
nit: o2[%s] was **equal** to o1[%s]
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
```suggestion return futures.stream().allMatch(Future::isDone); ```
Please include TopicDeletionDisabledException here.
This exception can't be thrown by DeleteTopics.
AK convention is to not use `set` setters or `get` getters.
Hmm, normally `IllegalArgumentException` indicates that the argument to a function is bogus, right? That's not really the case here-- the function argument was fine, but the topic wasn't set up correctly. This can probably just be a generic `RuntimeException`, since we don't have a need to make it something fancier.
Yeah, that works, too, and is more align with the current code.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
How about `return admin.endOffsets(assignment);`
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
placeholder may not be required for exception
nit: don't need the type params on the next three lines
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
This should be three tests.
You can remove this `assertThat` and the loop below as you've already proven this is true in the test above. So no need to assert it again.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
as above: we need to remove adminPrefix configs
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
nit: `anf` -> `and`
It might be simpler to just use `int transactionTimeout` -- Java will auto-cast to long in ``` if (transactionTimeout < commitInterval) { ```
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
Can you explain this a bit further? Why do we return null when the group is stable, I assume stable means everyone is successfully on the same generation? So why return null rather than the actual generation.
I think you should have a separate method `generationIfStable` or something.
How much effort would it be to have a test case for this? We have a few LeaveGroup tests in `ConsumerCoordinatorTest`.
SGTM. If we find it flooding the logs and not helpful we can reconsider
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
I think we may be able to get rid of some of the `bytesRead` bookkeeping. As far as I can tell, it is only used in the exception message below and it seems redundant there (i.e. sizeOfBodyInBytes - bytesRemaining = bytesRead).
`keySerde` -> `valueSerde`
nit: both lines missing . at end
Nit `.` at the end
nit: single parameter per line
above (some some more times below)
nit: Please fix code style.
see my question above about using mocks.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
Nit: can be `final`
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
```suggestion "The desired Unix precision for the timestamp. Used to generate the output when type=unix " + ```
`orderInGroup` param is duplicated for key & value converter
nit: fits in one line
nit: blank line missing here
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
This test went from checking for exactly one message, to checking at least one message. If you think that is fine (I dont have the full scope so I dont know) then LGTM! :)
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Incredibly unlikely to be a problem, but this should be `>=`. Actually, this is a bit risky anyway because one timestamp is from the driver machine, the other is from the worker machine where the connector is running. (We ntp the workers during setup, so they are reasonably in sync. I can't remember the exact details, but I think we had issues with this previously due to clock skew; iirc it was in code that generated security certs on the host and then tried to use them on the workers.)
Should this be `num_lines=3` (cf. L116 and L126)
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
Why are we generating the connect internal topic names here? if there's a rule for the topic naming convention it should be defined in one place which is `ReplicationPolicy`.
> only when MM2 is running in standalone mode. They are created in any mode if there is no value for DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG > If the user is running MM2 in connect mode, the user is responsible for configuring DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, DistributedConfig.CONFIG_TOPIC_CONFIG, etc. It is what you are meaning. Right? Small clarification, users can use `DistributedConfig` with any mode (even standalone) to override the name of these topics. And they always had the power to do so, even before KIP-690, and if this new topic name didn't match the `isInternalTopic` policy, it would replicate. The PR's approach is trying to control the Connect topics that MM2 needs to set up using the separator; this is where I am not sure it's a minor fix or something that requires a KIP that follows KIP-690. My suggestion, is to introduce the minor fix first and propose another KIP if you believe that Connect internal topics created by MM2 Workers should to be controlled by the separator as well.
Method should not be `final`. Additionally the `final` keyword for method arguments and local variables is not required and does not improve readability of the code here. Indeed Java does not distinguish between readonly and read-write variables. But unless an anonymous class is declared (this requirement is removed after Java 8) or the variable is used further down in the code (improved readability) marking every single readonly variable as final does not make things better IMHO.
nit: config is an overloaded term in the code, you might prefer to name this argument configInfos for instance.
Currently in this file the indentation style used is: ```java protected boolean maybeAddConfigErrors(ConfigInfos config, Callback<Created<ConnectorInfo>> callback) { ``` Still, once we move to single arguments per line it should be: ```java protected boolean maybeAddConfigErrors( ConfigInfos config, Callback<Created<ConnectorInfo>> callback ) { ``` I'd pick one of these. (First I confused `callback` for a local variable)
Blank line between description and params.
Also no need to use upper case in words within the sentence. E.g. `@param configInfos config infos to read errors from` ` @param callback a callback to add config error exception to` `@return true if errors were found in the config, otherwise false`
```suggestion info.userEndPoint(), taskManager().getTaskOffsetSums()) ```
req: Due to this deletion, line 327 becomes a no-op. Please remove it, too.
nit: line 365 above can be moved down now since it is only needed before line 379.
Not sure if we need to call out EOS in particular? It's incorrect in any case.
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nit: fix line break
Sorry, my bad! It doesn't matter whether it is called or not, since the `IllegalStateException` is thrown.
This test misses the verification whether the `StreamThread#setUncaughtExceptionHandler()` is called. This has been missing also before this PR.
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
Thanks for clarification.
Not sure I follow the logic of this test... We want to show that it doesn't block, but then we just assert that the call doesn't take longer than 30 seconds. It seems like this test is sensitive to timing, which is not ideal. I guess that what we really wanted to test is that the close method never calls `Thread.sleep()`/`Thread.join()` or maybe just never calls a specific list of blocking methods on the clients. Now that we have all the mocks above, it seems like we can make this assertion directly.
add `final` (also all other methods below)
would be nice to mark all params final whilst we are changing this
add `final` twice
nit: both lines missing . at end
Nit `.` at the end
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
A docstring for this method would be good :)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
You should use try with resources here too.
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
Could we try to avoid this copying if we assume it always wraps an array? I.e. ``` if (data.hasArray) return data.array(); ``` My syntax may not be perfect, and we need to double check the starting and offset in the backed array, but the general idea is as above.
Nitpick: I'd call this `deserialize`.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
@mjsax What you suggested sounds right to me.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
I think we can remove this variable to simplify the code further.
the method `restorePartition` is no longer used and can be removed
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
How do you feel about dropping `Number` from these names? This would be consistent with the methods we expose from `TransactionManager` itself (e.g. `lastAckedSequence()`).
Hmm.. In fact, there is not necessarily any relation between the partitions that are being consumed and those that are being written. Usually you would expect them not to overlap. I wonder if it actually makes more sense to track these offsets in a separate map.
Might help to have the partition in this message. Same thing in `add` below.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
This may be useful, since the log messages in TransformationChain do not print the schemas in `ConnectRecord`
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
SGTM, thanks @mjsax to bring to our attention.
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
typo: we want to test the **case** that poll() returns no records.
Same here, this exception message does not apply to the case this is trying to catch
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
Should this be `num_lines=3` (cf. L116 and L126)
Shouldn't need this line, it's handled by the superclass's constructor.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
I would maintain the terminology of "subscribe" to topics and "assigned" partitions, and say: "manually specify the partitions that are assigned to it"
I think `will go` should simply be `go`.
`while` seems to be missing
Worth noting the need to specify offsets when starting and after rebalance. Otherwise it'll just use the auto reset strategy.
I thought configuring consumer groups is not mandatory? (i.e. "consumer can configure a consumer group" but not "must").
Pls use caps :) (see your `ErrorReporter` above).
Just thinking about is once more: why do we need to make this interface public? We have `Named` as public method to use `NamedOperation` and other public control objects (`Consumed` etc) that implement it -- but I actually think, users don't need to know about this interface? \cc @fhussonnois @bbejeck @guozhangwang @vvcephei @ableegoldman
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
Oh, and a typo which I would like to make KNOWN (or UNKNOWN?! ... I would pick a pun over clarity any day :) )
Or https://github.com/google/guava/blob/master/guava/src/com/google/common/math/IntMath.java#L56-L72 It is safe to look as it is Apache License 2.0.
Can you do something like: ```java static final int tableSizeFor(int cap) { int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1); return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } ```
nit: `addMetadata` -> `put`
Also not clear why "numSegments - 1" here.
I know the naming thing has bit us in the past, is this same approach used elsewhere and/or how was it decided on? Specifically, metric name constraints really shouldn't be JMX specific if that is the case here, despite the fact that the metrics is so obviously JMX-inspired. I can easily find https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala#L46 but nothing else. Have we not had the same problems because metrics w/ topic names in them already have constraints on the naming? If I am remembering correctly, I think maybe both @gwenshap and @junrao were involved in some discussions, I think `-` vs `_` was a problem at some point? Maybe one of them could chime in here.
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
We normally use `assertThat()` in new and refactored code. Please also change the other occurrences. ```suggestion assertThat(hasStateTransition(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING), is(true)); ```
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
Could you please add some line breaks? This and some of the other verifications are too long.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
`return` is not necessary
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
nit: 'else' can be dropped
I don't think we really need this function any more... we can just submit to the executor from the other function.
typo: byteArrray -> byteArray
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
Might be better to use an `Exception` variable `firstException` and rethrow at the end if not `null` -- IIRC, behavior is undefined if we throw a second exception (ie, `finally` would executed after the first (outer) `catch` block.
`start` is not used.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Sensor names don't appear in JMX.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
I think it's just a computer-sciencey matter of principle. `clientsByTaskLoad` is a linear collection, so every `offer` would become `O(n)` if we did a `contains` call on it every time. Right now, it's only `O(n)` when we need to remove the prior record for the same client, and `O(log(n))` otherwise. Does it really matter? I'm not sure.
Gah! You're right. We should also _remove_ the client from `uniqueClients` when we `poll`.
Ah, sorry about that @ableegoldman ; I wasn't able (or was too lazy) to follow the `git praise` trail through the class movement. Well, kudos to you, then. :)
I'd just like to say what an awesome tool for optimization this class is. Kudos to you and @cadonna .
I do not remember having contributed to this awesomeness. It is all @ableegoldman 's merit.
I think @becketqin was suggesting that we can add a method to `Kafka` that would cause the thread waiting on `KafkaServer.shutdownLatch` to resume and call `System.exit()` with the appropriate exit status.
If there are multiple FatalExitError thrown, do we want call `System.exit()` only once instead of creating a thread for each of them? Maybe we can just log the error and return.
It should be 'false' by default
I'd also consider removing this one too if we are not using it.
Is this constructor used at all? If not, I'd not include it one should generally provide a message explaining more.
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
Thanks for the clarification @hachikuji
Yeah if it exists elsewhere let's just leave it as is for now.
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
+1 on assuming a single children, check-and-throw-otherwise
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
We should have a constant rather than using '262' directly
Do we need the if/else? Since this is a unit test, it seems OK to just assert that the first element is the rate and the second is the total.
Let's use try with resources here and the other test so that the file is closed after it's used.
Ah, nevermind, the topic is in the message's `toString()`.
typo: Woth -> With
It reminds me that sometimes I did the same thing of piggy-backing re-orging in the code base with actual changes that makes the reviewers life harder :P All of us should be careful in the future. That being said, for this PR since Boyang and I have bite it already, I think we can save reverting the piggy-backed reordering and just merge as this.
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
Ideally, we'd always use brackets on control flow operators.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
nit: use class `import` to avoid long name here
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
store not used
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Shouldn't need this line, it's handled by the superclass's constructor.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
`threadId` is no longer used.
Should we use "NO_CURRENT_THREAD" to replace "null"? It seems to me "NO_CURRENT_THREAD" is more readable.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
Ah got it, my bad :)
nit: no need to set `driver` to `null` each test gets it's own instance of the class, so it will always be null for the next test.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Note that Kafka only supports kerberos as the SASL mechanism.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Yes, this seems fine then.
remove this line
we need to remove this line, too. It is taken car of in `mapValues(ValueMapperWithKey...)`
Ack. Thanks for clarification.
This does not seem to be backward compatible as we ignore `keySerde` and `valSerde` now...
Nit: remove `this` (we try to avoid `this` wherever possible)
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
This seems to change the behaviour...
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
We should verify the actual timestamp.
nit: remove empty line
nit: add `final` (2x)
This is not required as contained in the check next line.
nit: remove (was tested already)
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
empty line needed
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
It was removed from the other versions of `group` but not from here.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Thanks for verifying @vvcephei!
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
Better name as "setCurrentNodeInProcessorContext"? And then in java docs mention that it returns the processor context with current node set.
nit: add `final`
nit: move to next line ("weird" formatting)
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
This will probably be subjective, but I'm ok with "hop" for now.
The name `restoreAvailableMemoryOnFailure` is a bit weird because we should always restore available memory on failure. Maybe we can name it `hasError` and set it to `false` right before `return buffer`.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
You are right. Never mind.
We can probably remove this line `restoreAvailableMemoryOnFailure = false`.
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
`consumer messages` should be `consume messages`
I suggested `alive` to be consistent with the method being called on the consumer. The main reason is that `init` and `start` are a bit too similar and it's a bit difficult to distinguish between the two. Anyway, ok to leave as is if you feel strongly about it.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Delete this block - this was a specific check for ensuring log output in the test_console_consumer
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Ok. Thanks for clarifying.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
if (comparator.compare(nextNoTimestamp.key.get(), nextWithTimestamp.key.get()) == 0) we need to advance on both ends while only returning the one from with-timestamp iterator, otherwise we may get duplicates returned.
This can be static
This can be static
This can be static
nit: What about extracting the construction in a small helper method `prepareDescribeLogDirsResponse` that create a response for one LogDir and TopicPartition? It seems that the same block of code is used in many tests.
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
```suggestion /** * The value of {@link StreamsConfig#APPLICATION_SERVER_CONFIG} configured for the Streams * client. * * @return {@link HostInfo} corresponding to the Streams client */ ```
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion * Metadata of a Kafka Streams client. ```
I'm not sure of the original motivation behind setting it to `50`. But if we are going to define it as `1000` i think it might be better to remove this and `reconnect_backoff_min_ms_config` definitations from here as they are the same as what is set in the `ProducerConfig` and `ConsumerConfig`, so it seem pretty pointless overriding them
Might be overkill if this is the only use case, but we could also add a composite validator.
`orderInGroup` param is duplicated for key & value converter
I don't think we want this in the base class -- its risky as it leads to accidental sharing as we had before. Instantiating it once statically in `SourceConnectorConfig` and `SinkConnectorConfig` seems fine, but keeping it here seems like we're inviting people to accidentally define additional parameters on it. It looks like we only use this in one place (in this file) so probably not a big deal to remove it.
```suggestion "The desired Unix precision for the timestamp. Used to generate the output when type=unix " + ```
Is this `null` assignment needed? Don't see the variable used after this.
This seems unnecessary since we're throwing away the collection anyway.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
It might be better to use a Kafkaesque schema definition.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
Should we use "NO_CURRENT_THREAD" to replace "null"? It seems to me "NO_CURRENT_THREAD" is more readable.
`threadId` is no longer used.
I would move line 328 and 329 to before this line.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
I would move line 330 and 331 to before this line.
This doesn't read well. Assuming it needs to be as long as `inactivity-gap` plus `grace-period` then the following reads better to me: ```suggestion * @param retentionPeriod length of time to retain data in the store (cannot be negative) * Note that the retention period must be at least as long as * inactivity-gap plus grace-period. ```
`{@link TimestampedWindowStoreBuilder}` should probably be `{@link TimestampedWindowStore}`.
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
We should test that delete twice in a row fails with `IllegalStateException`
Line too long (also some lines above and further below)
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
line too long
We should use `to()` and use a `TestOutputTopic` instead of the processor
nit: it was correct before
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
Yeah, that works, too, and is more align with the current code.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
Would `Integer.toHexString(value)` work here? Not sure if it's exactly the same as the current code.
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
Thanks for the explanation. A bit subtle as you had said. :)
nit: extract the call to this.interceptors.onConsume(new ConsumerRecords<>(records)) above line 1258 - its result would always be used
While you're here, I suggest fixing `fetchablePartitions` so that it doesn't do two `remove` calls on an `ArrayList` (in the worst case, it has to shift all the elements in the underlying array twice). We could pass a predicate to `subscriptions.fetchablePartitions()` and make it more efficient.
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
I've been trying to follow this. Looking into `ConsumerCoordinator`, I think I see what you're talking about, there are several opportunities to return early if the timer expires. But is `1ms` enough to guarantee that we'll actually get through to where we fetch the data? Is it possible to just modify the conditionals farther down to make sure that we can always make progress even when the timer is set to 0ms (i.e., the request is fully asynchronous). To me, a timeout of 0ms doesn't mean that the operation should take no time at all, just that it shouldn't block. It seems "within bounds" to still perform any operation we need in order to guarantee we make progress.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
Nit: maybe this can call the newly introduced `withTransactionalRecords`.
We'll need to fix this in a follow-up so that followers send the right replicaId.
Not really sure this has value if the test case expects the leader change correctly.
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
nit: A better general pattern is to use `assertEquals` comparing against empty list. Then if the assertion fails, the message will show what was in the collection.
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
Same as above mentioned, the validation didn't get handled in new API.
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
`before or after`
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Ok, I see that `rightWindowStart <= currentRecordTimestamp` isn't necessarily true when you call this from `processEarly` but I think you're kind of abusing this poor method  . I would keep things simple here and make sure the parameters always mean exactly the same thing when you call this, ie `rightWindowStart` should _always_ mean that "the start time of the right window for the record which is previous to the current record" . If that means some duplicated boolean checks here and there, so be it.
I guess "Necessary" still seems kind of open-ended/vague. By that you mean, "is not already created", right? But maybe we should wait until we see the final form of this method in case there are any further changes, and then we can go back and try to fish out a more specific name
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
nit: full-stop after the description.
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
`windowSize` should be `Duration`
And if they are not used yet, we can probably remove them for now.
I've been thinking about this and I wonder if this is the best way to handle this. It feels a bit error-prone to pass a `RecordMetadata` with almost nothing set. Have we considered passing the topic and nullable partition via the `exception` parameter? It's then easy to explain: if an error occurred, the second parameter is set, otherwise the first parameter is set. This would imply having a custom exception type that would include topic, nullable partition and the original exception.
@junrao The user would not have to cast because we would change the type of the second parameter from `Exception` to e.g. `RecordSendException`. This class would have 3 fields: `topic`, `partition` (nullable) and `cause` (original exception). It also means that if we ever need to add additional fields for the error case, it's easy. Wrapping and unwrapping the exception can be a bit annoying, but it's a common pattern in Java (`Future.get` throws an `ExecutionException`, which wraps the original exception for example).
The issue with passing topic/partition through exception is how the user would know which specific exception class to cast to. Also, when there is an error, the topic in general is available. However, partition may not be. So, you would still have the issue of including partial metadata. Passing any available metadata through RecordMetadata seems more natural to me.
Code convention nitpick: there should be a space before the colon.
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
nit: `final` (also next line)
nit: move to line above.
Oh right, forgot that it doesn't have the window times either. Nevermind then
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
Since this is at the end of runOnce, I'm wondering if it also makes sense to log whether we committed or punctuated, and whether/how many records we polled at the beginning of the method. Basically, it seems like, if it's a good idea to log some information once per cycle, then it's probably a good idea to summarize everything you'd want to know.
Is it important to make the distinction between "Processed zero records, going to poll again" and "Finished processing 0 records, going to poll again"? Also if we are not saying its processed anything maybe we should log what it is pulling from
Maybe we do this else where but can we log at DEBUG if there is nothing done and there is tasks assigned to the thread? It might be good to be able to confirm that its not getting stuck here or its actually not polling anything.
I have a similar question here, other than that LGTM
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
(Tbh that drives me crazy, I once spent like 4 hours debugging something only to realize that I wasn't using the correct TimeoutException  )
nit: move the import to the other `o.a.k.*` imports
Nit: move these two static factory methods above the non-static member variables, so all static and non-static members are together.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
How about "runs an external command for the worker."
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
```suggestion /** * The value of {@link StreamsConfig#APPLICATION_SERVER_CONFIG} configured for the Streams * client. * * @return {@link HostInfo} corresponding to the Streams client */ ```
```suggestion * Metadata of a Kafka Streams client. ```
never mind then. I'll leave this to AI.
these overrides don't seem to add much.
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
Not sure if we need this? If the shutdown is not clean, we logged an ERROR before in `StreamThread#run()`
nit: `clean flag` sound like an implementation detail, that may not be a good way to phrase it for an INFO log...
Ditto above: This is the only case where we pass in the parameter as `false`, but I think this check is not necessary as long as the state is guaranteed to be in `pending shutdown`.
@dguy If we need to access an inner function for lots of unit tests it usually indicates that our class design patterns are not good since unit test should be testing a class's out-facing behavior only; in other words cases that you need to trigger an inner function in unit tests should be rare, and I think reflection is fine as long as it is rarely used.
IMO, using reflection should be a last resort. It is pretty horrible and makes the tests harder to comprehend. I'm not a big fan of making methods visible just for testing, either, but I prefer this to having hacky test code using reflection. What would be better is if there was an easy way of testing this without either of the approaches.... That would require a fair amount of refactoring and smaller classes.
Not really sure this has value if the test case expects the leader change correctly.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.
We probably want all these `Long`s to be `long`s.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Could use `equalsIgnoreCase` directly.
Can this be a `byte`.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
I was checking the code in the protobuf repository and it turns out that the optimised version of this code is pretty complex: https://github.com/google/protobuf/blob/master/java/core/src/main/java/com/google/protobuf/CodedInputStream.java#L2473 I think it makes sense to start with the simple implementation, but we should file a JIRA for considering the optimisations in their implementation.
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
I think we can move this logic into ValueOrOtherValue as another static constructor.
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
nit: add `a {@link Named} config`
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
the method name changed to `windowedTable` and `windowSize` parameter is missing
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
Since this is a general `toString()`, we probably should handle manual assignment as well.
You don't want to pay the cost for the computation of the hashCode if it's never used. I still think it would be better to make the hashCode very cheap by just using the `id`.
Nit: why reuse `appId` -- if there is a bug, and we set `APPLICATION_ID_CONFIG` by mistake, it would not be detected. Maybe better to set a different id.
nit: no need to hyphenate unrecognized.
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
extra new line.
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Not sure what has changed here.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
I think it is still possible. Here's one scenario: 1. last checkpoint at offset 100; all writes goes to old CF. 2. continue writes to old CF til offset 110, but no checkpoint written yet. 3. non-graceful shutdown happens, and upon restarting new CF is used. 4. we start restoring from offset 100 to log-end-offset 110, to the new CF. Now we ended with data of offsets 100-110 in both CFs.
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: break line
nit: line too long
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
This was the checkstyle error that was failing your build.
More explanatory error, as discussed.
Dropped this unnecessary duplicate code, as we discussed.
If not, we should move the exception capturing logic inside the dbAccessor as well.
We shouldn't return `null`, but instead return a "unknown query" result.
Existing issue, space should be after the colon.
We should deprecate this one too I believe.
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
nit: extra newline here
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
nit: 4-space indention plus move `builder` down one line
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
ditto to `KStreamImpl`
prop: We could do ``` committed += taskManager.commitAllStandby(); if (committed > 0) ``` for the rest of the logic so that we don't need a separate `if (committed > 0)` later
Fine with me to keep the guard. Was just double checking.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
@mjsax , just to clarify, are you asking whether https://github.com/guozhangwang/kafka/pull/6 would conflict in some way with restructuring the commit logic? I don't believe so. I'm only changing how the tasks' lifecycles are managed, so it ought to be encapsulated from the thread's perspective.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
Oh, nevermind. I didn't see the following line.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
adding the registraiton part into Config file doesn't look good to me. Generally speaking Config classes are only there to express the config members and validation around them but not to take any action. SecurityUtils.java would be a better place.
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
ditto for the rest of the test
Might be simpler to use the mock deserializer only for values.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
none from what I can see, but I'm not sure it's worth holding up the PR for it.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
typo: `will does not`
Nit: remove empty line ;)
`statestore` -> `local stores` (to use same terminology as above)
If these aren't expected to change at runtime, I'd suggest initializing the `HashMap` in the static block as a temporary variable , then wrapping it to create an unmodifiable map which is then used for the constant here: ```java Map<String, File> pluginJars = new HashMap<>(); try { pluginJars.put(..., ...); // More of the same } finally { PLUGIN_JARS = Collections.unmodifiableMap(pluginJars); } ```
Seems the `fastTimeout` should be thread-safe since it will be accessed by both RequestSendThread and controller event thread. Perhaps `volatile` is a least requirement for memory visibility.
the test passes without the second poll. the first poll finishes the sync ``` INFO Successfully synced group in generation ``` before the second poll is triggered. The second poll notifies the assignor and gets committed offsets which i don't think is necessary in this test
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
nit: remove extra newlines
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Instead of using `.format` and `+` to create the string, maybe use same way as `cmd` is constructed (using % to format, and multiline string without `+` but by ending with `\`)
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
`start` is not used.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
I think we ditch the before/after methods as I previously recommended.
ditto on (what I think is) the impossibility of this condition being false.
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
I wonder if we want to print the actual list of partitions here, which might be long. And do it twice. I see the same pattern is applied elsewhere. I understand the value of explicit listing.
nit: move below the shortcut return below.
I was thinking it was odd that this wasn't reusing `flushAndCommitOffsets` since they are basically the same. But I see the exception handling is different, so that makes sense. But then I noticed there's no call to `onCommitCompleted` if there's an exception during the flush, which we do for every other path during commits. _Then_ I realized that this is actually for the special case of closing, and that the callbacks that are invoked by `commitOffsets` in this special case are weird since the seqno will never match and it is probably always getting logged as an "error" at debug level. This isn't critical since it's just at debug level, but should we have `onCommitCompleted` check for the sentinel value and ignore the callback in that case? But also, I think the seqno handling is kind of unnecessary now. I think this is a holdover from possibly 2 separate things. First, we previously handled offset commit differently with different threads. Second, the semantics of offset commit in the new consumer were very unclear at the time this code was original developed (that was back when it wasn't even fully implemented and I was trying to sort out what semantics we wanted, so ended up being somewhat defensive in this code). I believe it is the case now that you cannot have callbacks for offset commit come back out of order (despite the fact that they are async) and that since we guard offset commits with a check on whether we are currently committing, we can only end up with multiple because we explicitly allow commits to expire to allow a newer one to be submitted. But will this ever actually help? Because of the way the protocol works, won't the offset commits just get queued up anyway? Would it make sense to adjust this so we have to wait for the commit to finish regardless, but that we might do something like pause processing if it takes too long? Basically, since we don't do it synchronously, we allow processing to continue since it's nice to not have to stall during the commit, but at some point if we couldn't commit, we shouldn't just cancel that commit, we should wait until it actually completes. (And, of course, we also need better handling if the commits repeatedly fail.)
Since condition is just a comparison, you can put the comparison here directly
request1 and request 2 are not used.
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
The other constructor calls the parameter `sampledStat`. We should be consistent.
"this until it is" doesn't quite parse.
Oh, right, I forgot that the metrics registry returns the same copy of the sensor when all the name, description, and tags are the same... Thanks.
nit: I slightly doubt whether these exact match tests are necessary
I would suggest we co-locate the description with metrics and refactor out the sensor creation part, this may help reduce code redundancy.
Yes, each time the method is invoked (I.e., once per store), a separate copy of the string is placed on the heap. I previously didn't think this would be a big factor, but someone in the community profiled the memory usage of a long-running topology and found that these strings tend to accumulate over time. There's no need to worry about this for short-scoped strings like exception messages, but metrics are long-lived objects, and we benefit from making them static constants.
`Moving average duration` may be a bit confusing to readers, maybe just `Average duration of ..`.
My minor concern is that KeyFactory and ValueFactory may be only specific to key-value stores, not not any general stores; for example for database stores you may have some functions like ``` withSchema() ``` that defines the data types for each column but not using "withKey / Value" any more. But since it is only for future improvements let's revisit this nested mechanism later.
And if they are not used yet, we can probably remove them for now.
nit: remove empty link
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
nit: remove empty line
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
old code sets `retainDuplicates=true` but new code sets `retainDuplicates=false`
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
Thanks. Not a blocker.
If these aren't expected to change at runtime, I'd suggest initializing the `HashMap` in the static block as a temporary variable , then wrapping it to create an unmodifiable map which is then used for the constant here: ```java Map<String, File> pluginJars = new HashMap<>(); try { pluginJars.put(..., ...); // More of the same } finally { PLUGIN_JARS = Collections.unmodifiableMap(pluginJars); } ```
This method is also deprecated. We should throw same exception as for `childIndex`.
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
nit. Add `{ }` to block (we always use them). Same below.
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: top of class
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
ditto on removing before/after.
nit: remove empty line
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
@ewencp Actually I'd probably prefer to use separate explicitly named classes. For example: `StartConnectorCallable` and `StopConnectorCallable`. The reuse of the status enum seems a little confusing.
Can this be reverted? The existing formatting appears to be correct. ```suggestion public class ErrorHandlingMetrics { ```
Nit: let's remove this blank line, since it's unrelated to other changes.
Fails checkstyle, needs to be final
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
Maybe we should have a line for the old consumer as well. Regarding your question about the cluster id, the following line needs to be conditional on `from_kafka_version <= LATEST_0_10_0`: ``` assert self.zk.query("/cluster/id") is None ``` The code is just checking that the cluster id is generated after an upgrade from 0.10.0.x or lower to 0.10.1.x or higher.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
Nit: space missing before `timestamp_type`.
`topics` defined here and in next test, maybe move up to `init`
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
I think we can simplify these functions. Something like this: ```java private static byte readByte(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 1) readMore(buffer, input, bytesRemaining); return buffer.get(); } private static long readVarLong(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarlong(buffer); } private static int readVarInt(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarint(buffer); } ``` I think we shouldn't need more than one call to `readMore`.
Can we change `subject` to `rockDdStore` -- it's a weird name IMHO.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
Another name might be `seek()`.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
the method `restorePartition` is no longer used and can be removed
@mjsax What you suggested sounds right to me.
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
req: This is unnecessary
This is the same code as in `KTableFilter` -- we should refactor and share code.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
Not introduced in this patch: "is non" => "as non"
As mentioned above, we should defer all the checks of null keys to "repartition (before join or aggregation)", and the "join / aggregate" operators themselves. I think we are already doing this for most cases, but just double checking.
"empty" should be clear enough. Will update it in #10092
If @jeqo has not objections, I think we should remove it for both cases.
just in case, latest consumer-group reset-offset is comparing against `ListOffsetsResponse.UNKNOWN_OFFSET` instead of `null`. https://github.com/apache/kafka/blob/0bc394cc1d19f1e41dd6646e9ac0e09b91fb1398/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L680-L681
I think we need to pass this exception to the future instead of raising. Would be good to have a test case if we don't already.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
I don't fully understand why we check for a `processId` less than or greater than the ones defined above.
`hasItem(task01)` -> `equalTo(Collections.singleton(task01))`
As above: add more "randomness"
Just wondering if this test is good. Seems like a "RoundRobinAssigner" might compute the exact same assignment. Some more "randomness" would be good IMHO. (Not a must though.)
It's not about capacity, is it? It's about having not task that does not have the task assigned (as active or standby). -> `shouldNotAssignStandbyTaskReplicasWhenNoClientAvailableWithoutHavingTheTaskAssigned`
`final` is not required here.
I've seen alternative solutions floating around that use a configurable source here. Basically, the configuration passed to configure() is consulted to find the "source cluster", rather than looking at the topic name. That approach lets you return an actual source here, which obviates the new canTrackSource() method etc.
createMetadataTopic() is no longer used.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Don't need `Vin` here
Don't need `Vin` and `W extends Window` here
Can we call this something like `ensureCopartitioning` or `processRepartitions` or something? My take is that the copartitioning is the main point of this method so that's probably good to include in the name
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
original was better
original was better
original was better
original was better
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
nit: line too long
nit: break line
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
I think this could be done with `computeIfAbsent` like in "finishSnapshot" above
You might consider using `OptionalDouble`.
(very minor) nit: inconsistent naming of these in this class
Did you mean: ```suggestion setBrokerId(2). setBrokerEpoch(100). ```
nit: add a size? There are a few cases in here where we could do this.
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
super nit: extra blank line
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
Use `KafkaException` instead of `RuntimeException`
Use `KafkaException` instead of `RuntimeException`
We should add a `null` check to allow closing a deserializer that was not properly setup
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
this is the same as above: we should extract to a method to avoid inconsistencies if one might get updated but the other one slips...
We should, and I think it was actually a bug that we did not do that before.
It's not about being trivial or not, but about consistency. Code duplication has the danger to only update one copy of the code...
I am fine with consistency and clean-up, but I would like to have the clean-up in a separate PR.
You can inline the value of variable `key` here and remove `key`. ```suggestion final String fullSensorName = CLIENT_LEVEL_GROUP + SENSOR_NAME_DELIMITER + sensorName; ```
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
I stand by what i said, but I'll leave it up to you. It isn't a deal breaker for me!
I think this should be four different tests. One for each of the methods you are testing. I probably said this before, but it is much nicer if you can just read the test names to understand what should/shouldn't be happening
Regarding 1) and 2), I think it is not needed, and hence we can use the same `streams` object. 3) fair enough. 4) sounds good, keep it as is then.
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
maybe: `inputKeySerde` and `inputValSerde`
nit: ".. select the grouping key and the value to be aggregated".
records to it, and reading all records from it, such that
To be consistent, we should say "into a new instance of [a windowed] {@link KTable}".
an -> a
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Same here. We should use `builder.stream().toTable()`
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
As above, I think we should create both tables using `toTable()` operator
I am frankly not sure, if we need to test left-table-table join explicitly (the table-table join test from above should be sufficient)
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Yeah, it seems to me like we should remove it.
We can use `ApiResult.completed()`
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
nit: maybe utility methods can be moved to the bottom of the class.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
nit: seems you can use `new ArrayList<>`
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
Could store `entry.getKey()` in a local variable since it is used several times
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
Map.Entry<String, String> to avoid the check below
`instantiateConfigProviders` since this is potentially creating multiple providers
As mentioned above, we can make this constructor default access.
Yeah, builders are nice for the reasons you mention. However, since it wasn't discussed in KIP-222, I think keeping it package private for now might be better.
Curious why you made this change. Lists are a bit more convenient for users.
One issue with `Collection` as that `equals` is not defined. So, one should be careful about returning it in public APIs. It's OK to receive it as a parameter though.
+1 for fixing the api. I would prefer a traditional api (either make C'tor private and `of` as a builder pattern w/o returning Optional or provide C'tors to create objects).
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
nit: We could revert this change as it does not bring much.
nit: We could revert this change as it does not bring much and re-align like it was before.
We can use `ApiResult.completed()`
I think we put args on the same line unless the list is too big
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
Seems like a no-op
I miss @shikhar!
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
Nit. use `{ }` for all code blocks
nit: parameter/line formatting
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
nit: add `final`
We already import the class on L26, so let's remove this import.
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
nit: add `{ }`
nit: remove empty line
Ditto on removing these before/after methods.
Ditto on removing before/after
ditto on removing before/after.
I think we ditch the before/after methods as I previously recommended.
Need to check if `group` is `null` in both `k1` and `k2`. Using this on, e.g., `DistributedConfig` from Kafka Connect doesn't currently work.
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
Yes that's correct as this PR stands now. But if we put the name check back to what it was originally, then this line is not needed.
All the callers seems already handles the `name == null` case so this seems unnecessary.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
For KStream-KTable join, the passed-in storeName of the table could be null, for example: `stream.join(table1.join(table2))` in which case the returned table from the table-table join has storeName `null` as it is not materialized anywhere.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
nit: formatting -> single parameter per line (same next line)
nit: 4-space indention plus move `builder` down one line
nit: avoid unnecessary `this.` prefix
nit: line too long
This was not addressed yet. the whole sentence can be removed
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
We really need a bug fix release for this! \cc @guozhangwang
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
ditto to `KStreamImpl`
nit: 4-space indention plus move `builder` down one line
Yeah, that would be the closest for primitive types.
Good to know the subtle difference. "acknowledged" sounds good to me.
Why are we using `LinkedList`? It's very rare where it should be used.
@dajac Won't this create a list regardless of whether there are timed out entries? Let's see whether this addresses @ijuma 's concern on the other PR.
I don't have a better suggestion, but targetTopicPartitionsUpstream is a kinda confusing name.
`seems existing already but it doesn't` -- this might be confusion. What about: ``` Could not create topic {}. Topic is properly marked for deletion (number of partitions is unknown). Will retry to create this topic in {} ms (to let broker finish async delete operation first). Error message was: {} ```
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
I think it would read better if we remove `config` from the sentence.
Nit: `.` full stop missing.
We should explain why the key ("temp") is hard-coded here.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
nit: {@link KGroupedTable}
It seems that we haven't added the topicId yet.
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception.
It seems that we are logging at the debug level. I am wondering if we should log at WARN as before in ZK based appoach. ``` if (exception instanceof ApiException) { log.debug("{}: failed with {} in {} us", name, exception.getClass().getSimpleName(), deltaUs); return exception; } ```
In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.
Are we missing the ApiException here? I.e. error returned from servers that are not recoverable.
Nit: missing articles & punctuation in this description. "not available in buffer" -> "not available in the buffer". "available currently in buffer else return empty" -> "available currently in the buffer, else return empty"
What is the offset commit is positive and invalid? cc @hachikuji
I'm happy for this to be merged if @hachikuji is happy fwiw.
You mean if the offset is out of range? I'm not sure we have a good way to check this at the moment. It can't be done on the coordinator because we don't know what the valid offsets are for each topic partition, so that leaves the client where the check may end up stale anyway. By the way, there are a couple `commitSync` overloads that may need to be updated as well.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
Ditto above. I would recommend having consistent explanations here.
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
We don't need this for files, right? Just for directories (because of `file.deleteOnExit`)
We should update the Scala `TestUtils` to call this method.
This change is not needed.
Some of these lines have extra 4 space. Other lines have extra 8 spaces.
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
As above: add more "randomness"
Can't you use `createClient`? This call confused me... (same below)
Thanks for clarification. Does make sense now -- I did not consider the "task pairs" heuristic.
`hasItem(task01)` -> `equalTo(Collections.singleton(task01))`
I believe this is fixed in my next PR.
Do we need to set OP_READ? It seems it's always on.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
Harsha address this, I believe.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
should we not just add 10 records in a row (offsets `0...9`) and simulate commit marker at offset 10 via setting endOffset to 11 ? This setup would indicate, that the record at offset 10 is either a non-transactional message (we could have mixed writes) or it's in pending state (neither committed not aborted) and thus endOffset could not be 11L as "last stable offset" is 9 or 10 and endoffset must be smaller than "last stable offset")
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
nit: maybe just move into the line above? same above and below.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Sorry, missed this earlier: We are creating a new `selector` in `checkAuthentiationFailed`, so we should ensure that the previous selector is closed. You could call `selector.close()` just before calling `checkAuthenticationFailed` here and also a couple of lines below.
Sounds good. I just want to avoid someone trying to simplify the tests in the future without understanding that this test is verifying both features work together.
The "Swallowing the exception" is a bit alarming at first. It's true that if this method returns false then `TestUtils.waitForCondition` (which is using this method as the test condition) will fail at line 137. Pretty minor, but how about the following? ``` // Log the exception and return that the partitions were not assigned log.error("Could not check connector state info.", e); return false; ```
extract to variable
do we need this invalid config step here
```suggestion waitForCondition( this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each." ); ```
typo `direcctly` -> directly
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
I don't think we really need this function any more... we can just submit to the executor from the other function.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
`while` seems to be missing
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
Seems this duplicates `L733`. Might be good to extract into a small helper method.
It's a bit better if you move this inside the `try` and remove the `return` from the catch.
I think we're testing `testDir` Occupied here, not `AppDir`.
Is this line intentional? Unit tests normally don't need to print out to console.
Nitpick: I'd call this `getOrCreateFileChannel`.
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
This should probably just return a boolean
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
A docstring for this method would be good :)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Nit: let's avoid unrelated line additions.
adding `final` in every argument here was just noise but with not much added value. Given that we don't write constructors like this in Connect I think we should revert and add the new argument without the `final` specifier.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
req: this member should be removed.
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
Nevermind, I see that's the pattern we follow everywhere else
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
@mjsax Got it. Thanks for your response!
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.
Hmm, why do we need to remove for -1 broker? It doesn't seem that brokersToIsrs tracks that.
The test case `BrokersToIsrsTest.testNoLeader` suggests that it is a possible case. It looks like the path through `ReplicationControlManager.handleNodeDeactivated` could result in a `PartitionChangeRecord` which has leaderId set to -1.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
If these aren't expected to change at runtime, I'd suggest initializing the `HashMap` in the static block as a temporary variable , then wrapping it to create an unmodifiable map which is then used for the constant here: ```java Map<String, File> pluginJars = new HashMap<>(); try { pluginJars.put(..., ...); // More of the same } finally { PLUGIN_JARS = Collections.unmodifiableMap(pluginJars); } ```
Thanks. Not a blocker.
Can we just add https://checkstyle.sourceforge.io/config_sizes.html#LineLength so we never have to have this discussion again? :)
Are the `storeType` supposed to have the dash already? If not this should be `storeType + "-" + STORE_ID_TAG`.
In `threadLevelTagMap(String...)` there is a check: ``` if (tags != null) { if ((tags.length % 2) != 0) { throw new IllegalArgumentException("Tags needs to be specified in key-value pairs"); } ``` Should we do the same here
this is the same as above: we should extract to a method to avoid inconsistencies if one might get updated but the other one slips...
Maybe get the value of this expression on the line above. IMHO it will make it easier to see what the `assertThat` is doing.
Seems this duplicates `L733`. Might be good to extract into a small helper method.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
IMO, `createStreamThread()` would describe the behavior better.
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Should we be checking for null here? It's probably OK as this is only called from `abortIncompleteBatches`, but I thought I'd ask.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
That would work for me, but I'm OK with leaving as is as well.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
isFull is no longer used.
Right, so we should just reword Each connector gets their own dead letter queue topic to be more clear, so something like Each connectors dead letter queue is usually written to a different topic. Not super important, but Im just looking to eliminate potential usability issues.
Nit: reword to avoid "log" being ambiguous as verb or noun: "Writes errors and their context to application logs."
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
These can be final.
AK convention is to not use `set` setters or `get` getters.
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
nit: might be helpful adding a little helper since we do this a few times in here
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
@becketqin is right-- you should handle this case. Perhaps the server sent back bad data. The way to handle it is not to throw an exception, but to complete the relevant future(s) with an error. There are a few other cases where we handle bad server data by completing a future with failure in AdminClient.
If we do not expect this to happen. Shouldn't we throwI IllegalStateException? In this case, if the broker returned a replica that is not in the request, the broker may have somehow misplaced a replica. We should probably alert in this case.
nit: extra newline here
Is this necessary? The leader epoch is -1 by default.
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
I would call this one `topics()` as you did already in the request.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
Typo: should be "or larger than the number of available brokers"
Please include TopicDeletionDisabledException here.
We do not throw `InvalidTopicException` "if [the topic] is not found"
This exception can't be thrown by DeleteTopics.
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
Also, are we fine with the config logging being bumped up to `info` (which is what `logAll` does) vs `debug` (which is what it was here).
This could be `ConfigException` instead. With the `ConnectException` hierarchy we get a little bit stuck because we can't have `ConnectException` be the base of all our exceptions and not redefine some. There are some from core Kafka that are fine if triggered from within the framework, and `ConfigException` falls under that category -- the biggest value aside from standardization that asking for `ConnectException`/`RetriableException` from connectors/tasks is that it is a signal for how to handle the exception, but the runtime code doesn't require that everything inherits from those (and in fact we handle other exceptions from connectors as well, we just would prefer that the connector think about how it wants the framework to handle possible errors).
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
```suggestion Arrays.setAll(topics, i -> topics[i].trim()); ```
do we really need to process this in parallel? seems like `setAll()` should be simple and clean enough.
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
nit: `testAggregateRandomInput` to match up with other test names
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
nit: extra spaces after the `->`
```suggestion public void shouldDropWindowsOutsideOfRetention() { ```
Nit: Maybe `Use {@link #localThreadsMetadata()} to retrieve runtime information.`
We should mention it gets deprecated by two functions: 1) to get the static full topology description, one should use `Topology#describe()`; 2) to get the runtime thread metadata, one should use `#localThreadsMetadata()`;
I meant to have all "singulars" for consistency, i.e * Creates a * Starts the * Shuts down this * Does a clean up I'm OK with imperative style.
I see. I do not have a strong preference actually. But I remember we use singulars not plurals in many other classes and just wanted to be consistent. If it is actually the opposite case I'm happy to have them all to be plural.
I see, the assumption is that existing implementations will have overriden it.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Consider naming the topic "topic2" since there are only two topics in the test
nit: line too long
This should be new `ValueMapperWithKey`
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
Yeah. Even if it's "compatible" and does not break anything, it's still fall into the "public api change" category...
This seems to be a public API change that we cannot do without a KIP. Seem you added it so you can pass the different suppliers into `checkSupplier` ? Also not sure if `checkSupplier` must be as "complicated" as proposed.
either move `this` to next line, or fix indention.
"or null if this was not redirected"
What is the value in separating the `set` and `done` methods? The one place where I can see that they are separated by some other code it looks like moving the `set` next to the `done` doesn't affect the tests at all.
Thanks! Will push this shortly.
nit: 'else' can be dropped
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
Yeah it makes sense. Sorry for not getting back to you sooner
It seems relevant to the implementer whether the configs have been validated or not. If they're not guaranteed by the contract to have been validated then the implementer might need to duplicate some of the validation logic in this method.
Nit: using a new paragraph makes this stand out more. ```suggestion * <p>For backwards compatibility, the default implementation will return {@code null}, but connector developers are ```
"Connector authors" might be less ambiguous than "Developers".
WDYT about something like this: ``` /** * Signals whether the connector implementation is capable of defining the transaction boundaries for a * connector with the given configuration. This method is called before {@link #start(Map)}, only when the * runtime supports exactly-once and the connector configuration includes {@code transaction.boundary=connector}. * * <p>This method need not be implemented if the connector implementation does not support definiting * transaction boundaries. * * @param connectorConfig the configuration that will be used for the connector * @return {@link ConnectorTransactionBoundaries.SUPPORTED} if the connector will define its own transaction boundaries, * or {@link ConnectorTransactionBoundaries.UNSUPPORTED} otherwise. * @see TransactionContext */ ```
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
Nit: long line.
ditto for the rest of the test
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Missing a space after the comma, which is causing checkstyle to fail.
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
nit: `e` -> `fatal`
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
I think the name of the function is better defined as `interleaveTasksByConsumers`
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
My bad, mis read the lines :)
Is it ever possible that the `clientState.ownedPartitions().get(partition)` is not null but `allOwnedPartitions.contains(partition)` is false? It seems the second condition is redundant (and hence the `allOwnedPartitions` parameter throughout the `assign` function seems not needed).
nit: again this is not introduced in this PR, but let's use `AssignorError.NONE` here and elsewhere to be less vulnerable to enum changes.
By catching Throwable, aren't we silencing the error and not immediately failing the test? Also we should not be catching Throwable, if the JVM throws an Error we want it to fail the test and never want to handle it. There're a bunch of them in these files
Just let the Exception flow, that will automatically fail the test
What do we do if there's an exception? If it's expected, let's make it clear
Could the cluster aliases be constant as these are used all over the place
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
Yes, that is the reason for the inconsistency. Obviously it was a mistake to let `OffsetAndMetadata` be serializable in the first place.
nit: I think parenthesis are a little more conventional.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
I have a small preference for `Future<Map<Integer, Future>>` because it seems more aligned to how we do it for other APIs but I don't feel strong about it.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
What you had is fine.
Sorry for not catching this earlier, shall we try to be consistent when it comes to `toString` output? Here's an example of a recent Java class: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClientResponse.java#L67 What do you think? Also, it would be good to include an example of the logging output.
There's no benefit in using `StringBuilder` for something like this. Using `+` will have the same effect (the case where `StringBuilder` helps is when there's a loop.
This doesn't seem to be used.
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Ditto. Not using index.
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
nit: `final` is redundant for private static method
Typo, should be: "an ever-updating" counting table"
Typo: "new instance of [an] ever-updating KTable" Also, we do we say "ever-updating"? I'd remove it. Also, in other parts of the docs we do not mention "ever-updating" when we talk about KTable (see e.g. the `aggregateByKey` variant right above this one).
Ah, I see the difference: I suppose "ever-updating" refers to this method returning a _non-windowed_ KTable? If that's right, then we should perhaps clarify this a bit in the docstring (it's easy to miss -- these methods all have similar names).
"of [a] windowed..."
"of [an] ever-updating ..."
nit: use `assertThat` instead
nit: I would remove this line
nit: This last check is not needed, since it verifies functionality of the `Map` returned by `Collections.unmodifiableMap()` and not of the code under test.
specify generic -> `MockProducer<byte[], byte[]>`
Hmm.. I'm wondering how did we succeed in this test case, since in the above code `send()` call is only captured with `TimeoutException`? Note that we only set the KafkaException in the callback while here we throw exception directly. And in fact, you changed the expected exception from StreamsException to KafkaException in line 128 above.
We can remove this field now that it's unused
It seems ```exitProcedure``` and ```haltProcedure``` can be local variable
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
This is unused too
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
nit: move this `if` statement below/above version check on line 62
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
We can remove the code block line 82-85 above since it will be called here.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Could use `equalsIgnoreCase` directly.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
nit: seems these could be final? Same in `ConsumerUserData`.
Let's use `Map` on the left side instead of `HashMap`
Similar here, we can cache the result in case to be reused.
I hope we can get rid of those conversion in the future :)
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Yeah with KIP-429, during the rebalance some tasks would still be processed and hence records being sent out; so the resuming tasks could be in RUNNING state as well.
I think you need this line before line 32 to have the imports lexicographically sorted.
req: this member should be removed.
We can use `List<Class<? extends Connector>` to avoid the warning
This can go with the other `org.junit` imports below
This needs to be updated with the new constructor that accepts two arguments.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: Empty line could be removed.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: preserve empty line after `checkAndClearProcessResult`
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Could use `equalsIgnoreCase` directly.
Nah, I think we should actually keep this (although `IllegalStateException` seems to make more sense, can we change it?) -- we should just make sure we don't reach it
+1, we can rely on `storeManager#getStore` inside `StateManagerUtil` to check if the store is already registered.
Re: your concern, I don't think we can assume that a user's state store's `init` method is idempotent. AFAIK nothing should change that's relevant to the state store registration, but if something does (eg TaskCorrupted) we'd have to wipe out everything and start it all again anyways
Why adding a suffix? Is there any problem if the topic name is equal to the store name? This breaks ktable.
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Typo: "you can create [a] windowed ..."
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
`windowSize` should be `Duration`
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
nit: remove empty line
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
this doesn't seem to be used
Can we unify the `dataLength` computation into a single place? It's a little bit scattered at hard to follow. Also with different versions, I like a more explicit pattern using switch: ``` switch (version) { case 0: encodeV1(); break; case 1: encodeV2(); break; default: throw... } ```
nit: maybe use `finally` to restore this value.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
Hmm.. The function is aimed to be a public API in `Topology` in KIP-120 (which I think will be renamed from the current `TopologyBuilder`?), but here it is only used as the function of the `InternalTopologyBuilder` which seems incorrect.
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
nit: missing space between logPrefix and 'found'
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
prop: abortTransaction can also throw ProducerFenced.
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
I would like to propose the following change to take care of the source consumer group changes ```suggestion for (Map.Entry<TopicPartition, OffsetAndMetadata> convertedEntry : convertedUpstreamOffset.entrySet()) { TopicPartition topicPartition = convertedEntry.getKey(); for (Entry<TopicPartition, OffsetAndMetadata> idleEntry : group.getValue()) { if (idleEntry.getKey() == topicPartition) { long latestDownstreamOffset = idleEntry.getValue().offset(); // if translated offset from upstream is smaller than the current consumer offset // in the target, skip updating the offset for that partition long convertedOffset = convertedUpstreamOffset.get(topicPartition).offset(); if (latestDownstreamOffset >= convertedOffset) { log.trace("latestDownstreamOffset {} is larger than convertedUpstreamOffset {} for " + "TopicPartition {}", latestDownstreamOffset, convertedOffset, topicPartition); continue; } } } offsetToSync.put(convertedEntry.getKey(), convertedUpstreamOffset.get(topicPartition)); ```
Same here, for verifying the thrown cause
We should also verify the thrown cause
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
nit: is there a way to verify a function is never called? Like `consumer.commit()`
nit: we could use mkMap helper here as well.
Actually I think #2456 does not update the printed html for marking these as deprecated, so we still need to rebase + merge this PR after that one is merged.
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
`group` could be `null`
need a newline after the group before the underline, and a double newline after the underline
Should be two tests instead? (Leave it up to you to decide -- don't insist on a change)
nit: add `final` to parameters
Consider naming the topic "topic2" since there are only two topics in the test
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
nit: why not `k2` ? Should we use `A`, `B`, `C`, `D` for the values to make it easier to understand the expected result? It's unclear which A is which below.
Ok, let's just keep it in our back pocket for now.
We could detect if the processorTopology contains only `StaticTopicNameExtractors` and still throw in that case if the topic name isn't in the topology.
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
Your understanding is correct @mjsax .
nit: `final` params
Hmm, maybe even better: ``` java long startWaitNs = time.nanoseconds(); long timeNs; boolean waitingTimeElapsed; try { waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS); } catch (InterruptedException e) { this.waiters.remove(moreMemory); throw e; } finally { long endWaitNs = time.nanoseconds(); timeNs = Math.max(0L, endWaitNs - startWaitNs); this.waitTime.record(timeNs, time.milliseconds()); } ```
The name `restoreAvailableMemoryOnFailure` is a bit weird because we should always restore available memory on failure. Maybe we can name it `hasError` and set it to `false` right before `return buffer`.
We can probably remove this line `restoreAvailableMemoryOnFailure = false`.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
req: This is unnecessary
It's better to keep the parameters aligned (having same indentation)
We may as well use the more specific type `TimeoutException` given the name of the method.
I'm wondering if it still makes sense to consider metadata timeout and request timeout to determine the timeout for `selector.poll`, but maybe it can be discussed in another PR..
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
nit - "once once"
We probably want all these `Long`s to be `long`s.
Ah, you're right. I misread the second check.
Not really sure this has value if the test case expects the leader change correctly.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
This could probably also be an exception since this shouldn't be called with invalid target states.
Shouldn't this result in a change to a failed status? I think we'd either need to do that here (if thread safe) or have some way to communicate it to the original thread? We can defer this to another patch if we already weren't handling this case properly.
Nit: If we do fix up the above example of this it makes sense to fix this up too.
Personally I'd prefer to have decodeTasksData in which we hard-code the logic of doing both prevTasks and standbyTasks, we do not code-share for these two task sets but we share code of constructing the set for version two and version three. I guess we cannot get both code sharing, and since it is really a nit I'm fine either way :)
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
While you are there, can we fix this one? it is " Running at : " + miniKdc.getHost() + ":" + - miniKdc.getPort()
should we put `kafka` and `numThread` directly into `props` and reduce number of parameters here (would also simplify all the "passing through" code.
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
Yeah, we're still feeling out the best patterns for handling older versions.
I would say it's important _not_ to be able to create bogus requests. ;) We can introduce specific mechanisms for testing, but a public constructor for a request should do its own validation.
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
This doesn't seem to be used.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
We can compute this once and pass it to `removeAttribute`.
Let's also verify that `containsMBean` returns `true` before the removal. Also, it wouldn't hurt to check that the second `removeMetric` call removes it from `JmxReporter`.
'valueInZK' is poor name here - it's the name of the wildcard-suffixed pattern. 'intput' is actually a concrete resource name. `valueInZK` won't have the '*' suffix, (as discussed on the thread). - update the java doc accordingly too. Personally, I see a lot of scope for this being called in code where it shouldn't, (as it currently is). A more defensive API might be for the signature to be: ``` public static boolean matchWildcardSuffixed(Resource wildcard, Resource concrete) public static boolean matchWildcardSuffixed(ResourceFilter wildcard, Resource concrete) ``` And then to have the method throw if the wrong types of `wildcard` and `concrete` are passed in, i.e. `wildcard` should always have type `WILDCARD_SUFFIXED` and `concrete` should always to `LITERAL`
Hey @junrao, It was discussed on the dev channel that we shouldn't store or pass around the '*' the end as this then requires places to validate the resource name ends in a '*' on API calls and when loading from ZK. Also, with the rebrand of this from 'wildcard-suffixed' to simply 'prefixed' then I think we can drop the '*' completely. e.g. the user would add an ACL to any topic resource the has the prefix 'foo'. Look mum, no asterisks! This also helps separate this from the current 'wildcard' support i.e. '*'.
Ok. Then the KIP wiki needs to be updated.
Each unit test should test one thing, so break this into separate tests
Do you need this `int` cast? I thought I removed that in my branch and it worked fine.
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
Fair enough, let's just leave it as is then. Thanks for the explanation.
I used `equalToObject()` because it makes the intent more explicit.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
I think, we should use three different values to make sure that the different prefixes overwrite the configs for the corresponding clients. Looking into the test below, they seem to be redundant with this one? We can also remove this test and keep the other three (that would avoid redundancy, too)
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
> Unfortunately I don't think we can shutdown a thread until we have started it. Have a look at https://github.com/apache/kafka/blob/aeeb7b2f9a9abe8f49543a2278757722e5974cb3/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L976-L983
nit: could be useful to log the type of exception in the assertion message.
nit: remove empty line
nit: add `final` (2x)
nit: `child` -> `toChild`
nit: remove -- not used
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
Nice catch. And since we're here, why not make these `private`, too? They're currently not used outside of this class.
Nice tidy up of this test class :-)
Fair enough. Let's leave it as-is.
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
nit: Could we just call the above constructor here and avoid duplicating assignment logic? `this(startMs, durationMs, null)`
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
nit: remove `this` (not required)
Update return type to `L` (if we introduce `L`)
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
nit: remove (was tested already)
At least we try to not abuse Thread.sleep() ;-)
I am confused. If we change `store.range(hello, "zooom");` should `hasNext()` not return `true` (start and end are inclusive). Thus, to me it seems the test is wrong and there must be a bug in the iterator code.
I know this is just how the other tests are doing it, but it's not really an airtight way to validate the expected results...if nothing is returned then we never enter the `while` loop and the test passes, even if we did in fact expect there to be actual output. The important thing here was just to make sure it didn't throw an exception so it still does that, but it would be good to fix this up maybe in a followup PR
check `source != null` not necessary. In doubt add an assertion.
I'd also consider extracting: `source.getTimestampExtractor() != null ? ...` into a local as the line is quite long and it will make the code a bit easier to read.
add line: `TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();` and change to `RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
It would be nice if you made these `final` while you are doing this change.
key -> topic
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
Do we need to set OP_READ? It seems it's always on.
Harsha address this, I believe.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: allBuffered &= newInputPartitions.isEmpty();
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
I think we want to make the string computation lazy, but not lose info. We could do that by overriding `toString`.
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
Yeah I know that :) I was referring to the one with `SUM` not `COUNT` too but I copied the wrong name. We already have a public `addRateOfSumMetricToSensor` which is only used for rocksDB today, and I was wondering if we can leverage that.
nit ```suggestion expectedSensor, StreamsMetricsImpl.CACHE_LEVEL_GROUP, tagMap, hitRatio, HIT_RATIO_AVG_DESCRIPTION, HIT_RATIO_MIN_DESCRIPTION, HIT_RATIO_MAX_DESCRIPTION ); ```
I used `equalToObject()` because it makes the intent more explicit.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
It was removed from the other versions of `group` but not from here.
Oh, I just noticed. Then `synchronized` is not needed anymore.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
We really need a bug fix release for this! \cc @guozhangwang
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
nit: "active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
@ijuma Sorry, I don't know of a standard way of doing this,
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This is not necessary, since the for loop below would be a no-op.
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
```suggestion * A byte array comparator based on lexicographic ordering. ```
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I could not find any unit tests for this method.
since this wrapper just exposes a new constructor, consider making it a factory method instead
Another name might be `seek()`.
Maybe we should add one case where `position > 0`.
It would be good to verify that the buffer contents are correct as well.
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
Is this name correct? It seems like the buffer is never filled in this test.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
nit: remove empty line
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
ditto on (what I think is) the impossibility of this condition being false.
I think we ditch the before/after methods as I previously recommended.
We just need to make sure the extracted generation from all three processors are the same.
is it important to have 3 brokers for this test? I'm wondering if the tests would be more resilient and faster with just one broker and replica of each topic.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
Ok. Thanks for clarifying.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Thanks for clarifying!
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Shouldn't you verify if topology 1 still produces output records at this point? When I read the test name I would expect that verification here.
Nice coverage with different num.partitions, thanks!
nit: insert space `String... expected`
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks @vvcephei -- that is convincing.
nit: Could just to `new ArrayList<>();`
Yeah might as well change it I think, it results in shorter code
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
No need to pass the spec in the constructor here, as all the connections have access to the internal spec.
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
The other constructor calls the parameter `sampledStat`. We should be consistent.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: use the `SecurityManager` interface
Do we need to check if it is null here? I think it is probably ok if it doesn't throw any exceptions? Obviously it would be better if we could check that `loginManger.release()` was only called on the first invocation, but i appreciate that involves further refactoring
We should use `aasertThrows` and verify the error message similar to the `TransformerSupplier` test
as above -- guess some more below
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
Also set the store name in this test.
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
Ah, I see. Thanks for the explanation
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
Actually maybe we should wrap it in an `if hasPersistentStores` so that users won't get this warning if they don't have any persistent state
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
nit: remove `this` (not required)
Update return type to `L` (if we introduce `L`)
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Would it be simpler to check channel.isMuted() instead of channel.isInMutableState()? Then, the latter can be a private method in KafkaChannel.
Selector is shared between client and server. So, it's better not to mention server here.
It's just more proactive, right? If you're out of memory, you're not going to be able to do anything (beyond the handshake) on any channel anyway. I think the value is that instead of going through a more polling unnecessarily only to end up muting all the channels, you can just do so immediately.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
This can be static
nit: What about extracting the construction in a small helper method `prepareDescribeLogDirsResponse` that create a response for one LogDir and TopicPartition? It seems that the same block of code is used in many tests.
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say `assertDescriptions`, that verifies a descriptions map contains the information about a single log dir/topic partition? Something like `assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture)`.
This can be static
i think we should just stick with `joiner` for the name of this param. here and elsewhere
super nit: move method params up one line to start after `join(`
super nit: ditto from above
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: avoid inserting random blank lines.
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
nit: extra spaces after the `->`
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
Oh right, forgot that it doesn't have the window times either. Nevermind then
I'm not sure of the original motivation behind setting it to `50`. But if we are going to define it as `1000` i think it might be better to remove this and `reconnect_backoff_min_ms_config` definitations from here as they are the same as what is set in the `ProducerConfig` and `ConsumerConfig`, so it seem pretty pointless overriding them
I think I would say the second sentence as: "This avoids repeatedly sending requests in a tight loop under some failure scenarios." As it's phrased, it could be interpreted as meaning that we would repeatedly fail the same request.
spelling -> recrord -> record
It's better to keep the parameters aligned (having same indentation)
I would probably name this `reconnect.backoff.max.ms` to keep things consistent and to make the units explicit.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
nit: extra line
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
I think we'd want to override `parseResponse` for `API_VERSIONS` only.
That alternative looks good to me :)
> it doesn't seem particularly beneficial It seems to me the benefit is the error happens from runtime/test_runtime to build time (generate message code).
It seems to me this check should be moved to ```ApiMessageTypeGenerator``` as it is a generated code from json.
This doesn't seem to be used.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
Nit: add `final`
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
Do we need to change these signatures from `ConnectorTaskId` to `String`? The `ConnectorTaskId` gives us the ability to define tasks-specific client configuration properties if necessary/desired. I'm afraid that switching to `String` will make it harder and more invasive to add that back in. Plus, if there's not a good reason to remove these, let's leave that for smaller PRs.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
the type of pollTimeMs is *Duration*. It seems to me that the "ms" is a bit redundant.
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
Cool, yeah that addresses my concern 
Might be simpler to use the mock deserializer only for values.
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
This could throw a NPE. I think we should guard against this, and throw `ParseException` if NPE happens `ts.split("T")` is redundant and should be extracted into a variable.
nit: add `final`
Should we guard against NPE? (same blow)
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
nit: this can be final
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
I suggest passing all of the variables we access in this constructor via`SustainedConnectionWorker.this.spec` to be switched to parameters we pass upon instantiation
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
Actually, WDYT about adding this class in the "add configs" PR and then rebasing this PR on top of that? Then I could do the same (since I need this class in my next PR as well)
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
```suggestion for (final Map.Entry<TaskId, SortedSet<ClientIdAndLag<ID>>> taskToRankedClient : statefulTasksToRankedClients.entrySet()) { ``` Just to resolve a warning about referencing the subclass instead of the interface.
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
Hmm, this sounds to me that the StreamProducer's own `partitionsFor` did not return the num.partitions so we ended up calling `send` with `partition == null`, since otherwise we will get the `partition` as ``` partition = partitioner.partition(topic, key, value, partitions.size()); ``` where `partitioner` is the `StreamsPartitioner` and the producer's own partitioner should not be used.
Thanks for the explanation. Make sense.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
By the way, I wonder if we should just say it should be idempotent? Seems redundant to mention KafkaProducer.
nit: add `final
nit: add `final
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
prop: Use `assertThat()` here and in the other overload of this method.
nit: java style
Like I said, not a big deal and I'll commit as is. But by using entrySet, I didn't mean not to use key at all, just that something like: ``` for (Map.Entry<String, String> entry : configKeys.entrySet()) { String configName = entry.getKey(); Type type = entry.getValue(); ``` would avoid doing the extra lookup and would still let you use the key directly as needed.
I see, I guess `type` in `ConfigDef.convertToString` can be `null` due to this call, but why would we even use `ConfigDef.convertToString` in that case? It seems weird to use something from `ConfigDef` to do conversions when we know there isn't a type or `ConfigKey` associated with the field.
I believe that including the name of the property in the error message is redundant as that information will be available already in the REST response. I also think we may want to be clearer about the error message here. Users can't supply null values, but developers (by specifying `null` as the default value for a property in a `ConfigDef`, for example) definitely can, and we may want to make it clear which variety we're prohibiting. What do you think about this? ```suggestion .map(configEntry -> new ConfigValueInfo(configEntry.getKey(), "The JSON literal `null` may not be used in connector configurations")) ```
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
We also don't want to accept null addresses, right? I was thinking we could do this: ```java if (address == null || address.equals(NON_ROUTABLE_ADDRESS)) { throw new IllegalArgumentException("Invalid address " + address); } ```
I think we probably want to include both `host` and `url`. Maybe something like: ``` java log.warn("Removing server {} from {} as DNS resolution failed for {}", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host) ```
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
The point is that `getPort(url)` is outside the `try` block.
Note that the `getPort(url)` call a few lines above can fail with a `NumberFormatException`, so we probably need to put that inside the `try`.
Why did you remove the `return`? Without the `return`, the code will reach the `finally` block and log that the assignment took place.
Why is this an ERROR? If we receive a SHUTDOWN signal, it's just an "eager exit" to not create tasks IMHO. We might want to log a DEBUG though. Would also update the message: ``` log.debug("Skipping task creation in rebalance because we are already in shutdown phase."); ```
```suggestion log.debug("Skipping task creation in rebalance because we are already in {} state.", ``` (minor suggestion)
Shouldn't the indentation be as follows? ``` log.debug( "Skipping task creation in rebalance because we are already in {} state.", streamThread.state() ); ```
Shouldn't the indentation be as follows? ``` log.debug( "Encountered assignment error during partition assignment: {}. Will skip the task initialization", streamThread.assignmentErrorCode ); ``` The first parameter is one or two characters too long, though.
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
Should be larger
Typo: should be "or larger than the number of available brokers"
What is the offset commit is positive and invalid? cc @hachikuji
I'm happy for this to be merged if @hachikuji is happy fwiw.
You mean if the offset is out of range? I'm not sure we have a good way to check this at the moment. It can't be done on the coordinator because we don't know what the valid offsets are for each topic partition, so that leaves the client where the check may end up stale anyway. By the way, there are a couple `commitSync` overloads that may need to be updated as well.
Missing newline character.
Hmm. I think we're just following the same pattern used elsewhere, but I think the version check is redundant. We already know it's a valid version because the request constructor verifies it.
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
nit: Indicate that this needs shallow iterations on the entries.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
Might be simpler to use the mock deserializer only for values.
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this: ```java try { producer.beginTransaction() producer.send(...) producer.sendOffsetsToTransaction(...) producer.commitTransaction() } catch (Exception ) { producer.abortTransaction() } ```
nit: 'else' can be dropped
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
It is probably cleaner to have an explicit `EXPIRED` state.
`error` is unused
Relatedly, I think there might be some sort of checks in unit tests in maybe the producer or consumer that validate metrics are unregistered, might be able to use a similar approach here.
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
Hmm, good question. I may have actually been wrong about which values should be involved. I think @gwenshap and I had a long discussion about this awhile ago too and there are many ways you could define lag. I think the real problem here is that we may not be exposing enough information from the consumer to compute what I would really think of as lag -- FetchRequests include high watermark info so you know how many records are in the log but not yet returned to you, and the consumer creates metrics based on that. But we don't have access to that info. A connector that commits on every message would look like it has 0 lag, but it could be very far behind in the topic.
@mjsax Got it. Thanks for your response!
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
req: This is unnecessary
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
@ijuma Sorry, I don't know of a standard way of doing this,
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
line is too long
Nit: add `{}` to block
This is deprecated by scala (scala 2.12, not 0.12 :)). Here's the note from scala doc in 2.12.0, to explain why it is deprecated: > The transparent conversions provided here are considered fragile because they can result in unexpected behavior and performance. > Therefore, this API has been deprecated and JavaConverters should be used instead. I think that time, `CollectionConverters` haven't existed, so suggest to use `JavaConverters` instead. REF: https://www.scala-lang.org/api/2.12.0/scala/collection/JavaConversions$.html
Sorry, I was wrong. Have a 2nd look, and found `CollectionConverters` doesn't exist in scala 2.12. So, this change will not be compatible to scala 2.12. This change should be reverted, or maybe use `JavaConverters`? Thanks.
nit: typo in description
nit: `Arrays.asList` a bit more concise.
We can use `TestUtils.assertFutureThrows()` here too
nit: We could use `TestUtils.assertFutureThrows` here.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
This needs to be updated with the new constructor that accepts two arguments.
Passing in the current listener seems a little weird. I wonder if we're trying a little too hard to reuse the subscribeTopics method in SubscriptionState. If instead we had a method like SubscriptionState.subscribeMatchingTopic (or something like that), then we wouldn't need the flag and we wouldn't need to pass a listener. You could change this line to this: ``` subscription.subscribeMatchingTopics(topicsToSubscribe); metadata.setTopics(topicsToSubscribe) ```
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
nit: ```suggestion = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(new DBOptions(), new ColumnFamilyOptions()); ```
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
Can you elaborate? What do you mean by > otherwise the state won't proceed
Testing against log message is error-prone and hard to maintain, I think just making sure the thrown exception type is expected should be sufficient.
ditto here, should be moved to RocksDBStore#close
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
previous message => previous receive
Not sure about this. `SslTransportLayer#hasBytesBuffered` returns true if there is any data in `netReadBuffer`. If more data is needed to unwrap and no data arrives from the client, I think the handling of `keysWithBytesBuffered` results in a tight polling loop with timeout=0.
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
Any idea why these processors changed order? It could indicate a deeper problem.
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
Do we need this extra variable? It seems like `defaultValueBytes` is not used
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
Should this be simply `builder`? The current name sounds like it builds the definition.
Also we can just pass in the `StringBuilder` as an argument rather than create a new one here
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
Yeah if it exists elsewhere let's just leave it as is for now.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
That's correct. The implementation becomes a bit tricky, as we can't just use `Arrays.asList` and be done.
To a client of api its preferable if the contract is visible from the api signature. If we want this restriction, I think better to make it visible in the signature.
nit: Update the unit test to use this new method. Jose changed them to use C'tor.
Does this need to be public? Making it private forces use of `of` method, which I think is good.
+1 for fixing the api. I would prefer a traditional api (either make C'tor private and `of` as a builder pattern w/o returning Optional or provide C'tors to create objects).
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
You only need to crate that instance once, right? It can be a member of the class
`PrintForEachAction<>` to remove warning. Also in the `print` method
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
remove this line -- not required.
Nit: can be `final`
Nit: can be `final`
Nit: both parameters can be `final`
Nit: I think `"not-null"` might be confusion. I think a better naming would be `"null-encoding-that-is-not-just-'null'"`
Nit: can be `final`
super nit: extra blank line
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
Update return type to `L` (if we introduce `L`)
Same for all such calls.
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
`TimeWindows .of` is deprecated, right? Do we need to test old API? If yes, I think we need a `@SuppressWarning` to make the build pass
Same as above: need to check `clientResponse.hasResponse()`
`ClientRequest.destination` is the broker id.
Fair enough. Let's leave it as-is.
could use `assertFalse`
Could just use `false`
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
(especially given that below you use the simple name)
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
This may be useful, since the log messages in TransformationChain do not print the schemas in `ConnectRecord`
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
@rajinisivaram The login callback handler class isn't getting its #configure(Map<String, ?>, String, List<AppConfigurationEntry>) invoked. Perhaps it might be better to treat the login callback handler class the same way the client and server callback handler classes are treated, which is to create/configure them in SaslChannelBuilder? Note that the login callback handler class is potentially used both on the client side **and** on the server side (it is used on the broker when the mechanism is the inter-broker protocol).
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
Hah, I got the last word! Just kidding, fwiw I'm not trying to block this PR on the matter so it's fine by me if you merge as-is. Only wanted to make sure we're being fair to our assignor friends :P
Fine with me (although it does slightly detract from the opt-out possibility). WDYT about adding a retry backoff though? I'm a bit concerned we might just end up stuck in a loop of useless rebalancing, and waiting the full `probing.rebalance.interval` doesn't feel right either
Probably a better alternative to the marker interface is just an abstract assignor class that adds the lag info to the `ClientState` which the HATA and potentially future custom assignors could implement
I see your point. What I do not like so much is that it is not very intuitive to require successful lag computation for sticky assignor. I understand that if lag computation is not successful other parts of Streams will fail, but it is not the responsibility of this class to avoid that. I think what I am trying to say is that the verifications should be done where they are required to make the code easily comprehensible. I am just imagining me coming back to this code and trying to understand why the lag computation must be successful for the sticky assignor.
I buy the argument that it's weird (aka bad) to supply an input that may be invalid just because we know that particular implementation doesn't use the input. Of course, we don't actually input the lags directly; we just happen to supplement the `ClientState` with the computed lags _in some cases_, and the HATA just happens to use that. It seems only slightly less weird to _not_ invoke something because we haven't added some supplemental information that isn't even required in the first place. What if we add a marker interface (warning: terrible placeholder name coming) called `UsingClientLagsAssignor` that only HATA implements to indicate that it needs this extra information on the `ClientState` to do its job. This will also be useful if we want to eventually make the task assignor fully user customizable; I expect someone who implemented a custom assignor that does not require task lags would be surprised (and annoyed) to find out that their custom assignor was skipped because we couldn't compute superfluous client data. Or, we could just move the lag computation into the HATA. But personally I'd rather leave it in the SPA for the above reason
should this be `&&`? As it is, this loop could terminate even if we always return null and never non-null
we can get rid of the else block here and save some indentation. if we threw an exception then we're out of the function
nit: add `final`
I think we should also check for empty here, so that we could give a nicer error message than "all components don't sum to 100" etc.
This line is failing checkstyle.
Thanks for the background @rajinisivaram. By the way, I noticed one of the checks is defined inconsistently: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java#L344.
Was the boundary check wrong? You changed `>=` to `>`.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
ditto for the rest of the test
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
This should not have the `sink` suffix
Ack, I get it now. Thanks for clarifying.
same for the store
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
Sounds good. We can consider this resolved.
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
I think this can be written as a for loop over `requests` followed by `requests.clear()`.
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
They are using the same underlying enum, but `TaskStatus.State.DESTROYED` would probably be clearer here.
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
nit: remove `this` (not required)
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Nit: the same key.. ditto below.
Ditto above. I would recommend having consistent explanations here.
"of [a] windowed..."
"of [an] ever-updating ..."
@guozhangwang Yep, sounds good to me.
it treated => it is treated
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
@cmccabe I think you missed this change.
Did you mean: ```suggestion setBrokerId(2). setBrokerEpoch(100). ```
Nit: you can remove `value =`
nit: add a size? There are a few cases in here where we could do this.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
Yes, this seems fine then.
If we're just testing broker compatibility I don't think we even need this part of the test.
Note that Kafka only supports kerberos as the SASL mechanism.
assignedTopicPartitions could be updated concurrently and we are accessing it without lock protection here.
It seems that it's more natural for producerManager to create a CompletableFuture and return it through publishMessage() rather than passing in a CompletableFuture from the caller.
Hmm, this callback will be called from Producer's Sender thread and consumerManager.waitTillConsumptionCatchesUp() blocks until the timeout. This will block the Sender thread, which is not ideal.
extra new line.
Also: should only call onPartitionsLost on owned partitions that no longer exist
Maybe we could use a different value here.
You might consider using `OptionalDouble`.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
@vahidhashemian, yes, that's what I mean.
nit: The mention of join group comes out of nowhere
yes, it seems to be not what this test is checking on. I think we can drop it here.
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: add `final`
store not used
nit: add `final`
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
Also, not sure if we can make this easy, but it'd really be ideal if we could keep the logging line as a single line of code without requiring additional logic by the caller. (Maybe even if this requires allocating a special object to do that.)
Does it still make sense to have the if/else here? we log debug anyway and the exception should contain enough information to figure out the type of error.
Since we have several things that all need to be closed, perhaps we could use `ClientUtils.closeQuietly`? Maybe something like this ```java try { for (String id : connections) close(id); } finally { AtomicReference<Throwable> firstException = new AtomicReference<>(); closeQuietly(nioSelector, firstException); closeQuietly(sensors, firstException); closeQuietly(channelBuilder, firstException) if (firstException.get() != null) throw firstException.get() } ```
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
extra new line.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
+1 to this
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
Oh good point, we definitely need the key. But I think separating them turned out well
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
remove try-fail-catch and rewrite to ``` final StreamsException expected = assertThrows(StreamsException.class, () -> collector.flush()); assertTrue(expected.getCause() instanceof TimeoutException); assertTrue(expected.getMessage().endsWith(topic1TimeoutHint)); ```
Not 100% sure how `OnSubsequentCall` is meant either. But what you say seems to make sense and thus it should be different test. Thanks for the extra mile splitting them up!
nit: This last check is not needed, since it verifies functionality of the `Map` returned by `Collections.unmodifiableMap()` and not of the code under test.
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
Hmm.. I'm wondering how did we succeed in this test case, since in the above code `send()` call is only captured with `TimeoutException`? Note that we only set the KafkaException in the callback while here we throw exception directly. And in fact, you changed the expected exception from StreamsException to KafkaException in line 128 above.
Also need to change `keyValueStore` method to return `StateStoreSupplier<KeyValueStore>`
Can remove the first two null checks as they are covered in the overloaded `reduce`
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: should be `named` can't be null
EDIT: nvm, I think I understand it now.
Thanks. I will make another pass now.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
Maybe `Producer epoch...`. Also, not sure the exception message adds anything given what's already logged. Maybe we should remove that.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
Good point, thanks for clarifying.
At least we try to not abuse Thread.sleep() ;-)
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Fine with me to keep the guard. Was just double checking.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
nit: extra newline here
Ditto here for different exception types.
Thanks for the follow-up.
We should deprecate this one too I believe.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
I think `nanoseconds` should be `milliseconds`. It's a little more idiomatic to do it like this: ```java time.sleep(autoCommitIntervalMs); coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); ```
This test case passes without the fix. It doesn't look like it even goes through the auto-commit path.
The name should mention the fact that this case covers auto-commit.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
In order to reproduce this issue, we need to reset the generation via `maybeLeaveGroup` before the `onJoinComplete(gen.generationId, gen.memberId, gen.protocol, memberAssignment);` is triggered, but after the join-group response handler is exercised to set the generation id. I think this can still be doable with a single thread, to execute in the following ordering: 1. we only prepare the join-group response in the mock network client, but not the sync-group response, and we also make the MockTime to be able to advance time automatically, then by calling `joinGroupIfNeeded` with a small timeout instead of Long.MAX_VALUE, it should be able to finish the join-group round trip, trigger the handling logic to set the generation id, and then send the sync-group request, but then time out on https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L400 waiting for the sync-group response and return false. 2. Then we call `maybeLeaveGroup` within the same thread. 3. Then we prepare the sync-group response in the mock network client, and call `joinGroupIfNeeded` again, this time the response would be received, and the rest of the logic https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L406-L415 would be executed.
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
This statement is a bit misleading, how about "to the format indicated by the given magic value".
The try/catch is over the whole block, so the message seems a little odd. If this can only happen while reading the headers, we should probably move the try/catch to `getHeaders`. We should also probably rename `geHeader` to `readHeaders` or something like that.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
Ah, yes, the magic is hardcoded here.
No, I'm talking more about the `encode*` and `castTo*` methods in this class. I'm not suggesting we make them public, but instead I'm asking whether it would make sense to make these *protected* and *non-static* so that it would be easier to create subclasses. I guess the problem with that, though, is that they are implementation details and not part fo the public API. If anybody did create a subclass and we later changed the implementation, their subclass would no longer compile and they'd have problems running on different versions of Connect. Given that, I think it's fine the way it is: private non-static or private static doesn't really matter except for subclasses.
We can inline this like you've done for the byte[] case below
Is this always true? Can't the method be called with `schema` set to null? (The method does check for that condition.)
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
Actually, I now understand why the logic is where it is, and why the logical conversion doesn't need to be done. However, I still think the above logic using the schema's default and/or checking whether the schema is optional needs to only be performed when the `schema` is not null.
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
Just wondering if this test is good. Seems like a "RoundRobinAssigner" might compute the exact same assignment. Some more "randomness" would be good IMHO. (Not a must though.)
nit: use static imports to get rid of `Assert.`
add `final` twice
nit: remove empty line
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
same here -- sounds like CachingKeyValue with TimestampStore
I really like this class.
Good catch. I think you're right.
prop: I find `assertThat()` better readable than `assertEquals()`.
Btw, we should take the chance and make `version` and `commitId` final. Something like: ```java private static final String version; private static final String commitId; static { Properties props = new Properties(); try (InputStream resourceStream = AppInfoParser.class.getResourceAsStream("/kafka/kafka-version.properties")) { props.load(resourceStream); } catch (Exception e) { log.warn("Error while loading kafka-version.properties :" + e.getMessage()); } version = props.getProperty("version", "unknown").trim(); commitId = props.getProperty("commitId", "unknown").trim(); } ```
nit: just simplify to `throws Exception`
Fair enough, let's keep it inside StreamThread for now. In a longer term refactoring, maybe we could have an StreamsUtil class where such static functions / fields can be stuffed in.
`Not for this PR`: I think a better place for these static methods is StreamsConfig.
nit: add `final` -- same next line
Ouch! Sorry about that!
```suggestion final OffsetCheckpoint checkpoint = new OffsetCheckpoint( new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME)); ```
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
typo: "partitions that *are* no longer assigned"
I think we should not say that the commit can be retried. I would just say that the rebalance needs to be completed by calling `poll()` and that offsets to commit can be reconsidered after the group is rejoined.
nit: If a user tries to commit during an eager rebalance, the `CommitFailedException` will still be thrown -- we should consider making this explicit, I worry users might read this and think it's ok to commit during an (eager) rebalance as long as they catch the `RetriableCommitFailedException`
"Can be retriable" does seem to imply that in some circumstances it can be retried, while in others it _can't_ be retried at all. "Can be retried" implies it's up to the user whether they want to retry
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
Do we want a `ConnectException` here instead? Not sure.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
empty line needed
Oh, I just noticed. Then `synchronized` is not needed anymore.
Maybe a little subjective, but I think the test case would be more readable if we list these brokers directly. For example: ```java List<Integer> allBrokers = Arrays.asList(1, 2, 3, 4, 5); List<Integer> brokersToKeepUnfenced = Arrays.asList(1); ```
Perhaps we can use `Uuid.randomUuid`? It's a little weird for all brokers to have the same incarnationId.
nit: it's a small thing, but the assertion failure message is more useful if we use the `Errors` type. ```java assertEquals(Errors.NONE, Errors.forCode(createTopicsResponseData.topics().find("foo").errorCode())); ```
Is there a point to setting `min.insync.replicas` in this test? I am wondering why we don't just reuse the original create request.
You can call `fail` directly: `fail("Fencing of brokers did not process within expected time");`
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
@vahidhashemian, yes, that's what I mean.
I think we tend to use `forXxx` rather than `create` for mapping names to enums.
You might consider using `OptionalDouble`.
Nit: go with single parameter per line.
We shouldn't return `null`, but instead return a "unknown query" result.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
nit: move to line above.
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
Not sure what has changed here.
extra new line.
nit: move below the shortcut return below.
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
Another tab here that should be replaced.
You probably need to do the same in `toEnrichedRst()` and `toRst()`
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
Maybe we shouldn't say "error" when it's not an error. I'm just imagining the mailing list questions that will start pouring in... ```suggestion log.info("Received version probing code {}", AssignorError.VERSION_PROBING); ```
Nit: remove `this`
nit: break line (way too long)
Nit: fix line break
Cool, that's helpful. Thanks.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
This should say `AdminClient`, not `Consumer`.
Personally, yes, I prefer one call, but I leave it up to you.
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
Nit: add `final`
Same here and below
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
For `all()` function, its returned type should be `KafkaFuture<Void>`; ditto for other two Results as well.
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
Nit: maybe `("Topic: " + topic)`
We could refactor out a helper function here.
Sounds right to me.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
nit: maybe worth adding `this == o` to these `equals` implementations. This applies to all of the similar classes in this PR.
nit: add a size? There are a few cases in here where we could do this.
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
I think this could be done with `computeIfAbsent` like in "finishSnapshot" above
(very minor) nit: inconsistent naming of these in this class
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
Sorry, missed this earlier: We are creating a new `selector` in `checkAuthentiationFailed`, so we should ensure that the previous selector is closed. You could call `selector.close()` just before calling `checkAuthenticationFailed` here and also a couple of lines below.
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
instead of creating a new set, thoughts on just returning an empty collection? (`Collections.emptyNavigableSet()`)
nit: Provide a message to the `IllegalStateException` constructor
nit: Provide a message to the IllegalStateException constructor
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
Oh, I see. Well, either way works for me.
Nit: space missing before `timestamp_type`.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
Thanks @vvcephei -- that is convincing.
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
One issue with c. is that it works for new environments. If users already have MM2 running, it's using topics with the current names.
Why are we generating the connect internal topic names here? if there's a rule for the topic naming convention it should be defined in one place which is `ReplicationPolicy`.
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
What value does this test provide? Seems it verifies that `setup()` method is correct? Seems unnecessary to me.
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
IIUC, `rocksDBMetricsRecordingTriggerThread` is either going to be `null` because it didn't meet the criteria for creating the thread or it is going to be non null because we did create it. Given that, checking for `null` is a nicer way to determine if we need to shutdown because as long as we have a thread, regardless of the condition that created it (e.g. `RecordingLevel.DEBUG`), we should shut it down. This is also safer if the creation conditions ever change and we forget to update them here.
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
maybe instead to check that `rocksDBMetricsRecordingTriggerThread` is `null` instead? Here and elsewhere.
Same minor nitpick about whether or not we need to check for an empty group ID.
empty line needed
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
You only need to crate that instance once, right? It can be a member of the class
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
The number has changed and 5 is no longer relevant.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
Hmm, if we performs an unclean leader election, the only replica in ISR should just be the new leader since the data in existing ISR is not guaranteed to match with the new leader.
Typo: "you can create [a] windowed ..."
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
And again with `final` if you don't mind
My minor concern is that KeyFactory and ValueFactory may be only specific to key-value stores, not not any general stores; for example for database stores you may have some functions like ``` withSchema() ``` that defines the data types for each column but not using "withKey / Value" any more. But since it is only for future improvements let's revisit this nested mechanism later.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
Is this also logged if there is nothing to be committed? Just double checking to ensure we don't introduce a new issue -- we only commit, if we have anything to commit IIRC.
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
Nit: we don't normally use exclamation marks in Kafka log messages.
We should just throw an "InvalidArgumentException" here.
should we put `kafka` and `numThread` directly into `props` and reduce number of parameters here (would also simplify all the "passing through" code.
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
nit: additional new line
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
nit: additional new line
I don't think we need extra `toGiveUpTopicPartitions` to store the partitions to be deleted. We can log the warning message in L1103 here directly
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
ditto on (what I think is) the impossibility of this condition being false.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
Semantically really bad to forward `0x3`, but well, it is what it is.
@eliaslevy Since this part is covered in other unit test case, we want to remove redundant coverage to leave the unit test as succinct as possible.
I think this can be simplified a little bit more by getting rid of this variable. How about this? ```java boolean userConfiguredTransactions = this.originalsContainsKey(TRANSACTIONAL_ID_CONFIG); boolean idempotenceEnabled = this.getBoolean(ENABLE_IDEMPOTENCE_CONFIG); if (!idempotenceEnabled && userConfiguredTransactions) { throw new ConfigException("Cannot set a " + ProducerConfig.TRANSACTIONAL_ID_CONFIG + " without also enabling idempotence."); } return idempotenceEnabled;
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
nit: move `logContext` to its own line
Thinking about this some more, not sure there's a lot of value in forcing users to set `idempotence=false` in cases where they're setting `acks=1|0` or `retries=0`. So, I'd change the warning to info for these cases. `max.in.flight.requests.per.connection` is different since it's an implementation constraint that `idempotence` doesn't work when it's > 5 vs inherent to the configuration. For this one, I'd have a warning and mention that it will become an error in Kafka 4.0.
Fair enough, let's just leave it as is then. Thanks for the explanation.
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
I used `equalToObject()` because it makes the intent more explicit.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
This is a nitpick, but perhaps we should not use `at the same time` in `close` or `configure` because it won't actually happen at the same time (it will most likely happen sequentially, but we don't have to specify that).
Could we try to avoid this copying if we assume it always wraps an array? I.e. ``` if (data.hasArray) return data.array(); ``` My syntax may not be perfect, and we need to double check the starting and offset in the backed array, but the general idea is as above.
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
nit: rename `ivtwks`
nit: add `final`
nit: add `final`
nit: add `final`
nit: `final` I know it wasn't before, but let's stick to making params as final
True, will we ever want to have this ability? But the change seems fine to me.
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
Is this designed to have thread-level be the parent of the cache-level? I think originally we want to have task-level be the parent of cache-level (but there is a bug for that so it may not actually be the case).
I didn't think of that before, but now that you mention it, the change makes sense to me.
why do we make lines longer? harder to read now
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
Looks good. I like the additional checking that you're doing here.
Do we actually have to mock `generation()` and `rawConfig()` for this test? Looking at `connector()`, it looks like it only relies on the snapshot.
I don't think this mock is needed. Same below.
We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
I don't see bucketing
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
a tab is missing here for alignment too.
nit: probably checkstyle won't catch this, but if you format the block, there's an extra tab that you can remove here too.
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
another ideaa: add an assertion here? so that we are guaranteed that this method is called.
we can make method this public in `EmbeddedConnectCluster`.
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
this global variable isn't great. Can't we hit some rest endpoint that can return this internal values out of the extension? probably makes for a better end to end test too.
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
not be => not be able to
Changed it locally.
newuntil => newUntil
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
public access? I can see this being accessed by another package too (such as `rest.resources`)
Oh, and a typo which I would like to make KNOWN (or UNKNOWN?! ... I would pick a pun over clarity any day :) )
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
Cheating the compiler, woohoo!
This doesn't need to be declared outside the loop (it can be final at the assignment).
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
Yes, this seems fine then.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
Shouldn't need this line, it's handled by the superclass's constructor.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
If you pass the new one, then you can probably get rid of `changedTopicId`
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
Yes, I was just pointing out that there is still a gap.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
I see what you mean, and yea that is a fair point 
This is an internal class -- no need to mark as deprecated.
nit: add `final`
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
