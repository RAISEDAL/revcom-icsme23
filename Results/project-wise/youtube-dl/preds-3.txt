The test page has a perfectly good `application/ld+json` script element with `@type: VideoObject` which should be the default source for metadata.
```suggestion description = try_get(additional_data, lambda x: x['caption']['text']) ``` otherwise this would error out when no caption object
Have you checked if this works with the multi-video post test? IIRC you'd need to extract `carousel_media` for it also ```suggestion best_quality = items0['video_versions'][0] ``` or make the array indexing optional
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
I mean you must pass both regexes.
Should not match `varflashPlayerOptions...`.
This statement does not conform to PEP8.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
No need to escape `{}`.
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
additional info can be extracted from `video` request.
no, there is `season_number`, `episode_number`, `timestamp`, etc...
By providing username and password in params obviously.
Use more relaxed version, e.g. `[^/]+` to match segments and `[^/?#&]+` to match id.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
try to parse multiple formats, set `vcodec` to `none`.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
Must be numeric not string.
Meaning that you must provide account credentials for testing or whatever else is needed.
Using preferences causes invalid sorting.
@zachee @dstftw I had a go at removing the duplication. Is this any good? **Function** ``` def _get_first_valid_downloaded_webpage(urls, video_id, headers): for url in urls: webpage = self._download_webpage(url, video_id, headers=headers) if not ('File not found' in webpage or 'deleted by the owner' in webpage): return webpage raise ExtractorError('File not found', expected=True, video_id=video_id) ``` **Usage** ``` def _real_extract(self, url): video_id = self._match_id(url) url = 'https://openload.co/embed/%s/' % video_id url2 = 'https://openload.co/f/%s/' % video_id headers = { 'User-Agent': self._USER_AGENT, } webpage = self._get_first_valid_downloaded_webpage([url, url2], video_id, headers) ```
@cacdpa: That's better. Just don't use the name ```url``` - it's already used as a parameter of ```_real_extract```.
`note` and `errnote` of `_download_json` instead.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
`'%s'` is very unlikely to be a helpful error message.
There should be spaces between `%`.
Carry long lines.
`.get()` idiom is used for optional fields only.
```XimilayaIE.ie_key()``` is better
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
No need to escape `]` is character set.
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
yt-dlp's `--print` is a bit more complex, allowing printing data at multiple stages. Eg: `-O "after_move:%(filepath)s" will print the final file path. This function is needed to (easily) support that syntax, (and also other similar options)
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
More relaxed regex.
This is not true either. Login may be achieved via authorized cookies.
Just remove the `try:` and `except:` lines.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
`self._html_search_meta` is better here.
This regex does not make any sense.
Do not capture groups you don't use.
No captures for groups you don't use.
No need for that check it's already checked in `parse_iso8601`.
`_html_search_regex` already calls `unescapeHTML`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Read: coding conventions, optional fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
All formats must be extracted.
I believe their generic name is `YouPorn`, so you can just delete this line.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
`<span[^>]+class="name">...` is better.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
This should not be fatal.
This should be extracted in the first place.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
Upps, that's wrong, the results are indeed tuples.
Breaks on `None` title.
Use the original scheme and host.
Don't capture groups you don't use.
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
No escape for `/`.
Do not capture empty strings.
Breaks if not arr.
Must be int.
It's artist not an album artist.
Optional fields should not break extraction if missing.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
it better to extract all the urls in the `mpath` array.
There is no point in that.
Must be extracted first.
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
No, parse as JSON. Or from ld+json.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
No, use a value that matches the user agent used by youtube-dl(`chrome`).
use the already extracted value(`video_url`).
should not break the extraction here if a request fails or the `video` field is not accessible.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
you already have `pgm_id` and `pgm_no` variables now.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Do not touch existing `AtomicParsley` code. `mutagen` path must be a fallback if `AtomicParsley` is missing.
You must output to a temp file not the original file.
Won't work. See how this is done for output template.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
This should not process the whole page. Regex should be relaxed.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Use fatal=False or default=None in _html_search_regex to handle fallbacks instead of try-except patterns.
Just use ```'ÐÐ¾ÑÐ»Ðµ ÑÐ¸ÑÑÐ¾Ð². Â«ÐÑÐ¸Ð¾Ð»Ð¸Ð½Â»'```
url and formats are not used together.
`SoundcloudIE.ie_key() if SoundcloudIE.suitable(permalink_url) else None` at the place of passing `ie`.
This must be passed in `url_result`.
This won't skip empty strings.
Extractor must not return None.
Mandatory. Read coding conventions.
Remove and make dvr path mandatory.
Do not escape `/`.
It also accepts `/video/bbq-salon/WHATEVER_I_WANT_AND_MAKES_SEO_SENSE-63940306` but that doesn't make it `id`, does it? Also when you examine their graphql query there is a attribute called `dotId` which has exactly that value eg. it is `ID` your `video_id` is in their data on attribute `urlName`. Do as you think, but this will be an issue when merging into master.
There's no need to name a group if not used.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
End users do not read source codes thus will never find this advice.
will return invalid URL if `search_url` is `null`.
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
Use regular string format syntax instead.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
It does not check the other path since you neither download it here nor test the `url` not to contain `mp4:`. Thus you can't be sure whether this test involved the other path or not.
Idiomatic name is `video_id`.
Second argument should be an id extracted from `_VALID_URL`.
The same. Should use https if ```url``` use https.
Same issue for re.search
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
The whole code until here can be simplified to `page_id = self._match_id(url)`
The argument will already be a character string, no need to decode it.
Ok, It's up to you.
this does not handle the case where `contentUrl` value is `None`.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
No need to use `sanitized_Request` request here, pass url directly to download method.
Use `self._parse_html5_media_entries` instead.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
API URLs should be used.
I've already pointed out: API URLs should be used.
Change 1 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI. Change 2 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/media/_hqLjQ95yx8Z.
As well as this.
It's a matching only test. Testing two identical URLs twice does not make any sense.
Still does not handle aforementioned URLs.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
This should actually be just `self.url_result(embedded_url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No. Use fatal search regex instead.
Dots not escaped.
Must not be fatal for playlist.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This does not make any sense, you already have `url`.
Relax `id` group.
Capture between tags.
Remove all garbage.
Plain `for x in l`.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
same for `episode_number`(both lines where highlighted).
`created_at_i` as `timestamp`.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
Must be separate extractor.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
Preferably for such a long field use `'md5:...'`
I would prefer indent similar to the former code.
This regex should be split into multiple lines for bettercode perception.
1 is ok.
All formats should be extracted.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Should not be fatal.
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
This video is georestricted.
Use `self._match_id` is better.
`self._search_regex` is enough here.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
This is never reached cause sort formats will throw on `not formats`.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
These dependencies are unacceptable. Moreover you don't use them.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
The playlists are public: there should be tests for them. Watch this space.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
It shouldn't fail if `user` or `username` is missing.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Remove all debug output.
`video_id` may be `None`.
Regex should be relaxed. Dots should be escaped.
No such field.
This should be split into building url and extracting formats.
Query should be passed as `query` parameter.
Uppercase is not honored.
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
still the check for element class is missing.
The third parameter of `_html_search_regex` is name but not ID.
`'id'` is required.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
make one of the tests an `only_matching` test.
there is not need for excess verbosity.
no longer needed.
Remove unnecessary verbosity.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Network connections in your browser.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Use `self._og_search_*` functions here.
Use a for loop to eliminate duplicated codes.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
Read coding conventions.
You should not throw everything under try/except. Also read coding conventions.
surround only the part that will threw the exception.
This will return non existent path if conversion fails.
`{}` won't work in python 2.6.
Why do you disallow `story` to be an id? Because you do not want it to be an id in your particular case that is clearly an ad hoc hack cause other extractors may use `_generic_id` and may allow `story` to be an id.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
This is pointless.
All of these will break extraction on unexpected data.
Must be `int`.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
This branch is never reached.
`default` is not used with `fatal`.
This does not necessarily mean that. There is a clear captured error message that should be output.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
Don't capture groups you don't use.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
Instead of this extraction should be delegated to `WSJIE` via `url_result`.
Also pass `video_id` since it's known beforehand.
Should not be fatal.
`[]` is useless.
Move method near the place of usage.
1. Breaks if div is not found. 2. `re.finall`.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `self.url_result(inner_url, 'Generic')` instead.
All debug code should be removed.
Should be delegated via `url_result`.
This duplicates code from `MedialaanIE`.
Depending on where in the page the target may be, consider `self._html_search_regex()` which unescapes the returned match (eg, if it contains `&amp;` that should be `&`, or just `&#0049;` that should be `1`).
`self._search_regex` is enough here.
Matched data-video should not be empty.
No exact URLs.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`for key, value in media.get('images', {}).items():`
This code looks similar to `sd` format and can be extracted to a function.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion new = '' ```
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
use single quotes consistently.
check that extracted description is what is expected(should not contain html tags).
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
Re-read my post with eyes please.
This breaks all non ks embeds. ks part must be optional.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This should be rewritten in terms of [`YoutubeIE._extract_urls`](https://github.com/rg3/youtube-dl/commit/66c9fa36c10860b380806b9de48f38d628289e03).
Do not shadow built-in names.
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
Use `utils.xpath_text` instead, again with `fatal=False`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `compat.compat_str` instead.
This is never reachable.
This is default.
Must not be fatal.
Bitrate should go to corresponding format meta field.
`self._parse_html5_media_entries` for formats extraction.
This should be a ```list```.
Breaks if no `rate` key in `stream`.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
Should not be greedy.
`{}` doesn't work in python 2.6.
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
If you accept this to be missing then it must be `default=` not `fatal`.
This is never reached cause sort formats will throw on `not formats`.
All of these will break extraction on unexpected data.
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
Read and follow code conventions. Check code with flake8.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Should not be greedy.
All formats must be extracted.
Do not use leading underscore for locals.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
Uppercase is used for const.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
Either **do** or remove.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
should be in the `else` block of the `for` loop.
the same for `streaming` key.
`id` must not contain any irrelevant parts.
Formats not sorted.
Should be tolerate to missing keys in `media`.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
No brackets needed.
This is no longer actual.
This is useless at the end.
`<h3>` is intentional.
It should not match `h|`.
87-90 code duplication.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_search_regex` is enough here.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
`fatal` must be added to `_extract_info`. No changes to core code.
Add `fatal` flag.
Do not change the order of extraction.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
`compat_str()`, here and in l.96.
Parenthesis are superfluous.
None of the optional metadata should break the extraction when missing.
All these regexes should be relaxed.
This test is pointless. You must test iframe extraction not regex match.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
This matches multiple videos.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
Side note: Wow, these guys are military, but don't support https? Oh my...
To get regex highlighting, this string should be prefixed with `r`. That's just convention, but plain neat.
This is a tuple, that can't be right. Also, a test is only really useful with some things to test against, like title and md5sum of the image. You can run it with `python tests/test_download TestDownload.test_DefenseGouvFr`.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Instead of this extractor you should add an extractor for `https://20.detik.com/embed/...` URLs and add detection of such embeds in generic extractor.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
It's better to keep the original ID for existing patterns, or --download-archive will be broken.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This should be split into building url and extracting formats.
Update `video_info` and return it directly.
It won't be longer.
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
The `list` call is superfluous here and can be safely removed.
`else` is superfluous.
No need to escape `]` is character set.
`--no-playlist` is not respected.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
No. You should not shadow the original explicitly provided password.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
There are lots of Content-Type calls. Please merge them together
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Use compat_HTTPError instead
default and fatal are not used together. `True` is default.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`if not content_url:`.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
It should be robust in case of some missing fields.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
it whould be better to iterate once and extract the needed information.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
`title` is mandatory. Move flags into regex itself.
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I think it's safe to use `avi` here since we use extensions for this option. Moreover, it will simplify [this code](https://github.com/aurium/youtube-dl/blob/master/youtube_dl/postprocessor/ffmpeg.py#L297-L301) a bit.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
`<display_id> is not a video`.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
No such meta field.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
All tests that test similar extraction scenario should be `only_matching`.
Will never happen.
Never use bare except.
You don't need to call `report_extraction` here and above, since the extraction is over already.
Oh, alright, leave it in. I was suprised to find a `\x` instead of a `\u`, but specifically for the non-breaking space, that should be fine.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
Add `fatal` flag.
Do not change the order of extraction.
`fatal` must be added to `_extract_info`. No changes to core code.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
Use `video_id`. Remove `.replace('\n', '')`.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
Must not be fatal.
Breaks on None.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
No need to escape `]` is character set.
`--no-playlist` is not respected.
Use bare `re.match`.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
You have some unmerged lines here
There should be an `id` group.
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Use single quotes consistently.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Possibly referenced before assignment.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Possibly referenced before assignment.
This regex does not look like generic embed. Provide several examples that use this embedding.
url and formats are not used together.
No trailing $, override suitable.
Don't capture groups you don't use. Use proper regex to match all country codes.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
No direct URLs in tests.
Extract id once before the loop.
If nothing matches `None` will be returned.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
and the function that normally used to encode postdata is `urlencode_postdata`.
JSON layout has changed. Error is not captured properly anymore.
Error handling broken.
This does not necessarily mean that. There is a clear captured error message that should be output.
There's no need to use `u` prefix given `unicode_literals` is declared.
This will process the same URL twice overwriting the previous results.
This line is unnecessary, webpage is never used.
breaks the extraction if `clips` is `None`.
will break the extraction if `profiles` is empty or `None`.
same if one of the values is `None`.
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Correct field name is `format_id`.
`ext` here makes no sense.
Also pass `m3u8_id='hls'`.
`/?` is senseless at the end.
Use display id.
`default` is not used with `fatal`.
Re-read my post with eyes please.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
A minor bit: don't use a capturing group if it's not used.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
This is in the test suite, and in the test suite, warnings are an error of us, aren't they? So why isn't the implementation ``` raise Exception(message) ```
We should really provide a better interface to test against, something along the lines of `download(url)`.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
This doc string does not match the function now.
It should always return a list.
It worth adding a doc string.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
Must not be fatal. Read coding conventions on optional/mandatory fields.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
No. _extract_urls of youtube extractor.
This line can just be removed.
Don't capture groups you don't use.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Use `self.url_result(inner_url, 'Generic')` instead.
Make more relaxed and add title regex as fallback.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
If they switch to strings this will break formats' sorting. Same should be done for width & height.
Change to `config_files.get('hls', {}).get('all')`.
Make it non fatal, provide correct `ext`, consider forcing to native hls by default, and set same `preference` as for direct links since for some videos hd is only available via hls (e.g. https://player.vimeo.com/video/98044508).
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
No, use a value that matches the user agent used by youtube-dl(`chrome`).
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
```python for path, format_id in (('', 'audio'), ('video', 'sd'), ('videohd', 'hd')): self._download_xml( 'https://www.heise.de/ct/uplink/ctuplink%s.rss' % path, video_id, 'Downloading %s XML' % format_id, fatal=False) ```
Must not be fatal.
We may use proper XML parsing here and simply call `self._download_xml`
No escape for `/`.
Do not capture empty strings.
Avoid unrelated changes.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
The description is always optional, so there should be a `fatal=False` in here.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
Must only contain description.
Carry long lines. Read coding conventions.
Must only contain title.
Use `self._parse_html5_media_entries` instead.
This should be a ```list```.
I would always return a `multi_video` result.
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Extracting duplicate code into a function obviously.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
None of the optional fields should break extraction if missing.
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
You have some unmerged lines here
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This is not matched by `_VALID_URL`.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`ext` here makes no sense.
Correct field name is `format_id`.
Also pass `m3u8_id='hls'`.
Do not capture groups you don't use.
No exact URLs here.
Query to `query`.
Before these changes there were 4 references and extraction followed them strictly in order of reference declaration.
At least move iframe extraction after video tag extraction.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
Read coding conventions on optional and mandatory data extraction.
No, provide subtitles as URL.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
I suggest `default=video_id` in the `_html_search_regex` call.
will be extracted from the URL.
Again: float_or_none, not parse_duration.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
There should be fallbacks for these values since they are more or less static.
This should not be fatal.
It's already done at L151.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
Read coding conventions on how mandatory data should be accessed.
`[]` is superfluous in group with single character.
Must not be fatal.
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
All integral numeric metafields should be wrapped in `int_or_none`.
Title is mandatory.
This may change as well. Add a fallback that just processes all videos without differentiation.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
This may change as well. Add a fallback that just processes all videos without differentiation.
This should actually be just `self.url_result(embedded_url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
Separate variable is superfluous.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
`enumerate` on for range.
Too broad regex.
87-90 code duplication.
No trailing markers - override `suitable`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
use the extension extracted from `determine_ext`.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
instead of this it's better to just put result of `determine_ext` in a variable and use it.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
Don't use bare `except:`
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Move flags into regex.
This check is not necessary now as you test pathconf anyway.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
I would prefer testing with `hasattr` instead to avoid [possible breakages](https://github.com/rg3/youtube-dl/commit/22603348aa0b3e02c520589dea092507a04ab06a) I've learned about the hard way.
Use bare `re.match`.
`else` is superfluous.
`--no-playlist` is not respected.
Re-read my post with eyes please.
A minor bit: don't use a capturing group if it's not used.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
Should not break if missing.
Python 3.2 doesn't like u-literals.
Optional fields should not break extraction if missing.
Removing useless noise.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
Breaks. Read coding conventions.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Code duplication 173, 213. There is no sense to extract fields explicitly.
```suggestion if not (season_id and video_id): ```
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
For rtmp it's always flv despite of the extension.
`{}` doen't work in python 2.6.
Name an example URL where `og:description` has HTML tags.
Instead of escaping the inner double quotes you could single-quote the string.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
`{}` does not work in python 2.6
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
This doc should be updated.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
works also for me :+1:
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
No need in another test.
It's obvious from `'is_live': True`.
It's a field name not a step name.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Dot is pointless here.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
This does not mean it should not be included.
This is done automatically.
Breaks. Read coding conventions.
Default note is enough.
Capturing empty string is senseless. If you expect a dict then capture group must contain `{}`.
This should be extracted from `_VALID_URL` with `self._match_id`.
Default part should be removed since there is no default.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
try_get is pointless here. Read coding conventions.
`get` is pointless since availability of result key is mandatory.
Simple string concatenation is enough here.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
What's the point of this? Use `url` as base.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Actually, it's an opposite. It's a check for successful login.
Code duplication. This is already implemented in `CeskaTelevizeIE`.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
Either `text` or `html` is mandatory.
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
no longer needed.
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
Query to `query=`.
Should not be fatal.
Should be tolerate to missing keys in `media`.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`creator` should be preserved for backward compatibility.
Redundant or not it should be preserved either by providing manually or by copying from `artist` automatically when not present (that is not implemented) since it can be used by someone already.
This is not matched by `_VALID_URL`.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
`lxml` is not part of the standard library, you'll need to use regular expresions.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Network connections in your browser.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
`id` and `display_id` should be tested as well.
Use quotes consistently - only a single or only a double. Use different kind of quotes only if there is no way to use same kind of quotes.
As already mentioned `id` and `display_id` should be extracted.
There is no need to test URLs.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
I've already pointed out: API URLs should be used.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
Do not escape quotes inside triple quotes.
this should be done once(in `_real_initialize`).
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
`compat_str()`, here and in l.96.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
This does not mean it should not be included.
This is done automatically.
Use more python idiomatic `.get('title')` and `if alt_title:` instead.
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
> Also append_url_to_file may be renamed to something better reflecting it's actual content. the function can be removed completely and move the code directly to the for loop.
121, 124 - DRY.
As above, you can omit this and let the core set it from `release_timestamp`: ```suggestion ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
No `upload_date` is guaranteed to be present.
Breaks on `-F`.
I've already pointed out this does not work.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
Should not be fatal.
This is already fatal.
No such meta field.
This does not work in python 2.6.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
34-35 can be easily moved into `for`.
1. Don't shadow outer names. 2. `url_or_none`.
Real id is in widget dict.
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
`title` must never be `None`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks extraction if `release_date[0:4]` is not `int`.
Must be list, not a string.
Use `_parse_json` instead.
`id` should be extracted via `_VALID_URL` when present.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
All methods only used once should be explicitly inlined.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Automatic captions are also available.
Caps is used for constants.
Should not break extraction is download fails.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
You're not guaranteed `thumbnails` is a list.
Should be better with ```isinstanceof(filename, BytesIO)```
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
you can get json output by appending `&format=json` to the api request url.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Should not break if there is no `resolution` key.
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
`url` should not be `None`.
Rename to something else.
You must delegate with `url_result` instead.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
`.*/?` is pointless at the end.
**Never ever** use `eval` on data you don't control.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Avoid unrelated changes.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
```XimilayaIE.ie_key()``` is better
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
`flashvars[k]` is `v`.
This is pointless.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This does not make any sense, you already have `url`.
Do not remove the old pattern.
No escape for `/`.
Must not be fatal. Do not capture empty string.
Use `self.url_result(inner_url, 'Generic')` instead.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Filter invalid URLs.
You technically can't login with this extractor apart from using cookies.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
Don't lookup twice.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
`objectID` does not match the id from `AmericasTestKitchenIE`.
No escapes for slash.
Breaks on None.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
If a method is used by at least two extractors, it can be moved to ```common.py```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
As we're always going to want `type` later (but that's a Python keyword, so rename it): ```suggestion main_id, type_ = re.match(self._VALID_URL, url).group('id', 'type') ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
I think it's customary to use `_VALID_URL` for id matching if possible.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
We should really provide a better interface to test against, something along the lines of `download(url)`.
This will break unicode strings under python 2.
Not having archive != having dummy `Archive` object.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
Must be `int`.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
There should be a hardcoded fallback since it's always the same.
There should be an `id` group.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
will be extracted from the URL.
I suggest `fatal=False`
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
`_search_regex` is enough here.
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
No need for this check, this is already checked in `_sort_formats`.
Request wrapping code can be moved to the base class.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Plain `for x in l`.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Extract `height` for each format.
All formats must be extracted.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
Do not remove the old pattern.
Video URL is mandatory. Read coding conventions.
This should be recursively delegated to pbs extractor instead.
This is too broad and detects the same video twice.
From what I've seen there is always only one video on the page thus no need in playlist.
This is usually called `display_id` and included in info dict as well.
Uppercase is used for const.
Again: remove. Any download attempt must have clear explicit message.
Read coding conventions.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
Query should go in `query=` .
Should not break extraction is download fails.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Don't shadow built-in names.
No need to escape `]` is character set.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Breaks if no URLs extracted.
[`determine_ext`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L901) is more robust. However, usually you don't need to specify `ext` in formats dictionary.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
[Correct way](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/pluralsight.py#L95-L100) to implement order independent argument matching.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
Part after `\?` should be removed since it's not used anymore.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Must be extracted first.
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
Use `self._html_search_meta` instead.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
All formats must be extracted.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
Breaks on `None`.
Breaks on `default=False`. `title` must be fatal.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Original regexes should be tested first.
These will be needed later: ```suggestion from ..utils import ( get_element_by_id, get_element_by_class, int_or_none, js_to_json, MONTH_NAMES, qualities, unified_strdate, ) ```
Still does not handle aforementioned URLs.
Don't capture groups you don't use.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Breaks if no `name`.
`default=None` for the first.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
Both used only once, move to the place where used. Also relax both regexes.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This may change as well. Add a fallback that just processes all videos without differentiation.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
It's already extracted as video_id.
Wrong fallback value in `thumbnail`, `description` and `creator`.
All these regexes should be relaxed.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Remove unnecessary verbosity.
This must be an id of the media.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
This is fatal.
`note` and `errnote` of `_download_json` instead.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Use `self._download_json` instead.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
Should not break if `published_at` is missing.
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Field name is supposed to be `key` not `long_video_id`.
`_search_regex` is enough here.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All methods only used once should be explicitly inlined.
No. It's a job if the video extractor.
Should handle `src` with `https?:` also.
A minor bit: don't use a capturing group if it's not used.
Code duplication with base class method.
_og_search_description is a youtube-dl function
Use ```_og_search_description()``` as a fallback instead.
This line is unnecessary as there's already self._og_search_description
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Does not work as expected in all cases.
Do not change the order of extraction scenarios.
`{}` won't on python 2.6.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
This doc should be updated.
I did this so I can deprecate `--get-url` (along with all other --get options). Unless you want to do that too, `urls` field isn't really needed
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
Course extraction must be in a separate extractor.
Read coding conventions on optional and mandatory data extraction.
Playlist title is optional, description breaks.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Never use bare except.
Instead of `resolution` and `preference` it should be extracted as `height`.
No need to escape forward slash.
Ok, It's up to you.
`formats` can be filtered out completely. ``` python -m youtube_dl nVlEpd92MJo -f "best[height<40]" [youtube] nVlEpd92MJo: Downloading webpage [youtube] nVlEpd92MJo: Downloading video info webpage [youtube] nVlEpd92MJo: Extracting video information [youtube] nVlEpd92MJo: Downloading DASH manifest [youtube] nVlEpd92MJo: Downloading DASH manifest Traceback (most recent call last): File "c:\Python\Python279\lib\runpy.py", line 162, in _run_module_as_main "__main__", fname, loader, pkg_name) File "c:\Python\Python279\lib\runpy.py", line 72, in _run_code exec code in run_globals File "C:\Dev\git\youtube-dl\master\youtube_dl\__main__.py", line 19, in <modul e> youtube_dl.main() File "youtube_dl\__init__.py", line 406, in main _real_main(argv) File "youtube_dl\__init__.py", line 396, in _real_main retcode = ydl.download(all_urls) File "youtube_dl\YoutubeDL.py", line 1614, in download url, force_generic_extractor=self.params.get('force_generic_extractor', Fals e)) File "youtube_dl\YoutubeDL.py", line 667, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "youtube_dl\YoutubeDL.py", line 713, in process_ie_result return self.process_video_result(ie_result, download=download) File "youtube_dl\YoutubeDL.py", line 1273, in process_video_result formats_to_download = list(format_selector(formats)) File "youtube_dl\YoutubeDL.py", line 991, in selector_function for format in f(formats): File "youtube_dl\YoutubeDL.py", line 1022, in selector_function yield formats[format_idx] IndexError: list index out of range ```
And `audio_selector` as well to catch `-f 'bestvideo+'`.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Breaks if no URLs extracted.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
I've already pointed out: use `display_id` until you get real id.
1. Provide clear evidence this field is supposed to store the video URL. According to http://atomicparsley.sourceforge.net/mpeg-4files.html `tvnn` is a TV Network Name and obviously has nothing to do with video URL at all. 2. This does not guarantee the actual container is `mp4`. 3. This won't work for audio.
This can be moved inside `if chapters:` condition.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
This does not make any sense, you already have `url`.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
No such meta field.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
This must be assert not exception.
`True` is default.
This is pointless, you don't have any fallback.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
If you accept this to be missing then it must be `default=` not `fatal`.
All of these will break extraction on unexpected data.
This is never reached cause sort formats will throw on `not formats`.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
then you should use `_remove_duplicate_formats` method.
i'm not sure why you're adding this step.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Eliminate code duplication.
Use `_sort_formats` instead.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This is bitrate, not quality.
Breaks if no `rate` key in `stream`.
Should be `mp4`.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
Also pass `m3u8_id='hls'`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Read coding conventions and fix all optional meta fields.
No need to specify this.
Should be `flv`.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
No hardcodes. Use API.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
This video is georestricted.
Use `self._match_id` is better.
Just copy paste the whole line I've posted.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
All debug garbage must be removed.
It should match all non empty domain names.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
The argument will already be a character string, no need to decode it.
Please use `print()` syntax, as we support also Python 3(.3+)
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Relying on `userMayViewClip` is probably a [bad idea](https://github.com/rg3/youtube-dl/commit/2b6bda1ed86e1b64242b33c032286dc315d541ae#diff-0b85b33765d2939b773726fda5a55b06).
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
No. Must be fatal. Read coding conventions on optional fields.
There is no need in this method.
Same as in some previous PR.
I would always return a `multi_video` result.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
For 2.6 (yes, I know) compatibiility, `{0}` rather than `{}`, apparently. And number other formats elsewhere.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
You don't need to call `report_extraction` here and above, since the extraction is over already.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
1. Don't shadow outer names. 2. `url_or_none`.
34-35 can be easily moved into `for`.
Real id is in widget dict.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Fix test: ```suggestion 'ext': 'mp4', ```
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
This is never reached if Content-length is not set.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Just copy paste the whole line I've posted.
it's about the way you're setting `playlist_title`. `playlist_title` should not be set directly, it should be set in the playlist result title(by using the `playlist_result` method or using the playlist result type).
Playlist metadata must not be fatal.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Must be extracted first.
None is default.
Lack of information is denoted by `None` not `0`.
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
If audio_info does not have category_name, categories will become [None, 'xxx']. It should be only ['xxx']
errnote still needs a fix. "try to parse web page" sounds wrong
group=1 is equivalent to the default behavior of _html_search_regex; just drop that.
Just use `'thumbnail'` to match the default image URL.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
There is no need in this method.
All these regexes should be relaxed.
Again: float_or_none, not parse_duration.
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
This is default.
This is never reachable.
Playlist title is optional.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `video_data` may be `None`.
Use `re.sub` instead.
Read code conventions on optional fields.
Nothing changed. Also there is a video id available in JSON.
What I actually meant is to put them in a tuple or a list.
For tv/se `og:title` contains unnecessary suffix.
Just cut it with `utils.remove_end`.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
All formats should be extracted.
`.*` at the end does not make any sense.
Avoid unrelated changes.
Consistently use flags inside regex.
This is not true at the moment.
Plays fine without any authentication in browser.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Check code with flake8.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Instead of such hacks you can name group differently and capture it without any issue.
```suggestion 'url': 'http://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/', ``` This URL redirects to https://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/. This corresponds with the previous test L44 where we are using `play`.
There are two unrelated flags `_logged_in` and `logged_in`.
This is not true either. Login may be achieved via authorized cookies.
Insert this alphabetically (after veehd and before veoh).
Don't touch unrelated stuff.
Keep this test as `only_matching`.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
DRY with `_limelight_result`.
Extract `media_data['streamInfo']['sourceId']` into variable.
Some tests for these embeds could be useful, at least for reference.
No point checking this.
There is no point checking `url`.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
All formats should be extracted.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
I've already pointed out: API URLs should be used.
API URLs should be used.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
Field name is supposed to be `key` not `long_video_id`.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
No. Override `suitable`.
This should not match playlist URLs.
`for key, value in media.get('images', {}).items():`
Right, that's fine for `display_id`.
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
The most simple way is to use `utils.extract_attributes`.
Direct URLs should also be extracted.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
Extract human readable title from the `webpage`.
Move flags into regex.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
EntryId must be extracted the very first.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Breaks. Read coding conventions.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
There's no need to name a group if not used.
`show = data.get('show') or {}`
Lack of information is denoted by `None`.
Should not break if `published_at` is missing.
Any test case using this approach? I can't find it.
Also pass `m3u8_id='hls'`.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
`{}` won't work in python 2.6.
Won't work. See how this is done for output template.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
``` python course_id = self._search_regex( (r'data-course-id=["\'](\d+)', r'&quot;id&quot;: (\d+)'), webpage, 'course id') ```
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Breaks when `get_element_by_class` returns `None`.
Must be fatal.
No. Must be fatal. Read coding conventions on optional fields.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
Also `_valueless_option` is probably a better name for it.
Second check should be removed.
Yes, argumentless options handling should be moved in separate method as well: ``` python def _argless_option(self, command_option, param, expected_value=True): ... return [command_option] if param == expected_value else [] ```
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Carry long lines. Bother to finally read coding conventions.
You are **delegating** to brightcove that provides the id.
You must make it **not break** as you delegate.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Don't do this manually use `_download_webpage_handle`
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
`title` must be mandatory I've already told about this.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
```suggestion if not (playlist_files and isinstance(playlist_files, list)): ```
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
There are more video formats available that should be extracted as well.
This should be removed. `player_url` is used for RTMP.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
Move flags into regex.
`try_get`, single quotes.
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
Does not work as expected in all cases.
If `_search_regex` fails `None` will be passed to `_parse_json`.
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
both are know beforehand, so there is no need to use `urljoin`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use `utils.xpath_text` instead, again with `fatal=False`.
Everything apart from `url` is optional.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
there is not need for excess verbosity.
no longer needed.
incorrect URLs for Cook's Country.
Above, it's still possible that the JSON download works but doesn't result in a `dict`. So this would be better (setting `None` on error to help with the second point below): ``` status = try_get(info, lambda x: x['status']) ``` Then, if the API makes a breaking change without us noticing, is that `expected` or not? As the site is unlikely to revert the change, it becomes our bug and so not `expected`. I suggest `expected` should correspond to the API returning an actual status that is not OK, and nothing else, like so: ``` raise ExtractorError(status or 'something went wrong', expected=status not in ('ok', None)) ``` But you are obviously familiar with the API and I'm not ...
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
``` content_el = itemdoc.find(self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/')) duration = float_or_none(content_el.attrib.get('duration')) if content_el is not None else None ``` or ``` content_el = find_xpath_attr(itemdoc, self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/'), 'duration') duration = float_or_none(content_el.attrib['duration']) if content_el is not None else None ```
Use `compat_urllib_parse_unquote_plus` instead.
Use `self._match_id` instead.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Network connections in your browser.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Read coding conventions on mandatory metadata.
Using preferences causes invalid sorting.
do you have an example with TTML subtitles? all the videos that i've tested with has only VTT subtitles.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
This should be split into building url and extracting formats.
All of these will break extraction on unexpected data.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
will easily match outside the element.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
Use single quotes consistently.
m3u8 is also available.
What's the point of `# match self._live_title` here? Remove.
Breaks on `None` title.
Don't capture groups you don't use.
Query to `query`,
Instead of such hacks you can name group differently and capture it without any issue.
Don't capture groups you don't use.
All debug garbage must be removed.
```suggestion get_element_by_class, int_or_none, ```
* the first element of the path can have more than 1 digit * the tail `/player\.html` isn't needed (unless there are other tails that should find different media with the same id): ```suggestion _VALID_URL = r'https?://(?:www\.)?embed\.vidello\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)' ```
The playlists are public: there should be tests for them. Watch this space.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
No escape for `/`.
If these `,`s are thousands separators, might they be `.`s for some locales (plainly not a decimal point for a count)? ```suggestion view_count = str_to_int(self._html_search_regex( (r'<strong>([\d,.]+)</strong> views', r'Views\s*:\s*<strong>([\d,.]+)</strong>'), webpage, 'view count', fatal=False)) ``` (and `from ..utils import str_to_int` at the top).
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Then just keep this code and > create default fallbacks if extraction of these fails
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Avoid shadowing built-in names.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
you can get json output by appending `&format=json` to the api request url.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
There's no need to name a group if not used.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
`.*` at the end does not make any sense.
Read coding conventions on mandatory metadata.
1. No `{}`. 2. Inline. 3. Query to `query`.
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
This is already checked in `float_or_none`.
1. Do not remove fallback to previous duration value. 2. `scale` of `float_or_none` instead.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
`id` should not be optional. No need in trailing `/`.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
See how browser calls it.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
No need in these variables.
Playlist title is optional, description breaks.
You must make it **not break** as you delegate.
You are **delegating** to brightcove that provides the id.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
yes, this is correct.
Breaks extraction if `release_date[0:4]` is not `int`.
`title` must never be `None`.
Breaks extraction if `json.loads` fails.
Same as in some previous PR.
You should have only one single method for extracting info.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Don't do this manually use `_download_webpage_handle`
This is superfluous, the extension can be extracted automatically.
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Don't use `;` unless you need to write more than 1 statement per line (personally, I would also avoid doing that)
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
This will process the same URL twice overwriting the previous results.
The description is always optional, so there should be a `fatal=False` in here.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Just rethrow active exception.
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
Read and follow code conventions. Check code with flake8.
If `video` not in `media` empty formats will be returned that does not make any sense.
`for key, value in media.get('images', {}).items():`
No need to escape `]` is character set.
`'id'` is required.
Should not be fatal.
Never use bare except.
JSON should be parsed as JSON.
`_search_regex` per each field. Add fallback.
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
dict comprehensions don't work in python 2.6.
I've already pointed out: `.*$` is pointless at the end.
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
It does since there may be no postprocessing at all.
Opening message is already applied for `multi_video`, I don't see any reason for closing message not to be applied for it as well.
No `upload_date` is guaranteed to be present.
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Should extract chunklists via `self._extract_m3u8_formats` here.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Superfluous. Move directly into the method call.
Does not match `var IDEC='`.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
Breaks. Read coding conventions.
Breaks if no `name`.
Either sloppy code or an anti-scraping measure.
Audio is not 128.
`width` and `height` instead.
- - [ ]
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
You don't need list here. Just return it directly.
Outer parentheses are not idiomatic in python.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use single quotes consistently.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
1. Extract dict if you expect dict. 2. Relax regex. 3. Escape dots.
Again: float_or_none, not parse_duration.
DRY. url is mandatory.
Do not shadow built-in names.
Capturing empty string does not make any sense. What's the point capturing this at all? id and path occur only once in webpage.
Should not be fatal.
`video_id` is already a string.
No need in these variables.
See how browser calls it.
No exact URLs, use `re:`.
No exact URLs.
Use whitespace characters consistently.
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
The `list` call is superfluous here and can be safely removed.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
No. It must be a soft dependency. Message requesting installation of `AtomicParsley` or `mutagen` should only be output when thumbnail embedding is requested and neither of these dependencies is found.
Some extractors use the domain name as `IE_DESC`, so I guess it's OK.
Geo restricted. Test will always fail.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This does not matter, if you open https://youtu.be/BaW_jenozKc there is a video embedded. If you open https://redirect.invidious.io/watch?v=BaW_jenozKc there is no video embedded. >used for URLs floating over the net to share youtube videos. Prove that.
There is no video on this page.
`{}` does not work in python 2.6.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Oh, I see. Thanks!
Use default. Read coding conventions and fix code.
Why did you remove this test? It does not work with your changes.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
You should extract `partner_id` and `entry_id` and return `kaltura:...` shortcut.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
There should be an `id` group.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
Must be extracted first.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This should actually be just `self.url_result(embedded_url)`.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
`note` and `errnote` of `_download_json` instead.
This is fatal.
Breaks extraction if there is no `stream-labels` key.
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
`split` returns a list, video_id must be a string.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
Move before youtube-dl imports.
```suggestion timestamp = unified_timestamp(rights.get('validFrom')) ```
Don't touch the old test.
this should be done once(in `_real_initialize`).
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
Must be numeric.
This field is height not quality.
Nothing changed. Breaks in case regex does not match.
It also failed in your previous commit: https://travis-ci.org/rg3/youtube-dl/jobs/8459585. b64encode returns bytes, you must decode to get a string : `base64.b64decode(googleString).decode('ascii')`, I'm not sure since I have little experience with base64.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
This should actually be just `self.url_result(embedded_url)`.
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
```suggestion 'ext': ext, ``` fix flake8 check
Read coding conventions on optional metadata.
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
Breaks if not arr.
Won't work. See how this is done for output template.
This has no effect. Postprocessors work on info dict copy.
avconv does not support `-cookies`, use `-headers` instead. You should also pass all headers.
Do not pass `default=None` to `_html_search_regex` instead.
`title` must be mandatory I've already told about this.
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
Breaks on `default=False`. `title` must be fatal.
Breaks on `None`.
Code duplication. Must be single call to search regex.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Omit expected type.
All formats must be extracted.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
`_parse_json`. Read coding conventions.
No need to escape `/`.
`id` should be extracted via `_VALID_URL` when present.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Don't capture groups you don't use. Use proper regex to match all country codes.
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
Don't capture unused groups
`<span[^>]+class="name">...` is better.
Use bare `re.match`.
No need to escape `]` is character set.
- use single quotes consistently. - i think it would be better to keep errnote closer to note.
```suggestion self._API_BASE_URL + 'authentication/login', None, 'Logging in', data=urlencode_postdata({ ```
Inline everything used only once.
This should be split into building url and extracting formats.
Should check for a list.
Breaks if no videos in season.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`user_info` may be `None`.
`created_at_i` as `timestamp`.
No need to escape `/`.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
There is no need in this method.
Again: float_or_none, not parse_duration.
All these regexes should be relaxed.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
All formats must be extracted.
Read code conventions on optional fields.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
Allow arbitrary whitespace and both quote types.
Move everything into `_download_webpage`.
`expected_status` to `_download_json` instead.
What's the point of this extractor? It's covered by album extractor. Remove.
This should be removed.
Yes, if it tests the same extraction scenario. There is only one extraction scenario in this code.
will break if `item` is `None`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
More ways to get `title`: ```suggestion title = ( self._og_search_title(webpage, default=None) or get_element_by_class('my_video_title', webpage) or self._html_search_regex(r'<title\b[^>]*>([^<]+)</title\b', webpage, 'title')) ```
Inline all these.
This will result is `[None]` is no category extracted.
This will break extraction if no `id` present.
`.*/?` is pointless at the end.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
```suggestion # coding: utf-8 from __future__ import unicode_literals ```
Coding cookie is only required for non-ASCII sources.
```suggestion try_get, url_or_none, ```
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
`not json_lds` already does the second part.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
I think that this line would produce unexpected results with multiple urls. The first one would use the generic extractor and the rest would use the normal extractors. (I may have misunderstood it)
No `upload_date` is guaranteed to be present.
I've already pointed out this does not work.
`id` is of arbitrary length.
1. `id` is **not necessarily** 3 digits. 2. `id` must not contain irrelevant words.
All debug code must be removed.
If either of these attrs is missing whole playlist extraction is broken.
This should not be here as done by downloader.
No direct URLs in tests.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
Single loop for all sources without any unnecessary intermediates.
Matching empty data is senseless.
Must not be `None`.
Breaks on `None`.
Move into `_download_json`.
Do not use `sanitized_Request`.
Do not use sanitized_Request.
Title part should be optional.
Code duplication 173, 213. There is no sense to extract fields explicitly.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
there is not need for excess verbosity.
no longer needed.
will return invalid URL if `search_url` is `null`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All methods only used once should be explicitly inlined.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
The semantics of tests should be kept. This test should test `*-videoplayer_size-[LMS].html` URL. Same for others: `*-videoplayer.html`, `*-audioplayer.html` (on two different domains).
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
`self._parse_html5_media_entries` for formats extraction.
Should not be fatal.
`True` is default.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
Couldn't we simply look at the extension (below) to get the format map? Then we don't have to update this list.
160 is a video format.
and 139 is a audio format
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
Use `compat_urllib_parse_unquote_plus` instead.
Code duplication in 70-102 and 104-137.
This field is added automatically no need to add it by hand.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
This TODO needs work
PEP8 mandates two empty lines here.
This line does not help and is not necessary.
Side note: Wow, these guys are military, but don't support https? Oh my...
Don't capture groups you don't use.
Don't capture groups if you are not going to use them.
`/?` does not make any sense.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
All these regexes should be relaxed.
Must be int.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
eg find type by URL ext
`expected_status` to `_download_json` instead.
Read coding conventions on optional/mandatory meta fields.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
This won't skip empty strings.
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
Debug code must be removed.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
The code should match the content. If it's not possible to figure out the lang code there is no point placing it in subtitles. Moreover it's not time bound so can't even barely be treated as subtitles.
It's not always an .lrc since it does not always follow the format and not bound to time tags.
This is not always the case. Here is the `en` lyrics http://y.qq.com/#type=song&mid=001JyApY11tIp6.
Do not touch the old patterns.
This test is identical to the first. Revert.
Lack of data must be expressed by `None` not empty string.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
This should go into `YoutubeDL.py`
-`images that appear when hovering the cursor over a video timeline` This may not apply to other sites.
Not having archive != having dummy `Archive` object.
Don't capture groups you don't use.
`.*/?` is pointless at the end.
Don't capture unused groups
There is already an extractor with such `IE_NAME`.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
No `(?i)`, no `$`. Dots must be escaped.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
Breaks on `None`.
Don't shadow built-ins.
Unite in single list comprehension.
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
If `_search_regex` fails `None` will be passed to `_parse_json`.
It seems you aren't using the module, remove this line.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
Doesn't match https://www.servus.com/de/p/Die-Gr%C3%BCnen-aus-Sicht-des-Volkes/AA-1T6VBU5PW1W12/?foo=bar. Should not match https://www.servus.com/de/p/Kleines-Geschenk-Set-Anti-Stress/SM129658/.
What are you even trying to do? `'1'` that's all.
Move to base class.
Do not match by plain text.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Harmonise with yt-dlp pt4: ```suggestion }] ```
Harmonise with yt-dlp pt3: ```suggestion ```
Just rethrow active exception.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
The argument will already be a character string, no need to decode it.
Since you now skip already downloaded segments the total fragment message displays wrong value after restarting: `[hlsnative] Total fragments: ...`. As a result - wrong download progress data.
121, 124 - DRY.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
do not use names of Python built-in functions(https://docs.python.org/3/library/functions.html#format).
`formats` should be sorted.
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
All these regexes are only used once thus make no sense as separate variables.
It should not. See the description of the field.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Again: relax regex.
It's a field name not a step name.
Lack of information is denoted by `None` not `0`.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
You should add support for this playlist-alike 3qsdn URLs in any non-breaking way.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
Non fatal, proper prefix id.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Since this extractor by itself can't provide too much info, maybe it would be better to remove the _real_extract and _VALID_URL, don't import it in `__init__`and do something similar to`MTVServicesInfoExtractor`, which is the base class for the extractors that need it.
Inline all these.
You could consider using the library routine `parse_resolution()` (`utils.py`) ``` thumb_info = parse_resolution(res) thumb_info['url'] = base_url.replace(replace, res) thumb_info['preference'] = n # as above thumbnails.append(thumb_info) ```
Breaks if no `rate` key in `stream`.
There is no point to use `get` here.
`if not videos:`.
`ad_free_formats` is never empty here since `_sort_formats` will throw if's empty.
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
Don't change extractor name.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Pass `default` to `_og_search_title` instead.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
default and fatal are not used together.
Will never happen since it does not throw if not fatal.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Already pointed out: must be `ExtractorError`.
No. It's a job if the video extractor.
Network connections in your browser.
Query should be passed as `query` parameter.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
The same. Should use https if ```url``` use https.
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Add a rationale for that.
Consistently use single quotes.
Usually display_id is used before the actual video_id is extracted.
Must only contain description.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Rename to something else.
`url` should not be `None`.
You must delegate with `url_result` instead.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
`compat_str()`, here and in l.96.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
The order of dictionaries is not deterministic before Python 3.6. For example: ``` $ PYTHONHASHSEED=0 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: ok ---------------------------------------------------------------------- Ran 1 test in 0.004s OK $ PYTHONHASHSEED=1 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: FAIL ====================================================================== FAIL: test_dfxp2srt (__main__.TestUtil) ---------------------------------------------------------------------- Traceback (most recent call last): File "test/test_utils.py", line 1093, in test_dfxp2srt self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data) AssertionError: u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... != u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... Diff is 663 characters long. Set self.maxDiff to None to see it. ---------------------------------------------------------------------- Ran 1 test in 0.005s FAILED (failures=1) ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
will break if `item` is `None`.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Always return a playlist.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Don't add list items if 'url' is None.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
`--no-playlist` is not respected.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
The idiomatic way to extract `id` is to use a group in `_VALID_URL`.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
What's the point of this? `canonical_url` is the same as `url`.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
`(server_json.get('id') or '?')` (or some other default value). Or server_json.get('id', '?') Similarly l.128.
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
Shouldn't be fatal
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
There is no need in this method.
All these regexes should be relaxed.
Playlist title is optional, description breaks.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Remove unused codes.
Remove debugging codes.
Use `self._sort_formats(formats)` instead.
46-47 code duplication.
`self._search_json_ld` should be improved instead.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
End users do not read source codes thus will never find this advice.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
This should be just `return info`
Same as above, we should create a `formats` array here.
We now have a fully-fledged format system which can be used.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
You should actually keep this line and change https://github.com/costypetrisor/youtube-dl/blob/autonumber_start/youtube_dl/YoutubeDL.py#L296, otherwise it won't be incremented.
You'll have to provide a default value for `autonumber_start` (like 1), because when using the youtube_dl module from python it will usually be missing.
This will break `--max-download`.
Not quite sure. Currently it redirects to `pornhub.com`. Possibly this was not the case in the past.
Should also match `pornhubpremium.net`. Or extractor should not match `pornhubpremium.net`.
You must provide account credentials/cookies for testing.
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
Use `self.playlist_result` instead.
This should be removed.
`{}` won't work in python 2.6.
This is determined automatically.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
should not return empty formats.
The indentation is messed up here, it should be 4 instead of 2 spaces. You may want to get a better editor - a modern editor should take care of indentation automatically.
The indentation is messed up here, it should be 4 instead of 3 spaces.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
Do not capture empty strings.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Trailing /? is not necessary here
Same question for 'contains'
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
Of course when it appears inside `script`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
Request wrapping code can be moved to the base class.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Lack of data is denoted by `None` not `0`.
This should be split into building url and extracting formats.
Same as in some previous PR.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
This TODO needs work
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
Remove all garbage.
**Do not remove** `_search_regex` part.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
To access both groups from the URL match, use `mobj = re.match(self._VALID_URL, url)` (`import re`) and then access the matches as `video_id = mobj.group('id')`. In this case the fact that the extractor is running means that the `id` and `display_id` groups matched. If you had an optional group, like `(?P<display_id>.+)?`, something like `display_id = mobj.groupdict().get('display_id')` would be appropriate.
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
Better to use unified_strdate for parsing dates.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
This is already imported and (in general) you should only use `import`s at the top level.
`{}` doesn't work in python 2.6.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
This does not make any sense, you already have `url`.
Relax `id` group.
Do not capture empty strings.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
You have some unmerged lines here
Remove all garbage.
This does not necessarily mean that. There is a clear captured error message that should be output.
This makes no sense - you already have it in `url`.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
For now this should not be printed or only printed in `verbose` mode.
This could be moved several lines up.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
This should be extracted right from `_VALID_URL`.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
There is no point in `or None` since `None` is already default.
Title is mandatory.
`int_or_none` and `float_or_none` for all numeric fields.
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
`strip_or_none` no longer needed.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
Add `fatal` flag.
`fatal` must be added to `_extract_info`. No changes to core code.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Avoid unrelated changes.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Should not be fatal.
Should be tolerate to missing keys in `media`.
Query to `query=`.
Must be separate extractor delegating to CNBCIE.
First group is superfluous.
Must not be `None`.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
Title is mandatory.
None of the optional fields should break the extraction if missing. Read new extractor tutorial.
Has no effect on hls.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
Do not capture groups you don't use.
It should match all non empty domain names.
Should not allow empty 3rd level domain. Should not be greedy. Inner group is superfluous.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This is already fatal.
Should not be fatal.
Better to use `determine_ext` instead of `.endswith`
Breaks on missing file key.
Here you must use `urljoin`.
try_get is pointless here.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
This is never reached cause sort formats will throw on `not formats`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
`()` is a better 0-length iterable than `""`, which implies text.
Actually I think the line I fingered was fine, but the replacement's fine too.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
I've already suggested how to cover both scenarios without getting any error. > Looks like old videos with **5 digit length video id** are still available via xstream in much better quality than vgtv. New video ids are 6 digit length.
Looks like old videos with 5 digit length video id are still available via xstream in much better quality than vgtv.
Use formatted strings.
Do not shadow existing variables.
According to the [W3C HTML5 syntax spec](http://www.w3.org/TR/html5/syntax.html), using `'\s'` to match whitespaces is better here.
Don't add list items if 'url' is None.
`args` may be `None` here.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Something like this (not tested): ```python args = self._downloader.params.get('postprocessor_args') if args is None: return default if isinstance(args, (list, tuple)): # for backward compatibility return args assert isinstance(args, dict) pp_args = args.get(pp_key) if pp_args is not None: return pp_args pp_args = args.get('default') # for backward compatibility if pp_args is not None: return pp_args return default ```
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
TODO: do ;)
This value looks an awful lot like a `display_id`
I meant a _webpage that uses embed.ly to embed some non-video content and also contains a video that is detected by code after this_. I concur that the best action would be to wait for a test case, and then decide how we can exclude it or improve some other code.
Broken python 3. Must be bytes. `urlencode_postdata`.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
`try_get` is useless here.
This fails on Python 2. (duh!) Instead, we should fix SSL support in general.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
Should not be greedy.
That's very brittle.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
do you have an example with subtitles.
if the `ext` can't be detected than fallback to `vtt` as it's the `ext` that most likely to be.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
`{}` won't work in python 2.6.
It's better to fail instead of fallback.
Should be more relaxed.
Pass video id.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Move data and query into `_download_webpage` call.
`False` is not a valid note.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
`\s*` make no sense at the end.
There is no point checking `url`.
No point checking this.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
There's no need to use `u` prefix given `unicode_literals` is declared.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Breaks on None.
No escapes for slash.
This is never reached cause sort formats will throw on `not formats`.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Read coding conventions on how mandatory data should be accessed.
What's the point? It's not alphabetic altogether anyway.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
No newline at the end of file.
All video formats must be extracted.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
Duration calculation is incorrect.
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
`default` implies non `fatal`.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Just `video_url = urljoin('https://ndtvod.bc-ssl.cdn.bitgravity.com/23372/ndtv/', filename)`.
1. This will never be reached. 2. Don't change the order.
Each test the same scenario = duplicate.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
Use `self._search_regex` and `utils.unified_strdate` instead.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
`default=None` for the first.
Both used only once, move to the place where used. Also relax both regexes.
it better to extract all the urls in the `mpath` array.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
`ExtractorError` is not raised when `fatal=False`.
Breaks if not arr.
No need to escape whitespace.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
`/?` does not make any sense.
Don't capture unused groups
Do not carry dict values.
Breaks on None.
No escapes for slash.
Nothing changed. Also there is a video id available in JSON.
It should match at least one character.
extraction must be tolerate to missing fields.
This should be extracted first.
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
No hardcodes. Use API.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Code in bracket is a set of matching characters. `or`ing won't work.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
Don't shadow built-in names.
Already pointed out: must be `ExtractorError`.
Will break if `picture_url` is `None`.
Use `self._html_search_meta()` instead.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
Never ever use bare except.
Playlist title is optional, description breaks.
Extraction should be tolerate to missing fields.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
This worked for me.
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks. Read coding conventions.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
1. Relax regex. 2. Do not capture empty dict.
Omit expected type.
Invalid arguments for 4-5.
this will fail if `type` is not present.
you can iterate here using `values` method and make it in a single line without checking if `Item` is present: ```python for metadata in video_data.get('__children', {}).get('Item', {}).values(): ```
no need to create a method when it will be used once.
This is only used once.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
`{}` won't work in python 2.6.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
I've already pointed out: I won't accept this.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
The URL should have been `compat_str` already, so the regex groups will be too.
Do not escape quotes inside triple quotes.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
No such meta field.
For `url` type any metadata here have no effect.
Query should be passed as `query` parameter.
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
Move flags into regex. Regex should match `runParams={`.
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
`default=None` for the first.
Same for re.search
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
Captures `?#&` ending as id.
`None` is not an id.
Use raw strings.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
This is not a generic embed.
This is too broad. It must not capture plain text URLs.
The other case was some legacy code. I don't see much sense in such messages since it's clear what extractor is delegated to since all messages from the final extractor are prefixed with `IE_NAME`.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
These 2 lines could be replaced by: ```python uploader_url, creator = creator_data[0][0:2] ```
Remove useless code.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
Also looks like they redesigned the site so that extractor does not work any longer.
Matching empty id is senseless.
Although I think you may have taken "specific" more literally than I intended!
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
By the way, the pythonic way is to just evaluate `thumb_list`
Remove useless code.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
metavar should be `FILE`. `type` is already string by default.
Just `File to read configuration from`.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Extractor should not return `None`.
Relax `id` group.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Escape dot. No need to split URL.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
I'm addressing this concrete line of code.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
```dict_get``` makes codes even shorter
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
87-90 code duplication.
Too broad regex.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
yes, remove duplicate formats if the qualities are not available for all programs.
from first test of `SverigesRadioPublicationIE`: https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=low&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a32.m4a", "duration": 132, "codingFormat": 12, "state": 0, "isGeoblockEnabled": false } ``` https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=medium&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a96.m4a", "duration": 132, "codingFormat": 13, "state": 0, "isGeoblockEnabled": false } ```
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
Right, that's fine for `display_id`.
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
Playlist title is optional, description breaks.
No, it's not the point. It **must not** skip items than belong to the range specified.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
That's completely different videos.
Ids must stay intact.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Usually display_id is used before the actual video_id is extracted.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Carry long lines.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
Instead of such hacks you can name group differently and capture it without any issue.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
Groups around `video` and `sptv/spiegeltv` are superfluous.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`.*/?` is pointless at the end.
The URL should have been `compat_str` already, so the regex groups will be too.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
1. No umask respected. 2. There is no `os.chmod` in python 3.2 according to python [docs](https://docs.python.org/3/library/os.html#os.chmod). 3. flake8.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
`--ap-mso` should not be touched since it's a separate stand-alone mechanism that has stable unique MSO identifiers for TV providers across all extractors while this options is not.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
This is pointless, you don't have any fallback.
56-71 code duplication.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
will return invalid URL if `search_url` is `null`.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
We should use the various `compat_*` down below and delete these imports, so that the code also runs on Python 3.
Do not capture groups you don't use.
`https?://` is better in this case. openload supports both HTTP and HTTPS.
Do not shadow existing variables.
You have some unmerged lines here
`{}` doesn't work in python 2.6.
The class name should ends with FD
The number 25 should be configurable
Don't capture groups if you are not going to use them.
Again: relax regex.
It's a field name not a step name.
Relax regex, make group unnamed, don't capture empty dict.
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
Not all URLs contain video id.
Should be `display_id`.
No, it's not. If video_id extraction from page fails whole extraction fails.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
```suggestion if episodes: ```
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Use _parse_json and js_to_json here
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
This introduces an ambiguity in case of several mso available. That's why it won't be accepted.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
Never use bare except.
No need to escape forward slash.
Instead of `resolution` and `preference` it should be extracted as `height`.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
All these regexes should be relaxed.
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
`_search_regex` is enough.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
Query should be passed as `query`.
Looks like, it's off then.
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
`_match_id`. Do not shadow built-in names.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
No trailing `$`, override `suitable`.
No escapes for slash.
Capture with /album. Capture non greedy.
`for k, v in flashvars.items()`.
`flashvars[k]` is `v`.
Must be numeric.
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
26-29, 32-37 code duplication.
Use self._report_warning instead
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
Actually, it's an opposite. It's a check for successful login.
`ext` should be mp4.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
It's obvious from `'is_live': True`.
What's the point of `# match self._live_title` here? Remove.
Again: relax regex.
If there's only one format, just use `'url'`.
All formats should be extracted.
Use `self._search_regex` and `utils.unified_strdate` instead.
first_three_chars = int(float(ol_id[:3])) is cleaner
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
If `_search_regex` fails `None` will be passed to `_parse_json`.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
OK. I'll open an issue for discussing this. For now you can remove this line.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
falback to a static URL.
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
Sure, that would be fine.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
Or at least, it's not `unicode`. In yt-dl `str` should almost always be `compat_str`, but as above it's not needed here.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
This fails on python 3 and doesn't look too good.
You can import `try_rm` from helper
Title is mandatory.
No need to use named group when there is only one group.
`player_id` is not extracted in this fallback but used at 71.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
`if height and filesh:`
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Optional data should not break extraction if missing. Read coding conventions.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
What's the point of this? Remove.
You should capture a part of URL that represents a video in unique way...
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
I think you don't need `locals()` here.
This can be moved inside `if chapters:` condition.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
OK. I'll open an issue for discussing this. For now you can remove this line.
These looks like candidates for generalization and extracting into a separate method.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Breaks if no `name`.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Do not match by plain text.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
This will break extraction if no `id` present.
Must be `list`.
This will result is `[None]` is no category extracted.
Again: no, do not touch token extraction code. Just eliminate duplication.
Again: you must eliminate code duplication. In both places.
Can't be None.
just use a hardcoded value for now.
accept `audio` `mediaType`.
use `query` argument.
Just put original texts: æ§ããã«è¨ã£ã¦é å¼µã£ã¦ã
url and formats are not used together.
No trailing $, override suitable.
No trailing $, override suitable.
All these regexes should be relaxed.
Must be int.
Bitrate should go to corresponding format meta field.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
No trailing $, override suitable.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
I've already suggested using `Downloading` as idiomatic wording.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
Sorry I didn't check it, it should look into `<div class='more_info'>`
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
`title` must be mandatory I've already told about this.
The whole code until here can be simplified to `page_id = self._match_id(url)`
This looks like `orderedSet(m.group(1) for m in re.finditer(r'href="/video([0-9_]+)"')`. Also, since it only gets called once, feel free to move it in the main function.
Why is the `m?` group in here? If it's optional anyways, you can just leave it out ;)
Again: it must contain `vbr or abr` if available.
The starting part that is already used for it.
This should be fixed as well.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Use `self._parse_html5_media_entries` instead.
Sorry, we actually request 10KiB, and `K` does stand for Kilobyte in head.
`K` in head stands for `Kibibyte`, the test uses Kilobytes.
`id` and `display_id` should be tested as well.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Using preferences causes invalid sorting.
Breaks if no `rate` key in `stream`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
`flashvars[k]` is `v`.
`for k, v in flashvars.items()`.
This field is height not quality.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
Don't shadow built-in names.
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
`/?` does not make any sense.
Do not carry dict values.
Do not capture empty strings.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
Optional fields should not break extraction if missing.
`--rm-cache-dir` wipes the whole cache thus should never be suggested to use.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
This prevents from authenticating with `--cookies`.
This video is georestricted.
Use `self._match_id` is better.
There's no need to name a group if not used.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need for this check, this is already checked in `_sort_formats`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
You should have only one single method for extracting info.
170-172 - code duplication.
`title` is mandatory. Move flags into regex itself.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
These looks like mandatory fields.
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
Real id is in widget dict.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
1. Don't shadow outer names. 2. `url_or_none`.
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
Must not be `None`.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
If both test the same extraction scenario leave only one.
Don't capture groups if you are not going to use them.
Don't capture unused groups
No need to escape `/`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
26-29, 32-37 code duplication.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
Do not shadow `url` variable. `fatal` has no effect when `default` is provided.
Code duplication at 58-60 and 68-70.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
No need to escape `#`. No need to capture groups you don't use.
`int_or_none` and `float_or_none` for all numeric fields.
Use regular string format syntax instead.
`/?` makes no sense at the end.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
I guess something like `base_url` is a better name than `url`.
I guess using `content_type` instead of `ct` improves readability.
`r'(?s)(<(?P<tag>video|audio)[^>]*>)(.*?)</(?P=tag)>'` is better as numbers may change in the future.
It's better to fail instead of fallback.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
Playlist title is optional.
`_parse_json`. Read coding conventions.
Will never happen.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
OK. I'll open an issue for discussing this. For now you can remove this line.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
`'id'` is required.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Breaks on missing key, breaks on download failure.
As already said: no trailing $. Override `suitable`.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Do not escape `/`.
Read coding conventions on optional metadata.
this will be done for you by just providing `width` and `height`
The most simple way is to use `utils.extract_attributes`.
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
Unless it supports videos from other sites, the IE name already says for what it is.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `video_id`. Remove `.replace('\n', '')`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Breaks extraction if no `followBar`.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
`default` implies non fatal.
Place right here in the following order: 1. If no title extract with `_og_search_title`. 2. If still no title extract from `<title>`. 3. Extract `playlist_description`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Also do not use double quotes for string literals.
Don't shadow outer names. No need for bracket when using single character.
Matching between quotes is incorrect.
Do not remove existing tests.
1. This will never be reached. 2. Don't change the order.
All these regexes should be relaxed.
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
I guess it a typo? now -> not
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
Weird. Yesterday it was working: ``` python -m youtube_dl http://mobile-ondemand.wdr.de/CMS2010/mdb/ondemand/weltweit/fsk0/75/752868/752868_8108527.mp4 [download] Destination: 8108527-752868.mp4 [download] 1.4% of 291.34MiB at 8.53MiB/s ETA 00:33 ERROR: Interrupted by user ``` But not now.
[Use `utils.qualities` instead](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/npo.py#L123).
still the check for element class is missing.
The third parameter of `_html_search_regex` is name but not ID.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Use `self._match_id` is better.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
I suggest extracting from `mediaLink` element instead of matching the JS URL as it may change in the future. For example: ``` Python media_link_obj = self._parse_json(self._html_search_regex( r'class="mediaLink\b[^"]*"[^>]+data-extension="([^"]+)"', webpage, 'media link'), display_id, transform_source=js_to_json) js_url = media_link_obj['mediaObj']['url'] ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
1. This must only take place when it's not available from player JSON. 2. Query must be passed as `query`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
As already pointed out: you must delegate to `SBSIE` extractor not inherit from it.
Not acceptable. Search `url_result`.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
No exact URLs here.
Do not capture groups you don't use.
Query to `query`.
Don't carry URLs. Read coding conventions.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Name an example URL where `og:description` has HTML tags.
`player_id` is not extracted in this fallback but used at 71.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
`(byte_counter / rate_limit) - elapsed` sometimes takes negative values (tested on python2). Negative delay to `sleep` results in `IOError` and failed download. There should be at least a check for that.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
That's all correct code [53-93], do not remove it. Fix `add_m3u8_format` instead.
Should not be fatal.
This is already fatal.
No such meta field.
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
no need to set `fatal=True`, this is the default.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
Do not remove the old pattern.
No escape for `/`.
Use `\s*` instead.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
As already said: no trailing $. Override `suitable`.
It `md5` flag should not go here as an argument. Instead it should be extracted in particular downloader from `self.params`.
Calculate it in chunks after the final file is on disk. As said it makes no sense to calculate it immediately since the final file may be modified by postprocessor.
This is missing a `cwd=` spec at the latest. If we need a git revision number, we should really think about releasing more often instead.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
`playlist_description` can be extracted from the same data.
This is already fatal.
Should not be fatal.
Capturing empty URL is senseless.
parentheses not needed.
fallback to other available values.
use single quotes consistently.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
All debug garbage must be removed.
You have some unmerged lines here
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
There should be an `id` group.
Use the original scheme and host.
Don't shadow built-in names.
Breaks on `None` title.
Inline everything used only once.
Do not match by plain text.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
It won't be longer.
Revert. >Checking download_**url** video format **URL** makes even less sense.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
Plain `for` is enough.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
- - [ ]
Do not match exact URL.
No exact matches for URLs.
No such field.
No such field.
This should be split into building url and extracting formats.
Python 3.2 doesn't like u-literals.
Should not break if missing.
Optional fields should not break extraction if missing.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Capturing empty string is senseless. `\n` instead of `root[__env]`.
Use raw strings.
Breaks if no videos in season.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
yes, this is correct.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
`/?` is senseless at the end.
This branch is never reached.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
You have some unmerged lines here
``` for i, video_url in enumerate(video_urls): ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
`compat_str()`, here and in l.96.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
This line can just be removed.
170-172 - code duplication.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Don't shadow built-ins.
fatal=True is default.
What's the point of this? Use `url` as base.
Query to `query`,
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
Move data and query into `_download_webpage` call.
Any test case using this approach? I can't find it.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Also pass `m3u8_id='hls'`.
It should be robust in case of some missing fields.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
No bare except.
It's better to use `self._download_webpage(url, video_id)`
The argument will already be a character string, no need to decode it.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Allow arbitrary whitespace and both quote types.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
160 is a video format.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
It's already extracted as video_id.
Request wrapping code can be moved to the base class.
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
Don't change extractor name.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
Carry long lines.
Conversion between different date formats is redundant. Just return Unix timestamps.
Whats the point reconstructing the URL? You already have it in `url`.
It does not necessarily mean that.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
This is pointless, you don't have any fallback.
`True` is default.
You should not call this yourself instead you should define `_GEO_COUNTRIES`.
Playlist extraction should only take place after no video formats found.
This change breaks MTVServicesEmbeddedIE
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
Do not remove existing tests.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Change double quotes to single quotes
OK. How about creating one base class, define the outside functions there, and make derivative classes for playlists that return `playlist_result()`. This way you'll reduce the boilerplate code
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Part after `\?` should be removed since it's not used anymore.
This should actually be just `self.url_result(embedded_url)`.
`'%s'` is very unlikely to be a helpful error message.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Don't capture unused groups./
Must be separate extractor.
Instead of such hacks you can name group differently and capture it without any issue.
This is checked by `_search_regex`.
Use display id.
`default` is not used with `fatal`.
This should be in `_real_initialize`. Same for all other occurrences.
You must provide account credentials/cookies for testing.
`acodec == 'none'`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
No brackets needed.
This is no longer actual.
This is useless at the end.
This must be an id of the media.
Pattern should include `<iframe` part.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
Sorry for the late response. This check may give a false alert if someday afreeca.tv decides to use a different name than `./track/flag`. `./track/video/file` is more reliable. The overall extraction workflow should be: 1. Check `./track/video/file` 2. Raise an error if no entries 3. Check other fields
If there's only one format, just use `'url'`.
then I think it would be better to use `fatal=False` instead of `default=None`.
looking again at this, there are multiple problems with the `og:published_time`, the timezone is important to calculate the correct timestamp so the `og:published_time` value for embeds is malformed, and also there is a discrepancy between the values from the video page and the values from the embed page, so unless there is a way to determine the correct value, it might be better to drop it.
> so I guess I'll remove it at that's it. right? yes, it would be better to remove it.
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
Again: it's not part of the video's title and must not be in the title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
` - Servus TV` should not be in title.
This does not match anything.
I've already pointed out: `.*$` is pointless at the end.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
No need to escape forward slash. Should be `https?`. Part after `(?P<id>\d+)` should be optional since `id` is only used for extraction.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
Why did you remove this test? It does not work with your changes.
Don't touch the old test.
Must be separate extractor.
Again: you should not bother with that. It will be automatically extracted from the download URL.
1. This will duplicate source format when multiple non source formats are available. 2. Source format must be named `source`.
Just replace the part appended to non source formats with empty string.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Code duplication in 142-145 and 146-149.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
should not return empty formats.
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
Such cases should be handled, too. I guess a possible approach is creating a table with common video and audio codecs. If given codecs are not on the table, fallback to `video,audio` order.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
And this as well.
`.*/?` is pointless at the end.
```suggestion get_element_by_class, int_or_none, ```
This check is not necessary now as you test pathconf anyway.
Use `utils.get_filesystem_encoding` instead.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
All similar tests should be `only_matching`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
Better to have some URLs in _TESTS with ```'only_matching': True``` for changes to _VALID_URL
create a seperate extractor.
`[]` is useless.
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
Use `\s*` instead.
Else branch is useless.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
This regex does not make any sense.
Network connections in your browser.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Use `compat_urllib_parse_unquote_plus` instead.
Unnumbered placeholders are not supported in Python 2.6.
Avoid shadowing built-in names.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Move it right after `title = info['title']`.
`utils.int_or_none` should be used instead.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
All formats should be extracted.
`self._search_regex` is enough here.
Matched data-video should not be empty.
If `_search_regex` fails `None` will be passed to `_parse_json`.
It should not. See the description of the field.
Breaks if not arr.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
hls and rtmp are available as well.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
do not capture groups that you're not using.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
54-58, 71-75 code duplication.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
All formats should be extracted not only mp4.
could you add description from perex key
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
All methods only used once should be explicitly inlined.
This may change as well. Add a fallback that just processes all videos without differentiation.
**Do not remove** `_search_regex` part.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
Does not match https://narando.com/r/b2t4t789kxgy9g7ms4rwjvvw.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Also pass `m3u8_id='hls'`.
`ext` here makes no sense.
Correct field name is `format_id`.
`.*` on both ends of regex make no sense. Also when capturing at least one character is required - there is no sense capturing empty video id.
...and output it here instead of const string `video_id` here that makes no sense.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
What's the point of this base class? It's only inherited once.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Breaks if no URLs extracted.
Escape dot. No need to split URL.
and the function that normally used to encode postdata is `urlencode_postdata`.
both are know beforehand, so there is no need to use `urljoin`.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Query to `query`.
should not return empty formats.
Do not capture groups you don't use.
Let's do it, then, if you like.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
As in, me personally? I think I knew that, but protocol.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
`/?` is senseless at the end.
Breaks. Read coding conventions.
No brackets needed.
This is no longer actual.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
Does not match `var IDEC='`.
Also pass `video_id` since it's known beforehand.
Use bare `re.match`.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
Matching empty data is senseless.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Default part should be removed since there is no default.
Single quotes. `item` is already a string.
Request wrapping code can be moved to the base class.
Recursion should be replaced with plain loop.
It worth adding a doc string.
This doc string does not match the function now.
It should always return a list.
No trailing `$`, override `suitable`.
Capture with /album. Capture non greedy.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
It's not an album id.
Not a video id.
Dot is pointless here.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
`vcodec` to 'none'.
Put `raise` after `if` section at the same indent.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
`parse_duration()` should work.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Possibly referenced before assignment.
This should be removed.
Use `self.playlist_result` instead.
That should work. Open a new PR with two commits: - `[myspace] Use play_path for faster download` - `[myspace] Add extractor for albums` (it could be done in this PR but you would need to use `git rebase` and `git push --force`)
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
I've already pointed out - title **must be fatal**.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Breaks extraction if no `followBar`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Use `video_id`. Remove `.replace('\n', '')`.
`{}` does not work in python 2.6
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`skip_download` is needed for the test to pass similar to the first test.
Must be extracted first.
Should not break if there is no `resolution` key.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`<span[^>]+class="name">...` is better.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
This is not true at the moment.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Option should be called something like `--date-playlist-order` and should accept `asc`, `desc` or `none`. Code should process playlist according to it.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Invalid syntax at all.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
Use `\s*` instead.
All these fields should be `fatal=False`.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
Real id is in widget dict.
Should not break if there is no `resolution` key.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
No brackets needed.
This is no longer actual.
This is useless at the end.
Do not capture groups you don't use.
Groups around `video` and `sptv/spiegeltv` are superfluous.
only_matching, move to the end.
Make it match URLs from the first post obviously.
This no longer matches http://v.youku.com/player.php/sid/XNDgyMDQ2NTQw/v.swf and http://player.youku.com/v_show/id_XMTc1ODE5Njcy.html.
Don't capture groups you don't use.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
1. `v` may not be dict. 2. `v.get('slug')`.
`video_data` is totally useless. Write directly to id variable when found.
Extraction should be tolerate to missing fields.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Optional data should not break extraction if missing. Read coding conventions.
try_get is pointless here.
Mandatory. Read coding conventions.
Id from URL is not always a video id. Correct id is in JSON.
No need to escape `{}`.
It should be robust in case of some missing fields.
Matching empty data is senseless.
Must not be `None`.
`id` by no means should be `None`.
Title is mandatory.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Regex should be relaxed. Dots should be escaped.
Remove all debug output.
`video_id` may be `None`.
`note` and `errnote` of `_download_json` instead.
This is fatal.
What's the point of lines 104-108? `ext` is already flv.
Breaks if no URLs extracted.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
Just modify mimetype2ext rather than introducing hacks in individual extractors
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
Breaks if not a list.
Outer parentheses are not idiomatic in python.
You don't need list here. Just return it directly.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
Should not break if there is no `resolution` key.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Using preferences causes invalid sorting.
That's completely different videos.
Better to use integers for supported_resolutions and use str() here
Dots should be escaped
Use ```query``` parameter of ```_download_webpage``` instead.
Whether it has changed or not does not mean there should be a format with invalid URL.
Must not be `None`.
`field_preference` must be a list or a tuple.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
You should be able to replace those 2 lines with ```python3 subtitles[s['language']].extend({'url': s['url'], 'ext': s['category']} for s in ysubs) ``` . Untested but it should become a generator which gets implicitly iterated, which should be faster than appending 1 by 1.
Doh, somehow I didn't realize the ref to `s` didn't exist if switching to that. My bad. Will need to use PyCharm 1st ...
Is it really required for a single video URL? All tests you provided end up with empty `data` array and construct `playlist` it from [the original `url`](https://github.com/hlintala/youtube-dl/blob/yle/youtube_dl/extractor/yle.py#L109).
This breaks streaming to stdout.
Add a rationale for that.
These are not used.
Oh, ok. That makes sense. Shoulda read the statement above it.
Here `if episode_json is False` can/should be replaced with `if not episode_json`, which is also the way you're doing it later.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
This must be done right after title extraction.
This is never reachable.
Playlist title is optional.
`{}` won't on python 2.6.
Read coding conventions on how mandatory data should be accessed.
Must not be fatal.
If ```options[0]``` _is_ ```{```, options should be returned.
1. ```options``` should be escaped - ```re.escape``` 2. Nested variables won't be detected. For example: (from https://developer.jwplayer.com/jw-player/docs/developer-guide/customization/configuration-reference/) ``` var config = { "playlist": [{ "file": "/assets/sintel.mp4", "image": "/assets/sintel.jpg", "title": "Sintel Trailer", "mediaid": "ddra573" },{ "file": "/assets/bigbuckbunny.mp4", "image": "/assets/bigbuckbunny.jpg", "title": "Big Buck Bunny Trailer", "mediaid": "ddrx3v2" }] }; ```
Sorry for previous noises. I just actually checked the HTML source of your example and found that this PR doesn't work as expected. ("resolving the variable to the JSON string") Try to print the value of ```mobj``` after this line.
Make `www\.` part optional.
Must only contain title.
Do not escape `/`.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Upper case is idiomatic for constants.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
Breaks if not arr.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
All formats should be extracted.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
Yes, it should accept any variation of whitespace.
Invalid arguments for 4-5.
Omit expected type.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
i'm not sure why you're adding this step.
then you should use `_remove_duplicate_formats` method.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
`xrange` is not defined in Python 3.x
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Unnumbered placeholders are not supported in Python 2.6.
Use `compat_urllib_parse_unquote_plus` instead.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
