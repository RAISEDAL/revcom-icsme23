The test page has a perfectly good `application/ld+json` script element with `@type: VideoObject` which should be the default source for metadata.
```suggestion description = try_get(additional_data, lambda x: x['caption']['text']) ``` otherwise this would error out when no caption object
Have you checked if this works with the multi-video post test? IIRC you'd need to extract `carousel_media` for it also ```suggestion best_quality = items0['video_versions'][0] ``` or make the array indexing optional
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Must not break extraction if missing.
Should contain `quality` key.
Breaks. Read coding conventions.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Consts should be in uppercase.
Merge in single list comprehension.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
This is always true.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
I mean you must pass both regexes.
Should not match `varflashPlayerOptions...`.
This statement does not conform to PEP8.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This regex does not make any sense.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
No need to escape `{}`.
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
Id from URL is not always a video id. Correct id is in JSON.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
That's incorrect. `\1` must be a number of capture group.
This should include `iframe` part.
Remove all garbage.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
1 is ok.
```/tv/tags/[^/]*?``` => ```/tv/tags/[^/]+```
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
additional info can be extracted from `video` request.
no, there is `season_number`, `episode_number`, `timestamp`, etc...
By providing username and password in params obviously.
Read coding conventions on optional/mandatory meta fields.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
the same for `streaming` key.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
Use more relaxed version, e.g. `[^/]+` to match segments and `[^/?#&]+` to match id.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
There is no point checking `url`.
No point checking this.
`\s*` make no sense at the end.
This is determined automatically.
All tests that test similar extraction scenario should be `only_matching`.
Will never happen.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
I've already suggested using single quotes.
This code duplication may be eliminated.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
Use display id.
This is checked by `_search_regex`.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
`/?` is senseless at the end.
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
You can import `try_rm` from helper
This is superfluous, the extension can be extracted automatically.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Use `self.url_result(inner_url, 'Generic')` instead.
``` for i, video_url in enumerate(video_urls): ```
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Lack of information is denoted by `None` not `0`.
```suggestion new = '' ```
None is default.
No need for escapes inside a brace group, all dots outside must be escaped.
try to parse multiple formats, set `vcodec` to `none`.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
should not break the extraction if the field is not available.
use `query` argument.
just use a hardcoded value for now.
accept `audio` `mediaType`.
This is superfluous since you provide `formats`.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
No need in this message.
You can import `try_rm` from helper
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
This will fail if neither mplayer nor mpv is available.
Must be numeric not string.
Meaning that you must provide account credentials for testing or whatever else is needed.
Using preferences causes invalid sorting.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
That's completely different videos.
Ids must stay intact.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
Title is invalid.
``` for i, video_url in enumerate(video_urls): ```
@zachee @dstftw I had a go at removing the duplication. Is this any good? **Function** ``` def _get_first_valid_downloaded_webpage(urls, video_id, headers): for url in urls: webpage = self._download_webpage(url, video_id, headers=headers) if not ('File not found' in webpage or 'deleted by the owner' in webpage): return webpage raise ExtractorError('File not found', expected=True, video_id=video_id) ``` **Usage** ``` def _real_extract(self, url): video_id = self._match_id(url) url = 'https://openload.co/embed/%s/' % video_id url2 = 'https://openload.co/f/%s/' % video_id headers = { 'User-Agent': self._USER_AGENT, } webpage = self._get_first_valid_downloaded_webpage([url, url2], video_id, headers) ```
@cacdpa: That's better. Just don't use the name ```url``` - it's already used as a parameter of ```_real_extract```.
`note` and `errnote` of `_download_json` instead.
Breaks on None.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Read coding conventions on how mandatory data should be accessed.
Must not be fatal.
Temp file is not removed in this case.
Sorry - dismiss that
The current working directory is not always writable.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
`'%s'` is very unlikely to be a helpful error message.
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
Indenting is messed up here.
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
Formats not sorted.
There should be spaces between `%`.
Carry long lines.
`.get()` idiom is used for optional fields only.
Better to use `determine_ext` instead of `.endswith`
`formats` is always a list of dictionaries.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
This is superfluous since you provide `formats`.
`/video/ in url`.
Formats not sorted.
```XimilayaIE.ie_key()``` is better
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
The same. Should use https if ```url``` use https.
Same issue for re.search
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
No need to escape `]` is character set.
`'id'` is required.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Read coding conventions on optional fields.
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This will result is `[None]` is no category extracted.
Must be `list`.
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Let's try to force the CI tests, and also ensure no final `/`: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)).rstrip('/') ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
As above, you can omit this and let the core set it from `release_timestamp`: ```suggestion ```
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
To access both groups from the URL match, use `mobj = re.match(self._VALID_URL, url)` (`import re`) and then access the matches as `video_id = mobj.group('id')`. In this case the fact that the extractor is running means that the `id` and `display_id` groups matched. If you had an optional group, like `(?P<display_id>.+)?`, something like `display_id = mobj.groupdict().get('display_id')` would be appropriate.
No need for escapes inside a brace group, all dots outside must be escaped.
Lack of information is denoted by `None` not `0`.
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
yt-dlp's `--print` is a bit more complex, allowing printing data at multiple stages. Eg: `-O "after_move:%(filepath)s" will print the final file path. This function is needed to (easily) support that syntax, (and also other similar options)
You only need a simplified version of this. Actually, optparse's built-in action `append` should be sufficient here
Post-processors are already identified by `key` in API same should be used here.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
`autonumber` is not reset to zero in the first place.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
More relaxed regex.
This is not true either. Login may be achieved via authorized cookies.
This will also apply when `online` key does not exist.
Network connections in your browser.
Do not reformat code and remove irrelevant changes.
There is no need in `fatal` when `default` is provided.
Lack of data must be expressed by `None` not empty string.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
Just remove the `try:` and `except:` lines.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
`self._html_search_meta` is better here.
> Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields. - the playlist title and description are not mandatory. - extraction should not fail if any of the fields `blocks`, `sets`, and `id` are not available.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
combine into a single call to `_html_search_meta`.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
All these fields should be `fatal=False`.
I suggest `fatal=False`
This regex does not make any sense.
Do not capture groups you don't use.
No captures for groups you don't use.
Network connections in your browser.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
End users do not read source codes thus will never find this advice.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
No need for that check it's already checked in `parse_iso8601`.
`_html_search_regex` already calls `unescapeHTML`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use single quotes consistently.
Use `self.url_result(inner_url, 'Generic')` instead.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Noway. See other extractors on how to delegate properly.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Read: coding conventions, optional fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
Must be int.
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
I believe their generic name is `YouPorn`, so you can just delete this line.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
The argument will already be a character string, no need to decode it.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Why have you changed the quotes here? It's not that important, but we strive to use `'` when possible, and have when possible a consistent quote character in a file.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `self.url_result(inner_url, 'Generic')` instead.
`<span[^>]+class="name">...` is better.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need to escape `]` is character set.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
`else` is superfluous.
`--no-playlist` is not respected.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
This code looks similar to `sd` format and can be extracted to a function.
This should not be fatal.
This should be extracted in the first place.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
`scale` passed to `float_or_none` instead.
What's the point changing old time-proven regexes with another ones? That's not an improvement.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Breaks on unexpected data.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
All these regexes should be relaxed.
Use `\s*` instead.
All these fields should be `fatal=False`.
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Breaks if not arr.
Breaks. Read coding conventions.
Breaks if no `name`.
Formats in webpage are still available and should be extracted.
This is pointless.
Remove useless code.
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Remove all debug output.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
Sorry - dismiss that
Temp file is not removed in this case.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Breaks. Read coding conventions.
All formats should be extracted.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion 'noplaylist': True, ```
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Re-read my post with eyes please.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Breaks on `None` title.
Use the original scheme and host.
Don't capture groups you don't use.
Query to `query`,
What's the point of this? Use `url` as base.
fatal=True is default.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
There are lots of Content-Type calls. Please merge them together
Just modify mimetype2ext rather than introducing hacks in individual extractors
This is equivalent to InfoExtractor._match_id
Never ignore generic exceptions
Use compat_HTTPError instead
No escape for `/`.
Do not capture empty strings.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
All these regexes should be relaxed.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
Must be int.
It's artist not an album artist.
Optional fields should not break extraction if missing.
Breaks. Read coding conventions.
Do not shadow existing variables.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Consts should be in uppercase.
Merge in single list comprehension.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
This is always true.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
This must be checked **before** any processing.
This must be checked **before** any processing.
Duration calculation is incorrect.
This has no effect. Postprocessors work on info dict copy.
I've already pointed out: I won't accept this.
You have some unmerged lines here
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Query should be passed as `query` parameter.
```suggestion 'noplaylist': True, ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
it better to extract all the urls in the `mpath` array.
There is no point in that.
Must be extracted first.
This wasn't used previously, is it really needed? If the answer is yes, you should use a `set` instead of a dictionary for keeping track of them.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
There are lots of Content-Type calls. Please merge them together
Use `compat_urllib_parse_unquote_plus` instead.
Never ignore generic exceptions
Use compat_HTTPError instead
Optional data should not break extraction if missing. Read coding conventions.
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
No `id` extracted.
I've already pointed out: use `display_id` until you get real id.
Pass `default` to `_og_search_title` instead.
This does not necessarily mean that. There is a clear captured error message that should be output.
Code duplication. `url` must not be `None`.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
No, parse as JSON. Or from ld+json.
Must be numeric.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
Invalid syntax at all.
Must be extracted first.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Don't shadow built-ins.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
We use single quotes as much as possible.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
No, use a value that matches the user agent used by youtube-dl(`chrome`).
use the already extracted value(`video_url`).
should not break the extraction here if a request fails or the `video` field is not accessible.
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
`int_or_none` for all int fields.
Read coding conventions on optional/mandatory meta fields.
All formats should be extracted.
No bare except.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
`ext` should be mp4.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
you already have `pgm_id` and `pgm_no` variables now.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
You have underindented here with just a tab instead of two. We uses spaces.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Do not touch existing `AtomicParsley` code. `mutagen` path must be a fallback if `AtomicParsley` is missing.
You must output to a temp file not the original file.
Won't work. See how this is done for output template.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
No, ADS is the equivalent of xattr. I'd rather have both in one option.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
Title is mandatory.
No such meta field.
`'thumb`' may not be present producing invalid thumbnail url.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
This should not process the whole page. Regex should be relaxed.
This should not process the whole page.
This is too relaxed and should only be matched in categories part of the webpage.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Formats in webpage are still available and should be extracted.
This is pointless.
Remove useless code.
Use `\s*` instead.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Everything after `id` may be skipped and not matched since http://www.rte.ie/radio/utils/radioplayer/rteradioweb.html#!rii=16:10507902 works just fine.
No need to be escaped either.
There is no need in `[]` around `#`.
I guess it's better to put it on single line since it's much shorter now.
Extraction should be tolerate to missing fields.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
Use fatal=False or default=None in _html_search_regex to handle fallbacks instead of try-except patterns.
Just use ```'ÐÐ¾ÑÐ»Ðµ ÑÐ¸ÑÑÐ¾Ð². Â«ÐÑÐ¸Ð¾Ð»Ð¸Ð½Â»'```
url and formats are not used together.
No trailing $, override suitable.
generator or PagedList instead of list.
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
`SoundcloudIE.ie_key() if SoundcloudIE.suitable(permalink_url) else None` at the place of passing `ie`.
This must be passed in `url_result`.
This won't skip empty strings.
Use `compat_urllib_parse_unquote` instead.
`{}` won't on python 2.6.
This line is unnecessary.
Pass `default` to `_og_search_title` instead.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Put `raise` after `if` section at the same indent.
You should not silence all another exceptions but re-raise.
Extractor must not return None.
Mandatory. Read coding conventions.
Remove and make dvr path mandatory.
Do not change this.
You already have it in get call.
No such meta field.
For `url` type any metadata here have no effect.
``` for i, video_url in enumerate(video_urls): ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do not escape `/`.
It also accepts `/video/bbq-salon/WHATEVER_I_WANT_AND_MAKES_SEO_SENSE-63940306` but that doesn't make it `id`, does it? Also when you examine their graphql query there is a attribute called `dotId` which has exactly that value eg. it is `ID` your `video_id` is in their data on attribute `urlName`. Do as you think, but this will be an issue when merging into master.
There's no need to name a group if not used.
No need to escape `/`.
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
All formats should be extracted not only mp4.
All methods only used once should be explicitly inlined.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
End users do not read source codes thus will never find this advice.
will return invalid URL if `search_url` is `null`.
Recursion should be replaced with plain loop.
Single quotes. `item` is already a string.
Unite in single list comprehension.
no longer needed.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
there is not need for excess verbosity.
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion 'noplaylist': True, ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion next_page, 'next page link', group='url', default=None)) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Note `video_data` may be `None`.
No such meta field.
`.*` at the end does not make any sense.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
No trailing $, override suitable.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`int_or_none` and `float_or_none` for all numeric fields.
`id` by no means should be `None`.
Use regular string format syntax instead.
No need to escape `/`.
No need to escape `#`. No need to capture groups you don't use.
To be removed.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
Must be fatal.
I've already pointed out - title **must be fatal**.
Breaks on `None`.
Breaks on `default=False`. `title` must be fatal.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Must be fatal.
Use regular string format syntax instead.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
There is no point in `or None` since `None` is already default.
Title is mandatory.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`id` by no means should be `None`.
To be removed.
Title is mandatory.
Read coding conventions and fix all optional meta fields.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
As already said: parse as JSON not with regexes.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
From what I've seen there is always only one video on the page thus no need in playlist.
It's already extracted as video_id.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
```suggestion new = '' ```
Dots should be escaped
`compat_str()`, here and in l.96.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
Shouldn't be fatal
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
It does not check the other path since you neither download it here nor test the `url` not to contain `mp4:`. Thus you can't be sure whether this test involved the other path or not.
Idiomatic name is `video_id`.
Second argument should be an id extracted from `_VALID_URL`.
This should match only integers.
rtmp's container is **always** flv.
Simplify to `rtmp://camwithher.tv/clipshare/%s' % ('mp4:%s.mp4' if int(video_id) > 2010 else video_id)`.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
`'%s'` is very unlikely to be a helpful error message.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
The same. Should use https if ```url``` use https.
Same issue for re.search
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Final bit, self._search_regex is better than re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
The whole code until here can be simplified to `page_id = self._match_id(url)`
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
Read coding conventions on optional fields.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Use `self.url_result(inner_url, 'Generic')` instead.
The argument will already be a character string, no need to decode it.
Ok, It's up to you.
this does not handle the case where `contentUrl` value is `None`.
To be removed.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
No `id` extracted.
I've already pointed out: use `display_id` until you get real id.
No bare except.
``` for i, video_url in enumerate(video_urls): ```
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
No need to use `sanitized_Request` request here, pass url directly to download method.
Use `self._parse_html5_media_entries` instead.
All these regexes should be relaxed.
This is determined automatically.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Must be int.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
API URLs should be used.
I've already pointed out: API URLs should be used.
There is no need to test URLs.
Formats should have `quality`.
Code duplication should be eliminated.
Using `FormatData` four times.
Still a code duplication.
Do not shadow existing variables.
```suggestion new = '' ```
Change 1 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI. Change 2 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/media/_hqLjQ95yx8Z.
As well as this.
It's a matching only test. Testing two identical URLs twice does not make any sense.
https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI is still available.
By convention, a title is required. Here, if the title isn't found, `.strip()` will crash. You could make `fatal=True`, or supply a default, say `default='Untitled video %s' % video_id`.
Use formatted strings.
Remove all useless noise.
Capturing empty URL is senseless.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Still does not handle aforementioned URLs.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
Don't capture groups you don't use.
There must be two different extractors: for videos and for playlists.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_search_regex` is enough here.
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Same, no such key possible.
Won't work for `info = {'title': None}`.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
This may change as well. Add a fallback that just processes all videos without differentiation.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
All methods only used once should be explicitly inlined.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
This should actually be just `self.url_result(embedded_url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No. Use fatal search regex instead.
No such meta field.
Not used with formats.
`'id'` is required.
`compat_str()`, here and in l.96.
```suggestion new = '' ```
Request wrapping code can be moved to the base class.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
Dots not escaped.
Must not be fatal for playlist.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
This does not make any sense, you already have `url`.
Relax `id` group.
Capture between tags.
Do not capture empty strings.
No exact URLs here.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Remove all garbage.
Plain `for x in l`.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
`/?` is senseless at the end.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
I've already pointed out: this must be removed.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
`ext` should be mp4.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
same for `episode_number`(both lines where highlighted).
`created_at_i` as `timestamp`.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`user_info` may be `None`.
Again: float_or_none, not parse_duration.
You can use self._download_json() here
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Depends on what does it mean.
No such meta field.
There is no need in this method.
Must be separate extractor.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be a separate extractor delegating to SBS.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
Instead of such hacks you can name group differently and capture it without any issue.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
All methods only used once should be explicitly inlined.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
Preferably for such a long field use `'md5:...'`
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
This may change as well. Add a fallback that just processes all videos without differentiation.
Breaks extraction if not available. Again read coding conventions.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Noway. See other extractors on how to delegate properly.
I would prefer indent similar to the former code.
This regex should be split into multiple lines for bettercode perception.
1 is ok.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
It should always return a list.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Never use bare except.
All formats should be extracted.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Should not be fatal.
Formats not sorted.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
No such meta field.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
```suggestion new = '' ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
We use single quotes as much as possible.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This video is georestricted.
Use `self._match_id` is better.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Also pass `m3u8_id='hls'`.
Correct field name is `format_id`.
`ext` here makes no sense.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
This is never reached cause sort formats will throw on `not formats`.
Use `self._search_regex` and `utils.unified_strdate` instead.
All these regexes should be relaxed.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
Must be int.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
Formats not sorted.
Should not be fatal.
All these regexes should be relaxed.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Must be int.
will be extracted from the URL.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
These dependencies are unacceptable. Moreover you don't use them.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
The playlists are public: there should be tests for them. Watch this space.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Carry long lines.
Extract common URL base.
Don't capture unused groups
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
Parse from flashvars JSON.
Parse from flashvars JSON.
Temp file is not removed in this case.
What's the point of this? Remove.
Breaks extraction if not available. Again read coding conventions.
Sorry - dismiss that
The current working directory is not always writable.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Read coding conventions on mandatory metadata.
Must be extracted first.
1. No `{}`. 2. Inline. 3. Query to `query`.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
It shouldn't fail if `user` or `username` is missing.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
This is already imported and (in general) you should only use `import`s at the top level.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
This will break extraction if no `id` present.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
This will result is `[None]` is no category extracted.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Remove all debug output.
`video_id` may be `None`.
Regex should be relaxed. Dots should be escaped.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Remove all unused code.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Use default. Read coding conventions and fix code.
Audio must have proper `vcodec` set.
No such field.
This should be split into building url and extracting formats.
Query should be passed as `query` parameter.
To be removed.
All debug code should be removed.
By providing username and password in params obviously.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Code duplication with tbs extractor.
It should not. See the description of the field.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Uppercase is not honored.
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
Must be `list`.
Playlist title is optional, description breaks.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Replace 245-257 with `entry.update({ ... })`.
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
still the check for element class is missing.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
`'id'` is required.
```suggestion new = '' ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
The third parameter of `_html_search_regex` is name but not ID.
`'id'` is required.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No such meta field.
Not used with formats.
`title` is mandatory. Move flags into regex itself.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Move to initial title assignment.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
```suggestion new = '' ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
make one of the tests an `only_matching` test.
there is not need for excess verbosity.
no longer needed.
incorrect URLs for Cook's Country.
will return invalid URL if `search_url` is `null`.
extraction should not break if an episode or all episodes couldn't be extracted.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
Remove unnecessary verbosity.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Network connections in your browser.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Extraction should not break if one of the formats is missing.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
`_search_regex`, `_parse_json`. Again: read coding conventions.
```python for path, format_id in (('', 'audio'), ('video', 'sd'), ('videohd', 'hd')): self._download_xml( 'https://www.heise.de/ct/uplink/ctuplink%s.rss' % path, video_id, 'Downloading %s XML' % format_id, fatal=False) ```
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
Use single quotes consistently.
Never use bare except.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You still duplicate the URL and unnecessary `if/elif` branches.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
yes, this is correct.
Use `self._og_search_*` functions here.
Use a for loop to eliminate duplicated codes.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Usually we use `md5:(md5 of the long text)` for such cases.
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
You don't need to call `report_extraction` here and above, since the extraction is over already.
No trailing $, override suitable.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This code looks similar to `sd` format and can be extracted to a function.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Read coding conventions.
You should not throw everything under try/except. Also read coding conventions.
surround only the part that will threw the exception.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
the same for `streaming` key.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
Better to use integers for supported_resolutions and use str() here
Same issue for urlh
Dots should be escaped
This will return non existent path if conversion fails.
`{}` won't work in python 2.6.
Why do you disallow `story` to be an id? Because you do not want it to be an id in your particular case that is clearly an ad hoc hack cause other extractors may use `_generic_id` and may allow `story` to be an id.
There should not be any ad hoc hacks in generic methods.
This has no effect. Postprocessors work on info dict copy.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
Just modify mimetype2ext rather than introducing hacks in individual extractors
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
You can import `try_rm` from helper
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
There is no need in this method.
`enumerate` on for range.
Optional data should not break extraction if missing. Read coding conventions.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
By providing username and password in params obviously.
I would always return a `multi_video` result.
Read coding conventions on optional/mandatory meta fields.
This is pointless.
All of these will break extraction on unexpected data.
Must be `int`.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
First group is superfluous.
Must not be `None`.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Either sloppy code or an anti-scraping measure.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/bigflix.py#L23-L25 All tests that test same extraction scenario should be made `only_matching`.
There are two scenarios: video without auth and video with auth. The rest are matching only.
These looks like mandatory fields.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
This branch is never reached.
`default` is not used with `fatal`.
This does not necessarily mean that. There is a clear captured error message that should be output.
This is checked by `_search_regex`.
Use display id.
`/?` is senseless at the end.
Move to base class.
Inline everything used only once.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
Title is mandatory field thus it should be `data['roominfo']['name']`.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
yes, this is correct.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
Don't capture groups you don't use.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
Empty string capture does not make any sense.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
Empty string capture does not make any sense.
`_search_regex` is enough.
This may change as well. Add a fallback that just processes all videos without differentiation.
Instead of this extraction should be delegated to `WSJIE` via `url_result`.
Also pass `video_id` since it's known beforehand.
Should not be fatal.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This does not make any sense, you already have `url`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
No need to escape a double quote inside a single (in fact that might break this due to `r'`).
```suggestion new = '' ```
`[]` is useless.
Move method near the place of usage.
1. Breaks if div is not found. 2. `re.finall`.
No need to escape whitespace.
Else branch is useless.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
Use `(?i)` in regex itself if you want case insensitivity.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
All debug code should be removed.
Should be delegated via `url_result`.
This duplicates code from `MedialaanIE`.
For now extract it in a new base IE class.
Consistently use flags inside regex.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
170-172 - code duplication.
You should have only one single method for extracting info.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Depending on where in the page the target may be, consider `self._html_search_regex()` which unescapes the returned match (eg, if it contains `&amp;` that should be `&`, or just `&#0049;` that should be `1`).
`self._search_regex` is enough here.
Matched data-video should not be empty.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
No exact URLs.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
What's the point of this? `canonical_url` is the same as `url`.
There is no need to provide full test for every URL schema if they don't test some special extraction scenario. [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/rds.py#L31-L32) is enough.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
Capturing empty string does not make any sense. What's the point capturing this at all? id and path occur only once in webpage.
This code looks similar to `sd` format and can be extracted to a function.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Everything apart from `url` is optional.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
use single quotes consistently.
check that extracted description is what is expected(should not contain html tags).
`strip_or_none` no longer needed.
`twitter:description` and `description` meta tags are also available.
combine into a single call to `_html_search_meta`.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
parentheses not needed.
fallback to other available values.
`title` is mandatory. Move flags into regex itself.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
same if one of the values is `None`.
this does not handle the case where `contentUrl` value is `None`.
breaks the extraction if `clips` is `None`.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
still fails if `uploader_data` not available.
Avoid shadowing built-in names.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Re-read my post with eyes please.
This breaks all non ks embeds. ks part must be optional.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This should include `iframe` part.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
A minor bit: don't use a capturing group if it's not used.
Remove all garbage.
Modify existing regex instead.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This should be rewritten in terms of [`YoutubeIE._extract_urls`](https://github.com/rg3/youtube-dl/commit/66c9fa36c10860b380806b9de48f38d628289e03).
Do not shadow built-in names.
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
You must use `default` if there is a fallback after it.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Should not be fatal.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Use `utils.xpath_text` instead, again with `fatal=False`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `compat.compat_str` instead.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
This code looks similar to `sd` format and can be extracted to a function.
This is never reachable.
This is default.
Must not be fatal.
Simplify: ```suggestion series_id = self._match_id(url) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Bitrate should go to corresponding format meta field.
`self._parse_html5_media_entries` for formats extraction.
This should be a ```list```.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Breaks if no `rate` key in `stream`.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
This is bitrate, not quality.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`_` is idiomatic way to denote unused variables.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Query should be passed as `query` parameter.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion 'noplaylist': True, ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
Uppercase is not honored.
`title` is not guaranteed to be present.
Most playlists don't have titles in their entries' at this time thus this option won't work for most of the cases.
87-90 code duplication.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
Should not be greedy.
`{}` doesn't work in python 2.6.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
Relax `id` group.
This does not make any sense, you already have `url`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will be extracted from the URL.
I suggest `fatal=False`
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
If you accept this to be missing then it must be `default=` not `fatal`.
This is never reached cause sort formats will throw on `not formats`.
All of these will break extraction on unexpected data.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
should not fail if the extraction of one set or item is not possible.
Use compat_HTTPError instead
Never ignore generic exceptions
There are lots of Content-Type calls. Please merge them together
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
Read and follow code conventions. Check code with flake8.
What's the point of this? `canonical_url` is the same as `url`.
Breaks if no `name`.
If `video` not in `media` empty formats will be returned that does not make any sense.
`for key, value in media.get('images', {}).items():`
It does not necessarily mean that.
Referring `url` from `url` looks like nonsense. Provide rationale.
Inline to actual call place.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
hls and rtmp are available as well.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
What's the point of lines 104-108? `ext` is already flv.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Should not be greedy.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
Lack of information is denoted by `None` not `0`.
```suggestion new = '' ```
Do not use leading underscore for locals.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Constant names should be in uppercase.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
Lack of data must be expressed by `None` not empty string.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
You could make the regex more robust: ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*(.*?)\s*</script' ``` * `\s*` for all the places where JS/HTML allow whitespace * `.*?` to avoid capturing "target_json</script>...<script...>...</script>", etc * `</` don't need to be escaped. Also, as you're presumably hoping to get some brace expression, maybe this? ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*({.*?});?\s*</script' ```
There's still a loop on `format_id` which doesn't seem right: ``` for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): if not isinstance(server_json, dict): continue for format_id in ('hls', 'dash'): if format_id not in server_json.keys(): continue if format_id == 'hls': formats.extend(self._extract_m3u8_formats( server_json[format_id], lecture_id, 'mp4', entry_protocol='m3u8_native', m3u8_id=format_id, note='Downloading %s m3u8 information' % server_json.get('id', '?') , elif format_id == 'dash': formats.extend(self._extract_mpd_formats( server_json[format_id], lecture_id, mpd_id=format_id, note='Downloading %s MPD manifest' % server_json.get('id', '?'), fatal=False)) self._sort_formats(formats) ... ```
`(server_json.get('id') or '?')` (or some other default value). Or server_json.get('id', '?') Similarly l.128.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
Use `self._sort_formats(formats)` instead.
Remove debugging codes.
surround only the part that will threw the exception.
Uppercase is used for const.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
`.*` at the end does not make any sense.
This is already fatal.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No such meta field.
Don't shadow built-ins.
Should not be fatal.
Nothing changed. Also there is a video id available in JSON.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
Either **do** or remove.
Carry long lines. Bother to finally read coding conventions.
This will process the same URL twice overwriting the previous results.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
Relax `id` group.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
All methods only used once should be explicitly inlined.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
should be in the `else` block of the `for` loop.
the same for `streaming` key.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
surround only the part that will threw the exception.
both are know beforehand, so there is no need to use `urljoin`.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
`id` must not contain any irrelevant parts.
Formats not sorted.
Should be tolerate to missing keys in `media`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`if mediatype == u'video':` is idiomatic Python.
We may use proper XML parsing here and simply call `self._download_xml`
This is fatal.
`note` and `errnote` of `_download_json` instead.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
Carry to the indented beginning of the line.
`int_or_none` for all int fields.
Carry to the indented beginning of the line.
No bare except.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Prefer `post.get()` for these two.
`enumerate` on for range.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Should contain `quality` key.
`<h3>` is intentional.
It should not match `h|`.
87-90 code duplication.
Query to `query`,
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Filter invalid URLs.
`/?` is senseless at the end.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
That's very brittle.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
```suggestion if not (season_id and video_id): ```
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_search_regex` is enough here.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Same, no such key possible.
No need for such checks.
Won't work for `info = {'title': None}`.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
cookie => cookies. There are 3 items.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`int_or_none` for all int fields.
No bare except.
All formats should be extracted.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Breaks on None.
Read coding conventions on how mandatory data should be accessed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
default and fatal are not used together. `True` is default.
`if not content_url:`.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
No trailing $, override suitable.
Code duplication should be eliminated.
generator or PagedList instead of list.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
Move into loop.
You're already building query with `query` param.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
It does not necessarily mean that.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
It should always return a list.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
Use `\s*` instead.
All these fields should be `fatal=False`.
This will result in reference to unassigned variable when time fields are missing.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
This can be moved inside `if chapters:` condition.
This has no effect. Postprocessors work on info dict copy.
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
That's completely different videos.
Ids must stay intact.
`fatal` must be added to `_extract_info`. No changes to core code.
Add `fatal` flag.
Do not change the order of extraction.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
Use `compat_urlparse.urljoin` instead.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Should not break if there is no `resolution` key.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Must be extracted first.
This will result in reference to unassigned variable when time fields are missing.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This code looks similar to `sd` format and can be extracted to a function.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
`compat_str()`, here and in l.96.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Parenthesis are superfluous.
None of the optional metadata should break the extraction when missing.
All these regexes should be relaxed.
Must be int.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This test is pointless. You must test iframe extraction not regex match.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
This matches multiple videos.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
EntryId must be extracted the very first.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
You must use `default` if there is a fallback after it.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
```suggestion if not (season_id and video_id): ```
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Side note: Wow, these guys are military, but don't support https? Oh my...
To get regex highlighting, this string should be prefixed with `r`. That's just convention, but plain neat.
This is a tuple, that can't be right. Also, a test is only really useful with some things to test against, like title and md5sum of the image. You can run it with `python tests/test_download TestDownload.test_DefenseGouvFr`.
`lxml` is not part of the standard library, you'll need to use regular expresions.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
According to [PEP8](http://www.python.org/dev/peps/pep-0008/), there should be a space after the comma. But again, better just move the contents of this function to the bottom.
What if the title does not start exactly 30 characters after this? Also, what if this string is not found.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
Roll this together: ```suggestion return self.url_result( 'francetv:' + video_id, ```
Currently these are right: ```suggestion 'title': 're:(?i)^Les Fondamentaux : Vocabulaire$', }, 'playlist_mincount': 25 ```
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
This will break extraction if no `id` present.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
This will result is `[None]` is no category extracted.
Must be `list`.
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
Merge in single list comprehension.
Instead of this extractor you should add an extractor for `https://20.detik.com/embed/...` URLs and add detection of such embeds in generic extractor.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Query to `query`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
Do not capture groups you don't use.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
It's better to keep the original ID for existing patterns, or --download-archive will be broken.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
No trailing $, override suitable.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
This should be split into building url and extracting formats.
Update `video_info` and return it directly.
It won't be longer.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
`{}` won't work in python 2.6.
This code looks similar to `sd` format and can be extracted to a function.
Revert. >Checking download_**url** video format **URL** makes even less sense.
Consistently use flags inside regex.
This is no longer actual.
No brackets needed.
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
The `list` call is superfluous here and can be safely removed.
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
This should actually be just `self.url_result(embedded_url)`.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Constant names should be in uppercase.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
There are lots of Content-Type calls. Please merge them together
`else` is superfluous.
No need to escape `]` is character set.
`--no-playlist` is not respected.
Use bare `re.match`.
Both test the same extraction scenario. The rule is one test per extraction scenario.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Query should be passed as `query` parameter.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
First group is superfluous.
Must not be `None`.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
No. You should not shadow the original explicitly provided password.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
Don't use bare `except:`
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
Did you use the netrc format for data stored in libsecret backends? If not don't use ```NetrcParseError```.
Better to use more meaningful names than u or p. That helps users to understand what youtube-dl are using from their secret storages.
ImportError and JSON parsing errors should be handled as well
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
There are lots of Content-Type calls. Please merge them together
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Use compat_HTTPError instead
Never ignore generic exceptions
This is equivalent to InfoExtractor._match_id
Just modify mimetype2ext rather than introducing hacks in individual extractors
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
default and fatal are not used together. `True` is default.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`if not content_url:`.
`'id'` is required.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`'%s'` is very unlikely to be a helpful error message.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
It should be robust in case of some missing fields.
For tv/se `og:title` contains unnecessary suffix.
Just cut it with `utils.remove_end`.
This may change as well. Add a fallback that just processes all videos without differentiation.
`title` must be mandatory I've already told about this.
This value looks an awful lot like a `display_id`
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
Breaks extraction if not available. Again read coding conventions.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
still fails if `uploader_data` not available.
Don't add list items if 'url' is None.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This should be just `return info`
```XimilayaIE.ie_key()``` is better
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
Same for re.search
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
it whould be better to iterate once and extract the needed information.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
`title` is mandatory. Move flags into regex itself.
at this point `sub_lang` is guaranteed to be in the subtitles dict.
keep the old fallback code.
no need to create a method when it will be used once.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
You should have only one single method for extracting info.
By providing username and password in params obviously.
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
It's a field name not a step name.
Again: relax regex.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`.*` at the end does not make any sense.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
I've already suggested using `Downloading` as idiomatic wording.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
Use single quotes consistently.
Never use bare except.
I think it's safe to use `avi` here since we use extensions for this option. Moreover, it will simplify [this code](https://github.com/aurium/youtube-dl/blob/master/youtube_dl/postprocessor/ffmpeg.py#L297-L301) a bit.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
Don't use bare `except:`
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
No. You should not shadow the original explicitly provided password.
Post-processors are already identified by `key` in API same should be used here.
`{}` won't work in python 2.6.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
`<display_id> is not a video`.
First group is superfluous.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Must not be `None`.
Must be `int`.
Use `self.url_result(inner_url, 'Generic')` instead.
Lack of data must be expressed by `None` not empty string.
Do not reformat code and remove irrelevant changes.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
No such meta field.
Not used with formats.
url and formats are not used together.
Do not capture empty strings.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
No trailing $, override suitable.
Unite in single list comprehension.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
`unescapeHTML(n[1])`? (`utils.py`) See also line 59 below.
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
Must not be fatal.
Rename to `KanalDIE`.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
All tests that test similar extraction scenario should be `only_matching`.
Will never happen.
Never use bare except.
Read coding conventions on optional and mandatory data extraction.
Please remove debugging lines.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Remove all debug garbage.
Noway. See other extractors on how to delegate properly.
You don't need to call `report_extraction` here and above, since the extraction is over already.
Oh, alright, leave it in. I was suprised to find a `\x` instead of a `\u`, but specifically for the non-breaking space, that should be fine.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
Need fatal=False or default=None
url and formats are not used together.
No trailing $, override suitable.
generator or PagedList instead of list.
Use `self._search_regex` and `utils.unified_strdate` instead.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
This code duplication may be eliminated.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
the same for `streaming` key.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
both are know beforehand, so there is no need to use `urljoin`.
Add `fatal` flag.
Do not change the order of extraction.
`fatal` must be added to `_extract_info`. No changes to core code.
Delegate to generic always when original path found nothing.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This code looks similar to `sd` format and can be extracted to a function.
This is not true in general.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
Use `video_id`. Remove `.replace('\n', '')`.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
It's already extracted as video_id.
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Breaks extraction if no `followBar`.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
If you pass cookie you must not pass credentials.
This does not look to be possible on a clean session.
Must not be fatal.
Breaks on None.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Read coding conventions on how mandatory data should be accessed.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Breaks on unexpected data.
Remove superfluous whitespace.
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
No need to escape `]` is character set.
`--no-playlist` is not respected.
Use bare `re.match`.
`else` is superfluous.
Both test the same extraction scenario. The rule is one test per extraction scenario.
`<span[^>]+class="name">...` is better.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
You have some unmerged lines here
There should be an `id` group.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
No trailing $, override suitable.
You should consult some git manual.
You don't check whether login succeeded or not.
You don't need to call `report_extraction` here and above, since the extraction is over already.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Use single quotes consistently.
m3u8 is also available.
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Relax regex, make group unnamed, don't capture empty dict.
Again: relax regex.
Move flags into regex.
Read coding conventions on optional fields.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
`autonumber` is not reset to zero in the first place.
Post-processors are already identified by `key` in API same should be used here.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
There is no point in `or None` since `None` is already default.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Possibly referenced before assignment.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Possibly referenced before assignment.
`compat_str()`, here and in l.96.
This code looks similar to `sd` format and can be extracted to a function.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
By providing username and password in params obviously.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This regex does not look like generic embed. Provide several examples that use this embedding.
url and formats are not used together.
No trailing $, override suitable.
Use `\s*` instead.
All these regexes should be relaxed.
As already said: parse as JSON not with regexes.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Style: ```suggestion return '/'.join(urlparts) ```
Don't capture groups you don't use. Use proper regex to match all country codes.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
No need to escape `/`.
Carry long lines.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
Formats not sorted.
No direct URLs in tests.
Extract id once before the loop.
If nothing matches `None` will be returned.
Playlist title is optional.
This should not be here as done by downloader.
If either of these attrs is missing whole playlist extraction is broken.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
Missing dot escape.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
That's very brittle.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Recursion should be replaced with plain loop.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
and the function that normally used to encode postdata is `urlencode_postdata`.
This regex does not make any sense.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
mobj would be None if nothing is matched
Better to use unified_strdate for parsing dates.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Need fatal=False or default=None
In python 3.X print is a function, for printing to screen use `self.to_screen`. But if it's a fatal error then `raise ExtractorError`
JSON layout has changed. Error is not captured properly anymore.
Error handling broken.
This does not necessarily mean that. There is a clear captured error message that should be output.
This branch is never reached.
`/?` is senseless at the end.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
I've already pointed out: this must be removed.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
There's no need to use `u` prefix given `unicode_literals` is declared.
This will process the same URL twice overwriting the previous results.
This line is unnecessary, webpage is never used.
Unnumbered placeholders are not supported in Python 2.6.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Do not capture empty strings.
`.*` at the end does not make any sense.
JSON should be parsed as JSON.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
breaks the extraction if `clips` is `None`.
will break the extraction if `profiles` is empty or `None`.
same if one of the values is `None`.
`<script>.*` this part does not play a roll in the regex, may be change to `<script[^>]*>\s*(var\s*)?`. Split Long lines(longer than 80 columns). ```suggestion hydration_data = self._search_regex( r'<script>.*hydrationData\s*=\s*({.+?})\s*</script>', webpage, 'hydration data', default='{}') ```
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
try put value that are used multiple times in a variable(ex: `author_info.get('id')`). i think the `{}` format is not supported in python 2.6.
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
None is default.
`_` is idiomatic way to denote unused variables.
Correct field name is `format_id`.
`ext` here makes no sense.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Matched data-video should not be empty.
`self._search_regex` is enough here.
This is superfluous since you provide `formats`.
There is no need in this method.
Instead of `resolution` and `preference` it should be extracted as `height`.
Optional data should not break extraction if missing. Read coding conventions.
`/?` is senseless at the end.
Use display id.
`default` is not used with `fatal`.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
This does not necessarily mean that. There is a clear captured error message that should be output.
`ext` should be mp4.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
This prevents from authenticating with `--cookies`.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
Re-read my post with eyes please.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
A minor bit: don't use a capturing group if it's not used.
This line can just be removed.
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
```suggestion from ..compat import compat_str ```
Don't capture groups if you are not going to use them.
`xrange` is not defined in Python 3.x
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
Extracting duplicate code into a function obviously.
There is no need in this method.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
EntryId must be extracted the very first.
There should be a hardcoded fallback since it's always the same.
You must use `default` if there is a fallback after it.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
This is in the test suite, and in the test suite, warnings are an error of us, aren't they? So why isn't the implementation ``` raise Exception(message) ```
We should really provide a better interface to test against, something along the lines of `download(url)`.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
You can import `try_rm` from helper
Instead of adding new parameters put them into `ctx`.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Alternatively you can just restore it after this PR is merged.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
You must output to a temp file not the original file.
No direct URLs in tests.
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks if not arr.
I'm not talking about capturing upload date. Do not capture AMPM.
Move flags into regex.
All these regexes should be relaxed.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This doc string does not match the function now.
It should always return a list.
It worth adding a doc string.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
Must not be fatal. Read coding conventions on optional/mandatory fields.
All debug garbage must be removed.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Formats not sorted.
Do not reformat code and remove irrelevant changes.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
Actually, it's an opposite. It's a check for successful login.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
No. _extract_urls of youtube extractor.
This line can just be removed.
Don't capture groups you don't use.
This breaks the extraction of prochan embeds. `[^"]+` is only applied for youtube group.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
Despite being keyword arguments avoid changing the original order.
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
This is the behavior of `_search_regex` with defaults.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
Use `(?i)` in regex itself if you want case insensitivity.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
All methods only used once should be explicitly inlined.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Make more relaxed and add title regex as fallback.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
According to pep8, there's a missing space after `+` in here.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This is pointless.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Remove useless code.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
From what I've seen there is always only one video on the page thus no need in playlist.
This is too broad and detects the same video twice.
This should be extracted right from `_VALID_URL`.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
The description is always optional, so there should be a `fatal=False` in here.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Must be int.
All these regexes should be relaxed.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
87-90 code duplication.
Query to `query`,
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Recursion should be replaced with plain loop.
No, it's not. If video_id extraction from page fails whole extraction fails.
This will break extraction if no `id` present.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks if not arr.
I'm not talking about capturing upload date. Do not capture AMPM.
All these regexes should be relaxed.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This code looks similar to `sd` format and can be extracted to a function.
If they switch to strings this will break formats' sorting. Same should be done for width & height.
Change to `config_files.get('hls', {}).get('all')`.
Make it non fatal, provide correct `ext`, consider forcing to native hls by default, and set same `preference` as for direct links since for some videos hd is only available via hls (e.g. https://player.vimeo.com/video/98044508).
This can now be omitted.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
Title is mandatory.
Breaks on unexpected data.
`[]` is superfluous in group with single character.
eg find type by URL ext
Optional data should not break extraction if missing. Read coding conventions.
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
230-264 no copy pastes.
All these fields should be `fatal=False`.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Read and follow code conventions. Check code with flake8.
Should be non fatal.
No, use a value that matches the user agent used by youtube-dl(`chrome`).
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
`int_or_none` for all int fields.
All formats should be extracted.
`note` and `errnote` of `_download_json` instead.
This is fatal.
No bare except.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
```python for path, format_id in (('', 'audio'), ('video', 'sd'), ('videohd', 'hd')): self._download_xml( 'https://www.heise.de/ct/uplink/ctuplink%s.rss' % path, video_id, 'Downloading %s XML' % format_id, fatal=False) ```
Must not be fatal.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
In both extraction functions you extract these fields from `webpage` (as fallbacks in the second case) but you do it differently (split on `|` here and as is in the second case). What's the rationale? `_search_regex` is fatal by default and will break extraction if it's not found for optional fields.
No escape for `/`.
Do not capture empty strings.
Avoid unrelated changes.
Make more relaxed and add title regex as fallback.
All these regexes should be relaxed.
Empty string capture does not make any sense.
Must be int.
All formats should be extracted.
Only whitespace is allowed between `videoInfo` and `=`.
Do not duplicate extractor.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Usually display_id is used before the actual video_id is extracted.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
Better to use unified_strdate for parsing dates.
mobj would be None if nothing is matched
Need fatal=False or default=None
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
The description is always optional, so there should be a `fatal=False` in here.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
Use `self._search_regex` and `utils.unified_strdate` instead.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Simplify: ```suggestion series_id = self._match_id(url) ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Must only contain description.
Carry long lines. Read coding conventions.
Must only contain title.
Should be tolerate to missing keys in `media`.
Query to `query=`.
Should not be fatal.
Must not be fatal.
Playlist title is optional.
This is never reachable.
This is default.
Use `self._parse_html5_media_entries` instead.
This should be a ```list```.
I would always return a `multi_video` result.
If `_search_regex` fails `None` will be passed to `_parse_json`.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
Breaks if no `name`.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Need fatal=False or default=None
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
There are lots of Content-Type calls. Please merge them together
Just modify mimetype2ext rather than introducing hacks in individual extractors
This is equivalent to InfoExtractor._match_id
Never ignore generic exceptions
Use compat_HTTPError instead
Extracting duplicate code into a function obviously.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
None of the optional fields should break extraction if missing.
`try_get`, single quotes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
If `_search_regex` fails `None` will be passed to `_parse_json`.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Either sloppy code or an anti-scraping measure.
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
cookie => cookies. There are 3 items.
All formats should be extracted.
`int_or_none` for all int fields.
No bare except.
`'%s'` is very unlikely to be a helpful error message.
Indenting is messed up here.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
You have some unmerged lines here
``` for i, video_url in enumerate(video_urls): ```
`acodec == 'none'`.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
Read coding conventions on optional/mandatory meta fields.
This will result in reference to unassigned variable when time fields are missing.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
`ext` should be mp4.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This is not matched by `_VALID_URL`.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Upper case is idiomatic for constants.
Then just keep this code and > create default fallbacks if extraction of these fails
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
`_` is idiomatic way to denote unused variables.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
It should be robust in case of some missing fields.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
This code looks similar to `sd` format and can be extracted to a function.
Use `self.url_result(inner_url, 'Generic')` instead.
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
`ext` here makes no sense.
Correct field name is `format_id`.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Matched data-video should not be empty.
`self._search_regex` is enough here.
There is no need in this method.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Optional data should not break extraction if missing. Read coding conventions.
You should have only one single method for extracting info.
Do not capture groups you don't use.
No exact URLs here.
Query to `query`.
`vcodec` to 'none'.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
Move `_match_id` into `_extract_audio`.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
it's either one of two cases: they are identical -> keep them as they are(formats of the same entry). they are different -> separate them into a playlist with two entries.
Before these changes there were 4 references and extraction followed them strictly in order of reference declaration.
At least move iframe extraction after video tag extraction.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
`ref:` should not be removed from video id.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
Noway. See other extractors on how to delegate properly.
```dict_get``` makes codes even shorter
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
Read coding conventions on optional and mandatory data extraction.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
`/?` is senseless at the end.
I've already pointed out: this must be removed.
There are also vtt subtitles available.
Check code with flake8.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
I suggest `default=video_id` in the `_html_search_regex` call.
will be extracted from the URL.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `fatal=False`
will easily match outside the element.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
Again: float_or_none, not parse_duration.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This code looks similar to `sd` format and can be extracted to a function.
`int_or_none` for all int fields.
No bare except.
All formats should be extracted.
`note` and `errnote` of `_download_json` instead.
This is fatal.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
There should be fallbacks for these values since they are more or less static.
This should not be fatal.
It's already done at L151.
Use `query` for query.
`acodec == 'none'`.
Move into `_download_json`.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
Move to the place of usage.
`info_dict['formats'] = formats`.
`ext` should be mp4.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
**Always** check code with flake8.
Request wrapping code can be moved to the base class.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
`'%s'` is very unlikely to be a helpful error message.
Indenting is messed up here.
Read coding conventions on how mandatory data should be accessed.
`[]` is superfluous in group with single character.
Must not be fatal.
Breaks on unexpected data.
Breaks on unexpected data.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Breaks on None.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
`only_once` parameter isn't in yt-dl, yet.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Read coding conventions and fix all optional fields.
`{}` does not work in python 2.6.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
All integral numeric metafields should be wrapped in `int_or_none`.
Title is mandatory.
This may change as well. Add a fallback that just processes all videos without differentiation.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`int_or_none` for all int fields.
No bare except.
All formats should be extracted.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
This may change as well. Add a fallback that just processes all videos without differentiation.
Breaks if no `name`.
Dots should be escaped
Same issue for urlh
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
This should actually be just `self.url_result(embedded_url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
`if mediatype == u'video':` is idiomatic Python.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
The argument will already be a character string, no need to decode it.
Remove useless code.
Separate variable is superfluous.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
`enumerate` on for range.
Use `_sort_formats` instead.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
There is no point in that.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Optional data should not break extraction if missing. Read coding conventions.
Too broad regex.
87-90 code duplication.
No trailing markers - override `suitable`.
Must not return `None`.
`default=None` for the first.
Empty string capture does not make any sense.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Empty string capture does not make any sense.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
`self._search_regex` is enough here.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Correct field name is `format_id`.
Use `self._match_id` is better.
`ext` here makes no sense.
`title` is mandatory. Move flags into regex itself.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
Title is mandatory field thus it should be `data['roominfo']['name']`.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
yes, this is correct.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
I guess this was deleted by a mistake.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Consistently use flags inside regex.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Code duplication with tbs extractor.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
``` for i, video_url in enumerate(video_urls): ```
use the extension extracted from `determine_ext`.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
instead of this it's better to just put result of `determine_ext` in a variable and use it.
at this point `sub_lang` is guaranteed to be in the subtitles dict.
What's the point of this? `canonical_url` is the same as `url`.
You're already building query with `query` param.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
Inline to actual call place.
It does not necessarily mean that.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
Title is mandatory.
No such meta field.
`'thumb`' may not be present producing invalid thumbnail url.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will be extracted from the URL.
I suggest `fatal=False`
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
Don't use bare `except:`
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. You should not shadow the original explicitly provided password.
All methods only used once should be explicitly inlined.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
``` for i, video_url in enumerate(video_urls): ```
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
Network connections in your browser.
`{}` does not work in python 2.6.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Extraction should not break if one of the formats is missing.
This is not true in general.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
The `return` is not needed anymore.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
1. Do not remove the old pattern. 2. Relax regex.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Move flags into regex.
Relax regex, make group unnamed, don't capture empty dict.
It's a field name not a step name.
Again: relax regex.
Read coding conventions on optional fields.
Carry to the indented beginning of the line.
None is default.
Carry to the indented beginning of the line.
This check is not necessary now as you test pathconf anyway.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
I would prefer testing with `hasattr` instead to avoid [possible breakages](https://github.com/rg3/youtube-dl/commit/22603348aa0b3e02c520589dea092507a04ab06a) I've learned about the hard way.
`os.pathconf` and `os.pathconf_names`. You should also test whether `os.pathconf_names` contains `PC_NAME_MAX`. And probably catch an `OSError`.
Use `utils.get_filesystem_encoding` instead.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Use bare `re.match`.
`else` is superfluous.
`--no-playlist` is not respected.
No need to escape `]` is character set.
Both test the same extraction scenario. The rule is one test per extraction scenario.
`<span[^>]+class="name">...` is better.
Query should be passed as `query` parameter.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Request wrapping code can be moved to the base class.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Re-read my post with eyes please.
A minor bit: don't use a capturing group if it's not used.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
Instead of such hacks you can name group differently and capture it without any issue.
This line can just be removed.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
You can't know whether it's an article with no video or video id pattern just changed. Original approach should be reverted.
This is superfluous.
Should not break if missing.
Python 3.2 doesn't like u-literals.
Optional fields should not break extraction if missing.
Do not shadow existing variables.
Read: coding conventions, optional fields.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Same for re.search
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Removing useless noise.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
Won't work for `info = {'title': None}`.
No such key `thumbnailUrl` possible in `info`.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
```suggestion if not (season_id and video_id): ```
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
These looks like mandatory fields.
Consistently use flags inside regex.
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
Note `video_data` may be `None`.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
No trailing $, override suitable.
It does not necessarily mean that.
Breaks. Read coding conventions.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion next_page, 'next page link', group='url', default=None)) ```
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Code duplication 173, 213. There is no sense to extract fields explicitly.
```suggestion if not (season_id and video_id): ```
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
remove the duplication as much as possible(`https://www.vvvvid.it/vvvvid/ondemand/%s/` is always used).
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
Playlist metadata must not be fatal.
Playlist id and title should not be fatal.
By providing username and password in params obviously.
For rtmp it's always flv despite of the extension.
`{}` doen't work in python 2.6.
Name an example URL where `og:description` has HTML tags.
Use `self._search_regex` and `utils.unified_strdate` instead.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Either sloppy code or an anti-scraping measure.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Instead of escaping the inner double quotes you could single-quote the string.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
`{}` does not work in python 2.6
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Parentheses make this clearer although not required: ```suggestion uploader_url = ('https://parler.com/' + uploader_id) if uploader_id else None ```
```suggestion new = '' ```
"" -> ''
This is always true.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
This doc should be updated.
Just modify mimetype2ext rather than introducing hacks in individual extractors
`.*/?` is pointless at the end.
This is equivalent to InfoExtractor._match_id
No exact matches for URLs.
Do not match exact URL.
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This should be just `return info`
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Prefer `post.get()` for these two.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
The description is always optional, so there should be a `fatal=False` in here.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
works also for me :+1:
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
This worked for me.
Video id length is 8.
All formats should be extracted.
`note` and `errnote` of `_download_json` instead.
All methods only used once should be explicitly inlined.
This is fatal.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Breaks if no URLs extracted.
No need in another test.
It's obvious from `'is_live': True`.
It's a field name not a step name.
Carry to the indented beginning of the line.
Again: relax regex.
Relax regex, make group unnamed, don't capture empty dict.
Lack of information is denoted by `None` not `0`.
Move flags into regex.
Formats not sorted.
No need for escapes inside a brace group, all dots outside must be escaped.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
Because all of them use the same extraction scenario.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
This does not work in python 2.6.
Keep test with non-ad rendition.
Should be more relaxed.
Pass video id.
I think it's customary to use `_VALID_URL` for id matching if possible.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
According to [PEP8](http://www.python.org/dev/peps/pep-0008/), there should be a space after the comma. But again, better just move the contents of this function to the bottom.
What if the title does not start exactly 30 characters after this? Also, what if this string is not found.
Carry long lines. Bother to finally read coding conventions.
Check code with flake8.
All methods only used once should be explicitly inlined.
Use `_search_regex`, it reports an error message if the regex doesn't match.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Dot is pointless here.
It's not an album id.
Not a video id.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
This does not mean it should not be included.
This is done automatically.
Breaks. Read coding conventions.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
Referring `url` from `url` looks like nonsense. Provide rationale.
No brackets needed.
This is no longer actual.
This is useless at the end.
Default note is enough.
Capturing empty string is senseless. If you expect a dict then capture group must contain `{}`.
This should be extracted from `_VALID_URL` with `self._match_id`.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
All video formats must be extracted.
No newline at the end of file.
Shouldn't be fatal
No need for that.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Default part should be removed since there is no default.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
Post-processors are already identified by `key` in API same should be used here.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
try_get is pointless here. Read coding conventions.
`get` is pointless since availability of result key is mandatory.
Simple string concatenation is enough here.
try_get is pointless here.
Here you must use `urljoin`.
Breaks on missing file key.
`tracks` is not guaranteed to be iterable.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
Incorrect. find returns -1 on failure that is Trueish value.
Must not be fatal.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
What's the point of this? Use `url` as base.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Use `self.url_result(inner_url, 'Generic')` instead.
Not a video id.
But it would need to modify `gen_extractors`, it's not a great deal.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
You can import `try_rm` from helper
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Actually, it's an opposite. It's a check for successful login.
Code duplication. This is already implemented in `CeskaTelevizeIE`.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
strip_jsonp should work here
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`_search_regex` is enough.
Just like I expected storage location has changed meanwhile so this code is completely broken. As I've already pointed out: match **only** `data-source` attribute.
This may change as well. Add a fallback that just processes all videos without differentiation.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Either `text` or `html` is mandatory.
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
This is useless at the end.
This is no longer actual.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
No brackets needed.
Referring `url` from `url` looks like nonsense. Provide rationale.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
no longer needed.
there is not need for excess verbosity.
will return invalid URL if `search_url` is `null`.
make one of the tests an `only_matching` test.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Must be `int`.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
The JSON returned by the API call is in `application/ld+json` format; we have a method `_json_ld()` for handling that, but the data here uses constructs that it doesn't (yet) support. Anyhow, try handling all the `asset_format`s here; maybe this version will work; only the actual format URL is mandatory: ```suggestion # actually, quality processing doesn't improve on just the width # a filesize is available but is 0 or doesn't change with quality qualities = qualities('hq', 'mq', 'lq') for asset_format in asset_formats: f_url = url_or_none(asset_format.get('contentUrl')) if not f_url: continue ext = determine_ext(f_url) transcodingFormat = try_get(asset_format, lambda x: x['transcodingFormat'], dict) or {} label = (strip_or_none(transcodingFormat.get('label') or '').split('-') extra = ( ('width', int_or_none(transcodingFormat.get('videoWidth'))), ('quality', qualities(label[0])), ('language', asset_format.get('language')), ) if ext == 'm3u8': # expect 'mimeType': 'application/vnd.apple.mpegurl' fmts = self._extract_m3u8_formats( # if the yt-dl HLS downloader doesn't work: `entry_protocol='m3u8'` f_url, video_id, ext=mp4, entry_protocol='m3u8_native', m3u8_id=transcodingFormat.get('formatType'), fatal=False)) for f in fmts: f.update((k, v) for k, v in extra if f.get(k) is None) formats.extend(fmts) else: # expect 'mimeType': 'video/mp4' fmt = {'url': f_url} fmt.update(extra) formats.append(fmt) ``` Then call `self._sort_formats(formats)` either here or, better, in `_real_extract()` after returning from here: if there aren't any formats, it raises.
After this, check for mandatory `title` field, and use the value in the returned info_dict: ```suggestion video_data = self.get_video_data(url, video_id) title = video_data['title'] ```
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
As above, `final_asset.get("contentUrl")` could return `None`. Get the mandatory value early; crash if that fails.
Query to `query=`.
Should not be fatal.
Should be tolerate to missing keys in `media`.
Carry long lines. Read coding conventions.
Must not be fatal.
Must only contain description.
Must only contain title.
This is never reachable.
Playlist title is optional.
This is default.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
still fails if `uploader_data` not available.
I suggest `fatal=False`
use `query` argument.
should not break the extraction if the field is not available.
try to parse multiple formats, set `vcodec` to `none`.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
`creator` should be preserved for backward compatibility.
Redundant or not it should be preserved either by providing manually or by copying from `artist` automatically when not present (that is not implemented) since it can be used by someone already.
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
I've already suggested using single quotes.
Use _ (underline) instead of webpage if the value is not used.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Are all of these necessary? I think youtube-dl defaults suffice.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
`lxml` is not part of the standard library, you'll need to use regular expresions.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Network connections in your browser.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
All video formats must be extracted.
```suggestion if episodes: ```
Shouldn't be fatal
No newline at the end of file.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
The description is always optional, so there should be a `fatal=False` in here.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Must be int.
All these regexes should be relaxed.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
It should always return a list.
This doc string does not match the function now.
`id` and `display_id` should be tested as well.
Use quotes consistently - only a single or only a double. Use different kind of quotes only if there is no way to use same kind of quotes.
As already mentioned `id` and `display_id` should be extracted.
python's `json` use escape sequences, but both are supported. I prefer to directly use the character (imaging having to use escape sequences for titles in chines, arabic ...).
I don't see any reason to use unicode escape sequences here, you can directly use `'Ã­'`.
Do not shadow built-in names.
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
All tests that test similar extraction scenario should be `only_matching`.
Will never happen.
Please remove debugging lines.
There is no need to test URLs.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
I've already pointed out: API URLs should be used.
API URLs should be used.
Formats should have `quality`.
Code duplication should be eliminated.
Do not shadow existing variables.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Still a code duplication.
Using `FormatData` four times.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
should not break the extraction if the field is not available.
use `query` argument.
just use a hardcoded value for now.
accept `audio` `mediaType`.
This is superfluous since you provide `formats`.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Do not escape quotes inside triple quotes.
this should be done once(in `_real_initialize`).
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
Move to the place of usage.
`info_dict['formats'] = formats`.
Move into `_download_json`.
`acodec == 'none'`.
It's already done at L151.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
This is equivalent to InfoExtractor._match_id
```suggestion new = '' ```
The argument will already be a character string, no need to decode it.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`compat_str()`, here and in l.96.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Use `sanitize_url()`, or let the core code, which does so, fix it.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
`else` is superfluous.
Do not pass `default=None` to `_html_search_regex` instead.
Use compat_HTTPError instead
Never ignore generic exceptions
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
This does not mean it should not be included.
This is done automatically.
Use more python idiomatic `.get('title')` and `if alt_title:` instead.
What's the point of this? `canonical_url` is the same as `url`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
I did this so I can deprecate `--get-url` (along with all other --get options). Unless you want to do that too, `urls` field isn't really needed
To be removed.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
I've already pointed out: this must be removed.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
> Also append_url_to_file may be renamed to something better reflecting it's actual content. the function can be removed completely and move the code directly to the for loop.
121, 124 - DRY.
These are not used.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
For now this should not be printed or only printed in `verbose` mode.
This could be moved several lines up.
Since you now skip already downloaded segments the total fragment message displays wrong value after restarting: `[hlsnative] Total fragments: ...`. As a result - wrong download progress data.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
As above, you can omit this and let the core set it from `release_timestamp`: ```suggestion ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Let's try to force the CI tests, and also ensure no final `/`: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)).rstrip('/') ```
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
Again: float_or_none, not parse_duration.
Lack of information is denoted by `None` not `0`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
No `upload_date` is guaranteed to be present.
Breaks on `-F`.
I've already pointed out this does not work.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
No, it's not the point. It **must not** skip items than belong to the range specified.
It does since there may be no postprocessing at all.
I'm addressing this concrete line of code.
This will result is `[None]` is no category extracted.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
Carry long lines. Bother to finally read coding conventions.
hls and rtmp are available as well.
Network connections in your browser.
What's the point of lines 104-108? `ext` is already flv.
Should not be fatal.
This is already fatal.
No such meta field.
Not used with formats.
This is superfluous since you provide `formats`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
`compat_str()`, here and in l.96.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This does not work in python 2.6.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
I guess `filename` can also be included.
This will fail if neither mplayer nor mpv is available.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
You must output to a temp file not the original file.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
There are lots of Content-Type calls. Please merge them together
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
34-35 can be easily moved into `for`.
1. Don't shadow outer names. 2. `url_or_none`.
Real id is in widget dict.
There should be a hardcoded fallback since it's always the same.
`.*` at the end does not make any sense.
Dots not escaped.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
This does not make any sense, you already have `url`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
Use `{}` dicts.
Read coding conventions and fix optional meta fields.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
`title` must never be `None`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks extraction if `release_date[0:4]` is not `int`.
Code duplication in 70-102 and 104-137.
I've already pointed out: I won't accept this.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
Code duplication in 142-145 and 146-149.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
Must be list, not a string.
Use `_parse_json` instead.
`id` should be extracted via `_VALID_URL` when present.
Sholdn not break the extraction if missing.
Must be numeric.
This should be extracted first.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`flashvars[k]` is `v`.
`int_or_none` for all int fields.
No bare except.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Use `compat_urlparse.urljoin` instead.
This code looks similar to `sd` format and can be extracted to a function.
m3u8 is also available.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Read coding conventions on optional/mandatory meta fields.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
No bare except.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
This is fatal.
Use _ (underline) instead of webpage if the value is not used.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
still fails if `uploader_data` not available.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
Replace with `'md5:'`.
It's already extracted as video_id.
`video_data` is totally useless. Write directly to id variable when found.
As already said: parse as JSON not with regexes.
1. `v` may not be dict. 2. `v.get('slug')`.
Do not touch this.
Do not touch this.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
Don't capture groups you don't use.
What's the point of this? Use `url` as base.
Code duplication 173, 213. There is no sense to extract fields explicitly.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Breaks downloading of videos that does not require authentication.
There are two scenarios: video without auth and video with auth. The rest are matching only.
https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/bigflix.py#L23-L25 All tests that test same extraction scenario should be made `only_matching`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use single quotes consistently.
Automatic captions are also available.
Caps is used for constants.
Should not break extraction is download fails.
Query should go in `query=` .
Don't lookup `lang_code` twice.
I did not suggest moving `subtitle_original_lang_code` into loop.
Move into loop.
Don't lookup twice.
You're already building query with `query` param.
92-115, 133-157 codes are identical apart from entry tag and query.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
You're not guaranteed `thumbnails` is a list.
Should be better with ```isinstanceof(filename, BytesIO)```
There is no point to use `remove_start` since line is always a string.
Consistently use single quotes.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
No, ADS is the equivalent of xattr. I'd rather have both in one option.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
you can get json output by appending `&format=json` to the api request url.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
Avoid shadowing built-in names.
Again: float_or_none, not parse_duration.
Read coding conventions on optional and mandatory data extraction.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Should not break if there is no `resolution` key.
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Optional data should not break extraction if missing. Read coding conventions.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Merge in single list comprehension.
Consts should be in uppercase.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Merge in single list comprehension.
Consts should be in uppercase.
`url` should not be `None`.
Rename to something else.
You must delegate with `url_result` instead.
`show = data.get('show') or {}`
Lack of information is denoted by `None`.
Should not break if `published_at` is missing.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
`.*/?` is pointless at the end.
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
All these import are unused, check your code with flake8.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
If both test the same extraction scenario leave only one.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
**Never ever** use `eval` on data you don't control.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Breaks on None.
Matching empty string is senseless.
Either sloppy code or an anti-scraping measure.
Temp file is not removed in this case.
Sorry - dismiss that
The current working directory is not always writable.
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
Right, that's fine for `display_id`.
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
Read coding conventions on optional and mandatory data extraction.
Remove all useless noise.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
Noway. See other extractors on how to delegate properly.
Avoid unrelated changes.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Remove useless code.
This is pointless.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
```XimilayaIE.ie_key()``` is better
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
The same. Should use https if ```url``` use https.
Same issue for re.search
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
Relax `id` group.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
`flashvars[k]` is `v`.
This is pointless.
Formats in webpage are still available and should be extracted.
This code looks similar to `sd` format and can be extracted to a function.
Remove useless code.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
Use `self._sort_formats(formats)` instead.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This does not make any sense, you already have `url`.
Relax `id` group.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This field is added automatically no need to add it by hand.
Simplify to `rtmp://camwithher.tv/clipshare/%s' % ('mp4:%s.mp4' if int(video_id) > 2010 else video_id)`.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This code looks similar to `sd` format and can be extracted to a function.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Do not remove the old pattern.
No escape for `/`.
Must not be fatal. Do not capture empty string.
All formats should be extracted.
Use `\s*` instead.
Do not capture empty strings.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
All these fields should be `fatal=False`.
Must be int.
All these regexes should be relaxed.
Use `self.url_result(inner_url, 'Generic')` instead.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
```suggestion new = '' ```
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Either sloppy code or an anti-scraping measure.
Filter invalid URLs.
You technically can't login with this extractor apart from using cookies.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
```suggestion 'noplaylist': True, ```
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
Don't lookup twice.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
I did not suggest moving `subtitle_original_lang_code` into loop.
Move into loop.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
Don't lookup `lang_code` twice.
92-115, 133-157 codes are identical apart from entry tag and query.
You're already building query with `query` param.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
Use single quotes consistently.
idk what the maintainers think about this, but I personally think this change is out of the scope of this PR. If this function is desired, it can be added separately. For now, you could just replace `self._match_valid_url(url)` with `re.match(self._VALID_URL, url)` as many other extractors already do.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Use ```query``` parameter of ```_download_webpage``` instead.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
Use `\s*` instead.
`self._html_search_meta` is better here.
Better to use integers for supported_resolutions and use str() here
No escapes for slash.
Breaks on None.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Nothing changed. Also there is a video id available in JSON.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Breaks on `None`.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Don't shadow built-ins.
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
If a method is used by at least two extractors, it can be moved to ```common.py```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
You don't need to call `report_extraction` here and above, since the extraction is over already.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
As we're always going to want `type` later (but that's a Python keyword, so rename it): ```suggestion main_id, type_ = re.match(self._VALID_URL, url).group('id', 'type') ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
I think it's customary to use `_VALID_URL` for id matching if possible.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
`'%s'` is very unlikely to be a helpful error message.
Indenting is messed up here.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
We should really provide a better interface to test against, something along the lines of `download(url)`.
This will break unicode strings under python 2.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
You can import `try_rm` from helper
No, ADS is the equivalent of xattr. I'd rather have both in one option.
I don't know if there's an actual performance gain, but it's common to use a list: ``` ret = [] for x in ...: ret.append(newstring) return ''".join(ret) ```
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
Must be `int`.
Must not be `None`.
First group is superfluous.
This matches multiple videos.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
All debug code must be removed.
No. Revert as it was.
No. `_search_regex` is already fatal and reports proper error message. Revert.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
The `return` is not needed anymore.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
1. Do not remove the old pattern. 2. Relax regex.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
There should be a hardcoded fallback since it's always the same.
This should not be fatal.
This matches multiple videos.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
There should be an `id` group.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I guess it a typo? now -> not
There's no need to `return` after `raise`.
You should consult some git manual.
You don't check whether login succeeded or not.
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
It may be interesting to allow to add new items just as strings, for example : ``` IEs = [ ('Youtube', ['YoutubePlaylistIE', 'YoutubeChannelIE', 'YoutubeUserIE', 'YoutubeSearchIE', 'YoutubeIE']), 'Generic', ] ``` It would make easier to add simple IEs.
This doc should be updated.
I think you don't need `locals()` here.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
But it would need to modify `gen_extractors`, it's not a great deal.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
will be extracted from the URL.
I suggest `fatal=False`
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
More relaxed regex.
```suggestion new = '' ```
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
This TODO needs work
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
`_search_regex` is enough here.
No need for this check, this is already checked in `_sort_formats`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
`<span[^>]+class="name">...` is better.
Should extract chunklists via `self._extract_m3u8_formats` here.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
No need to escape `]` is character set.
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
This must be checked **before** any processing.
This must be checked **before** any processing.
Duration calculation is incorrect.
This has no effect. Postprocessors work on info dict copy.
I've already pointed out: I won't accept this.
You have some unmerged lines here
No need for this check, this is already checked in `_sort_formats`.
Request wrapping code can be moved to the base class.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Don't do this manually use `_download_webpage_handle`
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Plain `for x in l`.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Extract `height` for each format.
Inline everything used only once.
Do not match by plain text.
Escape dot. No need to split URL.
Place on a single line.
Again: it will capture empty string for `data-playlist-item=""`.
Capturing empty string does not make any sense.
Must not be fatal for playlist.
All formats must be extracted.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Read code conventions on optional fields.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Don't shadow built-ins.
Lack of information is denoted by `None` not `0`.
None is default.
Nothing changed. Also there is a video id available in JSON.
No need for escapes inside a brace group, all dots outside must be escaped.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
Do not remove the old pattern.
Video URL is mandatory. Read coding conventions.
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
Remove all garbage.
All these regexes should be relaxed.
Empty string capture does not make any sense.
This should be just `return info`
Same as above, we should create a `formats` array here.
We may use proper XML parsing here and simply call `self._download_xml`
This is too broad and detects the same video twice.
From what I've seen there is always only one video on the page thus no need in playlist.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Code duplication should be eliminated.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
Uppercase is used for const.
Again: remove. Any download attempt must have clear explicit message.
Read coding conventions.
Breaks on `None`.
This is already fatal.
Capturing empty URL is senseless.
No such meta field.
Don't shadow built-ins.
Should not be fatal.
Nothing changed. Also there is a video id available in JSON.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All formats should be extracted.
`int_or_none` for all int fields.
No bare except.
m3u8 is also available.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Query should go in `query=` .
Should not break extraction is download fails.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
Don't lookup `lang_code` twice.
Don't lookup twice.
Move into loop.
I did not suggest moving `subtitle_original_lang_code` into loop.
92-115, 133-157 codes are identical apart from entry tag and query.
You're already building query with `query` param.
You can use a `dict` expression, and the return value shouldn't be a list: ```py return { 'url': video_url, 'ext': 'webm', 'id': video_id, 'title': video_id, 'license': licenze, } ```
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Don't shadow built-in names.
No need to escape `]` is character set.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
generator or PagedList instead of list.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
No escape for `/`.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Breaks if no URLs extracted.
Never use bare except.
You must make it **not break** as you delegate.
You are **delegating** to brightcove that provides the id.
Playlist title is optional, description breaks.
Will never happen.
Remove all debug garbage.
Noway. See other extractors on how to delegate properly.
[`determine_ext`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L901) is more robust. However, usually you don't need to specify `ext` in formats dictionary.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
[Correct way](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/pluralsight.py#L95-L100) to implement order independent argument matching.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
Part after `\?` should be removed since it's not used anymore.
`'%s'` is very unlikely to be a helpful error message.
Indenting is messed up here.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All methods only used once should be explicitly inlined.
This is fatal.
Query should be passed as `query` parameter.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
Not used with formats.
No such meta field.
No. Use fatal search regex instead.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Must be extracted first.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion if episodes: ```
Same issue for urlh
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
Use `self._html_search_meta` instead.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
There is no such field, must be `uploader`.
This should actually be just `self.url_result(embedded_url)`.
`/kenh-truyen-hinh/ in url`.
`/video/ in url`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Formats not sorted.
```suggestion new = '' ```
All formats must be extracted.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
Code in bracket is a set of matching characters. `or`ing won't work.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
EntryId must be extracted the very first.
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Breaks on `None`.
Breaks on `default=False`. `title` must be fatal.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
Use whitespace characters consistently.
Must be fatal.
No escapes for `/`.
Code duplication. Must be single call to search regex.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
I've already pointed out - title **must be fatal**.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Original regexes should be tested first.
**Do not remove** `_search_regex` part.
>Adding another fallback here does not make much sense since title is not used when delegated to kaltura anyway.
If `_search_regex` fails `None` will be passed to `_parse_json`.
`title` is mandatory. Move flags into regex itself.
No need to escape whitespace.
fallback to other available values.
This is not true at the moment.
These will be needed later: ```suggestion from ..utils import ( get_element_by_id, get_element_by_class, int_or_none, js_to_json, MONTH_NAMES, qualities, unified_strdate, ) ```
Still does not handle aforementioned URLs.
Don't capture groups you don't use.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
Apparently the page data has changed: ```suggestion 'uploader': 'chixa33', 'description': 'md5:5b067801318e33c2e6eea4ab90b1fdd3', ```
Add data to help `upload_date` extraction and a sorting key: ```suggestion _MONTH_NAMES_KA = ['áááááá á', 'ááááá áááá', 'ááá á¢á', 'ááá ááá', 'áááá¡á', 'ááááá¡á','ááááá¡á', 'ááááá¡á¢á', 'á¡áá¥á¢ááááá á', 'áá¥á¢ááááá á', 'ááááááá á', 'áááááááá á'] _quality = staticmethod(qualities(('SD', 'HD'))) ```
Oh, alright, leave it in. I was suprised to find a `\x` instead of a `\u`, but specifically for the non-breaking space, that should be fine.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
yes, this is correct.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You still duplicate the URL and unnecessary `if/elif` branches.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
should not fail if the extraction of one set or item is not possible.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Breaks if no `name`.
Do not use leading underscore for locals.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Extracting duplicate code into a function obviously.
Read coding conventions on optional/mandatory meta fields.
Constant names should be in uppercase.
`default=None` for the first.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
Both used only once, move to the place where used. Also relax both regexes.
All these regexes should be relaxed.
Empty string capture does not make any sense.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion if episodes: ```
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This may change as well. Add a fallback that just processes all videos without differentiation.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
Should be non fatal.
This code looks similar to `sd` format and can be extracted to a function.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Code in bracket is a set of matching characters. `or`ing won't work.
EntryId must be extracted the very first.
There should be a hardcoded fallback since it's always the same.
You must use `default` if there is a fallback after it.
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
It's already extracted as video_id.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
As already said: parse as JSON not with regexes.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Wrong fallback value in `thumbnail`, `description` and `creator`.
All these regexes should be relaxed.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
JSON should be parsed as JSON.
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Should not be fatal.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Remove unnecessary verbosity.
This must be an id of the media.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
Not used with formats.
No such meta field.
No. Use fatal search regex instead.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
This is fatal.
`note` and `errnote` of `_download_json` instead.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
`int_or_none` for all int fields.
No bare except.
All methods only used once should be explicitly inlined.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Use `self._download_json` instead.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Request wrapping code can be moved to the base class.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
It also failed in your previous commit: https://travis-ci.org/rg3/youtube-dl/jobs/8459585. b64encode returns bytes, you must decode to get a string : `base64.b64decode(googleString).decode('ascii')`, I'm not sure since I have little experience with base64.
Make more relaxed and add title regex as fallback.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Read coding conventions on optional fields.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
You must delegate with `url_result` instead.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
This is equivalent to InfoExtractor._match_id
Rename to something else.
`url` should not be `None`.
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`_` is idiomatic way to denote unused variables.
None is default.
Field name is supposed to be `key` not `long_video_id`.
`_search_regex` is enough here.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
No need for this check, this is already checked in `_sort_formats`.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Remove useless code.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All methods only used once should be explicitly inlined.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
That's very brittle.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`'id'` is required.
generator or PagedList instead of list.
You technically can't login with this extractor apart from using cookies.
Should handle `src` with `https?:` also.
A minor bit: don't use a capturing group if it's not used.
Code duplication with base class method.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`{}` does not work in python 2.6.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
_og_search_description is a youtube-dl function
Use ```_og_search_description()``` as a fallback instead.
This line is unnecessary as there's already self._og_search_description
This may match across several `meta`s.
Must not be fatal. Do not capture empty string.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
More relaxed regex.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
Parentheses make this clearer although not required: ```suggestion uploader_url = ('https://parler.com/' + uploader_id) if uploader_id else None ```
"" -> ''
Read coding conventions and fix code failsafeness.
This is always true.
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
accept `audio` `mediaType`.
should not break the extraction if the field is not available.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
try to parse multiple formats, set `vcodec` to `none`.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
extract mandatory information(title and formats) first, and sort formats.
No exact URLs here.
Does not work as expected in all cases.
Do not change the order of extraction scenarios.
`{}` won't on python 2.6.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
I suggest `fatal=False`
will be extracted from the URL.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
Do not touch existing tests.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Set empty `vcodec`.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Referring `url` from `url` looks like nonsense. Provide rationale.
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Capture as `id` obviously.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
No need to escape `]` is character set.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
This doc should be updated.
I think you don't need `locals()` here.
But it would need to modify `gen_extractors`, it's not a great deal.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
This TODO needs work
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
At this point `iteminfo` isn't known to be a dict. If it isn't, or it has no key `status_code`, this will crash, when it might be preferable to catch all these potential failures in the ExtractorError. ``` ... iteminfo = self._download_json('https://www.douyin.com/web/api/v2/aweme/iteminfo', video_id, query={'item_ids': video_id}) or {} status_code = iteminfo.get('status_code', 'status_code missing') if status_code: raise ExtractorError('%s (%s)' % (iteminfo.get('status_msg', 'status_msg missing'), status_code), video_id=video_id) ... ```
I did this so I can deprecate `--get-url` (along with all other --get options). Unless you want to do that too, `urls` field isn't really needed
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
Playlist metadata must not be fatal.
you already have `pgm_id` and `pgm_no` variables now.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
there is no need for an `else` block.
```suggestion if not (pgm_id and pgm_no): ```
Extracting duplicate code into a function obviously.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
Course extraction must be in a separate extractor.
Read coding conventions on optional and mandatory data extraction.
Playlist title is optional, description breaks.
Breaks if no URLs extracted.
No `ExtractorError` is raised here, `except` will never trigger.
Will never happen.
Remove all debug garbage.
Noway. See other extractors on how to delegate properly.
Check code with flake8.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Matching empty data is senseless.
Must not be `None`.
This may change as well. Add a fallback that just processes all videos without differentiation.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Never use bare except.
Instead of `resolution` and `preference` it should be extracted as `height`.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Use single quotes consistently.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
I've already suggested using `Downloading` as idiomatic wording.
Ok, It's up to you.
`formats` can be filtered out completely. ``` python -m youtube_dl nVlEpd92MJo -f "best[height<40]" [youtube] nVlEpd92MJo: Downloading webpage [youtube] nVlEpd92MJo: Downloading video info webpage [youtube] nVlEpd92MJo: Extracting video information [youtube] nVlEpd92MJo: Downloading DASH manifest [youtube] nVlEpd92MJo: Downloading DASH manifest Traceback (most recent call last): File "c:\Python\Python279\lib\runpy.py", line 162, in _run_module_as_main "__main__", fname, loader, pkg_name) File "c:\Python\Python279\lib\runpy.py", line 72, in _run_code exec code in run_globals File "C:\Dev\git\youtube-dl\master\youtube_dl\__main__.py", line 19, in <modul e> youtube_dl.main() File "youtube_dl\__init__.py", line 406, in main _real_main(argv) File "youtube_dl\__init__.py", line 396, in _real_main retcode = ydl.download(all_urls) File "youtube_dl\YoutubeDL.py", line 1614, in download url, force_generic_extractor=self.params.get('force_generic_extractor', Fals e)) File "youtube_dl\YoutubeDL.py", line 667, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "youtube_dl\YoutubeDL.py", line 713, in process_ie_result return self.process_video_result(ie_result, download=download) File "youtube_dl\YoutubeDL.py", line 1273, in process_video_result formats_to_download = list(format_selector(formats)) File "youtube_dl\YoutubeDL.py", line 991, in selector_function for format in f(formats): File "youtube_dl\YoutubeDL.py", line 1022, in selector_function yield formats[format_idx] IndexError: list index out of range ```
And `audio_selector` as well to catch `-f 'bestvideo+'`.
I would add an additional check here to catch syntax errors like `-f 'bestvideo,,best'`. ``` python if not current_selector: raise syntax_error('Expected a selector', start) ```
Breaks on unexpected data.
Breaks on unexpected data.
`[]` is superfluous in group with single character.
``` for i, video_url in enumerate(video_urls): ```
I've already pointed out: use `display_id` until you get real id.
No `id` extracted.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Same, no such key possible.
Won't work for `info = {'title': None}`.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
Breaks if no URLs extracted.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Will never happen.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Never use bare except.
Playlist title is optional, description breaks.
No `ExtractorError` is raised here, `except` will never trigger.
Remove all debug garbage.
Noway. See other extractors on how to delegate properly.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
I've already pointed out: use `display_id` until you get real id.
No `id` extracted.
This is checked by `_search_regex`.
Use display id.
`default` is not used with `fatal`.
Check code with flake8.
There are also vtt subtitles available.
Read coding conventions on optional/mandatory meta fields.
1. Provide clear evidence this field is supposed to store the video URL. According to http://atomicparsley.sourceforge.net/mpeg-4files.html `tvnn` is a TV Network Name and obviously has nothing to do with video URL at all. 2. This does not guarantee the actual container is `mp4`. 3. This won't work for audio.
This can be moved inside `if chapters:` condition.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
1. Single quotes. 2. `expected`.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
This does not make any sense, you already have `url`.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will easily match outside the element.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Use _parse_json and js_to_json here
Formats not sorted.
```suggestion new = '' ```
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
No such meta field.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
- - [ ]
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
No trailing $, override suitable.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
This must be assert not exception.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
Query should be passed as `query` parameter.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
`True` is default.
This is pointless, you don't have any fallback.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
Query should be passed as `query`.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
56-71 code duplication.
Default sorting is just fine. Remove `field_preference`.
```suggestion new = '' ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If you accept this to be missing then it must be `default=` not `fatal`.
All of these will break extraction on unexpected data.
This is never reached cause sort formats will throw on `not formats`.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
* the first element of the path can have more than 1 digit * the tail `/player\.html` isn't needed (unless there are other tails that should find different media with the same id): ```suggestion _VALID_URL = r'https?://(?:www\.)?embed\.vidello\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)' ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
The same. Should use https if ```url``` use https.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
Same issue for re.search
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
Default part should be removed since there is no default.
`autonumber` is not reset to zero in the first place.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
then you should use `_remove_duplicate_formats` method.
i'm not sure why you're adding this step.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
again, extract all formats, there is no guarentee that those formats are available in all videos.
as i said there is no guarantee that this will be the case all the time.
capture only what is needed for the extraction.
- extract all formats. - try not complicating things, try to avoid using hardcoded values.
i think the use of `videotitle` is not reliable(it also has a problem with escaped double quotes), instead of this cleanup, other sources should be prefered.
add more fallbacks and extract `timestamp`.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Request wrapping code can be moved to the base class.
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
The third parameter of `_html_search_regex` is name but not ID.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Eliminate code duplication.
Use `_sort_formats` instead.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Formats in webpage are still available and should be extracted.
This is pointless.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This is bitrate, not quality.
Breaks if no `rate` key in `stream`.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`_` is idiomatic way to denote unused variables.
None is default.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
We use single quotes as much as possible.
Also pass `m3u8_id='hls'`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Correct field name is `format_id`.
Matched data-video should not be empty.
`self._search_regex` is enough here.
`ext` here makes no sense.
170-172 - code duplication.
`title` is mandatory. Move flags into regex itself.
Read coding conventions and fix all optional meta fields.
No need to specify this.
Should be `flv`.
Title is mandatory.
No such meta field.
`'thumb`' may not be present producing invalid thumbnail url.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Lack of information is denoted by `None` not `0`.
No need to escape `#`. No need to capture groups you don't use.
m3u8 is also available.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
No hardcodes. Use API.
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
Dots should be escaped.
According to [PEP8](http://www.python.org/dev/peps/pep-0008/), there should be a space after the comma. But again, better just move the contents of this function to the bottom.
Code in bracket is a set of matching characters. `or`ing won't work.
What if the title does not start exactly 30 characters after this? Also, what if this string is not found.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
Carry long lines. Bother to finally read coding conventions.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Code duplication at 58-60 and 68-70.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
If `_search_regex` fails `None` will be passed to `_parse_json`.
`title` is mandatory. Move flags into regex itself.
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
This video is georestricted.
Use `self._match_id` is better.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Also pass `m3u8_id='hls'`.
Correct field name is `format_id`.
`ext` here makes no sense.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
All debug garbage must be removed.
It should match all non empty domain names.
Do not capture a group if you don't use it.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
It's not an album id.
Dot is pointless here.
Not a video id.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
This condition is not needed, t is always None here.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
Dots should be escaped
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
You can import `try_rm` from helper
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Do not capture empty strings.
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
url and formats are not used together.
No trailing $, override suitable.
Rename to `KanalDIE`.
The argument will already be a character string, no need to decode it.
Please use `print()` syntax, as we support also Python 3(.3+)
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
You have some unmerged lines here
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Use `\s*` instead.
We may use proper XML parsing here and simply call `self._download_xml`
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This should be just `return info`
We now have a fully-fledged format system which can be used.
Relying on `userMayViewClip` is probably a [bad idea](https://github.com/rg3/youtube-dl/commit/2b6bda1ed86e1b64242b33c032286dc315d541ae#diff-0b85b33765d2939b773726fda5a55b06).
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Better to use `unified_strdate()`.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
Read coding conventions on mandatory meta fields.
Parse drupal settings.
this does not handle the case where `contentUrl` value is `None`.
`parse_duration()` should work.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
No. Must be fatal. Read coding conventions on optional fields.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
There is no point in that.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
Optional data should not break extraction if missing. Read coding conventions.
There is no need in this method.
Same as in some previous PR.
I would always return a `multi_video` result.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
This code looks similar to `sd` format and can be extracted to a function.
This is pointless.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Again: float_or_none, not parse_duration.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
For 2.6 (yes, I know) compatibiility, `{0}` rather than `{}`, apparently. And number other formats elsewhere.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
All methods only used once should be explicitly inlined.
Remove all useless noise.
Use formatted strings.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
All similar tests should be `only_matching`.
Of course when it appears inside `script`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
You must use `default` if there is a fallback after it.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
You don't need to call `report_extraction` here and above, since the extraction is over already.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
1. Don't shadow outer names. 2. `url_or_none`.
34-35 can be easily moved into `for`.
Real id is in widget dict.
Relax `id` group.
This does not make any sense, you already have `url`.
resolution is defined as "Textual description of width and height". "medium" does not fit.
`width` and `height` instead.
- - [ ]
Playlist title is optional, description breaks.
This code looks similar to `sd` format and can be extracted to a function.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Fix test: ```suggestion 'ext': 'mp4', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
It should always return a list.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
This is never reached if Content-length is not set.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
This should only be printed when sleep interval is provided. It's kind of misleading and unrelevant to see when you didn't want any sleep interval.
This does not make any sense since you already sleep in `_request_webpage`.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Just copy paste the whole line I've posted.
Also pass `m3u8_id='hls'`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
`_search_regex`, `_parse_json`. Again: read coding conventions.
`default` implies non `fatal`.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
it's about the way you're setting `playlist_title`. `playlist_title` should not be set directly, it should be set in the playlist result title(by using the `playlist_result` method or using the playlist result type).
Playlist metadata must not be fatal.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
fallback to other available values.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`if not videos:`.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
This must be assert not exception.
Must be extracted first.
None is default.
Lack of information is denoted by `None` not `0`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
No bare except.
All formats should be extracted.
This is fatal.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Everything after `id` may be skipped and not matched since http://www.rte.ie/radio/utils/radioplayer/rteradioweb.html#!rii=16:10507902 works just fine.
Extraction should be tolerate to missing fields.
There is no need in this method.
`_` is idiomatic way to denote unused variables.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
Empty string capture does not make any sense.
This should be just `return info`
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
All formats should be extracted.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
If audio_info does not have category_name, categories will become [None, 'xxx']. It should be only ['xxx']
errnote still needs a fix. "try to parse web page" sounds wrong
group=1 is equivalent to the default behavior of _html_search_regex; just drop that.
(?:[^>]+)? can be simplified as [^>]*
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
Same issue for re.search
The same. Should use https if ```url``` use https.
Just use `'thumbnail'` to match the default image URL.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
There is no need in this method.
All these regexes should be relaxed.
Again: float_or_none, not parse_duration.
This code looks similar to `sd` format and can be extracted to a function.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I would always return a `multi_video` result.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Breaks if no `name`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
Must be fatal.
Breaks on `None`.
I've already pointed out - title **must be fatal**.
Breaks on `default=False`. `title` must be fatal.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Must be fatal.
This is default.
This is never reachable.
Playlist title is optional.
This must be done right after title extraction.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Query to `query=`.
Should not be fatal.
Should be tolerate to missing keys in `media`.
Must not be fatal.
```suggestion new = '' ```
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `video_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No trailing $, override suitable.
Use `re.sub` instead.
Read code conventions on optional fields.
Nothing changed. Also there is a video id available in JSON.
Read coding conventions on optional fields.
All formats must be extracted.
Don't shadow built-ins.
Lack of information is denoted by `None` not `0`.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
None is default.
No need for escapes inside a brace group, all dots outside must be escaped.
What I actually meant is to put them in a tuple or a list.
For tv/se `og:title` contains unnecessary suffix.
Just cut it with `utils.remove_end`.
If `_search_regex` fails `None` will be passed to `_parse_json`.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
No need for this check, this is already checked in `_sort_formats`.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Breaks if no URLs extracted.
All formats should be extracted.
`.*` at the end does not make any sense.
Avoid unrelated changes.
Must be extracted first.
url and formats are not used together.
No trailing $, override suitable.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Consistently use flags inside regex.
This is not true at the moment.
Plays fine without any authentication in browser.
`title` is mandatory. Move flags into regex itself.
Should be delegated via `url_result`.
All debug code should be removed.
For now extract it in a new base IE class.
This duplicates code from `MedialaanIE`.
170-172 - code duplication.
You should have only one single method for extracting info.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
This code duplication may be eliminated.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Can we resolve these IDs? There may also be a time encoded in there.
Check code with flake8.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Instead of such hacks you can name group differently and capture it without any issue.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
Don't capture groups you don't use.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
```suggestion 'url': 'http://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/', ``` This URL redirects to https://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/. This corresponds with the previous test L44 where we are using `play`.
There are two unrelated flags `_logged_in` and `logged_in`.
This is not true either. Login may be achieved via authorized cookies.
This is not necessarily true. Login errors should be detected and output.
You should **capture** error message and **output** it.
`default` implies non `fatal`.
There is no need in `fatal` when `default` is provided.
Code duplication should be eliminated.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Insert this alphabetically (after veehd and before veoh).
Don't touch unrelated stuff.
Keep this test as `only_matching`.
https://www.3sat.de/wissen/nano/190822-sendung-100.html for 3SAT https://www.zdf.de/kinder/wickie-und-die-starken-maenner/der-wegezoll-102.html for ZDF both running with the patch above download klappt mit dem Patch
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
All debug output should be removed. `{}` does not work in python 2.6.
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
This may change as well. Add a fallback that just processes all videos without differentiation.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
DRY with `_limelight_result`.
Extract `media_data['streamInfo']['sourceId']` into variable.
Some tests for these embeds could be useful, at least for reference.
I see. By the way, `cinematique` branch makes no sense since there is no such extractor.
Avoid shadowing built-in names, `id` in this case.
You can use self._download_json() here
Some not covered example URLs: https://www.nowness.com/category/art-and-design/antony-donaldson-robert-graham-soft-orange https://cn.nowness.com/category/èºæ¯ä¸è®¾è®¡/å¯¼æ¼åªè¾ä¸åº-antony-donaldson
`id` should also be relaxed to allow hieroglyphs.
Course extraction must be in a separate extractor.
Playlist title is optional, description breaks.
No point checking this.
There is no point checking `url`.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Carry long lines. Bother to finally read coding conventions.
`\s*` make no sense at the end.
No need to escape `/`.
This is determined automatically.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
Use display id.
This is checked by `_search_regex`.
This is superfluous, the extension can be extracted automatically.
Now you break extraction if any of these keys is missing.
No, override `suitable` instead. Also do not split strings.
There are also vtt subtitles available.
Check code with flake8.
All formats should be extracted.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
This is fatal.
No bare except.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All methods only used once should be explicitly inlined.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No need to escape `#`. No need to capture groups you don't use.
I've already pointed out: API URLs should be used.
API URLs should be used.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
Do not shadow existing variables.
`'%s'` is very unlikely to be a helpful error message.
Read coding conventions on optional and mandatory data extraction.
m3u8 is also available.
This code looks similar to `sd` format and can be extracted to a function.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
This code looks similar to `sd` format and can be extracted to a function.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
No need to escape `{}`.
Id from URL is not always a video id. Correct id is in JSON.
`'%s'` is very unlikely to be a helpful error message.
```suggestion new = '' ```
Indenting is messed up here.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
Field name is supposed to be `key` not `long_video_id`.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
`_search_regex` is enough here.
Should extract chunklists via `self._extract_m3u8_formats` here.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
No need for this check, this is already checked in `_sort_formats`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No. Override `suitable`.
This should not match playlist URLs.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
What's the point of this? `canonical_url` is the same as `url`.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
capture only what is needed for the extraction.
Right, that's fine for `display_id`.
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
No such meta field.
Read coding conventions on optional and mandatory data extraction.
```suggestion new = '' ```
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
The most simple way is to use `utils.extract_attributes`.
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
Else branch is useless.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
Direct URLs should also be extracted.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
Extract human readable title from the `webpage`.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Move flags into regex.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
Move to the place of usage.
`acodec == 'none'`.
Use `query` for query.
`info_dict['formats'] = formats`.
Move into `_download_json`.
It's already done at L151.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
This is already imported and (in general) you should only use `import`s at the top level.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
This will break extraction if no `id` present.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
This will result is `[None]` is no category extracted.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Breaks. Read coding conventions.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Read coding conventions on optional fields.
I would always return a `multi_video` result.
This will result is `[None]` is no category extracted.
This will break extraction if no `id` present.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
`show = data.get('show') or {}`
Lack of information is denoted by `None`.
Should not break if `published_at` is missing.
Rename to something else.
`url` should not be `None`.
You must delegate with `url_result` instead.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Breaks extraction if not available. Again read coding conventions.
Any test case using this approach? I can't find it.
Also pass `m3u8_id='hls'`.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Unite in single list comprehension.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
fallback to other available values.
parentheses not needed.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
`{}` won't work in python 2.6.
Won't work. See how this is done for output template.
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
This must be calculated once.
Don't relax regexes.
This must be checked **before** any processing.
This has no effect. Postprocessors work on info dict copy.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
Formats not sorted.
Carry long lines.
`.*` at the end does not make any sense.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
``` python course_id = self._search_regex( (r'data-course-id=["\'](\d+)', r'&quot;id&quot;: (\d+)'), webpage, 'course id') ```
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
`/?` is senseless at the end.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Check code with flake8.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
I've already pointed out: this must be removed.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Breaks when `get_element_by_class` returns `None`.
Must be fatal.
`default` and `fatal` are not used together.
Pass as list of regexes, don't bulk.
`'` can be used between `"` quotes and vice versa.
Matching between quotes is incorrect.
Also do not use double quotes for string literals.
Don't shadow outer names. No need for bracket when using single character.
Only whitespace is allowed between `videoInfo` and `=`.
No. Must be fatal. Read coding conventions on optional fields.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
There is no point in that.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
Optional data should not break extraction if missing. Read coding conventions.
Also `_valueless_option` is probably a better name for it.
Second check should be removed.
Yes, argumentless options handling should be moved in separate method as well: ``` python def _argless_option(self, command_option, param, expected_value=True): ... return [command_option] if param == expected_value else [] ```
121, 124 - DRY.
This does not look to be possible on a clean session.
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
All debug output should be removed. `{}` does not work in python 2.6.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
This will result in reference to unassigned variable when time fields are missing.
Unite in single list comprehension.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
Either sloppy code or an anti-scraping measure.
Carry long lines. Bother to finally read coding conventions.
You are **delegating** to brightcove that provides the id.
You must make it **not break** as you delegate.
Playlist title is optional, description breaks.
`.get()` idiom is used for optional fields only.
`'id'` is required.
Read coding conventions on optional and mandatory data extraction.
Rename to `KanalDIE`.
Should not be fatal.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Don't do this manually use `_download_webpage_handle`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Can we resolve these IDs? There may also be a time encoded in there.
In python 3.X print is a function, for printing to screen use `self.to_screen`. But if it's a fatal error then `raise ExtractorError`
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
The argument will already be a character string, no need to decode it.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
`title` must be mandatory I've already told about this.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
This may change as well. Add a fallback that just processes all videos without differentiation.
It should be robust in case of some missing fields.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
```suggestion if not (playlist_files and isinstance(playlist_files, list)): ```
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
This can now be omitted.
All formats must be extracted.
Bitrate should go to corresponding format meta field.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Read: coding conventions, optional fields.
I guess this was deleted by a mistake.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
There are more video formats available that should be extracted as well.
This should be removed. `player_url` is used for RTMP.
Either **do** or remove.
There should be a hardcoded fallback since it's always the same.
Or at least, it's not `unicode`. In yt-dl `str` should almost always be `compat_str`, but as above it's not needed here.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
EntryId must be extracted the very first.
This may change as well. Add a fallback that just processes all videos without differentiation.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
Everything apart from `url` is optional.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
This code looks similar to `sd` format and can be extracted to a function.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Unite in single list comprehension.
`_` is idiomatic way to denote unused variables.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
Move flags into regex.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
```suggestion new = '' ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
`try_get`, single quotes.
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Either sloppy code or an anti-scraping measure.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Does not work as expected in all cases.
If `_search_regex` fails `None` will be passed to `_parse_json`.
All these regexes should be relaxed.
Empty string capture does not make any sense.
Mandatory data must be accessed with `[]` not `get`.
Empty string capture does not make any sense.
Must be int.
`if images` implies the second check.
`images` is not guaranteed to be a list.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
Please use `print()` syntax, as we support also Python 3(.3+)
both are know beforehand, so there is no need to use `urljoin`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
falback to a static URL.
There is no point in that.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Indenting is messed up here.
`'%s'` is very unlikely to be a helpful error message.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
Shouldn't be fatal
```suggestion new = '' ```
`compat_str()`, here and in l.96.
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
Use single quotes consistently.
Never use bare except.
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
230-264 no copy pastes.
All these fields should be `fatal=False`.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Read and follow code conventions. Check code with flake8.
Should be non fatal.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
strip_jsonp should work here
Use ```query``` parameter of ```_download_webpage``` instead.
Better to use integers for supported_resolutions and use str() here
Dots should be escaped
Same issue for urlh
Use `utils.xpath_text` instead, again with `fatal=False`.
Everything apart from `url` is optional.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
Remove all garbage.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
Escape dot. No need to split URL.
As above, `final_asset.get("contentUrl")` could return `None`. Get the mandatory value early; crash if that fails.
Extract `height` for each format.
there is not need for excess verbosity.
no longer needed.
incorrect URLs for Cook's Country.
extraction should not break if an episode or all episodes couldn't be extracted.
will return invalid URL if `search_url` is `null`.
make one of the tests an `only_matching` test.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
Above, it's still possible that the JSON download works but doesn't result in a `dict`. So this would be better (setting `None` on error to help with the second point below): ``` status = try_get(info, lambda x: x['status']) ``` Then, if the API makes a breaking change without us noticing, is that `expected` or not? As the site is unlikely to revert the change, it becomes our bug and so not `expected`. I suggest `expected` should correspond to the API returning an actual status that is not OK, and nothing else, like so: ``` raise ExtractorError(status or 'something went wrong', expected=status not in ('ok', None)) ``` But you are obviously familiar with the API and I'm not ...
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
Actually I think the line I fingered was fine, but the replacement's fine too.
`()` is a better 0-length iterable than `""`, which implies text.
`info.get('description')` ? Not a mandatory item. Similarly with `info['uploaded_at']` below.
This is always true.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
``` content_el = itemdoc.find(self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/')) duration = float_or_none(content_el.attrib.get('duration')) if content_el is not None else None ``` or ``` content_el = find_xpath_attr(itemdoc, self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/'), 'duration') duration = float_or_none(content_el.attrib['duration']) if content_el is not None else None ```
Use `compat_urllib_parse_unquote_plus` instead.
Use `self._match_id` instead.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Whats the point reconstructing the URL? You already have it in `url`.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Avoid shadowing built-in names.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Network connections in your browser.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Should not be fatal.
As already said: parse as JSON not with regexes.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Do not touch this.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
Read coding conventions on mandatory metadata.
Using preferences causes invalid sorting.
do you have an example with TTML subtitles? all the videos that i've tested with has only VTT subtitles.
Read coding conventions and fix optional meta fields.
Use `{}` dicts.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Title is invalid.
That's completely different videos.
Ids must stay intact.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
This should be split into building url and extracting formats.
All of these will break extraction on unexpected data.
It does not necessarily mean that.
no need to create a method when it will be used once.
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
Hmm, okay. Not the best soulution but it's also implemented in other extractores
This must be assert not exception.
Inline to actual call place.
By providing username and password in params obviously.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
Don't lookup `lang_code` twice.
Don't lookup twice.
Move into loop.
I did not suggest moving `subtitle_original_lang_code` into loop.
92-115, 133-157 codes are identical apart from entry tag and query.
You're already building query with `query` param.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
```suggestion if re.match(r'^preview_url\d$', k): ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
Avoid unrelated changes.
this is basically the same code repeated twice. It can be generalized
You should use `self._request_webpage`, preferably with a HEAD request
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Remove useless code.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Optional data should not break extraction if missing. Read coding conventions.
will easily match outside the element.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
It should be robust in case of some missing fields.
Simplify: ```suggestion series_id = self._match_id(url) ```
Use single quotes consistently.
m3u8 is also available.
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Again: relax regex.
Read coding conventions on optional fields.
Breaks on `None` title.
Don't capture groups you don't use.
Query to `query`,
What's the point of this? Use `url` as base.
Use the original scheme and host.
fatal=True is default.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Instead of such hacks you can name group differently and capture it without any issue.
Don't capture groups you don't use.
All debug garbage must be removed.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
` - Servus TV` should not be in title.
Again: it's not part of the video's title and must not be in the title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
All methods only used once should be explicitly inlined.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
```suggestion get_element_by_class, int_or_none, ```
* the first element of the path can have more than 1 digit * the tail `/player\.html` isn't needed (unless there are other tails that should find different media with the same id): ```suggestion _VALID_URL = r'https?://(?:www\.)?embed\.vidello\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)' ```
The playlists are public: there should be tests for them. Watch this space.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Use `_request_webpage` instead.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
No escape for `/`.
If these `,`s are thousands separators, might they be `.`s for some locales (plainly not a decimal point for a count)? ```suggestion view_count = str_to_int(self._html_search_regex( (r'<strong>([\d,.]+)</strong> views', r'Views\s*:\s*<strong>([\d,.]+)</strong>'), webpage, 'view count', fatal=False)) ``` (and `from ..utils import str_to_int` at the top).
All these regexes should be relaxed.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks if not arr.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
It's a field name not a step name.
Again: relax regex.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`.*` at the end does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Then just keep this code and > create default fallbacks if extraction of these fails
Upper case is idiomatic for constants.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Raise `ExtractorError` instead.
Fix test: ```suggestion 'ext': 'mp4', ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
`_` is idiomatic way to denote unused variables.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Same, no such key possible.
Won't work for `info = {'title': None}`.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Use `\s*` instead.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
i think the use of `videotitle` is not reliable(it also has a problem with escaped double quotes), instead of this cleanup, other sources should be prefered.
add more fallbacks and extract `timestamp`.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Avoid shadowing built-in names.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
you can get json output by appending `&format=json` to the api request url.
Must be int.
Code duplication in 70-102 and 104-137.
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
Title is mandatory.
It's not a display id.
No `url` and `formats` at the same time.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
This is superfluous since you provide `formats`.
230-264 no copy pastes.
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
Same issue for urlh
```suggestion new = '' ```
There's no need to name a group if not used.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
Do not shadow existing variables.
Carry to the indented beginning of the line.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
This may change as well. Add a fallback that just processes all videos without differentiation.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
This should be a ```list```.
```[\s]``` => ```\s```
Better to use unified_strdate for parsing dates.
mobj would be None if nothing is matched
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
Do not capture empty strings.
`self._search_json_ld` should be improved instead.
`unescapeHTML(n[1])`? (`utils.py`) See also line 59 below.
No trailing $, override suitable.
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Read coding conventions on mandatory metadata.
1. No `{}`. 2. Inline. 3. Query to `query`.
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This code looks similar to `sd` format and can be extracted to a function.
Breaks on unexpected data.
`[]` is superfluous in group with single character.
This is fatal.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
`'%s'` is very unlikely to be a helpful error message.
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
Indenting is messed up here.
This is already checked in `float_or_none`.
1. Do not remove fallback to previous duration value. 2. `scale` of `float_or_none` instead.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
Plays fine without any authentication in browser.
This is not true at the moment.
Use `self._search_regex` and `utils.unified_strdate` instead.
`title` is mandatory. Move flags into regex itself.
This should be extracted in the first place.
You should have only one single method for extracting info.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
`id` should not be optional. No need in trailing `/`.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
`http://api.nowness.com/api/` part can also be moved to `api_request`.
From what I've seen there is always only one video on the page thus no need in playlist.
This is too broad and detects the same video twice.
`split` returns a list, video_id must be a string.
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
`default=None` for the first.
See how browser calls it.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
No need in these variables.
`video_id` is already a string.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Playlist title is optional, description breaks.
You must make it **not break** as you delegate.
You are **delegating** to brightcove that provides the id.
`/?` is senseless at the end.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
There are also vtt subtitles available.
I've already pointed out: this must be removed.
No, override `suitable` instead. Also do not split strings.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
Check code with flake8.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
yes, this is correct.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Title is mandatory field thus it should be `data['roominfo']['name']`.
You still duplicate the URL and unnecessary `if/elif` branches.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
You should make extraction tolerate to these fields missing not remove them.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
Breaks extraction if `release_date[0:4]` is not `int`.
`title` must never be `None`.
Breaks extraction if `json.loads` fails.
No trailing $, override suitable.
`parse_duration()` should work.
`player_id` is not extracted in this fallback but used at 71.
Use `self._search_regex` and `utils.unified_strdate` instead.
No need to use named group when there is only one group.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Same as in some previous PR.
You should have only one single method for extracting info.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Move to initial title assignment.
This is default.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
There is no need in this method.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I would always return a `multi_video` result.
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Don't do this manually use `_download_webpage_handle`
This is superfluous, the extension can be extracted automatically.
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
strip_jsonp should work here
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
Never ignore generic exceptions
Use compat_HTTPError instead
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Remove all debug output.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
`'thumb`' may not be present producing invalid thumbnail url.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
No such meta field.
Title is mandatory.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
Read coding conventions and fix all optional meta fields.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
surround only the part that will threw the exception.
the same for `streaming` key.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
both are know beforehand, so there is no need to use `urljoin`.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Use `compat_urlparse.urljoin` instead.
This code looks similar to `sd` format and can be extracted to a function.
m3u8 is also available.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Read coding conventions on optional/mandatory meta fields.
Don't use `;` unless you need to write more than 1 statement per line (personally, I would also avoid doing that)
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
`int_or_none` and `float_or_none` for all numeric fields.
This change is unrelated, you must open a new PR for it. That's one of the reasons why it's a good idea to use a new branch for each PR.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This should be just `return info`
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
No need to escape `#`. No need to capture groups you don't use.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
This will process the same URL twice overwriting the previous results.
There should be a hardcoded fallback since it's always the same.
Relax `id` group.
EntryId must be extracted the very first.
This does not make any sense, you already have `url`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion new = '' ```
The description is always optional, so there should be a `fatal=False` in here.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Must be int.
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Just rethrow active exception.
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
This line is unnecessary.
Pass `default` to `_og_search_title` instead.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
Read and follow code conventions. Check code with flake8.
If `video` not in `media` empty formats will be returned that does not make any sense.
`for key, value in media.get('images', {}).items():`
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
What's the point of this? `canonical_url` is the same as `url`.
This is pointless. If no formats can be extracted extraction should stop immediately.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
No need to escape `]` is character set.
`'id'` is required.
Should not be fatal.
Query to `query=`.
Should be tolerate to missing keys in `media`.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Never use bare except.
JSON should be parsed as JSON.
`_search_regex` per each field. Add fallback.
No need to use named group when there is only one group.
`player_id` is not extracted in this fallback but used at 71.
Title is mandatory.
No `ExtractorError` is raised here, `except` will never trigger.
Missing dot escape.
No need to escape a double quote inside a single (in fact that might break this due to `r'`).
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
dict comprehensions don't work in python 2.6.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
I'm not talking about capturing upload date. Do not capture AMPM.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Breaks if not arr.
Move flags into regex.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
I've already pointed out: `.*$` is pointless at the end.
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
This will break extraction if no `id` present.
Must be `list`.
Read coding conventions on optional fields.
This will result is `[None]` is no category extracted.
Replace 245-257 with `entry.update({ ... })`.
No. Override `suitable`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
It does since there may be no postprocessing at all.
Opening message is already applied for `multi_video`, I don't see any reason for closing message not to be applied for it as well.
No `upload_date` is guaranteed to be present.
I've already pointed out this does not work.
Read coding conventions on optional fields.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
No, it's not the point. It **must not** skip items than belong to the range specified.
This will break extraction if no `id` present.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
That's incorrect. `\1` must be a number of capture group.
This should include `iframe` part.
Remove all garbage.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
1 is ok.
```/tv/tags/[^/]*?``` => ```/tv/tags/[^/]+```
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Consts should be in uppercase.
Merge in single list comprehension.
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
This code looks similar to `sd` format and can be extracted to a function.
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Should extract chunklists via `self._extract_m3u8_formats` here.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
Field name is supposed to be `key` not `long_video_id`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need for this check, this is already checked in `_sort_formats`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Carry to the indented beginning of the line.
Lack of information is denoted by `None` not `0`.
None is default.
No need for escapes inside a brace group, all dots outside must be escaped.
Nothing changed. Also there is a video id available in JSON.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Superfluous. Move directly into the method call.
Does not match `var IDEC='`.
Read coding convention on optional fields and fix all issues.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
Request wrapping code can be moved to the base class.
None of the optional fields should break extraction if missing.
Course extraction must be in a separate extractor.
Extracting duplicate code into a function obviously.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
remove the duplication as much as possible(`https://www.vvvvid.it/vvvvid/ondemand/%s/` is always used).
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Breaks. Read coding conventions.
Breaks if no `name`.
Either sloppy code or an anti-scraping measure.
There is no point in that.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Should be non fatal.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Shouldn't be fatal
No such meta field.
Audio is not 128.
`width` and `height` instead.
- - [ ]
All formats should be extracted.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Do not match exact URL.
No exact matches for URLs.
Instead of `resolution` and `preference` it should be extracted as `height`.
I've already suggested using `Downloading` as idiomatic wording.
This code looks similar to `sd` format and can be extracted to a function.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
This is not true at the moment.
I suggest `fatal=False`
Plays fine without any authentication in browser.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You don't need list here. Just return it directly.
Outer parentheses are not idiomatic in python.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Not a video id.
It's not an album id.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
These looks like mandatory fields.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
Some `display_id` should be extracted from `url` and shown instead of `None`.
There are two scenarios: video without auth and video with auth. The rest are matching only.
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Remove superfluous whitespace.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
I suggest `default=video_id` in the `_html_search_regex` call.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
`{}` won't on python 2.6.
will be extracted from the URL.
I suggest `fatal=False`
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
Again: ```suggestion {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie}, separators=(',', ':')) message = 'nobody{0}use{1}md5forencrypt'.format( URL, request_text).encode('latin1') ```
Py2.6 compat (!): ```suggestion 'requestId': '{0}_{1:04}'.format(now, rand), ```
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
1. Extract dict if you expect dict. 2. Relax regex. 3. Escape dots.
Again: float_or_none, not parse_duration.
DRY. url is mandatory.
No such meta field.
Depends on what does it mean.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
`default` and `fatal` are not used together.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Do not shadow built-in names.
Capturing empty string does not make any sense. What's the point capturing this at all? id and path occur only once in webpage.
Should not be fatal.
No such meta field.
Not used with formats.
then I think it would be better to use `fatal=False` instead of `default=None`.
> so I guess I'll remove it at that's it. right? yes, it would be better to remove it.
looking again at this, there are multiple problems with the `og:published_time`, the timezone is important to calculate the correct timestamp so the `og:published_time` value for embeds is malformed, and also there is a discrepancy between the values from the video page and the values from the embed page, so unless there is a way to determine the correct value, it might be better to drop it.
don't use the image that has the play icon(`image_full_play`).
don't use `modified_time` as publish timestamp.
`video_id` is already a string.
No need in these variables.
See how browser calls it.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
No exact URLs, use `re:`.
No exact URLs.
Use whitespace characters consistently.
Must be fatal.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
I've already pointed out - title **must be fatal**.
Breaks on `None`.
No escapes for `/`.
Breaks on `default=False`. `title` must be fatal.
Matching between quotes is incorrect.
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
The `list` call is superfluous here and can be safely removed.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
```not (foo is None)``` => ```foo is not None```
Recursion should be replaced with plain loop.
Uppercase is not honored.
Single quotes. `item` is already a string.
It does since there may be no postprocessing at all.
Prefer `post.get()` for these two.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
This code looks similar to `sd` format and can be extracted to a function.
No. It must be a soft dependency. Message requesting installation of `AtomicParsley` or `mutagen` should only be output when thumbnail embedding is requested and neither of these dependencies is found.
Some extractors use the domain name as `IE_DESC`, so I guess it's OK.
Geo restricted. Test will always fail.
Must only contain title.
Removing useless noise.
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
Must be numeric.
`for k, v in flashvars.items()`.
`flashvars[k]` is `v`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
This has no effect. Postprocessors work on info dict copy.
Duration calculation is incorrect.
You've forgot to pass `info_dict` to `supports()`.
avconv does not support `-cookies`, use `-headers` instead. You should also pass all headers.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
This does not matter, if you open https://youtu.be/BaW_jenozKc there is a video embedded. If you open https://redirect.invidious.io/watch?v=BaW_jenozKc there is no video embedded. >used for URLs floating over the net to share youtube videos. Prove that.
There is no video on this page.
`{}` does not work in python 2.6.
Extraction should not break if one of the formats is missing.
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Do not remove existing tests.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
Uppercase is not honored.
`title` is not guaranteed to be present.
Most playlists don't have titles in their entries' at this time thus this option won't work for most of the cases.
87-90 code duplication.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Oh, I see. Thanks!
Use default. Read coding conventions and fix code.
Audio must have proper `vcodec` set.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Remove all debug output.
`video_id` may be `None`.
Remove all unused code.
Why did you remove this test? It does not work with your changes.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All these regexes should be relaxed.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
You should extract `partner_id` and `entry_id` and return `kaltura:...` shortcut.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
You must use `default` if there is a fallback after it.
EntryId must be extracted the very first.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
There should be an `id` group.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I guess it a typo? now -> not
There's no need to `return` after `raise`.
You don't check whether login succeeded or not.
You should consult some git manual.
Code duplication. This is already implemented in `CeskaTelevizeIE`.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
**Always** check code with flake8.
Request wrapping code can be moved to the base class.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Must be extracted first.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This should actually be just `self.url_result(embedded_url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
`note` and `errnote` of `_download_json` instead.
This is fatal.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
All formats should be extracted.
No bare except.
Read coding conventions on optional/mandatory meta fields.
`ext` should be mp4.
Breaks extraction if there is no `stream-labels` key.
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
Code duplication should be eliminated.
Extraction should be tolerate to missing fields.
No need to escape `#`. No need to capture groups you don't use.
This should be just `return info`
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Formats not sorted.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
`split` returns a list, video_id must be a string.
In python 3.X print is a function, for printing to screen use `self.to_screen`. But if it's a fatal error then `raise ExtractorError`
Can we resolve these IDs? There may also be a time encoded in there.
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
Use `self.playlist_result` instead.
The argument will already be a character string, no need to decode it.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
`[]` is superfluous in group with single character.
```suggestion 'noplaylist': True, ```
Read coding conventions on how mandatory data should be accessed.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Either sloppy code or an anti-scraping measure.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
This is already fatal.
Not used with formats.
Nothing changed. Also there is a video id available in JSON.
Should not be fatal.
No such meta field.
Move before youtube-dl imports.
```suggestion timestamp = unified_timestamp(rights.get('validFrom')) ```
Don't touch the old test.
`,` is fine here.
use `datetime` class directly.
that's why i think it's better to directly use python's datetime builtin module to parse the non standardized format.
All these import are unused, check your code with flake8.
keep the old fallback code.
If there's only one format, just use `'url'`.
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
this should be done once(in `_real_initialize`).
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
Do not escape quotes inside triple quotes.
Consistently use flags inside regex.
`ext` should be mp4.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
Must be numeric.
This field is height not quality.
Nothing changed. Breaks in case regex does not match.
Invalid syntax at all.
This should be extracted first.
Sholdn not break the extraction if missing.
`for k, v in flashvars.items()`.
Must be numeric.
`flashvars[k]` is `v`.
```suggestion new = '' ```
It also failed in your previous commit: https://travis-ci.org/rg3/youtube-dl/jobs/8459585. b64encode returns bytes, you must decode to get a string : `base64.b64decode(googleString).decode('ascii')`, I'm not sure since I have little experience with base64.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
This should actually be just `self.url_result(embedded_url)`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This will process the same URL twice overwriting the previous results.
Note `dl_data` may be `None`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
```suggestion 'ext': ext, ``` fix flake8 check
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
All formats should be extracted not only mp4.
this will be done for you by just providing `width` and `height`
could you add description from perex key
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
All formats should be extracted not only mp4.
this will be done for you by just providing `width` and `height`
could you add description from perex key
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Won't work. See how this is done for output template.
This has no effect. Postprocessors work on info dict copy.
avconv does not support `-cookies`, use `-headers` instead. You should also pass all headers.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Alternatively you can just restore it after this PR is merged.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
You must output to a temp file not the original file.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
Do not pass `default=None` to `_html_search_regex` instead.
`title` must be mandatory I've already told about this.
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
After this, check for mandatory `title` field, and use the value in the returned info_dict: ```suggestion video_data = self.get_video_data(url, video_id) title = video_data['title'] ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
As above, `final_asset.get("contentUrl")` could return `None`. Get the mandatory value early; crash if that fails.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
This code looks similar to `sd` format and can be extracted to a function.
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Breaks on `default=False`. `title` must be fatal.
Breaks on `None`.
Code duplication. Must be single call to search regex.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
No escapes for `/`.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
I've already pointed out - title **must be fatal**.
Don't shadow outer names. No need for bracket when using single character.
Also do not use double quotes for string literals.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
This can be moved inside `if chapters:` condition.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Omit expected type.
Must not break extraction if missing.
If there's only one format, just use `'url'`.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Read coding conventions on optional/mandatory meta fields.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
No brackets needed.
This is no longer actual.
All formats must be extracted.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
`_parse_json`. Read coding conventions.
Read: coding conventions, optional fields.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Use `compat_urllib_parse_unquote_plus` instead.
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
No need to escape `/`.
`id` should be extracted via `_VALID_URL` when present.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Use `_parse_json` instead.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
you can get json output by appending `&format=json` to the api request url.
Must be list, not a string.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Don't capture groups you don't use. Use proper regex to match all country codes.
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
Don't capture unused groups
It's obvious from `'is_live': True`.
What's the point of `# match self._live_title` here? Remove.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
```suggestion 'description': compat_str, ```
Harmonise with yt-dlp pt2: ```suggestion 'age_limit': 18, ```
Again: relax regex.
It's a field name not a step name.
`<span[^>]+class="name">...` is better.
Use bare `re.match`.
No need to escape `]` is character set.
`--no-playlist` is not respected.
`else` is superfluous.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
- use single quotes consistently. - i think it would be better to keep errnote closer to note.
```suggestion self._API_BASE_URL + 'authentication/login', None, 'Logging in', data=urlencode_postdata({ ```
Inline everything used only once.
Do not match by plain text.
`ext` should be mp4.
By providing username and password in params obviously.
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
This should be split into building url and extracting formats.
Should check for a list.
Breaks if no videos in season.
`default=None` for the first.
`_` is idiomatic way to denote unused variables.
> Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields. - the playlist title and description are not mandatory. - extraction should not fail if any of the fields `blocks`, `sets`, and `id` are not available.
Playlist title is optional.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
Inline to actual call place.
By providing username and password in params obviously.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Lack of information is denoted by `None` not `0`.
All formats should be extracted.
This is fatal.
`user_info` may be `None`.
`created_at_i` as `timestamp`.
No need to escape `/`.
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
No need to escape `\`.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
you can get json output by appending `&format=json` to the api request url.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Never use bare except.
Lack of information is denoted by `None` not `0`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
There is no need in this method.
Again: float_or_none, not parse_duration.
All these regexes should be relaxed.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
This code looks similar to `sd` format and can be extracted to a function.
Breaks if no `name`.
I would always return a `multi_video` result.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Breaks if not arr.
This code looks similar to `sd` format and can be extracted to a function.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
I would always return a `multi_video` result.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
`self._search_regex` is enough here.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Correct field name is `format_id`.
Use `self._match_id` is better.
`ext` here makes no sense.
`title` is mandatory. Move flags into regex itself.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
both are know beforehand, so there is no need to use `urljoin`.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
That's very brittle.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
```suggestion 'noplaylist': True, ```
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
All formats must be extracted.
Read code conventions on optional fields.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Don't shadow built-ins.
Lack of information is denoted by `None` not `0`.
None is default.
No need for escapes inside a brace group, all dots outside must be escaped.
Nothing changed. Also there is a video id available in JSON.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
By providing username and password in params obviously.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
_ is not a special character in Python's regular expressions. There's no need to escape it.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
Groups around `video` and `sptv/spiegeltv` are superfluous.
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Allow arbitrary whitespace and both quote types.
Move everything into `_download_webpage`.
`expected_status` to `_download_json` instead.
eg find type by URL ext
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
Use `query` for query.
Move into `_download_json`.
`acodec == 'none'`.
Can we resolve these IDs? There may also be a time encoded in there.
What's the point of this extractor? It's covered by album extractor. Remove.
This should be removed.
Yes, if it tests the same extraction scenario. There is only one extraction scenario in this code.
All duplicate tests should be set `only_matching`.
Use `self.playlist_result` instead.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
It's not an album id.
Not a video id.
This is already imported and (in general) you should only use `import`s at the top level.
From what I've seen there is always only one video on the page thus no need in playlist.
will break if `item` is `None`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Use `\s*` instead.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
This has no effect. Postprocessors work on info dict copy.
Duration calculation is incorrect.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Must be extracted first.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
More ways to get `title`: ```suggestion title = ( self._og_search_title(webpage, default=None) or get_element_by_class('my_video_title', webpage) or self._html_search_regex(r'<title\b[^>]*>([^<]+)</title\b', webpage, 'title')) ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Inline all these.
This will result is `[None]` is no category extracted.
This will break extraction if no `id` present.
Replace 245-257 with `entry.update({ ... })`.
Must be `list`.
Course extraction must be in a separate extractor.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Read coding conventions on optional fields.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
`.*/?` is pointless at the end.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
I think it's better to just make DailymotionIE and YoutubeIE a sublcass of the subtitles IE class
Sorry, I meant that YoutubeIE and DailymotionIE should directly inherit from SubtitlesIE or NoAutoSubtilesIE, without an intermediate class.
Just modify mimetype2ext rather than introducing hacks in individual extractors
This should actually be just `self.url_result(embedded_url)`.
This is equivalent to InfoExtractor._match_id
`xrange` is not defined in Python 3.x
```suggestion # coding: utf-8 from __future__ import unicode_literals ```
Coding cookie is only required for non-ASCII sources.
```suggestion try_get, url_or_none, ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
```suggestion get_element_by_class, int_or_none, ```
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
Don't capture unused groups
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
Use `_request_webpage` instead.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
Use ```query``` parameter of ```_download_webpage``` instead.
strip_jsonp should work here
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _ (underline) instead of webpage if the value is not used.
Are all of these necessary? I think youtube-dl defaults suffice.
`not json_lds` already does the second part.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
No, it won't. Bother to read it carefully. `(["\']?)` is idiomatic and correct way to say `(["\']|)`.
Invalid. Either both quotes must be present or none. Not one of them.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
`self._search_json_ld` should be improved instead.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
I think that this line would produce unexpected results with multiple urls. The first one would use the generic extractor and the rest would use the normal extractors. (I may have misunderstood it)
No `upload_date` is guaranteed to be present.
I've already pointed out this does not work.
It does since there may be no postprocessing at all.
Never ever use bare except.
Breaks on `-F`.
No, it's not the point. It **must not** skip items than belong to the range specified.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
`id` is of arbitrary length.
1. `id` is **not necessarily** 3 digits. 2. `id` must not contain irrelevant words.
All debug code must be removed.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
This matches multiple videos.
No. Revert as it was.
No. `_search_regex` is already fatal and reports proper error message. Revert.
First group is superfluous.
Must not be `None`.
Must be `int`.
If either of these attrs is missing whole playlist extraction is broken.
This should not be here as done by downloader.
No direct URLs in tests.
Extract id once before the loop.
If nothing matches `None` will be returned.
Playlist title is optional.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Never ignore generic exceptions
Use compat_HTTPError instead
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Either sloppy code or an anti-scraping measure.
Matching empty data is senseless.
Must not be `None`.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Capturing empty URL is senseless.
No. Use fatal search regex instead.
This may change as well. Add a fallback that just processes all videos without differentiation.
All methods only used once should be explicitly inlined.
No such meta field.
Move into `_download_json`.
Do not use `sanitized_Request`.
Do not use sanitized_Request.
1. This must be downloaded after JSON. 2. This must not be fatal.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
This should actually be just `self.url_result(embedded_url)`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Code duplication. This is already implemented in `CeskaTelevizeIE`.
Title part should be optional.
Code duplication 173, 213. There is no sense to extract fields explicitly.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Playlist id and title should not be fatal.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
You can import `try_rm` from helper
Playlist metadata must not be fatal.
`strip_or_none` no longer needed.
Matching empty string is senseless.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
This condition is not needed, t is always None here.
youtube-dl does not use non-browser user agent.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
This is in the test suite, and in the test suite, warnings are an error of us, aren't they? So why isn't the implementation ``` raise Exception(message) ```
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
there is not need for excess verbosity.
no longer needed.
will return invalid URL if `search_url` is `null`.
incorrect URLs for Cook's Country.
extraction should not break if an episode or all episodes couldn't be extracted.
make one of the tests an `only_matching` test.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All methods only used once should be explicitly inlined.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
`note` and `errnote` of `_download_json` instead.
This is fatal.
All formats should be extracted.
`int_or_none` for all int fields.
No bare except.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
The semantics of tests should be kept. This test should test `*-videoplayer_size-[LMS].html` URL. Same for others: `*-videoplayer.html`, `*-audioplayer.html` (on two different domains).
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
`self._parse_html5_media_entries` for formats extraction.
Bitrate should go to corresponding format meta field.
All debug output should be removed. `{}` does not work in python 2.6.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
I suggest extracting from `mediaLink` element instead of matching the JS URL as it may change in the future. For example: ``` Python media_link_obj = self._parse_json(self._html_search_regex( r'class="mediaLink\b[^"]*"[^>]+data-extension="([^"]+)"', webpage, 'media link'), display_id, transform_source=js_to_json) js_url = media_link_obj['mediaObj']['url'] ```
We now have a fully-fledged format system which can be used.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
Should not be fatal.
`True` is default.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
Avoid shadowing built-in names.
Code duplication in 155-163 and 165-172.
Code duplication in 142-145 and 146-149.
Code duplication in 70-102 and 104-137.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
Couldn't we simply look at the extension (below) to get the format map? Then we don't have to update this list.
160 is a video format.
and 139 is a audio format
`_sort_formats` should always be called for non youtube videos(to break the extraction with a proper message when no formats has been extracted).
This URL is available from JSON.
This should be extracted first.
Sholdn not break the extraction if missing.
Must be numeric.
Must be numeric.
`for k, v in flashvars.items()`.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
Use `compat_urllib_parse_unquote_plus` instead.
Code duplication in 70-102 and 104-137.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Everything apart from `url` is optional.
Code duplication in 155-163 and 165-172.
Code duplication in 142-145 and 146-149.
This field is added automatically no need to add it by hand.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
This TODO needs work
you can get json output by appending `&format=json` to the api request url.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
This should actually be just `self.url_result(embedded_url)`.
This will break the entire extraction if there is no match for some format.
This is equivalent to InfoExtractor._match_id
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
PEP8 mandates two empty lines here.
This line does not help and is not necessary.
Side note: Wow, these guys are military, but don't support https? Oh my...
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
[Correct way](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/pluralsight.py#L95-L100) to implement order independent argument matching.
To get regex highlighting, this string should be prefixed with `r`. That's just convention, but plain neat.
This is a tuple, that can't be right. Also, a test is only really useful with some things to test against, like title and md5sum of the image. You can run it with `python tests/test_download TestDownload.test_DefenseGouvFr`.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
This should not be fatal.
Don't capture groups you don't use.
Don't capture groups if you are not going to use them.
`/?` does not make any sense.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Don't capture unused groups
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
No need to escape `\`.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
`xrange` is not defined in Python 3.x
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
All these regexes should be relaxed.
Must be int.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Capturing empty URL is senseless.
No. Use fatal search regex instead.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Need fatal=False or default=None
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
eg find type by URL ext
`expected_status` to `_download_json` instead.
Read coding conventions on optional/mandatory meta fields.
`expected_status` to `_download_json` instead.
Use display id.
This is checked by `_search_regex`.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
Mandatory data must be accessed with `[]` not `get`.
`images` is not guaranteed to be a list.
`if images` implies the second check.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
Not a video id.
Use `self.playlist_result` instead.
It's not an album id.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
From what I've seen there is always only one video on the page thus no need in playlist.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`if playlist_size:` is enough.
This won't skip empty strings.
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
No such meta field.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Prefer `post.get()` for these two.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
For these 3, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], int), ``` (equivalent in effect to wrapping in `int_or_none()`).
Debug code must be removed.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
`<span[^>]+class="name">...` is better.
`_search_regex` is enough here.
Either implement or remove.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
Temp file is not removed in this case.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Carry to the indented beginning of the line.
Lack of information is denoted by `None` not `0`.
None is default.
Nothing changed. Also there is a video id available in JSON.
No need for escapes inside a brace group, all dots outside must be escaped.
The code should match the content. If it's not possible to figure out the lang code there is no point placing it in subtitles. Moreover it's not time bound so can't even barely be treated as subtitles.
It's not always an .lrc since it does not always follow the format and not bound to time tags.
This is not always the case. Here is the `en` lyrics http://y.qq.com/#type=song&mid=001JyApY11tIp6.
As well as this.
Just update the dictionary with `subtitles` key when `.lrc` detected. There is no need in code duplication. Also replace `zh-CN` with something more generic like `origin`.
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
I've already suggested using single quotes.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
This code duplication may be eliminated.
Do not touch the old patterns.
This test is identical to the first. Revert.
Lack of data must be expressed by `None` not empty string.
JSON should be parsed as JSON.
Unite in single list comprehension.
Never use bare except.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Rename to `KanalDIE`.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
The `return` is not needed anymore.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
1. Do not remove the old pattern. 2. Relax regex.
This should go into `YoutubeDL.py`
-`images that appear when hovering the cursor over a video timeline` This may not apply to other sites.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
This will break unicode strings under python 2.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Don't capture groups you don't use.
`.*/?` is pointless at the end.
Don't capture unused groups
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
No need to use `sanitized_Request` request here, pass url directly to download method.
Just modify mimetype2ext rather than introducing hacks in individual extractors
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
This is equivalent to InfoExtractor._match_id
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
There is already an extractor with such `IE_NAME`.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
No `(?i)`, no `$`. Dots must be escaped.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
Let's do it, then, if you like.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
As in, me personally? I think I knew that, but protocol.
fallback to other available values.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
All formats should be extracted not only mp4.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Breaks on `None`.
Don't shadow built-ins.
Unite in single list comprehension.
None is default.
Nothing changed. Also there is a video id available in JSON.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
Use ```query``` parameter of ```_download_webpage``` instead.
strip_jsonp should work here
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _ (underline) instead of webpage if the value is not used.
Are all of these necessary? I think youtube-dl defaults suffice.
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
None of the optional fields should break extraction if missing.
It seems you aren't using the module, remove this line.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
Doesn't match https://www.servus.com/de/p/Die-Gr%C3%BCnen-aus-Sicht-des-Volkes/AA-1T6VBU5PW1W12/?foo=bar. Should not match https://www.servus.com/de/p/Kleines-Geschenk-Set-Anti-Stress/SM129658/.
If it can be downloaded with native hls without any problem it should be forced in `_extract_m3u8_formats` and removed from test.
If you want to force this just pass `entry_protocol='m3u8_native'` to `_extract_m3u8_formats`.
All these import are unused, check your code with flake8.
Because all of them use the same extraction scenario.
For 2.6 (yes, I know) compatibiility, `{0}` rather than `{}`, apparently. And number other formats elsewhere.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Move flags into regex. Regex should match `runParams={`.
What are you even trying to do? `'1'` that's all.
Move to base class.
Do not match by plain text.
Inline everything used only once.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Plain `for x in l`.
Extract `height` for each format.
Escape dot. No need to split URL.
Place on a single line.
Capturing empty string does not make any sense.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Also pass `m3u8_id='hls'`.
fallback to other available values.
parentheses not needed.
`default` implies non `fatal`.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Harmonise with yt-dlp pt4: ```suggestion }] ```
Harmonise with yt-dlp pt3: ```suggestion ```
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
No bare except.
Just rethrow active exception.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
The argument will already be a character string, no need to decode it.
`int_or_none` for all int fields.
No `id` extracted.
I've already pointed out: use `display_id` until you get real id.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
No bare except.
Since you now skip already downloaded segments the total fragment message displays wrong value after restarting: `[hlsnative] Total fragments: ...`. As a result - wrong download progress data.
121, 124 - DRY.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
These are not used.
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
> Also append_url_to_file may be renamed to something better reflecting it's actual content. the function can be removed completely and move the code directly to the for loop.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Unite in single list comprehension.
A typo? `pariod`
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
do not use names of Python built-in functions(https://docs.python.org/3/library/functions.html#format).
`formats` should be sorted.
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
it's either one of two cases: they are identical -> keep them as they are(formats of the same entry). they are different -> separate them into a playlist with two entries.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
26-29, 32-37 code duplication.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
Don't capture groups you don't use.
Breaks on `None` title.
`'id'` is required.
Query to `query`,
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
All these regexes are only used once thus make no sense as separate variables.
It should not. See the description of the field.
If `_search_regex` fails `None` will be passed to `_parse_json`.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
This produces invalid results when being called in a non-english speaking country. At least for me, in germany, titles will have `, - Anschauen auf Crunchyroll` appended (which is the same phrase being cut off here, just in german) This was not the case before
Function calls are complex. For example: ``` from youtube_dl.jsinterp import JSInterpreter jsi = JSInterpreter(''' function a(x) { return x; } function b(x) { return x; } function c() { return [a, b][0](0); } ''') print(jsi.call_function('c')) ```
All these regexes should be relaxed.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Again: relax regex.
It's a field name not a step name.
Lack of information is denoted by `None` not `0`.
Relax regex, make group unnamed, don't capture empty dict.
No need for escapes inside a brace group, all dots outside must be escaped.
Move flags into regex.
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Read coding conventions on optional fields.
None is default.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
Bitrate should go to corresponding format meta field.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Must be extracted first.
Optional data should not break extraction if missing. Read coding conventions.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
`title` is mandatory. Move flags into regex itself.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
This is not true at the moment.
Plays fine without any authentication in browser.
You should add support for this playlist-alike 3qsdn URLs in any non-breaking way.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
Use `query` for query.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
`acodec == 'none'`.
Move into `_download_json`.
This code looks similar to `sd` format and can be extracted to a function.
This is fatal.
Non fatal, proper prefix id.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Since this extractor by itself can't provide too much info, maybe it would be better to remove the _real_extract and _VALID_URL, don't import it in `__init__`and do something similar to`MTVServicesInfoExtractor`, which is the base class for the extractors that need it.
`formats` is always a list of dictionaries.
Better to use `determine_ext` instead of `.endswith`
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
This is superfluous since you provide `formats`.
Read coding conventions on optional/mandatory meta fields.
Must not be fatal.
Inline all these.
You could consider using the library routine `parse_resolution()` (`utils.py`) ``` thumb_info = parse_resolution(res) thumb_info['url'] = base_url.replace(replace, res) thumb_info['preference'] = n # as above thumbnails.append(thumb_info) ```
Breaks if no `rate` key in `stream`.
With this: ``` for n, res in enumerate(common_res): ``` you could replace `len(thumbnails)` by `n` as the value of `preference`.
This should be split into building url and extracting formats.
Should be `mp4`.
Use `query` for query.
Replace 245-257 with `entry.update({ ... })`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
There is no point to use `get` here.
`if not videos:`.
`ad_free_formats` is never empty here since `_sort_formats` will throw if's empty.
If such reconstructed URL is already processed it should be skipped.
This must be assert not exception.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
Use `_download_webpage` since you're not using handle anynway.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Parentheses make this clearer although not required: ```suggestion uploader_url = ('https://parler.com/' + uploader_id) if uploader_id else None ```
"" -> ''
"" -> ''
Read coding conventions and fix code failsafeness.
This is always true.
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
Don't change extractor name.
Using preferences causes invalid sorting.
Inline to actual call place.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Title is invalid.
Ids must stay intact.
That's completely different videos.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Pass `default` to `_og_search_title` instead.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
You don't check whether login succeeded or not.
You should consult some git manual.
This line is unnecessary.
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
default and fatal are not used together.
Will never happen since it does not throw if not fatal.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
Must not be fatal.
You are using different ids in url and here. Don't do that.
`compat_str()`, here and in l.96.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Read coding conventions on mandatory metadata.
Must be extracted first.
1. No `{}`. 2. Inline. 3. Query to `query`.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
The `return` is not needed anymore.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
1. Do not remove the old pattern. 2. Relax regex.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will be extracted from the URL.
I suggest `fatal=False`
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This code looks similar to `sd` format and can be extracted to a function.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Already pointed out: must be `ExtractorError`.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
Don't shadow built-in names.
You must delegate with `url_result` instead.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
will break if `item` is `None`.
Rename to something else.
`url` should not be `None`.
Network connections in your browser.
Query should be passed as `query` parameter.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Rename to `KanalDIE`.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
```suggestion if episodes: ```
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Unite in single list comprehension.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
The same. Should use https if ```url``` use https.
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
Same issue for re.search
Trailing /? is not necessary here
Final bit, self._search_regex is better than re.search
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
`'id'` is required.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
Can we resolve these IDs? There may also be a time encoded in there.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Add a rationale for that.
Consistently use single quotes.
There is no point in that.
This breaks streaming to stdout.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
Then just keep this code and > create default fallbacks if extraction of these fails
Upper case is idiomatic for constants.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Either sloppy code or an anti-scraping measure.
Usually display_id is used before the actual video_id is extracted.
Must only contain description.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
Should not be fatal.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Use `self._parse_html5_media_entries` instead.
Carry long lines. Read coding conventions.
Rename to something else.
`url` should not be `None`.
You must delegate with `url_result` instead.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
```suggestion 'noplaylist': True, ```
Unite in single list comprehension.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
`compat_str()`, here and in l.96.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
The order of dictionaries is not deterministic before Python 3.6. For example: ``` $ PYTHONHASHSEED=0 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: ok ---------------------------------------------------------------------- Ran 1 test in 0.004s OK $ PYTHONHASHSEED=1 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: FAIL ====================================================================== FAIL: test_dfxp2srt (__main__.TestUtil) ---------------------------------------------------------------------- Traceback (most recent call last): File "test/test_utils.py", line 1093, in test_dfxp2srt self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data) AssertionError: u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... != u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... Diff is 663 characters long. Set self.maxDiff to None to see it. ---------------------------------------------------------------------- Ran 1 test in 0.005s FAILED (failures=1) ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
All debug output should be removed. `{}` does not work in python 2.6.
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
will break if `item` is `None`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
Same as in some previous PR.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Always return a playlist.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Don't capture empty list. `_search_regex`, `_parse_json`. Read coding conventions.
No need for that.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This will result in reference to unassigned variable when time fields are missing.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
```suggestion new = '' ```
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Don't add list items if 'url' is None.
It's better to fix thumbnail extraction instead of remove the test.
Put it in this one is OK.
This intermediate dict is completely pointless. Build formats directly.
still fails if `uploader_data` not available.
extract all formats.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
All formats should be extracted not only mp4.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Same, no such key possible.
Won't work for `info = {'title': None}`.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
`--no-playlist` is not respected.
Prefer `post.get()` for these two.
I would always return a `multi_video` result.
Extractor should not return `None`.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Request wrapping code can be moved to the base class.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
Must not be fatal.
Must be fatal. Must be extracted very first.
What's the point of this? `canonical_url` is the same as `url`.
Don't lookup twice.
92-115, 133-157 codes are identical apart from entry tag and query.
You're already building query with `query` param.
The idiomatic way to extract `id` is to use a group in `_VALID_URL`.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Playlist title is optional, description breaks.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Remove all useless noise.
Use formatted strings.
Either sloppy code or an anti-scraping measure.
What's the point of this? `canonical_url` is the same as `url`.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
```suggestion new = '' ```
Use `self.url_result(inner_url, 'Generic')` instead.
Noway. See other extractors on how to delegate properly.
Remove all debug garbage.
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
`(server_json.get('id') or '?')` (or some other default value). Or server_json.get('id', '?') Similarly l.128.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
There's still a loop on `format_id` which doesn't seem right: ``` for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): if not isinstance(server_json, dict): continue for format_id in ('hls', 'dash'): if format_id not in server_json.keys(): continue if format_id == 'hls': formats.extend(self._extract_m3u8_formats( server_json[format_id], lecture_id, 'mp4', entry_protocol='m3u8_native', m3u8_id=format_id, note='Downloading %s m3u8 information' % server_json.get('id', '?') , elif format_id == 'dash': formats.extend(self._extract_mpd_formats( server_json[format_id], lecture_id, mpd_id=format_id, note='Downloading %s MPD manifest' % server_json.get('id', '?'), fatal=False)) self._sort_formats(formats) ... ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
Shouldn't be fatal
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Lack of information is denoted by `None` not `0`.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
There is no need in this method.
All these regexes should be relaxed.
Playlist title is optional, description breaks.
Do not shadow existing variables.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Breaks on None.
This code looks similar to `sd` format and can be extracted to a function.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I would always return a `multi_video` result.
No `ExtractorError` is raised here, `except` will never trigger.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Breaks on `None` title.
Don't capture groups you don't use.
Query to `query`,
No need to escape `]` is character set.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
both are know beforehand, so there is no need to use `urljoin`.
the same for `streaming` key.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
Never use bare except.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
Remove unused codes.
Remove debugging codes.
Use `self._sort_formats(formats)` instead.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Upper case is idiomatic for constants.
Then just keep this code and > create default fallbacks if extraction of these fails
You've traded bad for worse. Just parse it with regex in `_search_regex`.
This code duplication may be eliminated.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Either sloppy code or an anti-scraping measure.
46-47 code duplication.
`self._search_json_ld` should be improved instead.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
Should not break if there is no `resolution` key.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
Update `video_info` and return it directly.
End users do not read source codes thus will never find this advice.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion if episodes: ```
Lack of data must be expressed by `None` not empty string.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
Unite in single list comprehension.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
There should be a hardcoded fallback since it's always the same.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
EntryId must be extracted the very first.
Empty string capture does not make any sense.
You must use `default` if there is a fallback after it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
This should be just `return info`
Same as above, we should create a `formats` array here.
We now have a fully-fledged format system which can be used.
`if mediatype == u'video':` is idiomatic Python.
We may use proper XML parsing here and simply call `self._download_xml`
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
The argument will already be a character string, no need to decode it.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This is determined automatically.
This does not make any sense, you already have `url`.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
You should actually keep this line and change https://github.com/costypetrisor/youtube-dl/blob/autonumber_start/youtube_dl/YoutubeDL.py#L296, otherwise it won't be incremented.
You'll have to provide a default value for `autonumber_start` (like 1), because when using the youtube_dl module from python it will usually be missing.
This will break `--max-download`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
```not (foo is None)``` => ```foo is not None```
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
`autonumber` is not reset to zero in the first place.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
This code looks similar to `sd` format and can be extracted to a function.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Not quite sure. Currently it redirects to `pornhub.com`. Possibly this was not the case in the past.
Should also match `pornhubpremium.net`. Or extractor should not match `pornhubpremium.net`.
You must provide account credentials/cookies for testing.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Will break if `episode_title` is `None`.
This should be in `_real_initialize`. Same for all other occurrences.
`split` returns a list, video_id must be a string.
It's better to use `self._download_webpage(url, video_id)`
This should actually be just `self.url_result(embedded_url)`.
You have some unmerged lines here
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
`int_or_none` for all int fields.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
This will result in reference to unassigned variable when time fields are missing.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No bare except.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Prefer `post.get()` for these two.
Use `self.playlist_result` instead.
This should be removed.
`{}` won't work in python 2.6.
They are actually different :`MIGcBg` vs. `MIGmBg`
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Can we resolve these IDs? There may also be a time encoded in there.
This is determined automatically.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Do not touch `only_matching` tests.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
should not return empty formats.
`formats` should be sorted.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
26-29, 32-37 code duplication.
`vcodec` to 'none'.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
Move `_match_id` into `_extract_audio`.
The indentation is messed up here, it should be 4 instead of 2 spaces. You may want to get a better editor - a modern editor should take care of indentation automatically.
The indentation is messed up here, it should be 4 instead of 3 spaces.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
The variable name `shand` is non-descriptive
Not having archive != having dummy `Archive` object.
The variable name `phand` is non-descriptive
This will break unicode strings under python 2.
Should extract chunklists via `self._extract_m3u8_formats` here.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
Do not capture empty strings.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
`skip_download` is needed for the test to pass similar to the first test.
No trailing $, override suitable.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Use formatted strings.
Shouldn't be fatal
Remove all useless noise.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Consts should be in uppercase.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Merge in single list comprehension.
This is always true.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
Trailing /? is not necessary here
Same question for 'contains'
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Same for re.search
The same. Should use https if ```url``` use https.
Same issue for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
This will process the same URL twice overwriting the previous results.
Use _parse_json and js_to_json here
This should not be touched.
`if not formats` is enough.
In both extraction functions you extract these fields from `webpage` (as fallbacks in the second case) but you do it differently (split on `|` here and as is in the second case). What's the rationale? `_search_regex` is fatal by default and will break extraction if it's not found for optional fields.
```suggestion new = '' ```
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
should not break the extraction if the field is not available.
use `query` argument.
just use a hardcoded value for now.
accept `audio` `mediaType`.
This is superfluous since you provide `formats`.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Of course when it appears inside `script`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
You must use `default` if there is a fallback after it.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Unite in single list comprehension.
```suggestion new = '' ```
Request wrapping code can be moved to the base class.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
You can use self._download_json() here
fallback to other available values.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Use `self._search_regex` and `utils.unified_strdate` instead.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Unite in single list comprehension.
I would always return a `multi_video` result.
Lack of data is denoted by `None` not `0`.
This should be split into building url and extracting formats.
Same as in some previous PR.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Query should be passed as `query` parameter.
Referring `url` from `url` looks like nonsense. Provide rationale.
By providing username and password in params obviously.
No brackets needed.
This is useless at the end.
This is no longer actual.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
Lack of information is denoted by `None` not `0`.
This is fatal.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
There should be a fallback title if `_og_search_title()` returns `None`.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
`--no-playlist` is not respected.
I'm pretty sure PEP8 mandates the `break` to be in a new line, but that's not that important.
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
This TODO needs work
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
fatal=True is already the default
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Trailing /? is not necessary here
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
This code looks similar to `sd` format and can be extracted to a function.
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
This line is unnecessary.
You should consult some git manual.
You don't check whether login succeeded or not.
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
There's no need to `return` after `raise`.
I guess it a typo? now -> not
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This does not make any sense, you already have `url`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Must be extracted first.
151-165 code duplication.
Remove all garbage.
**Do not remove** `_search_regex` part.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
If `_search_regex` fails `None` will be passed to `_parse_json`.
This produces invalid results when being called in a non-english speaking country. At least for me, in germany, titles will have `, - Anschauen auf Crunchyroll` appended (which is the same phrase being cut off here, just in german) This was not the case before
Avoid unrelated changes.
It should not. See the description of the field.
`player_id` is not extracted in this fallback but used at 71.
No `ExtractorError` is raised here, `except` will never trigger.
To access both groups from the URL match, use `mobj = re.match(self._VALID_URL, url)` (`import re`) and then access the matches as `video_id = mobj.group('id')`. In this case the fact that the extractor is running means that the `id` and `display_id` groups matched. If you had an optional group, like `(?P<display_id>.+)?`, something like `display_id = mobj.groupdict().get('display_id')` would be appropriate.
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
Better to use unified_strdate for parsing dates.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
mobj would be None if nothing is matched
Need fatal=False or default=None
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Read coding conventions on optional/mandatory meta fields.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Referring `url` from `url` looks like nonsense. Provide rationale.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
This is already imported and (in general) you should only use `import`s at the top level.
`{}` doesn't work in python 2.6.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
It shouldn't fail if `user` or `username` is missing.
Single quotes. `item` is already a string.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
Recursion should be replaced with plain loop.
This will break extraction if no `id` present.
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
This does not make any sense, you already have `url`.
Relax `id` group.
Do not capture empty strings.
Capture between tags.
No exact URLs here.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
You have some unmerged lines here
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Style: ```suggestion return '/'.join(urlparts) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Sorry - dismiss that
The current working directory is not always writable.
Temp file is not removed in this case.
Remove all garbage.
This does not necessarily mean that. There is a clear captured error message that should be output.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
`/?` is senseless at the end.
I've already pointed out: this must be removed.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
`ext` should be mp4.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
All methods only used once should be explicitly inlined.
`int_or_none` for all int fields.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
No bare except.
All formats should be extracted.
`note` and `errnote` of `_download_json` instead.
This is fatal.
For now this should not be printed or only printed in `verbose` mode.
This could be moved several lines up.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
If you pass cookie you must not pass credentials.
This does not look to be possible on a clean session.
This is fatal.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
Do not touch existing tests.
All these regexes should be relaxed.
Either sloppy code or an anti-scraping measure.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This code looks similar to `sd` format and can be extracted to a function.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
As already said: parse as JSON not with regexes.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
From what I've seen there is always only one video on the page thus no need in playlist.
It's already extracted as video_id.
There is no point in `or None` since `None` is already default.
Title is mandatory.
`int_or_none` and `float_or_none` for all numeric fields.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`id` by no means should be `None`.
Use regular string format syntax instead.
No need to escape `/`.
No need to escape `#`. No need to capture groups you don't use.
To be removed.
Title is mandatory.
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
Optional fields should not break extraction if missing.
No newline at the end of file.
hls and rtmp are available as well.
`title` is mandatory. Move flags into regex itself.
What's the point of lines 104-108? `ext` is already flv.
Extraction should be tolerate to missing fields.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
`strip_or_none` no longer needed.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
Will never happen since it does not throw if not fatal.
`{}` does not work in python 2.6.
Extraction should not break if one of the formats is missing.
This is not true in general.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
Add `fatal` flag.
`fatal` must be added to `_extract_info`. No changes to core code.
Do not change the order of extraction.
This line can just be removed.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This should actually be just `self.url_result(embedded_url)`.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
still fails if `uploader_data` not available.
Don't add list items if 'url' is None.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Avoid unrelated changes.
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
All formats should be extracted.
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
```suggestion from ..compat import compat_str ```
Don't capture groups if you are not going to use them.
`xrange` is not defined in Python 3.x
Should not be fatal.
Should be tolerate to missing keys in `media`.
Query to `query=`.
Carry long lines. Read coding conventions.
Must not be fatal.
Must only contain description.
Must only contain title.
This is never reachable.
Playlist title is optional.
This is default.
Must be separate extractor delegating to CNBCIE.
First group is superfluous.
Must not be `None`.
Despite being keyword arguments avoid changing the original order.
Shouldn't be fatal
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
All similar tests should be `only_matching`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Use _parse_json and js_to_json here
This should be a ```list```.
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
Better to use unified_strdate for parsing dates.
```[\s]``` => ```\s```
mobj would be None if nothing is matched
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
I'm **not talking about any particular test case**. I'm talking about a potential situation when this code: ``` <td class="key">Title:</td> <td class="value"></td> ``` **is not present on the webpage**. 46 will **fail** in this case because it has `fatal=True` and you will not reach a "fallback" in this case.
Dots should be escaped. Query may contain more arguments that will be incorrectly captured by this regex as path.
There are more metadata to be extracted: `duration`, `abr`, `asr`, `filesize_approx`.
It's more rational to use `False` as default since it's used more often in this code.
Move to the place where it's first used.
This will break extraction if some field is missing. Optional fields should not break extraction.
And `_html_search_regex` will fail breaking the extraction completely.
Title is mandatory.
None of the optional fields should break the extraction if missing. Read new extractor tutorial.
Has no effect on hls.
To be removed.
No need for escapes inside a brace group, all dots outside must be escaped.
Fix: ```suggestion urlparts = video_url.split('/') ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
No need to specify this.
Should be `flv`.
Read coding conventions and fix all optional meta fields.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
From what I've seen there is always only one video on the page thus no need in playlist.
No need to escape `]` is character set.
This is never reachable.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
Do not capture groups you don't use.
It should match all non empty domain names.
Should not allow empty 3rd level domain. Should not be greedy. Inner group is superfluous.
No captures for groups you don't use.
Must be separate extractor.
All debug garbage must be removed.
Don't. Must be a separate article extractor.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
This regex does not make any sense.
All methods only used once should be explicitly inlined.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
m3u8 is also available.
Lack of information is denoted by `None` not `0`.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
This is fatal.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
This is already fatal.
Should not be fatal.
Better to use `determine_ext` instead of `.endswith`
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
There should be spaces between `%`.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
try to parse multiple formats, set `vcodec` to `none`.
Breaks on missing file key.
Here you must use `urljoin`.
try_get is pointless here.
`tracks` is not guaranteed to be iterable.
`get` is pointless since availability of result key is mandatory.
try_get is pointless here. Read coding conventions.
`raise_geo_restricted` and specify `countries`.
`emotion` is not guaranteed to be a dict.
Incorrect. find returns -1 on failure that is Trueish value.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
idk what the maintainers think about this, but I personally think this change is out of the scope of this PR. If this function is desired, it can be added separately. For now, you could just replace `self._match_valid_url(url)` with `re.match(self._VALID_URL, url)` as many other extractors already do.
This condition is not needed, t is always None here.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
This does not look to be possible on a clean session.
If you pass cookie you must not pass credentials.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
No unrelated changes.
Remove all unrelated changes.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
This is never reached cause sort formats will throw on `not formats`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
`'http:' + None = TypeError`
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`()` is a better 0-length iterable than `""`, which implies text.
Actually I think the line I fingered was fine, but the replacement's fine too.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
`info.get('description')` ? Not a mandatory item. Similarly with `info['uploaded_at']` below.
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
Above, it's still possible that the JSON download works but doesn't result in a `dict`. So this would be better (setting `None` on error to help with the second point below): ``` status = try_get(info, lambda x: x['status']) ``` Then, if the API makes a breaking change without us noticing, is that `expected` or not? As the site is unlikely to revert the change, it becomes our bug and so not `expected`. I suggest `expected` should correspond to the API returning an actual status that is not OK, and nothing else, like so: ``` raise ExtractorError(status or 'something went wrong', expected=status not in ('ok', None)) ``` But you are obviously familiar with the API and I'm not ...
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
This is always true.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I've already suggested how to cover both scenarios without getting any error. > Looks like old videos with **5 digit length video id** are still available via xstream in much better quality than vgtv. New video ids are 6 digit length.
Looks like old videos with 5 digit length video id are still available via xstream in much better quality than vgtv.
Use formatted strings.
Remove all useless noise.
All methods only used once should be explicitly inlined.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
This code looks similar to `sd` format and can be extracted to a function.
Unite in single list comprehension.
Do not shadow existing variables.
According to the [W3C HTML5 syntax spec](http://www.w3.org/TR/html5/syntax.html), using `'\s'` to match whitespaces is better here.
Don't add list items if 'url' is None.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
Can we resolve these IDs? There may also be a time encoded in there.
`args` may be `None` here.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Something like this (not tested): ```python args = self._downloader.params.get('postprocessor_args') if args is None: return default if isinstance(args, (list, tuple)): # for backward compatibility return args assert isinstance(args, dict) pp_args = args.get(pp_key) if pp_args is not None: return pp_args pp_args = args.get('default') # for backward compatibility if pp_args is not None: return pp_args return default ```
`cli_configuration_args` is supposed to be used with `params` dict, if you are working with `postprocessor_args` on your own you should not use it.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
`pp` should be matched against the list of available post-processors that should be build similarly to `gen_extractor_classes`.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Alternatively you can just restore it after this PR is merged.
Same. The `filter` doesn't make sense to me
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Breaks if no URLs extracted.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
It should always return a list.
TODO: do ;)
This value looks an awful lot like a `display_id`
I meant a _webpage that uses embed.ly to embed some non-video content and also contains a video that is detected by code after this_. I concur that the best action would be to wait for a test case, and then decide how we can exclude it or improve some other code.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`skip_download` is needed for the test to pass similar to the first test.
If there's only one format, just use `'url'`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Sorry for the late response. This check may give a false alert if someday afreeca.tv decides to use a different name than `./track/flag`. `./track/video/file` is more reliable. The overall extraction workflow should be: 1. Check `./track/video/file` 2. Raise an error if no entries 3. Check other fields
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
`try_get` is useless here.
Merge in single list comprehension.
Consts should be in uppercase.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
This is always true.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
This fails on Python 2. (duh!) Instead, we should fix SSL support in general.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
m3u8 is also available.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
That's very brittle.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion 'noplaylist': True, ```
do you have an example with subtitles.
if the `ext` can't be detected than fallback to `vtt` as it's the `ext` that most likely to be.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
`enumerate` on for range.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Read coding conventions on optional/mandatory meta fields.
151-165 code duplication.
Breaks if not a list.
Optional data should not break extraction if missing. Read coding conventions.
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
`{}` won't work in python 2.6.
It's better to fail instead of fallback.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
No. You should not shadow the original explicitly provided password.
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
No such meta field.
No such meta field.
Should be more relaxed.
Pass video id.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
Because all of them use the same extraction scenario.
Mandatory. Read coding conventions.
will break the extraction if `profiles` is empty or `None`.
should not break the extraction if the field is not available.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
API URLs should be used.
I've already pointed out: API URLs should be used.
Playlist title is optional.
`try_get`, single quotes.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
None of the optional fields should break extraction if missing.
Move data and query into `_download_webpage` call.
`False` is not a valid note.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
This does not necessarily mean that. There is a clear captured error message that should be output.
`default` is not used with `fatal`.
There should be an `id` group.
Use display id.
This is checked by `_search_regex`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
All formats should be extracted.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
The description is always optional, so there should be a `fatal=False` in here.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
`\s*` make no sense at the end.
There is no point checking `url`.
No point checking this.
This is determined automatically.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Should not be fatal.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
All formats should be extracted.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
Use `self.url_result(inner_url, 'Generic')` instead.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
There's no need to use `u` prefix given `unicode_literals` is declared.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Never use bare except.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
```suggestion new = '' ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Breaks on None.
No escapes for slash.
This is never reached cause sort formats will throw on `not formats`.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Formats in webpage are still available and should be extracted.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`if not videos:`.
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Read coding conventions on how mandatory data should be accessed.
Breaks on unexpected data.
Must not be fatal.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Breaks on None.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
What's the point? It's not alphabetic altogether anyway.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
Actually, it's an opposite. It's a check for successful login.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
No newline at the end of file.
All video formats must be extracted.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
This should be extracted from `_VALID_URL` with `self._match_id`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
This code looks similar to `sd` format and can be extracted to a function.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Everything apart from `url` is optional.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
`ext` should be mp4.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Consistently use flags inside regex.
This will result in reference to unassigned variable when time fields are missing.
All debug code should be removed.
Should be delegated via `url_result`.
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
Duration calculation is incorrect.
`{}` won't work in python 2.6.
This has no effect. Postprocessors work on info dict copy.
The current working directory is not always writable.
Sorry - dismiss that
Temp file is not removed in this case.
Prefer `post.get()` for these two.
``` for i, video_url in enumerate(video_urls): ```
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
`default` implies non `fatal`.
This is not true either. Login may be achieved via authorized cookies.
There are two unrelated flags `_logged_in` and `logged_in`.
Should be `display_id`.
Not all URLs contain video id.
No, it's not. If video_id extraction from page fails whole extraction fails.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
You technically can't login with this extractor apart from using cookies.
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
still the check for element class is missing.
Just `video_url = urljoin('https://ndtvod.bc-ssl.cdn.bitgravity.com/23372/ndtv/', filename)`.
1. This will never be reached. 2. Don't change the order.
Each test the same scenario = duplicate.
All duplicate tests must be `only_matching`.
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
No such meta field.
Not used with formats.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
Use `self._search_regex` and `utils.unified_strdate` instead.
Name an example URL where `og:description` has HTML tags.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
surround only the part that will threw the exception.
falback to a static URL.
both are know beforehand, so there is no need to use `urljoin`.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
`default=None` for the first.
Both used only once, move to the place where used. Also relax both regexes.
Should be delegated via `url_result`.
Must be int.
All debug code should be removed.
All these regexes should be relaxed.
This is determined automatically.
Should not be fatal.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
it better to extract all the urls in the `mpath` array.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
you can get json output by appending `&format=json` to the api request url.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
All video formats must be extracted.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
No newline at the end of file.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
You can use a `dict` expression, and the return value shouldn't be a list: ```py return { 'url': video_url, 'ext': 'webm', 'id': video_id, 'title': video_id, 'license': licenze, } ```
`ExtractorError` is not raised when `fatal=False`.
Breaks if not arr.
No need to escape whitespace.
Formats in webpage are still available and should be extracted.
This is pointless.
Remove useless code.
`_search_regex` is enough.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Must not be fatal. Do not capture empty string.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
This should actually be just `self.url_result(embedded_url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
The argument will already be a character string, no need to decode it.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
This should actually be just `self.url_result(embedded_url)`.
There should be spaces between `%`.
Relax `id` group.
This does not make any sense, you already have `url`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`/?` does not make any sense.
Don't capture unused groups
Do not carry dict values.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Do not capture empty strings.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
url and formats are not used together.
No trailing $, override suitable.
generator or PagedList instead of list.
Breaks on None.
No escapes for slash.
Nothing changed. Also there is a video id available in JSON.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks on `None`.
This is pointless.
Formats in webpage are still available and should be extracted.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Lack of information is denoted by `None` not `0`.
Don't shadow built-ins.
It should match at least one character.
extraction must be tolerate to missing fields.
This should be extracted first.
Sholdn not break the extraction if missing.
Use `compat_urllib_parse_unquote_plus` instead.
This code looks similar to `sd` format and can be extracted to a function.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Everything apart from `url` is optional.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
Sorry - dismiss that
Temp file is not removed in this case.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Breaks. Read coding conventions.
All formats should be extracted.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion 'noplaylist': True, ```
No hardcodes. Use API.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Code in bracket is a set of matching characters. `or`ing won't work.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
All formats must be extracted.
No need to escape `/`.
Read: coding conventions, optional fields.
Relax `id` group.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
```suggestion new = '' ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Don't shadow built-in names.
Already pointed out: must be `ExtractorError`.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
Request wrapping code can be moved to the base class.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Query should be passed as `query` parameter.
No need to escape `]` is character set.
`else` is superfluous.
`--no-playlist` is not respected.
Use `self._html_search_meta()` instead.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
All methods only used once should be explicitly inlined.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
It should always return a list.
Use `\s*` instead.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Regex should be relaxed. Dots should be escaped.
Audio must have proper `vcodec` set.
Remove all unused code.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
Never ever use bare except.
Playlist title is optional, description breaks.
Extraction should be tolerate to missing fields.
fallback to other available values.
Use `self.playlist_result` instead.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
eg find type by URL ext
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Unite in single list comprehension.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Must not be fatal. Read coding conventions on optional/mandatory fields.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
This worked for me.
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
works also for me :+1:
Video id length is 8.
All formats should be extracted.
All methods only used once should be explicitly inlined.
`note` and `errnote` of `_download_json` instead.
This is fatal.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks. Read coding conventions.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
You have some unmerged lines here
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
```not (foo is None)``` => ```foo is not None```
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
1. Relax regex. 2. Do not capture empty dict.
Omit expected type.
Invalid arguments for 4-5.
Yes, it should accept any variation of whitespace.
Must not break extraction if missing.
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
Must not break extraction if missing.
Must not break extraction if missing.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
this will fail if `type` is not present.
you can iterate here using `values` method and make it in a single line without checking if `Item` is present: ```python for metadata in video_data.get('__children', {}).get('Item', {}).values(): ```
no need to create a method when it will be used once.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
By providing username and password in params obviously.
This is only used once.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
I'm pretty sure PEP8 mandates the `break` to be in a new line, but that's not that important.
No need in this message.
Please use `print()` syntax, as we support also Python 3(.3+)
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
You can use a `dict` expression, and the return value shouldn't be a list: ```py return { 'url': video_url, 'ext': 'webm', 'id': video_id, 'title': video_id, 'license': licenze, } ```
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
87-90 code duplication.
Query to `query`,
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Recursion should be replaced with plain loop.
No, it's not. If video_id extraction from page fails whole extraction fails.
This will break extraction if no `id` present.
`{}` won't work in python 2.6.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
I've already pointed out: I won't accept this.
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
This has no effect. Postprocessors work on info dict copy.
Mandatory data must be accessed with `[]` not `get`.
`images` is not guaranteed to be a list.
`if images` implies the second check.
`_process_data(x,...)` expects `x` to be a `dict`. The downloaded 'JSON' may turn out to be a list or just `False`. So: ``` info = self.download_json(api, song_id) info_data = try_get(info, lambda x: x['data'], dict) if not info_data: # if the site might often do this, not just when the API changes, add # `, expected=True` raise ExtractorError('API returned empty or invalid song data') return self._process_data(info_data, song_id, type_url) ```
It's not used inside FileDownloader.py so you probably don't need it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
The URL should have been `compat_str` already, so the regex groups will be too.
- we don't directly use `str` in the project, instead we use `compat_str` to keep the code compatible with all support versions of Python. - when you call `str/compat` on a dict it won't give a good result, so, i think it should be ommited.
Empty string capture does not make any sense.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
```suggestion if episodes: ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Do not escape quotes inside triple quotes.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
check the existence of the `contentUrl` before adding the format.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
this does not handle the case where `contentUrl` value is `None`.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
There are also vtt subtitles available.
Check code with flake8.
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
Should contain `quality` key.
Must not break extraction if missing.
`enumerate` on for range.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Must not break extraction if missing.
49-95 code duplication.
Must not break extraction if missing.
No such meta field.
For `url` type any metadata here have no effect.
Query should be passed as `query` parameter.
End users do not read source codes thus will never find this advice.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
Move flags into regex. Regex should match `runParams={`.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
All formats should be extracted.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This field is added automatically no need to add it by hand.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
`default=None` for the first.
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
```XimilayaIE.ie_key()``` is better
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Relax `id` group.
No need to escape `\`.
All methods only used once should be explicitly inlined.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
DRY. Extract json format as well.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
Note `video_data` may be `None`.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
`.*` at the end does not make any sense.
No trailing $, override suitable.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
`id` by no means should be `None`.
Title is mandatory.
There is no point in `or None` since `None` is already default.
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Captures `?#&` ending as id.
`None` is not an id.
Use raw strings.
Capturing empty string is senseless. `\n` instead of `root[__env]`.
`_match_id`. Do not shadow built-in names.
Breaks if no videos in season.
`True` is default.
default and fatal are not used together. `True` is default.
Should check for a list.
`if not content_url:`.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Use `sanitize_url()`, or let the core code, which does so, fix it.
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Why have you changed the quotes here? It's not that important, but we strive to use `'` when possible, and have when possible a consistent quote character in a file.
This intermediate dict is completely pointless. Build formats directly.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
Must be int.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
surround only the part that will threw the exception.
the same for `streaming` key.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
both are know beforehand, so there is no need to use `urljoin`.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Don't try logging in when `username` is `None` obviously.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
All methods only used once should be explicitly inlined.
`if playlist_size:` is enough.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
fatal=True is already the default
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
group=1 is equivalent to the default behavior of _html_search_regex; just drop that.
(?:[^>]+)? can be simplified as [^>]*
This is not a generic embed.
This is too broad. It must not capture plain text URLs.
The other case was some legacy code. I don't see much sense in such messages since it's clear what extractor is delegated to since all messages from the final extractor are prefixed with `IE_NAME`.
To be removed.
This regex does not look like generic embed. Provide several examples that use this embedding.
To be removed.
If a method is used by at least two extractors, it can be moved to ```common.py```
This should not be hardcoded.
It comes from http://www.visir.is/section/MEDIA?template=related_json&kat=5&subkat=73. `kat` and `subkat` come from a webpage.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
These 2 lines could be replaced by: ```python uploader_url, creator = creator_data[0][0:2] ```
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
No such meta field.
No such meta field.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
Use regular string format syntax instead.
To be removed.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
Also looks like they redesigned the site so that extractor does not work any longer.
Matching empty id is senseless.
Although I think you may have taken "specific" more literally than I intended!
No direct URLs here.
No need to escape `/`.
Matching empty data is senseless.
Must not be `None`.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
Move to initial title assignment.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Use `self.url_result(inner_url, 'Generic')` instead.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `(?i)` in regex itself if you want case insensitivity.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Recursion should be replaced with plain loop.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
You can import `try_rm` from helper
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
Not a video id.
Use `self.playlist_result` instead.
It's not an album id.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
From what I've seen there is always only one video on the page thus no need in playlist.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`if playlist_size:` is enough.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Never use bare except.
Lack of information is denoted by `None` not `0`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
remove the duplication as much as possible(`https://www.vvvvid.it/vvvvid/ondemand/%s/` is always used).
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
> Does that seem more helpful? If this error was to actually happen, it would mean that what the API returns has changed beyond what we expect. When the happens, I think all the user can do is report an issue here. there is a generic option to do this(`--dump-pages`), and we would ask to the user to use this option if needed. > Is it typical practice to say in error messages "report this error to youtube-dl"? If so I can add that as well. you can do that, by removing `expected=True` from the options passed to `ExtractorError`,
- we don't directly use `str` in the project, instead we use `compat_str` to keep the code compatible with all support versions of Python. - when you call `str/compat` on a dict it won't give a good result, so, i think it should be ommited.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
```suggestion if episodes: ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
By the way, the pythonic way is to just evaluate `thumb_list`
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
Use `\s*` instead.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Never use bare except.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This should be just `return info`
metavar should be `FILE`. `type` is already string by default.
Just `File to read configuration from`.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Should be extracted from `ytplayer_config`.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
It's better to fail instead of fallback.
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Optional data should not break extraction if missing. Read coding conventions.
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Extractor should not return `None`.
Relax `id` group.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
This does not make any sense, you already have `url`.
This is determined automatically.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
```suggestion new = '' ```
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Escape dot. No need to split URL.
Do not match by plain text.
Place on a single line.
Again: it will capture empty string for `data-playlist-item=""`.
Capturing empty string does not make any sense.
Plain `for x in l`.
Extract `height` for each format.
Must not be fatal for playlist.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
I'm addressing this concrete line of code.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
You can import `try_rm` from helper
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
```dict_get``` makes codes even shorter
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
Breaks if no URLs extracted.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
`enumerate` on for range.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Either sloppy code or an anti-scraping measure.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
The same. Should use https if ```url``` use https.
87-90 code duplication.
Too broad regex.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
`default=None` for the first.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
parentheses not needed.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
combine into a single call to `_html_search_meta`.
fallback to other available values.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
yes, remove duplicate formats if the qualities are not available for all programs.
from first test of `SverigesRadioPublicationIE`: https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=low&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a32.m4a", "duration": 132, "codingFormat": 12, "state": 0, "isGeoblockEnabled": false } ``` https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=medium&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a96.m4a", "duration": 132, "codingFormat": 13, "state": 0, "isGeoblockEnabled": false } ```
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
should not return empty formats.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
Extract common URL base.
26-29, 32-37 code duplication.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
try put value that are used multiple times in a variable(ex: `author_info.get('id')`). i think the `{}` format is not supported in python 2.6.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
will break the extraction if `profiles` is empty or `None`.
breaks the extraction if `clips` is `None`.
this does not handle the case where `contentUrl` value is `None`.
same if one of the values is `None`.
Right, that's fine for `display_id`.
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Read coding conventions on optional and mandatory data extraction.
Use formatted strings.
Remove all useless noise.
Playlist title is optional, description breaks.
No, it's not the point. It **must not** skip items than belong to the range specified.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
I've already pointed out this does not work.
Uppercase is not honored.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
This must be passed in `url_result`.
Must be `list`.
Replace 245-257 with `entry.update({ ... })`.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
That's completely different videos.
Ids must stay intact.
Using preferences causes invalid sorting.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Title is invalid.
Read coding conventions on optional/mandatory meta fields.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
surround only the part that will threw the exception.
both are know beforehand, so there is no need to use `urljoin`.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Usually display_id is used before the actual video_id is extracted.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
mobj would be None if nothing is matched
Better to use unified_strdate for parsing dates.
Need fatal=False or default=None
Carry long lines.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
You could just write this like so, as other extractors seem to do: ``` info_dict = { 'id': video_id, ... ```
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Better to use `determine_ext` instead of `.endswith`
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
There is no need in this method.
Instead of such hacks you can name group differently and capture it without any issue.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
Groups around `video` and `sptv/spiegeltv` are superfluous.
All methods only used once should be explicitly inlined.
`if mediatype == u'video':` is idiomatic Python.
We may use proper XML parsing here and simply call `self._download_xml`
We now have a fully-fledged format system which can be used.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Same as above, we should create a `formats` array here.
```suggestion new = '' ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`.*/?` is pointless at the end.
The URL should have been `compat_str` already, so the regex groups will be too.
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Do not capture groups you don't use.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `\`.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
1. No umask respected. 2. There is no `os.chmod` in python 3.2 according to python [docs](https://docs.python.org/3/library/os.html#os.chmod). 3. flake8.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
Won't work. See how this is done for output template.
Should be `fatal=False`.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
Of course when it appears inside `script`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
We use single quotes as much as possible.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
`--ap-mso` should not be touched since it's a separate stand-alone mechanism that has stable unique MSO identifiers for TV providers across all extractors while this options is not.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
`autonumber` is not reset to zero in the first place.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
yt-dlp's `--print` is a bit more complex, allowing printing data at multiple stages. Eg: `-O "after_move:%(filepath)s" will print the final file path. This function is needed to (easily) support that syntax, (and also other similar options)
You only need a simplified version of this. Actually, optparse's built-in action `append` should be sufficient here
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
Post-processors are already identified by `key` in API same should be used here.
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
I'm **not talking about any particular test case**. I'm talking about a potential situation when this code: ``` <td class="key">Title:</td> <td class="value"></td> ``` **is not present on the webpage**. 46 will **fail** in this case because it has `fatal=True` and you will not reach a "fallback" in this case.
Dots should be escaped. Query may contain more arguments that will be incorrectly captured by this regex as path.
There are more metadata to be extracted: `duration`, `abr`, `asr`, `filesize_approx`.
It's more rational to use `False` as default since it's used more often in this code.
Move to the place where it's first used.
This will break extraction if some field is missing. Optional fields should not break extraction.
And `_html_search_regex` will fail breaking the extraction completely.
This is pointless, you don't have any fallback.
56-71 code duplication.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
`True` is default.
Default sorting is just fine. Remove `field_preference`.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
I would always return a `multi_video` result.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
Not used with formats.
No such meta field.
No. Use fatal search regex instead.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
will return invalid URL if `search_url` is `null`.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
no longer needed.
there is not need for excess verbosity.
make one of the tests an `only_matching` test.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
```suggestion if not (season_id and video_id): ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
You technically can't login with this extractor apart from using cookies.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
We should use the various `compat_*` down below and delete these imports, so that the code also runs on Python 3.
Do not capture groups you don't use.
`https?://` is better in this case. openload supports both HTTP and HTTPS.
This doc should be updated.
You can import `try_rm` from helper
Use _ (underline) instead of webpage if the value is not used.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Do not shadow existing variables.
You have some unmerged lines here
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Extracting duplicate code into a function obviously.
Can we resolve these IDs? There may also be a time encoded in there.
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
The class name should ends with FD
The number 25 should be configurable
Don't capture groups if you are not going to use them.
You should capture a part of URL that represents a video in unique way...
Use `_request_webpage` instead.
...and output it here instead of const string `video_id` here that makes no sense.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
Again: relax regex.
It's a field name not a step name.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Read coding conventions on optional fields.
None is default.
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
The same. Should use https if ```url``` use https.
Not all URLs contain video id.
Should be `display_id`.
No, it's not. If video_id extraction from page fails whole extraction fails.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
There are two unrelated flags `_logged_in` and `logged_in`.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
```suggestion if episodes: ```
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
> Does that seem more helpful? If this error was to actually happen, it would mean that what the API returns has changed beyond what we expect. When the happens, I think all the user can do is report an issue here. there is a generic option to do this(`--dump-pages`), and we would ask to the user to use this option if needed. > Is it typical practice to say in error messages "report this error to youtube-dl"? If so I can add that as well. you can do that, by removing `expected=True` from the options passed to `ExtractorError`,
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
- we don't directly use `str` in the project, instead we use `compat_str` to keep the code compatible with all support versions of Python. - when you call `str/compat` on a dict it won't give a good result, so, i think it should be ommited.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
```[\s]``` => ```\s```
Better to use unified_strdate for parsing dates.
mobj would be None if nothing is matched
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
`--no-playlist` is not respected.
I'm pretty sure PEP8 mandates the `break` to be in a new line, but that's not that important.
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
This introduces an ambiguity in case of several mso available. That's why it won't be accepted.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
`ext` should be mp4.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
This line is unnecessary.
Pass `default` to `_og_search_title` instead.
You should not silence all another exceptions but re-raise.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Put `raise` after `if` section at the same indent.
Never use bare except.
No need to escape forward slash.
Instead of `resolution` and `preference` it should be extracted as `height`.
This will break the entire extraction if there is no match for some format.
Use single quotes consistently.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
No such meta field.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
`'thumb`' may not be present producing invalid thumbnail url.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
No such meta field.
Title is mandatory.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
Read coding conventions and fix all optional meta fields.
`_search_regex` is enough.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
Empty string capture does not make any sense.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
Empty string capture does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This may change as well. Add a fallback that just processes all videos without differentiation.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Query should be passed as `query` parameter.
```suggestion 'noplaylist': True, ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
parentheses not needed.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
combine into a single call to `_html_search_meta`.
fallback to other available values.
`title` is mandatory. Move flags into regex itself.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
Query should be passed as `query`.
`True` is default.
This is pointless, you don't have any fallback.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
56-71 code duplication.
Default sorting is just fine. Remove `field_preference`.
This code looks similar to `sd` format and can be extracted to a function.
All formats should be extracted.
Looks like, it's off then.
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
`_match_id`. Do not shadow built-in names.
`True` is default.
This is not supported in python 2.6.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
No exact URLs here.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
Use display id.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
Mandatory data must be accessed with `[]` not `get`.
`/?` is senseless at the end.
No trailing `$`, override `suitable`.
No escapes for slash.
Capture with /album. Capture non greedy.
Dot is pointless here.
`{}` won't work in python 2.6.
Not a video id.
It's not an album id.
Read coding convention on optional fields and fix all issues.
Will break if `episode_title` is `None`.
Will break if `picture_url` is `None`.
`for k, v in flashvars.items()`.
`flashvars[k]` is `v`.
Must be numeric.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
Invalid syntax at all.
This should be extracted first.
Sholdn not break the extraction if missing.
Must be numeric.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
26-29, 32-37 code duplication.
Move `_match_id` into `_extract_audio`.
should not return empty formats.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
`vcodec` to 'none'.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
`formats` should be sorted.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
Use self._report_warning instead
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
Actually, it's an opposite. It's a check for successful login.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`ext` should be mp4.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use single quotes consistently.
Consistently use flags inside regex.
All debug code should be removed.
Should be delegated via `url_result`.
It's obvious from `'is_live': True`.
What's the point of `# match self._live_title` here? Remove.
Again: relax regex.
It's a field name not a step name.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Carry to the indented beginning of the line.
Carry to the indented beginning of the line.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
If there's only one format, just use `'url'`.
All formats should be extracted.
Use `self._search_regex` and `utils.unified_strdate` instead.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
Remove useless code.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This is pointless.
Formats in webpage are still available and should be extracted.
first_three_chars = int(float(ol_id[:3])) is cleaner
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
If `_search_regex` fails `None` will be passed to `_parse_json`.
We may use proper XML parsing here and simply call `self._download_xml`
Same as above, we should create a `formats` array here.
It should not. See the description of the field.
Temp file is not removed in this case.
Breaks on unexpected data.
Sorry - dismiss that
The current working directory is not always writable.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
- - [ ]
`width` and `height` instead.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
All formats should be extracted.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
No, ADS is the equivalent of xattr. I'd rather have both in one option.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
surround only the part that will threw the exception.
Either sloppy code or an anti-scraping measure.
Use `self._search_regex` and `utils.unified_strdate` instead.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Name an example URL where `og:description` has HTML tags.
`{}` doen't work in python 2.6.
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
I've already pointed out: use `display_id` until you get real id.
No `id` extracted.
Pass `default` to `_og_search_title` instead.
This does not necessarily mean that. There is a clear captured error message that should be output.
Now you break extraction if any of these keys is missing.
Sure, that would be fine.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
Move data and query into `_download_webpage` call.
This condition is not needed, t is always None here.
`False` is not a valid note.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
Never ignore generic exceptions
Use compat_HTTPError instead
Or at least, it's not `unicode`. In yt-dl `str` should almost always be `compat_str`, but as above it's not needed here.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
There are more video formats available that should be extracted as well.
This should be removed. `player_url` is used for RTMP.
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
This fails on python 3 and doesn't look too good.
You can import `try_rm` from helper
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
This will break unicode strings under python 2.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Title is mandatory.
No need to use named group when there is only one group.
`player_id` is not extracted in this fallback but used at 71.
No `ExtractorError` is raised here, `except` will never trigger.
`_search_regex` per each field. Add fallback.
JSON should be parsed as JSON.
Never use bare except.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
I've already pointed out: API URLs should be used.
`try_get`, single quotes.
This is default.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
Extracting duplicate code into a function obviously.
`if height and filesh:`
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Optional data should not break extraction if missing. Read coding conventions.
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
Remove useless code.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
Read and follow code conventions. Check code with flake8.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
should not fail if the extraction of one set or item is not possible.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
What's the point of this? `canonical_url` is the same as `url`.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This does not make any sense, you already have `url`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
What's the point of this? Remove.
Parse from flashvars JSON.
Just output complete stringified flashvars and consume in python code as JSON.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
You should capture a part of URL that represents a video in unique way...
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
I think you don't need `locals()` here.
No exact URLs.
We should really provide a better interface to test against, something along the lines of `download(url)`.
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
...and output it here instead of const string `video_id` here that makes no sense.
I think it's customary to use `_VALID_URL` for id matching if possible.
`.*` on both ends of regex make no sense. Also when capturing at least one character is required - there is no sense capturing empty video id.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
This can be moved inside `if chapters:` condition.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
This has no effect. Postprocessors work on info dict copy.
The filename will end up as `.list.mkv` which doesn't make sense. It should be either `.mkv.list` or just `.list`
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
These looks like candidates for generalization and extracting into a separate method.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Breaks if no `name`.
Title is mandatory.
No `url` and `formats` at the same time.
It's not a display id.
`only_once` parameter isn't in yt-dl, yet.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
Inline to actual call place.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Do not match by plain text.
Plain `for x in l`.
Extract `height` for each format.
Escape dot. No need to split URL.
Place on a single line.
Capturing empty string does not make any sense.
Again: it will capture empty string for `data-playlist-item=""`.
Must not be fatal for playlist.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
the same for `streaming` key.
both are know beforehand, so there is no need to use `urljoin`.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
This will break extraction if no `id` present.
Must be `list`.
This will result is `[None]` is no category extracted.
Read coding conventions on optional fields.
Replace 245-257 with `entry.update({ ... })`.
Capture as `id` obviously.
I've already pointed out: `.*$` is pointless at the end.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Again: no, do not touch token extraction code. Just eliminate duplication.
Again: you must eliminate code duplication. In both places.
Can't be None.
no need to create a method when it will be used once.
All formats should be extracted.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
`id` by no means should be `None`.
After this, check for mandatory `title` field, and use the value in the returned info_dict: ```suggestion video_data = self.get_video_data(url, video_id) title = video_data['title'] ```
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
As above, `final_asset.get("contentUrl")` could return `None`. Get the mandatory value early; crash if that fails.
just use a hardcoded value for now.
accept `audio` `mediaType`.
use `query` argument.
should not break the extraction if the field is not available.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
```suggestion _VALID_URL = r'https?://(?:4d\.rtvslo\.si/(?:arhiv/[^/]+|embed)|www\.rtvslo\.si/(?:4d/arhiv|mmr/prispevek))/(?P<id>\d+)' ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Just put original texts: æ§ããã«è¨ã£ã¦é å¼µã£ã¦ã
url and formats are not used together.
No trailing $, override suitable.
`.*` at the end does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion new = '' ```
No trailing $, override suitable.
All these regexes should be relaxed.
Must be int.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
Bitrate should go to corresponding format meta field.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
No trailing $, override suitable.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
I've already suggested using `Downloading` as idiomatic wording.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
Never use bare except.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
No unrelated changes.
Remove all unrelated changes.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
It may be interesting to allow to add new items just as strings, for example : ``` IEs = [ ('Youtube', ['YoutubePlaylistIE', 'YoutubeChannelIE', 'YoutubeUserIE', 'YoutubeSearchIE', 'YoutubeIE']), 'Generic', ] ``` It would make easier to add simple IEs.
This doc should be updated.
I think you don't need `locals()` here.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
But it would need to modify `gen_extractors`, it's not a great deal.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
hls and rtmp are available as well.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
What's the point of lines 104-108? `ext` is already flv.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
Sorry I didn't check it, it should look into `<div class='more_info'>`
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
What's the point of this? Remove.
There is no point checking whole dicts and exact URLs.
Do not pass `default=None` to `_html_search_regex` instead.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use `self.url_result(inner_url, 'Generic')` instead.
The whole code until here can be simplified to `page_id = self._match_id(url)`
This looks like `orderedSet(m.group(1) for m in re.finditer(r'href="/video([0-9_]+)"')`. Also, since it only gets called once, feel free to move it in the main function.
Why is the `m?` group in here? If it's optional anyways, you can just leave it out ;)
This line is superfluous, the youtube-dl core can guess the extension better than that already.
You technically can't login with this extractor apart from using cookies.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Again: it must contain `vbr or abr` if available.
The starting part that is already used for it.
This should be fixed as well.
This will skip `format_id` completely even if `media_type` is available.
Breaks on unexpected data.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
There should be a fallback title if `_og_search_title()` returns `None`.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Use `self._parse_html5_media_entries` instead.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
This is fatal.
All formats should be extracted.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Sorry, we actually request 10KiB, and `K` does stand for Kilobyte in head.
`K` in head stands for `Kibibyte`, the test uses Kilobytes.
`id` and `display_id` should be tested as well.
This looks as if it would be better to split the code into two extractors. You can easily pass from one into the other by returning a `url_result`.
Do not shadow built-in names.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
According to pep8, there's a missing space after `+` in here.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
`note` and `errnote` of `_download_json` instead.
This is fatal.
Parse from flashvars JSON.
Use `self.url_result(inner_url, 'Generic')` instead.
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Again: ```suggestion {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie}, separators=(',', ':')) message = 'nobody{0}use{1}md5forencrypt'.format( URL, request_text).encode('latin1') ```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Py2.6 compat (!): ```suggestion 'requestId': '{0}_{1:04}'.format(now, rand), ```
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
should be in the `else` block of the `for` loop.
the same for `streaming` key.
Using preferences causes invalid sorting.
Breaks if no `rate` key in `stream`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Course extraction must be in a separate extractor.
You have some unmerged lines here
Matching empty string is senseless.
`_` is idiomatic way to denote unused variables.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
Check code with flake8.
`flashvars[k]` is `v`.
`for k, v in flashvars.items()`.
This field is height not quality.
Nothing changed. Breaks in case regex does not match.
Invalid syntax at all.
Must be numeric.
This should be extracted first.
Sholdn not break the extraction if missing.
Must be numeric.
```suggestion new = '' ```
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
Don't shadow built-in names.
This check is pointless.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
Query to `query`,
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
This should only be printed when sleep interval is provided. It's kind of misleading and unrelevant to see when you didn't want any sleep interval.
You have some unmerged lines here
This does not make any sense since you already sleep in `_request_webpage`.
`int_or_none` for all int fields.
No bare except.
There are also vtt subtitles available.
Check code with flake8.
`/?` does not make any sense.
Do not carry dict values.
Do not capture empty strings.
`.*` at the end does not make any sense.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Must not be fatal. Read coding conventions.
url and formats are not used together.
No trailing $, override suitable.
Rename to `KanalDIE`.
generator or PagedList instead of list.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
Optional fields should not break extraction if missing.
54-58, 71-75 code duplication.
`int_or_none` and `float_or_none` for all numeric fields.
this does not handle the case where `contentUrl` value is `None`.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
No need to escape `#`. No need to capture groups you don't use.
Use regular string format syntax instead.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
`--rm-cache-dir` wipes the whole cache thus should never be suggested to use.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
This prevents from authenticating with `--cookies`.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
`ext` should be mp4.
The argument will already be a character string, no need to decode it.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use single quotes consistently.
This video is georestricted.
Use `self._match_id` is better.
There's no need to name a group if not used.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
This is determined automatically.
Correct field name is `format_id`.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All methods only used once should be explicitly inlined.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
This code looks similar to `sd` format and can be extracted to a function.
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need for this check, this is already checked in `_sort_formats`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Field name is supposed to be `key` not `long_video_id`.
`_search_regex` is enough here.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
Should extract chunklists via `self._extract_m3u8_formats` here.
This is pointless.
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
accept `audio` `mediaType`.
should not break the extraction if the field is not available.
Please remove debugging lines.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
extract mandatory information(title and formats) first, and sort formats.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
You should have only one single method for extracting info.
170-172 - code duplication.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
Plays fine without any authentication in browser.
Consistently use flags inside regex.
This is not true at the moment.
All debug code should be removed.
Should be delegated via `url_result`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
These looks like mandatory fields.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/bigflix.py#L23-L25 All tests that test same extraction scenario should be made `only_matching`.
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
It should always return a list.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Actually, it's an opposite. It's a check for successful login.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I guess it a typo? now -> not
There's no need to `return` after `raise`.
Real id is in widget dict.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
1. Don't shadow outer names. 2. `url_or_none`.
34-35 can be easily moved into `for`.
Audio is not 128.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
This code looks similar to `sd` format and can be extracted to a function.
Code duplication in 155-163 and 165-172.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
`if not formats` is enough.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
`acodec == 'none'`.
Move into `_download_json`.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Should not be fatal.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Either sloppy code or an anti-scraping measure.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
Must not be `None`.
Move to initial title assignment.
`note` and `errnote` of `_download_json` instead.
This is fatal.
`int_or_none` for all int fields.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
No bare except.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
m3u8 is also available.
Lack of information is denoted by `None` not `0`.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
This is fatal.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
Avoid shadowing built-in names.
still fails if `uploader_data` not available.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
No bare except.
This is fatal.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
`_` is idiomatic way to denote unused variables.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
Use `query` for query.
`acodec == 'none'`.
Move into `_download_json`.
It's already done at L151.
Move to the place of usage.
`info_dict['formats'] = formats`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
If both test the same extraction scenario leave only one.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
Unite in single list comprehension.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Don't capture groups if you are not going to use them.
Don't capture unused groups
No need to escape `/`.
Should not break if `published_at` is missing.
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
You must delegate with `url_result` instead.
Rename to something else.
`url` should not be `None`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Request wrapping code can be moved to the base class.
Read coding conventions on optional and mandatory data extraction.
No need for this check, this is already checked in `_sort_formats`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Could you provide more info on that scenario? I was unable to find any example URL that wouldn't be covered by previous patterns.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
26-29, 32-37 code duplication.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
Move `_match_id` into `_extract_audio`.
should not return empty formats.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
`vcodec` to 'none'.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
`formats` should be sorted.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
Do not shadow `url` variable. `fatal` has no effect when `default` is provided.
Code duplication at 58-60 and 68-70.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
- - [ ]
`width` and `height` instead.
Audio is not 128.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
Read coding conventions on how mandatory data should be accessed.
No need to escape `#`. No need to capture groups you don't use.
`int_or_none` and `float_or_none` for all numeric fields.
Use regular string format syntax instead.
`id` by no means should be `None`.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
To be removed.
151-165 code duplication.
Breaks on unexpected data.
`/?` makes no sense at the end.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Set empty `vcodec`.
make one of the tests an `only_matching` test.
`playlist_description` can be extracted from the same data.
1. Extract dict if you expect dict. 2. Relax regex. 3. Escape dots.
Again: float_or_none, not parse_duration.
`int_or_none` and `float_or_none` for all numeric fields.
Use regular string format syntax instead.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
Use display id.
Mandatory data must be accessed with `[]` not `get`.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
`/?` is senseless at the end.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
230-264 no copy pastes.
Extracting duplicate code into a function obviously.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
No such meta field.
There is no need in this method.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
should not fail if the extraction of one set or item is not possible.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
What's the point of this? `canonical_url` is the same as `url`.
230-264 no copy pastes.
I guess something like `base_url` is a better name than `url`.
I guess using `content_type` instead of `ct` improves readability.
`r'(?s)(<(?P<tag>video|audio)[^>]*>)(.*?)</(?P=tag)>'` is better as numbers may change in the future.
`media_attributes` is somewhat misleading. It's the whole tag but not just attributes.
`(?s)` has no effects in regular expressions without `.`
A typo? `pariod`
This does not necessarily mean geo restriction, they may have other reasons for video unavailability. It's better checking the message inside `p` that explicitly states about blocking content in `your country`.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
It's better to fail instead of fallback.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
`{}` won't work in python 2.6.
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
Do not touch `only_matching` tests.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
This is determined automatically.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
No trailing $, override suitable.
We may use proper XML parsing here and simply call `self._download_xml`
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
Single loop for all sources without any unnecessary intermediates.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Too broad regex.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
There is no point in that.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
```suggestion new = '' ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
This code looks similar to `sd` format and can be extracted to a function.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
No need to escape `/`.
Use regular string format syntax instead.
To be removed.
All formats should be extracted.
Playlist title is optional.
`_parse_json`. Read coding conventions.
Will never happen.
Never use bare except.
Breaks if no URLs extracted.
Read coding conventions on optional and mandatory data extraction.
Playlist title is optional, description breaks.
Course extraction must be in a separate extractor.
Must not be fatal.
Noway. See other extractors on how to delegate properly.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
OK. I'll open an issue for discussing this. For now you can remove this line.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
This can be moved inside `if chapters:` condition.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
`'id'` is required.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
```suggestion new = '' ```
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
Do not touch existing tests.
All these regexes should be relaxed.
Either sloppy code or an anti-scraping measure.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This code looks similar to `sd` format and can be extracted to a function.
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
This field is added automatically no need to add it by hand.
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
If a method is used by at least two extractors, it can be moved to ```common.py```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
Single loop for all sources without any unnecessary intermediates.
Must not be fatal.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Breaks on missing key, breaks on download failure.
As already said: no trailing $. Override `suitable`.
`urls` is pointless. Build `entries` straightaway.
All debug garbage must be removed.
Must be separate extractor.
`playlist_description` can be extracted from the same data.
`_` is idiomatic way to denote unused variables.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
and the function that normally used to encode postdata is `urlencode_postdata`.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
`autonumber` is not reset to zero in the first place.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Don't use `;` unless you need to write more than 1 statement per line (personally, I would also avoid doing that)
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
The keys of the `dash_formats` dict are the `format_id` fields themselves, so you could directly `set(dash_formats)` or `set(dash_formats.keys())` to make it clearer.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Read coding conventions on optional/mandatory meta fields.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Referring `url` from `url` looks like nonsense. Provide rationale.
Do not escape `/`.
Read coding conventions on optional metadata.
this will be done for you by just providing `width` and `height`
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
All formats should be extracted not only mp4.
could you add description from perex key
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
The most simple way is to use `utils.extract_attributes`.
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `\s*` instead.
Unless it supports videos from other sites, the IE name already says for what it is.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Don't do this manually use `_download_webpage_handle`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
`split` returns a list, video_id must be a string.
Can we resolve these IDs? There may also be a time encoded in there.
It's better to use `self._download_webpage(url, video_id)`
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
All methods only used once should be explicitly inlined.
Use `video_id`. Remove `.replace('\n', '')`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Breaks extraction if no `followBar`.
Strings in JSON may contain `<`.
Move flags into regex. Regex should match `runParams={`.
I've already pointed out: use `float_or_none`.
The third parameter of `_html_search_regex` is name but not ID.
Lack of information is denoted by `None` not `0`.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No need for escapes inside a brace group, all dots outside must be escaped.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Audio must have proper `vcodec` set.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Remove all debug output.
`video_id` may be `None`.
Remove all unused code.
`default` implies non fatal.
Place right here in the following order: 1. If no title extract with `_og_search_title`. 2. If still no title extract from `<title>`. 3. Extract `playlist_description`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
No need to escape `]` is character set.
From what I've seen there is always only one video on the page thus no need in playlist.
`--no-playlist` is not respected.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Also do not use double quotes for string literals.
Don't shadow outer names. No need for bracket when using single character.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
Only whitespace is allowed between `videoInfo` and `=`.
No escapes for `/`.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
Code duplication. Must be single call to search regex.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Do not remove existing tests.
1. This will never be reached. 2. Don't change the order.
All these regexes should be relaxed.
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Must be int.
By convention, a title is required. Here, if the title isn't found, `.strip()` will crash. You could make `fatal=True`, or supply a default, say `default='Untitled video %s' % video_id`.
All tests that test similar extraction scenario should be `only_matching`.
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Put `raise` after `if` section at the same indent.
You should not silence all another exceptions but re-raise.
This line is unnecessary.
This value looks an awful lot like a `display_id`
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
There are two unrelated flags `_logged_in` and `logged_in`.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Weird. Yesterday it was working: ``` python -m youtube_dl http://mobile-ondemand.wdr.de/CMS2010/mdb/ondemand/weltweit/fsk0/75/752868/752868_8108527.mp4 [download] Destination: 8108527-752868.mp4 [download] 1.4% of 291.34MiB at 8.53MiB/s ETA 00:33 ERROR: Interrupted by user ``` But not now.
[Use `utils.qualities` instead](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/npo.py#L123).
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
Not used with formats.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
No such meta field.
Either sloppy code or an anti-scraping measure.
The third parameter of `_html_search_regex` is name but not ID.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Use `self._match_id` is better.
`self._search_regex` is enough here.
Matched data-video should not be empty.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
I suggest extracting from `mediaLink` element instead of matching the JS URL as it may change in the future. For example: ``` Python media_link_obj = self._parse_json(self._html_search_regex( r'class="mediaLink\b[^"]*"[^>]+data-extension="([^"]+)"', webpage, 'media link'), display_id, transform_source=js_to_json) js_url = media_link_obj['mediaObj']['url'] ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
Unite in single list comprehension.
Formats not sorted.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion 'noplaylist': True, ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
You're already building query with `query` param.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
It does not necessarily mean that.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
These are not used.
Instead of adding new parameters put them into `ctx`.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
1. This must only take place when it's not available from player JSON. 2. Query must be passed as `query`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Nothing changed. Also there is a video id available in JSON.
`_search_regex` with `default=None` instead.
Breaks on `None`.
Don't shadow built-ins.
All formats should be extracted not only mp4.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
As already pointed out: you must delegate to `SBSIE` extractor not inherit from it.
Not acceptable. Search `url_result`.
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
More relaxed regex.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
```suggestion if episodes: ```
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
This test is identical to the first. Revert.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
This is equivalent to InfoExtractor._match_id
All methods only used once should be explicitly inlined.
The argument will already be a character string, no need to decode it.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
This is fatal.
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
All these regexes should be relaxed.
```suggestion new = '' ```
Must be int.
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
You must delegate with `url_result` instead.
Rename to something else.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`url` should not be `None`.
`'id'` is required.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
No exact URLs here.
Do not capture groups you don't use.
Query to `query`.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
Move `_match_id` into `_extract_audio`.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
it's either one of two cases: they are identical -> keep them as they are(formats of the same entry). they are different -> separate them into a playlist with two entries.
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
`vcodec` to 'none'.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
Don't carry URLs. Read coding conventions.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
All these regexes should be relaxed.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
```suggestion new = '' ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Name an example URL where `og:description` has HTML tags.
`player_id` is not extracted in this fallback but used at 71.
No `ExtractorError` is raised here, `except` will never trigger.
Extraction should be tolerate to missing fields.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
Use display id.
This is checked by `_search_regex`.
This is superfluous, the extension can be extracted automatically.
Now you break extraction if any of these keys is missing.
No, override `suitable` instead. Also do not split strings.
There are also vtt subtitles available.
Check code with flake8.
`(byte_counter / rate_limit) - elapsed` sometimes takes negative values (tested on python2). Negative delay to `sleep` results in `IOError` and failed download. There should be at least a check for that.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
That's all correct code [53-93], do not remove it. Fix `add_m3u8_format` instead.
Duration calculation is incorrect.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Code duplication with tbs extractor.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Must be extracted first.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Should not be fatal.
This is already fatal.
No such meta field.
Not used with formats.
This is superfluous since you provide `formats`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
`compat_str()`, here and in l.96.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
no need to set `fatal=True`, this is the default.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
breaks the extraction if `clips` is `None`.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
will break the extraction if `profiles` is empty or `None`.
try put value that are used multiple times in a variable(ex: `author_info.get('id')`). i think the `{}` format is not supported in python 2.6.
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
same if one of the values is `None`.
Do not remove the old pattern.
No escape for `/`.
Use `\s*` instead.
All formats should be extracted.
All these regexes should be relaxed.
All these fields should be `fatal=False`.
Do not capture empty strings.
Must be int.
No need for this check, this is already checked in `_sort_formats`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Must ensure numeric.
Has no effect for url_transparent.
Merge in single list comprehension.
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Use `utils.xpath_text` instead, again with `fatal=False`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
As already said: no trailing $. Override `suitable`.
All debug garbage must be removed.
Must be separate extractor.
Do not reformat code and remove irrelevant changes.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Lack of data must be expressed by `None` not empty string.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Unite in single list comprehension.
It `md5` flag should not go here as an argument. Instead it should be extracted in particular downloader from `self.params`.
Calculate it in chunks after the final file is on disk. As said it makes no sense to calculate it immediately since the final file may be modified by postprocessor.
This is missing a `cwd=` spec at the latest. If we need a git revision number, we should really think about releasing more often instead.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Don't use print() directly as -j will be broken. Use to_screen or related functions instead.
Such logic should be implemented in get_suitable_downloader()
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
You must output to a temp file not the original file.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
`playlist_description` can be extracted from the same data.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
This is already fatal.
Should not be fatal.
Capturing empty URL is senseless.
No. Use fatal search regex instead.
No such meta field.
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
`compat_str()`, here and in l.96.
parentheses not needed.
fallback to other available values.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
combine into a single call to `_html_search_meta`.
`title` is mandatory. Move flags into regex itself.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
All debug garbage must be removed.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
Must not be fatal. Read coding conventions on optional/mandatory fields.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
hls and rtmp are available as well.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
You have some unmerged lines here
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
There should be an `id` group.
Use `self._search_regex` and `utils.unified_strdate` instead.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
url and formats are not used together.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
You don't need to call `report_extraction` here and above, since the extraction is over already.
No trailing $, override suitable.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
Use the original scheme and host.
Don't shadow built-in names.
Breaks on `None` title.
What's the point of this? Use `url` as base.
Don't capture groups you don't use.
Query to `query`,
fatal=True is default.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Inline everything used only once.
Do not match by plain text.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
`ext` should be mp4.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
It won't be longer.
Revert. >Checking download_**url** video format **URL** makes even less sense.
`{}` won't work in python 2.6.
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
Extracting duplicate code into a function obviously.
This code looks similar to `sd` format and can be extracted to a function.
Read coding conventions on optional/mandatory meta fields.
It does not necessarily mean that.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
Better to use `determine_ext` instead of `.endswith`
`formats` is always a list of dictionaries.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
Plain `for` is enough.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
use the extension extracted from `determine_ext`.
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
could you add description from perex key
All formats should be extracted not only mp4.
By providing username and password in params obviously.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
For now extract it in a new base IE class.
Should be delegated via `url_result`.
All debug code should be removed.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Code duplication at 58-60 and 68-70.
This check does not make any sense. If there are no formats extraction should stop immediately.
If `_search_regex` fails `None` will be passed to `_parse_json`.
These formats should not be removed.
`title` is mandatory. Move flags into regex itself.
Consistently use flags inside regex.
This is not true at the moment.
- - [ ]
Do not match exact URL.
No exact matches for URLs.
`width` and `height` instead.
Audio is not 128.
All formats should be extracted.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Query to `query=`.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
No such field.
No such field.
This should be split into building url and extracting formats.
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
This must be assert not exception.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
no need to create a method when it will be used once.
`if not videos:`.
By providing username and password in params obviously.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Python 3.2 doesn't like u-literals.
Should not break if missing.
Optional fields should not break extraction if missing.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
Trailing /? is not necessary here
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
This code looks similar to `sd` format and can be extracted to a function.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Capturing empty string is senseless. `\n` instead of `root[__env]`.
Use raw strings.
Breaks if no videos in season.
Eliminate excessive verbosity.
`None` is not an id.
Should check for a list.
Check for `compat_str` also.
`True` is default.
default and fatal are not used together. `True` is default.
`if not content_url:`.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
yes, this is correct.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
You still duplicate the URL and unnecessary `if/elif` branches.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
You should make extraction tolerate to these fields missing not remove them.
Title is mandatory field thus it should be `data['roominfo']['name']`.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
`/?` is senseless at the end.
This branch is never reached.
There are also vtt subtitles available.
Check code with flake8.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
I've already pointed out: this must be removed.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
No, override `suitable` instead. Also do not split strings.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
You have some unmerged lines here
``` for i, video_url in enumerate(video_urls): ```
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
Code duplication in 70-102 and 104-137.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
`ref:` should not be removed from video id.
Code duplication in 142-145 and 146-149.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
`compat_str()`, here and in l.96.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
This line can just be removed.
170-172 - code duplication.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
You technically can't login with this extractor apart from using cookies.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
Filter invalid URLs.
`ref:` should not be removed from video id.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Don't shadow built-ins.
The whole code until here can be simplified to `page_id = self._match_id(url)`
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
fatal=True is default.
What's the point of this? Use `url` as base.
Query to `query`,
Use the original scheme and host.
Breaks on `None` title.
Don't capture groups you don't use.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion 'noplaylist': True, ```
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
Move data and query into `_download_webpage` call.
`False` is not a valid note.
`expected_status` to `_download_json` instead.
This does not necessarily mean that. There is a clear captured error message that should be output.
This branch is never reached.
This should not be fatal.
`default` is not used with `fatal`.
Use display id.
Any test case using this approach? I can't find it.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Also pass `m3u8_id='hls'`.
Unite in single list comprehension.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
fallback to other available values.
parentheses not needed.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
It should be robust in case of some missing fields.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
This code looks similar to `sd` format and can be extracted to a function.
Use `self.url_result(inner_url, 'Generic')` instead.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
No bare except.
All methods only used once should be explicitly inlined.
`int_or_none` for all int fields.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
`note` and `errnote` of `_download_json` instead.
This is fatal.
No need to escape `#`. No need to capture groups you don't use.
It's better to use `self._download_webpage(url, video_id)`
The argument will already be a character string, no need to decode it.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Request wrapping code can be moved to the base class.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
Must be extracted first.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Allow arbitrary whitespace and both quote types.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Extracting duplicate code into a function obviously.
Only whitespace is allowed between `videoInfo` and `=`.
This code looks similar to `sd` format and can be extracted to a function.
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
160 is a video format.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This is not matched by `_VALID_URL`.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Then just keep this code and > create default fallbacks if extraction of these fails
Upper case is idiomatic for constants.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Do not reformat code and remove irrelevant changes.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
It's already extracted as video_id.
Request wrapping code can be moved to the base class.
1. `v` may not be dict. 2. `v.get('slug')`.
As already said: parse as JSON not with regexes.
No such meta field.
Not acceptable. Search `url_result`.
As already pointed out: you must delegate to `SBSIE` extractor not inherit from it.
I think this two lines are better using compat_urllib_parse_urlparse and the `_replace` method: - It's not immediately clear clear why you use `video_url[:10]`, it's better if you explicitly change the path - `rendition_url.rindex('.')` won't works as expected if the query contains a dot, and normal `index` will match the dot from the domain.
Extracting duplicate code into a function obviously.
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
Don't change extractor name.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
Using preferences causes invalid sorting.
Inline to actual call place.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
Title is invalid.
Ids must stay intact.
That's completely different videos.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
Carry long lines.
Better to use `determine_ext` instead of `.endswith`
`formats` is always a list of dictionaries.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
This is superfluous since you provide `formats`.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
Conversion between different date formats is redundant. Just return Unix timestamps.
Whats the point reconstructing the URL? You already have it in `url`.
It does not necessarily mean that.
DRY. url is mandatory.
Depends on what does it mean.
No such meta field.
No brackets needed.
Referring `url` from `url` looks like nonsense. Provide rationale.
This is no longer actual.
This is useless at the end.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
This is pointless, you don't have any fallback.
`True` is default.
Query should be passed as `query`.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
56-71 code duplication.
Default sorting is just fine. Remove `field_preference`.
I would always return a `multi_video` result.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
You should not call this yourself instead you should define `_GEO_COUNTRIES`.
Playlist extraction should only take place after no video formats found.
This change breaks MTVServicesEmbeddedIE
cookie => cookies. There are 3 items.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
So, looking at (1 of) the manifests, they're playlists and you could pass `preference=quality(version)` into `_extract_m3u8_formats()`. Probably also `entry_protocol='m3u8_native' as the manifests look harmless. Incidentally the API data appears to be in the webpage hydration JSON as well `window.__DUMPERT_STATE__ = JSON.parse(...)` but as you don't look at the page itself that doesn't matter.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
Can we resolve these IDs? There may also be a time encoded in there.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Extraction should not break if one of the formats is missing.
Use `_parse_json` instead.
No. Use fatal search regex instead.
No such meta field.
Not used with formats.
Capturing empty URL is senseless.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
Do not remove existing tests.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
Don't shadow outer names. No need for bracket when using single character.
Code duplication. Must be single call to search regex.
Also do not use double quotes for string literals.
No escapes for `/`.
Only whitespace is allowed between `videoInfo` and `=`.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Request wrapping code can be moved to the base class.
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
The third parameter of `_html_search_regex` is name but not ID.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Change double quotes to single quotes
OK. How about creating one base class, define the outside functions there, and make derivative classes for playlists that return `playlist_result()`. This way you'll reduce the boilerplate code
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
By providing username and password in params obviously.
`_` is idiomatic way to denote unused variables.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Part after `\?` should be removed since it's not used anymore.
This should actually be just `self.url_result(embedded_url)`.
`'%s'` is very unlikely to be a helpful error message.
`'id'` is required.
Indenting is messed up here.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This code looks similar to `sd` format and can be extracted to a function.
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
This will result in reference to unassigned variable when time fields are missing.
Query should be passed as `query` parameter.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Use `\s*` instead.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Never use bare except.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Don't capture unused groups./
Must be separate extractor.
Instead of such hacks you can name group differently and capture it without any issue.
All debug garbage must be removed.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
This is checked by `_search_regex`.
Use display id.
`default` is not used with `fatal`.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
No `id` extracted.
I've already pointed out: use `display_id` until you get real id.
Pass `default` to `_og_search_title` instead.
Code duplication. `url` must not be `None`.
Now you break extraction if any of these keys is missing.
This should be in `_real_initialize`. Same for all other occurrences.
You must provide account credentials/cookies for testing.
`acodec == 'none'`.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
Use `query` for query.
Move into `_download_json`.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
`ext` should be mp4.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
Parse from flashvars JSON.
Parse from flashvars JSON.
Temp file is not removed in this case.
What's the point of this? Remove.
Breaks extraction if not available. Again read coding conventions.
Sorry - dismiss that
The current working directory is not always writable.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
`enumerate` on for range.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Should contain `quality` key.
This must be an id of the media.
Pattern should include `<iframe` part.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Also pass `video_id` since it's known beforehand.
Extraction should be tolerate to missing fields.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Do not touch this.
Do not touch this.
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
Sorry for the late response. This check may give a false alert if someday afreeca.tv decides to use a different name than `./track/flag`. `./track/video/file` is more reliable. The overall extraction workflow should be: 1. Check `./track/video/file` 2. Raise an error if no entries 3. Check other fields
If there's only one format, just use `'url'`.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Code duplication in 70-102 and 104-137.
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
then I think it would be better to use `fatal=False` instead of `default=None`.
looking again at this, there are multiple problems with the `og:published_time`, the timezone is important to calculate the correct timestamp so the `og:published_time` value for embeds is malformed, and also there is a discrepancy between the values from the video page and the values from the embed page, so unless there is a way to determine the correct value, it might be better to drop it.
> so I guess I'll remove it at that's it. right? yes, it would be better to remove it.
don't use `modified_time` as publish timestamp.
don't use the image that has the play icon(`image_full_play`).
again, extract all formats, there is no guarentee that those formats are available in all videos.
as i said there is no guarantee that this will be the case all the time.
add more fallbacks and extract `timestamp`.
i think the use of `videotitle` is not reliable(it also has a problem with escaped double quotes), instead of this cleanup, other sources should be prefered.
- extract all formats. - try not complicating things, try to avoid using hardcoded values.
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
`only_once` parameter isn't in yt-dl, yet.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`{}` does not work in python 2.6.
Read coding conventions and fix all optional fields.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
Referring `url` from `url` looks like nonsense. Provide rationale.
Again: it's not part of the video's title and must not be in the title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
` - Servus TV` should not be in title.
Must only contain description.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Should not be fatal.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Note `video_data` may be `None`.
Any test case using this approach? I can't find it.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
This does not match anything.
I've already pointed out: `.*$` is pointless at the end.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Capture as `id` obviously.
Network connections in your browser.
No need to escape `]` is character set.
Read coding conventions on optional fields.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
No need to escape forward slash. Should be `https?`. Part after `(?P<id>\d+)` should be optional since `id` is only used for extraction.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
The third parameter of `_html_search_regex` is name but not ID.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
`'id'` is required.
All methods only used once should be explicitly inlined.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
The keys of the `dash_formats` dict are the `format_id` fields themselves, so you could directly `set(dash_formats)` or `set(dash_formats.keys())` to make it clearer.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
`info_dict['formats'] = formats`.
Move to the place of usage.
Must return info dict.
Read coding conventions on optional/mandatory meta fields.
Why did you remove this test? It does not work with your changes.
Don't touch the old test.
Must be separate extractor.
There is no need to test URLs.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
All methods only used once should be explicitly inlined.
Code duplication should be eliminated.
No exact URLs here.
Again: you should not bother with that. It will be automatically extracted from the download URL.
1. This will duplicate source format when multiple non source formats are available. 2. Source format must be named `source`.
Just replace the part appended to non source formats with empty string.
This is pointless. You already have `source_url` as marker.
`'id'` is required.
Extraction should not break if one of the formats is missing.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
All methods only used once should be explicitly inlined.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
``` for i, video_url in enumerate(video_urls): ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
Query to `query`,
Don't capture groups you don't use.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
should not fail if the extraction of one set or item is not possible.
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
No need to escape `{}`.
Direct URLs should also be extracted.
Id from URL is not always a video id. Correct id is in JSON.
Extract human readable title from the `webpage`.
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
should not return empty formats.
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
26-29, 32-37 code duplication.
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
`vcodec` to 'none'.
Do not capture groups you don't use.
Move `_match_id` into `_extract_audio`.
Such cases should be handled, too. I guess a possible approach is creating a table with common video and audio codecs. If given codecs are not on the table, fallback to `video,audio` order.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
And this as well.
`.*/?` is pointless at the end.
```suggestion get_element_by_class, int_or_none, ```
Don't capture groups if you are not going to use them.
Use `_request_webpage` instead.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
Don't capture unused groups
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Is it really required for a single video URL? All tests you provided end up with empty `data` array and construct `playlist` it from [the original `url`](https://github.com/hlintala/youtube-dl/blob/yle/youtube_dl/extractor/yle.py#L109).
This check is not necessary now as you test pathconf anyway.
Use `utils.get_filesystem_encoding` instead.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
I would prefer testing with `hasattr` instead to avoid [possible breakages](https://github.com/rg3/youtube-dl/commit/22603348aa0b3e02c520589dea092507a04ab06a) I've learned about the hard way.
`os.pathconf` and `os.pathconf_names`. You should also test whether `os.pathconf_names` contains `PC_NAME_MAX`. And probably catch an `OSError`.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
All similar tests should be `only_matching`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
These formats should not be removed.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Do not reformat code and remove irrelevant changes.
Noway. See other extractors on how to delegate properly.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
Do not touch existing tests.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Set empty `vcodec`.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Referring `url` from `url` looks like nonsense. Provide rationale.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
Better to have some URLs in _TESTS with ```'only_matching': True``` for changes to _VALID_URL
create a seperate extractor.
It should not match `h|`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Not all URLs contain video id.
`[]` is useless.
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
Formats in webpage are still available and should be extracted.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
- - [ ]
`width` and `height` instead.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
All formats should be extracted.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Use `\s*` instead.
Else branch is useless.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
All these fields should be `fatal=False`.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
None of the optional fields should break extraction if missing.
There is no need in this method.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
_ is not a special character in Python's regular expressions. There's no need to escape it.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
Groups around `video` and `sptv/spiegeltv` are superfluous.
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
It should be robust in case of some missing fields.
Use `self.url_result(inner_url, 'Generic')` instead.
This regex does not make any sense.
Network connections in your browser.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Query should be passed as `query` parameter.
`objectID` does not match the id from `AmericasTestKitchenIE`.
End users do not read source codes thus will never find this advice.
```suggestion if episodes: ```
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
Use `compat_urllib_parse_unquote_plus` instead.
Unnumbered placeholders are not supported in Python 2.6.
Avoid shadowing built-in names.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
It should match at least one character.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
`acodec == 'none'`.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Move it right after `title = info['title']`.
`utils.int_or_none` should be used instead.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
`'%s'` is very unlikely to be a helpful error message.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
No need to escape `#`. No need to capture groups you don't use.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
No `ExtractorError` is raised here, `except` will never trigger.
`player_id` is not extracted in this fallback but used at 71.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
There is no need in this method.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
All formats should be extracted.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
All these regexes should be relaxed.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
`_html_search_regex()` throws unless given a `default`(returns it silently) or `fatal=False` (returns `None` with a warning), so the OG search won't execute.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion new = '' ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
It should not. See the description of the field.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This code looks similar to `sd` format and can be extracted to a function.
Extracting duplicate code into a function obviously.
I would always return a `multi_video` result.
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
This is pointless.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
hls and rtmp are available as well.
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
What's the point of lines 104-108? `ext` is already flv.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
`_` is idiomatic way to denote unused variables.
Unite in single list comprehension.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
do not capture groups that you're not using.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
230-264 no copy pastes.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
`for key, value in media.get('images', {}).items():`
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
54-58, 71-75 code duplication.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
Never use bare except.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Lack of information is denoted by `None` not `0`.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
All formats should be extracted not only mp4.
could you add description from perex key
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Read coding conventions on optional/mandatory meta fields.
End users do not read source codes thus will never find this advice.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
By providing username and password in params obviously.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
All methods only used once should be explicitly inlined.
This may change as well. Add a fallback that just processes all videos without differentiation.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This code looks similar to `sd` format and can be extracted to a function.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
It should be robust in case of some missing fields.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Use `self.url_result(inner_url, 'Generic')` instead.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
**Do not remove** `_search_regex` part.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Remove debugging codes.
This should actually be just `self.url_result(embedded_url)`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This is pointless.
Formats in webpage are still available and should be extracted.
Remove useless code.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
There should be a fallback title if `_og_search_title()` returns `None`.
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
You should **capture** error message and **output** it.
`default` implies non `fatal`.
There are two unrelated flags `_logged_in` and `logged_in`.
Should be `display_id`.
Not all URLs contain video id.
No, it's not. If video_id extraction from page fails whole extraction fails.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Does not match https://narando.com/r/b2t4t789kxgy9g7ms4rwjvvw.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
No exact URLs here.
Don't capture unused groups
Do not capture empty strings.
Capture between tags.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
This has no effect. Postprocessors work on info dict copy.
Duration calculation is incorrect.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Must be extracted first.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Also pass `m3u8_id='hls'`.
`ext` here makes no sense.
Correct field name is `format_id`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Just copy paste the whole line I've posted.
Matched data-video should not be empty.
`self._search_regex` is enough here.
You should have only one single method for extracting info.
Optional data should not break extraction if missing. Read coding conventions.
`.*` on both ends of regex make no sense. Also when capturing at least one character is required - there is no sense capturing empty video id.
...and output it here instead of const string `video_id` here that makes no sense.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
This is fatal.
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
Use `self._html_search_meta` instead.
Same issue for urlh
Dots should be escaped
What's the point of this base class? It's only inherited once.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
`update()` one dict with another instead.
it whould be better to iterate once and extract the needed information.
Query should be passed as `query` parameter.
Capturing empty string is senseless. `\n` instead of `root[__env]`.
Use raw strings.
No such field.
no need to create a method when it will be used once.
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Breaks if no URLs extracted.
Escape dot. No need to split URL.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
`int_or_none` for all int fields.
``` for i, video_url in enumerate(video_urls): ```
I guess this was deleted by a mistake.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Dots not escaped.
both are know beforehand, so there is no need to use `urljoin`.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
falback to a static URL.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
should be in the `else` block of the `for` loop.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
This regex does not make any sense.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
```suggestion next_page, 'next page link', group='url', default=None)) ```
```suggestion 'noplaylist': True, ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Query to `query`.
should not return empty formats.
Do not capture groups you don't use.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
26-29, 32-37 code duplication.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
No exact URLs here.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
Move `_match_id` into `_extract_audio`.
Let's do it, then, if you like.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
As in, me personally? I think I knew that, but protocol.
This is already imported and (in general) you should only use `import`s at the top level.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
Single quotes. `item` is already a string.
`{}` doesn't work in python 2.6.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
Recursion should be replaced with plain loop.
It shouldn't fail if `user` or `username` is missing.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Usually display_id is used before the actual video_id is extracted.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
Better to use unified_strdate for parsing dates.
mobj would be None if nothing is matched
Need fatal=False or default=None
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
Pass `default` to `_og_search_title` instead.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Put `raise` after `if` section at the same indent.
You should not silence all another exceptions but re-raise.
This line is unnecessary.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
`/?` is senseless at the end.
I've already pointed out: this must be removed.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, override `suitable` instead. Also do not split strings.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
Check code with flake8.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Breaks. Read coding conventions.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Inline everything used only once.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
`enumerate` on for range.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Should contain `quality` key.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
There are two unrelated flags `_logged_in` and `logged_in`.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Does not match `var IDEC='`.
Also pass `video_id` since it's known beforehand.
Use bare `re.match`.
`--no-playlist` is not respected.
You can use self._download_json() here
Capture as `id` obviously.
`else` is superfluous.
The whole code until here can be simplified to `page_id = self._match_id(url)`
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
Read coding conventions on optional fields.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
Matching empty data is senseless.
Must not be `None`.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
This may change as well. Add a fallback that just processes all videos without differentiation.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
All methods only used once should be explicitly inlined.
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
I've already suggested using `Downloading` as idiomatic wording.
Never use bare except.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Default part should be removed since there is no default.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Post-processors are already identified by `key` in API same should be used here.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Single quotes. `item` is already a string.
Request wrapping code can be moved to the base class.
Recursion should be replaced with plain loop.
From what I've seen there is always only one video on the page thus no need in playlist.
Course extraction must be in a separate extractor.
Query should be passed as `query` parameter.
This will break extraction if no `id` present.
It should not match `h|`.
That's very brittle.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
It worth adding a doc string.
This doc string does not match the function now.
It should always return a list.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
No trailing `$`, override `suitable`.
Capture with /album. Capture non greedy.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
Dot is pointless here.
`{}` won't work in python 2.6.
Not a video id.
It's not an album id.
Relax `id` group.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Capture as `id` obviously.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
No need to escape `]` is character set.
It's not an album id.
Not a video id.
Dot is pointless here.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
What's the point of this? Use `url` as base.
fatal=True is default.
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
Extracting duplicate code into a function obviously.
Course extraction must be in a separate extractor.
1. Do not shadow `url`. 2. Regex should not match across several tags.
Use `{}` dicts instead.
`vcodec` to 'none'.
Put `raise` after `if` section at the same indent.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
This line is unnecessary.
You don't check whether login succeeded or not.
You should consult some git manual.
I guess it a typo? now -> not
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
All formats should be extracted.
`int_or_none` for all int fields.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Keep the old way also.
Code duplication at 58-60 and 68-70.
`parse_duration()` should work.
Better to use `unified_strdate()`.
`title` is mandatory. Move flags into regex itself.
This code looks similar to `sd` format and can be extracted to a function.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
`parse_duration()` should work.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Possibly referenced before assignment.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
This code looks similar to `sd` format and can be extracted to a function.
This wasn't used previously, is it really needed? If the answer is yes, you should use a `set` instead of a dictionary for keeping track of them.
This should be removed.
Use `self.playlist_result` instead.
That should work. Open a new PR with two commits: - `[myspace] Use play_path for faster download` - `[myspace] Add extractor for albums` (it could be done in this PR but you would need to use `git rebase` and `git push --force`)
They are actually different :`MIGcBg` vs. `MIGmBg`
Can we resolve these IDs? There may also be a time encoded in there.
It's not used inside FileDownloader.py so you probably don't need it.
Do we need it? It's not used anywhere else.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
If `video` not in `media` empty formats will be returned that does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
should not fail if the extraction of one set or item is not possible.
Read and follow code conventions. Check code with flake8.
I've already pointed out - title **must be fatal**.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Must be fatal.
Must be fatal.
Use whitespace characters consistently.
Breaks on `None`.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
No escapes for `/`.
Breaks on `default=False`. `title` must be fatal.
Breaks extraction if no `followBar`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Use `video_id`. Remove `.replace('\n', '')`.
Strings in JSON may contain `<`.
I've already pointed out: use `float_or_none`.
Move flags into regex. Regex should match `runParams={`.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
`{}` does not work in python 2.6
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Instead of escaping the inner double quotes you could single-quote the string.
All these regexes should be relaxed.
```suggestion new = '' ```
This should be just `return info`
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`skip_download` is needed for the test to pass similar to the first test.
Must be extracted first.
Should be non fatal.
Do not touch this.
Can we resolve these IDs? There may also be a time encoded in there.
Do not touch this.
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should not break if there is no `resolution` key.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Code duplication in 155-163 and 165-172.
This code looks similar to `sd` format and can be extracted to a function.
Must be extracted first.
Read coding conventions on optional/mandatory meta fields.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
In general we use single quotes unless there's a good reason. (single quotes in strings, etc.)
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
Code duplication in 70-102 and 104-137.
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
Do we need it? It's not used anywhere else.
It's not used inside FileDownloader.py so you probably don't need it.
`<span[^>]+class="name">...` is better.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
This is not true at the moment.
`title` is mandatory. Move flags into regex itself.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
If `_search_regex` fails `None` will be passed to `_parse_json`.
`urljoin` already does these checks.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
This is default.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Option should be called something like `--date-playlist-order` and should accept `asc`, `desc` or `none`. Code should process playlist according to it.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Post-processors are already identified by `key` in API same should be used here.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
Invalid syntax at all.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
`for k, v in flashvars.items()`.
Must be numeric.
This should be extracted first.
`flashvars[k]` is `v`.
Sholdn not break the extraction if missing.
Must be numeric.
```suggestion new = '' ```
Use `\s*` instead.
All these fields should be `fatal=False`.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
`self._html_search_meta` is better here.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
The argument will already be a character string, no need to decode it.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Simplify: ```suggestion series_id = self._match_id(url) ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Same, no such key possible.
Won't work for `info = {'title': None}`.
Real id is in widget dict.
Should not break if there is no `resolution` key.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
`width` and `height` instead.
Code duplication in 155-163 and 165-172.
Read coding conventions on optional/mandatory meta fields.
Code duplication in 142-145 and 146-149.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will be extracted from the URL.
I suggest `fatal=False`
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
`enumerate` on for range.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Should contain `quality` key.
Do not capture groups you don't use.
Groups around `video` and `sptv/spiegeltv` are superfluous.
only_matching, move to the end.
Don't capture groups you don't use.
Instead of such hacks you can name group differently and capture it without any issue.
Check code with flake8.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Make it match URLs from the first post obviously.
This no longer matches http://v.youku.com/player.php/sid/XNDgyMDQ2NTQw/v.swf and http://player.youku.com/v_show/id_XMTc1ODE5Njcy.html.
Don't capture groups you don't use.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
You have some unmerged lines here
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
``` for i, video_url in enumerate(video_urls): ```
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
This doc should be updated.
I think you don't need `locals()` here.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
This will break unicode strings under python 2.
We should really provide a better interface to test against, something along the lines of `download(url)`.
You can import `try_rm` from helper
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
1. `v` may not be dict. 2. `v.get('slug')`.
`video_data` is totally useless. Write directly to id variable when found.
Extraction should be tolerate to missing fields.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All formats should be extracted.
`int_or_none` for all int fields.
resolution is defined as "Textual description of width and height". "medium" does not fit.
`note` and `errnote` of `_download_json` instead.
This is fatal.
`'id'` is required.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Optional data should not break extraction if missing. Read coding conventions.
try_get is pointless here.
There is no need in this method.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
By providing username and password in params obviously.
This code looks similar to `sd` format and can be extracted to a function.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
Mandatory. Read coding conventions.
Id from URL is not always a video id. Correct id is in JSON.
No need to escape `{}`.
All these regexes should be relaxed.
This does not make any sense, you already have `url`.
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
These formats should not be removed.
This check does not make any sense. If there are no formats extraction should stop immediately.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
It should be robust in case of some missing fields.
Matching empty data is senseless.
Must not be `None`.
Move to initial title assignment.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
combine into a single call to `_html_search_meta`.
`id` by no means should be `None`.
Title is mandatory.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
Use regular string format syntax instead.
To be removed.
There is no need in this method.
151-165 code duplication.
Regex should be relaxed. Dots should be escaped.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Remove all unused code.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Audio must have proper `vcodec` set.
Use default. Read coding conventions and fix code.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
`note` and `errnote` of `_download_json` instead.
This is fatal.
What's the point of lines 104-108? `ext` is already flv.
Extraction should be tolerate to missing fields.
All methods only used once should be explicitly inlined.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
All formats should be extracted.
Breaks if no URLs extracted.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
JSON should be parsed as JSON.
Capturing empty URL is senseless.
No. Use fatal search regex instead.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Noway. See other extractors on how to delegate properly.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
Just modify mimetype2ext rather than introducing hacks in individual extractors
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
This is equivalent to InfoExtractor._match_id
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
There are lots of Content-Type calls. Please merge them together
Use compat_HTTPError instead
Never ignore generic exceptions
This should actually be just `self.url_result(embedded_url)`.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Breaks if not a list.
Outer parentheses are not idiomatic in python.
You don't need list here. Just return it directly.
Should be tolerate to missing keys.
Iteration over dict not the hardcoded list.
230-264 no copy pastes.
Breaks. Read coding conventions.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
If `video` not in `media` empty formats will be returned that does not make any sense.
No exact URLs here.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
Should not break if there is no `resolution` key.
No escape for `/`.
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
All formats should be extracted.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion new = '' ```
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
It's already extracted as video_id.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
As already said: parse as JSON not with regexes.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
default and fatal are not used together. `True` is default.
`if not content_url:`.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
No trailing $, override suitable.
Code duplication should be eliminated.
generator or PagedList instead of list.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
Extracting duplicate code into a function obviously.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
same if one of the values is `None`.
this does not handle the case where `contentUrl` value is `None`.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Using preferences causes invalid sorting.
That's completely different videos.
Ids must stay intact.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
Pass `default` to `_og_search_title` instead.
Read coding conventions on optional/mandatory meta fields.
Since this extractor by itself can't provide too much info, maybe it would be better to remove the _real_extract and _VALID_URL, don't import it in `__init__`and do something similar to`MTVServicesInfoExtractor`, which is the base class for the extractors that need it.
This can be placed in its own file.
Better to use integers for supported_resolutions and use str() here
Dots should be escaped
Use ```query``` parameter of ```_download_webpage``` instead.
Same issue for urlh
strip_jsonp should work here
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use _ (underline) instead of webpage if the value is not used.
Are all of these necessary? I think youtube-dl defaults suffice.
Whether it has changed or not does not mean there should be a format with invalid URL.
Must not be `None`.
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
This is superfluous since you provide `formats`.
`formats` is always a list of dictionaries.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Better to use `determine_ext` instead of `.endswith`
Code duplication with tbs extractor.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This should be just `return info`
```XimilayaIE.ie_key()``` is better
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
Same for re.search
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
No need to escape `/`.
`id` by no means should be `None`.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Title is mandatory.
There is no point in `or None` since `None` is already default.
You should be able to replace those 2 lines with ```python3 subtitles[s['language']].extend({'url': s['url'], 'ext': s['category']} for s in ysubs) ``` . Untested but it should become a generator which gets implicitly iterated, which should be faster than appending 1 by 1.
Doh, somehow I didn't realize the ref to `s` didn't exist if switching to that. My bad. Will need to use PyCharm 1st ...
Is it really required for a single video URL? All tests you provided end up with empty `data` array and construct `playlist` it from [the original `url`](https://github.com/hlintala/youtube-dl/blob/yle/youtube_dl/extractor/yle.py#L109).
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
All formats should be extracted not only mp4.
Unite in single list comprehension.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
This breaks streaming to stdout.
Add a rationale for that.
These are not used.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
This could be moved several lines up.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Consistently use single quotes.
Instead of adding new parameters put them into `ctx`.
Matching empty string is senseless.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Oh, ok. That makes sense. Shoulda read the statement above it.
Here `if episode_json is False` can/should be replaced with `if not episode_json`, which is also the way you're doing it later.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
No bare except.
There should be a fallback title if `_og_search_title()` returns `None`.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
Must not be `None`.
First group is superfluous.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
This matches multiple videos.
All debug code must be removed.
No. Revert as it was.
No. `_search_regex` is already fatal and reports proper error message. Revert.
This must be done right after title extraction.
This is never reachable.
Playlist title is optional.
This is default.
Carry long lines. Read coding conventions.
Should be tolerate to missing keys in `media`.
Should not be fatal.
Query to `query=`.
Must not be fatal.
Breaks if no `name`.
`{}` won't on python 2.6.
Read coding conventions on how mandatory data should be accessed.
Must not be fatal.
will be extracted from the URL.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Breaks on None.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Breaks on unexpected data.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
If ```options[0]``` _is_ ```{```, options should be returned.
1. ```options``` should be escaped - ```re.escape``` 2. Nested variables won't be detected. For example: (from https://developer.jwplayer.com/jw-player/docs/developer-guide/customization/configuration-reference/) ``` var config = { "playlist": [{ "file": "/assets/sintel.mp4", "image": "/assets/sintel.jpg", "title": "Sintel Trailer", "mediaid": "ddra573" },{ "file": "/assets/bigbuckbunny.mp4", "image": "/assets/bigbuckbunny.jpg", "title": "Big Buck Bunny Trailer", "mediaid": "ddrx3v2" }] }; ```
Sorry for previous noises. I just actually checked the HTML source of your example and found that this PR doesn't work as expected. ("resolving the variable to the JSON string") Try to print the value of ```mobj``` after this line.
Use `self._search_regex` and `utils.unified_strdate` instead.
This statement does not conform to PEP8.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Use `compat_urllib_parse_unquote_plus` instead.
Make `www\.` part optional.
Must only contain title.
Do not escape `/`.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
Plays fine without any authentication in browser.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
Empty string capture does not make any sense.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`_search_regex` is enough.
Just like I expected storage location has changed meanwhile so this code is completely broken. As I've already pointed out: match **only** `data-source` attribute.
This may change as well. Add a fallback that just processes all videos without differentiation.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Upper case is idiomatic for constants.
Then just keep this code and > create default fallbacks if extraction of these fails
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Raise `ExtractorError` instead.
EntryId must be extracted the very first.
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
`_` is idiomatic way to denote unused variables.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
No such meta field.
This is pointless.
Formats in webpage are still available and should be extracted.
Remove useless code.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Breaks when `get_element_by_class` returns `None`.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
```suggestion if not (season_id and video_id): ```
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
would still fail if `episodes` isn't available for a perticular `season`.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
strip_jsonp should work here
Use ```query``` parameter of ```_download_webpage``` instead.
Better to use integers for supported_resolutions and use str() here
Dots should be escaped
Same issue for urlh
Use ... instead of â¦ here. Non-ascii outputs may break random things.
All formats should be extracted.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
`'thumb`' may not be present producing invalid thumbnail url.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
Move into loop.
You're already building query with `query` param.
This is pointless. If no formats can be extracted extraction should stop immediately.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
It does not necessarily mean that.
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
Parentheses make this clearer although not required: ```suggestion uploader_url = ('https://parler.com/' + uploader_id) if uploader_id else None ```
"" -> ''
At this point `iteminfo` isn't known to be a dict. If it isn't, or it has no key `status_code`, this will crash, when it might be preferable to catch all these potential failures in the ExtractorError. ``` ... iteminfo = self._download_json('https://www.douyin.com/web/api/v2/aweme/iteminfo', video_id, query={'item_ids': video_id}) or {} status_code = iteminfo.get('status_code', 'status_code missing') if status_code: raise ExtractorError('%s (%s)' % (iteminfo.get('status_msg', 'status_msg missing'), status_code), video_id=video_id) ... ```
You could just write this like so, as other extractors seem to do: ``` info_dict = { 'id': video_id, ... ```
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
I guess `filename` can also be included.
This will fail if neither mplayer nor mpv is available.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
You must output to a temp file not the original file.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
There are lots of Content-Type calls. Please merge them together
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
Don't capture groups if you are not going to use them.
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
```suggestion 'language': compat_str(lang), ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
`note` and `errnote` of `_download_json` instead.
This is fatal.
Parse from flashvars JSON.
Use `self.url_result(inner_url, 'Generic')` instead.
Yes, it should accept any variation of whitespace.
Invalid arguments for 4-5.
Omit expected type.
`vardata` makes no sense.
Must not break extraction if missing.
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
Must not break extraction if missing.
Should contain `quality` key.
1. Relax regex. 2. Do not capture empty dict.
Must not break extraction if missing.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
i'm not sure why you're adding this step.
then you should use `_remove_duplicate_formats` method.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
again, extract all formats, there is no guarentee that those formats are available in all videos.
as i said there is no guarantee that this will be the case all the time.
capture only what is needed for the extraction.
- extract all formats. - try not complicating things, try to avoid using hardcoded values.
i think the use of `videotitle` is not reliable(it also has a problem with escaped double quotes), instead of this cleanup, other sources should be prefered.
add more fallbacks and extract `timestamp`.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
Use `self.url_result(inner_url, 'Generic')` instead.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
`xrange` is not defined in Python 3.x
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
No need to escape `\`.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
Just modify mimetype2ext rather than introducing hacks in individual extractors
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Unnumbered placeholders are not supported in Python 2.6.
Use `compat_urllib_parse_unquote_plus` instead.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Avoid shadowing built-in names.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
You technically can't login with this extractor apart from using cookies.
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
still the check for element class is missing.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Breaks when `player` is `False`.
This should be recursively delegated to pbs extractor instead.
Course extraction must be in a separate extractor.
It should not. See the description of the field.
That's very brittle.
No `ExtractorError` is raised here, `except` will never trigger.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`int_or_none` and `float_or_none` for all numeric fields.
`id` by no means should be `None`.
Use regular string format syntax instead.
No need to escape `/`.
No need to escape `#`. No need to capture groups you don't use.
To be removed.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
