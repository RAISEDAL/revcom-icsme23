The test page has a perfectly good `application/ld+json` script element with `@type: VideoObject` which should be the default source for metadata.
```suggestion description = try_get(additional_data, lambda x: x['caption']['text']) ``` otherwise this would error out when no caption object
Have you checked if this works with the multi-video post test? IIRC you'd need to extract `carousel_media` for it also ```suggestion best_quality = items0['video_versions'][0] ``` or make the array indexing optional
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Consts should be in uppercase.
Merge in single list comprehension.
I mean you must pass both regexes.
Should not match `varflashPlayerOptions...`.
This statement does not conform to PEP8.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This regex does not make any sense.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
No need to escape `{}`.
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
That's incorrect. `\1` must be a number of capture group.
This should include `iframe` part.
additional info can be extracted from `video` request.
no, there is `season_number`, `episode_number`, `timestamp`, etc...
By providing username and password in params obviously.
Read coding conventions on optional/mandatory meta fields.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
Use more relaxed version, e.g. `[^/]+` to match segments and `[^/?#&]+` to match id.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
There is no point checking `url`.
No point checking this.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
I've already suggested using single quotes.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
You can import `try_rm` from helper
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
try to parse multiple formats, set `vcodec` to `none`.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
should not break the extraction if the field is not available.
use `query` argument.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
No need in this message.
Must be numeric not string.
Meaning that you must provide account credentials for testing or whatever else is needed.
Using preferences causes invalid sorting.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
That's completely different videos.
@zachee @dstftw I had a go at removing the duplication. Is this any good? **Function** ``` def _get_first_valid_downloaded_webpage(urls, video_id, headers): for url in urls: webpage = self._download_webpage(url, video_id, headers=headers) if not ('File not found' in webpage or 'deleted by the owner' in webpage): return webpage raise ExtractorError('File not found', expected=True, video_id=video_id) ``` **Usage** ``` def _real_extract(self, url): video_id = self._match_id(url) url = 'https://openload.co/embed/%s/' % video_id url2 = 'https://openload.co/f/%s/' % video_id headers = { 'User-Agent': self._USER_AGENT, } webpage = self._get_first_valid_downloaded_webpage([url, url2], video_id, headers) ```
@cacdpa: That's better. Just don't use the name ```url``` - it's already used as a parameter of ```_real_extract```.
`note` and `errnote` of `_download_json` instead.
Breaks on None.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
`'%s'` is very unlikely to be a helpful error message.
Must not be fatal. Read coding conventions.
`.*` at the end does not make any sense.
There should be spaces between `%`.
Carry long lines.
`.get()` idiom is used for optional fields only.
Better to use `determine_ext` instead of `.endswith`
`formats` is always a list of dictionaries.
```XimilayaIE.ie_key()``` is better
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
No need to escape `]` is character set.
`'id'` is required.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Let's try to force the CI tests, and also ensure no final `/`: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)).rstrip('/') ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
yt-dlp's `--print` is a bit more complex, allowing printing data at multiple stages. Eg: `-O "after_move:%(filepath)s" will print the final file path. This function is needed to (easily) support that syntax, (and also other similar options)
You only need a simplified version of this. Actually, optparse's built-in action `append` should be sufficient here
Post-processors are already identified by `key` in API same should be used here.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
More relaxed regex.
This is not true either. Login may be achieved via authorized cookies.
This will also apply when `online` key does not exist.
Network connections in your browser.
Just remove the `try:` and `except:` lines.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
`self._html_search_meta` is better here.
> Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields. - the playlist title and description are not mandatory. - extraction should not fail if any of the fields `blocks`, `sets`, and `id` are not available.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
This regex does not make any sense.
Do not capture groups you don't use.
No captures for groups you don't use.
Network connections in your browser.
hls and rtmp are available as well.
No need for that check it's already checked in `parse_iso8601`.
`_html_search_regex` already calls `unescapeHTML`.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Use single quotes consistently.
Read: coding conventions, optional fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
EntryId must be extracted the very first.
I believe their generic name is `YouPorn`, so you can just delete this line.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
Please use `print()` syntax, as we support also Python 3(.3+)
The argument will already be a character string, no need to decode it.
We may use proper XML parsing here and simply call `self._download_xml`
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Why have you changed the quotes here? It's not that important, but we strive to use `'` when possible, and have when possible a consistent quote character in a file.
`<span[^>]+class="name">...` is better.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need to escape `]` is character set.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
This should not be fatal.
This should be extracted in the first place.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
`scale` passed to `float_or_none` instead.
What's the point changing old time-proven regexes with another ones? That's not an improvement.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Breaks if not arr.
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Remove all debug output.
Doesn't work in python 2.6.
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
Sorry - dismiss that
Temp file is not removed in this case.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Re-read my post with eyes please.
Breaks on `None` title.
Use the original scheme and host.
Don't capture groups you don't use.
Query to `query`,
What's the point of this? Use `url` as base.
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
No escape for `/`.
Do not capture empty strings.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
All these regexes should be relaxed.
Must be int.
It's artist not an album artist.
Optional fields should not break extraction if missing.
Breaks. Read coding conventions.
Do not shadow existing variables.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Broken python 3. Must be bytes. `urlencode_postdata`.
Consts should be in uppercase.
Merge in single list comprehension.
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
This must be checked **before** any processing.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
it better to extract all the urls in the `mpath` array.
There is no point in that.
Must be extracted first.
This wasn't used previously, is it really needed? If the answer is yes, you should use a `set` instead of a dictionary for keeping track of them.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
No, parse as JSON. Or from ld+json.
Must be numeric.
Nothing changed. Breaks in case regex does not match.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
We use single quotes as much as possible.
No, use a value that matches the user agent used by youtube-dl(`chrome`).
use the already extracted value(`video_url`).
should not break the extraction here if a request fails or the `video` field is not accessible.
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
you already have `pgm_id` and `pgm_no` variables now.
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
Do not touch existing `AtomicParsley` code. `mutagen` path must be a fallback if `AtomicParsley` is missing.
You must output to a temp file not the original file.
Won't work. See how this is done for output template.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
Alternatively you can just restore it after this PR is merged.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
Title is mandatory.
No such meta field.
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
This should not process the whole page. Regex should be relaxed.
This should not process the whole page.
This is too relaxed and should only be matched in categories part of the webpage.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Everything after `id` may be skipped and not matched since http://www.rte.ie/radio/utils/radioplayer/rteradioweb.html#!rii=16:10507902 works just fine.
No need to be escaped either.
Use fatal=False or default=None in _html_search_regex to handle fallbacks instead of try-except patterns.
Just use ```'ÐÐ¾ÑÐ»Ðµ ÑÐ¸ÑÑÐ¾Ð². Â«ÐÑÐ¸Ð¾Ð»Ð¸Ð½Â»'```
url and formats are not used together.
No trailing $, override suitable.
generator or PagedList instead of list.
`SoundcloudIE.ie_key() if SoundcloudIE.suitable(permalink_url) else None` at the place of passing `ie`.
This must be passed in `url_result`.
This won't skip empty strings.
Use `compat_urllib_parse_unquote` instead.
`{}` won't on python 2.6.
Extractor must not return None.
Mandatory. Read coding conventions.
Remove and make dvr path mandatory.
Do not change this.
You already have it in get call.
Do not escape `/`.
It also accepts `/video/bbq-salon/WHATEVER_I_WANT_AND_MAKES_SEO_SENSE-63940306` but that doesn't make it `id`, does it? Also when you examine their graphql query there is a attribute called `dotId` which has exactly that value eg. it is `ID` your `video_id` is in their data on attribute `urlName`. Do as you think, but this will be an issue when merging into master.
There's no need to name a group if not used.
No need to escape `/`.
```suggestion 'ext': ext, ``` fix flake8 check
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
End users do not read source codes thus will never find this advice.
will return invalid URL if `search_url` is `null`.
Recursion should be replaced with plain loop.
Single quotes. `item` is already a string.
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Note `video_data` may be `None`.
No such meta field.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`int_or_none` and `float_or_none` for all numeric fields.
`id` by no means should be `None`.
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
Must be fatal.
I've already pointed out - title **must be fatal**.
Use regular string format syntax instead.
`int_or_none` and `float_or_none` for all numeric fields.
No need to escape `#`. No need to capture groups you don't use.
There is no point in `or None` since `None` is already default.
Title is mandatory.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
Shouldn't be fatal
```suggestion new = '' ```
It does not check the other path since you neither download it here nor test the `url` not to contain `mp4:`. Thus you can't be sure whether this test involved the other path or not.
Idiomatic name is `video_id`.
Second argument should be an id extracted from `_VALID_URL`.
This should match only integers.
rtmp's container is **always** flv.
The same. Should use https if ```url``` use https.
Same issue for re.search
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Final bit, self._search_regex is better than re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
The whole code until here can be simplified to `page_id = self._match_id(url)`
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
Read coding conventions on optional fields.
The argument will already be a character string, no need to decode it.
Ok, It's up to you.
this does not handle the case where `contentUrl` value is `None`.
To be removed.
No point in base class.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
No need to use `sanitized_Request` request here, pass url directly to download method.
Use `self._parse_html5_media_entries` instead.
All these regexes should be relaxed.
This is determined automatically.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
API URLs should be used.
I've already pointed out: API URLs should be used.
There is no need to test URLs.
Formats should have `quality`.
Change 1 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI. Change 2 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/media/_hqLjQ95yx8Z.
As well as this.
It's a matching only test. Testing two identical URLs twice does not make any sense.
https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI is still available.
By convention, a title is required. Here, if the title isn't found, `.strip()` will crash. You could make `fatal=True`, or supply a default, say `default='Untitled video %s' % video_id`.
Still does not handle aforementioned URLs.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
Don't capture groups you don't use.
There must be two different extractors: for videos and for playlists.
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Same, no such key possible.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
This may change as well. Add a fallback that just processes all videos without differentiation.
Must be `int`.
This should actually be just `self.url_result(embedded_url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No. Use fatal search regex instead.
No such meta field.
Not used with formats.
Dots not escaped.
Must not be fatal for playlist.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
This does not make any sense, you already have `url`.
Relax `id` group.
Capture between tags.
Do not capture empty strings.
No exact URLs here.
Remove all garbage.
Plain `for x in l`.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
No, provide subtitles as URL.
`/?` is senseless at the end.
same for `episode_number`(both lines where highlighted).
`created_at_i` as `timestamp`.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`user_info` may be `None`.
Again: float_or_none, not parse_duration.
Must be separate extractor.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be a separate extractor delegating to SBS.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
Preferably for such a long field use `'md5:...'`
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
I would prefer indent similar to the former code.
This regex should be split into multiple lines for bettercode perception.
1 is ok.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
It should always return a list.
All formats should be extracted.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Should not be fatal.
Formats not sorted.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
This video is georestricted.
Use `self._match_id` is better.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Just copy paste the whole line I've posted.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
This is never reached cause sort formats will throw on `not formats`.
Use `self._search_regex` and `utils.unified_strdate` instead.
All these regexes should be relaxed.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
Formats not sorted.
Should not be fatal.
These dependencies are unacceptable. Moreover you don't use them.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
The playlists are public: there should be tests for them. Watch this space.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
What's the point of `# match self._live_title` here? Remove.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
Parse from flashvars JSON.
Parse from flashvars JSON.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
It shouldn't fail if `user` or `username` is missing.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
This is already imported and (in general) you should only use `import`s at the top level.
Single quotes. `item` is already a string.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Remove all debug output.
`video_id` may be `None`.
Regex should be relaxed. Dots should be escaped.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
No such field.
This should be split into building url and extracting formats.
Query should be passed as `query` parameter.
To be removed.
All debug code should be removed.
Uppercase is not honored.
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
still the check for element class is missing.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
The third parameter of `_html_search_regex` is name but not ID.
`'id'` is required.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No such meta field.
Not used with formats.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
make one of the tests an `only_matching` test.
there is not need for excess verbosity.
no longer needed.
incorrect URLs for Cook's Country.
will return invalid URL if `search_url` is `null`.
Remove unnecessary verbosity.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Network connections in your browser.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Extraction should not break if one of the formats is missing.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Use `self._og_search_*` functions here.
Use a for loop to eliminate duplicated codes.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Usually we use `md5:(md5 of the long text)` for such cases.
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Read coding conventions.
You should not throw everything under try/except. Also read coding conventions.
surround only the part that will threw the exception.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
the same for `streaming` key.
This will return non existent path if conversion fails.
`{}` won't work in python 2.6.
Why do you disallow `story` to be an id? Because you do not want it to be an id in your particular case that is clearly an ad hoc hack cause other extractors may use `_generic_id` and may allow `story` to be an id.
There should not be any ad hoc hacks in generic methods.
This has no effect. Postprocessors work on info dict copy.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
There is no need in this method.
`enumerate` on for range.
This is pointless.
All of these will break extraction on unexpected data.
Must be `int`.
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
Breaks downloading of videos that does not require authentication.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
This branch is never reached.
`default` is not used with `fatal`.
This does not necessarily mean that. There is a clear captured error message that should be output.
This is checked by `_search_regex`.
Use display id.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
Title is mandatory field thus it should be `data['roominfo']['name']`.
Don't capture groups you don't use.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
Empty string capture does not make any sense.
Instead of this extraction should be delegated to `WSJIE` via `url_result`.
Also pass `video_id` since it's known beforehand.
Should not be fatal.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
`[]` is useless.
Move method near the place of usage.
1. Breaks if div is not found. 2. `re.finall`.
No need to escape whitespace.
Else branch is useless.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
Use `(?i)` in regex itself if you want case insensitivity.
All debug code should be removed.
Should be delegated via `url_result`.
This duplicates code from `MedialaanIE`.
For now extract it in a new base IE class.
Consistently use flags inside regex.
Depending on where in the page the target may be, consider `self._html_search_regex()` which unescapes the returned match (eg, if it contains `&amp;` that should be `&`, or just `&#0049;` that should be `1`).
`self._search_regex` is enough here.
Matched data-video should not be empty.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
No exact URLs.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
This code looks similar to `sd` format and can be extracted to a function.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
use single quotes consistently.
check that extracted description is what is expected(should not contain html tags).
`strip_or_none` no longer needed.
`twitter:description` and `description` meta tags are also available.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
same if one of the values is `None`.
this does not handle the case where `contentUrl` value is `None`.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
Re-read my post with eyes please.
This breaks all non ks embeds. ks part must be optional.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This should include `iframe` part.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
This should be rewritten in terms of [`YoutubeIE._extract_urls`](https://github.com/rg3/youtube-dl/commit/66c9fa36c10860b380806b9de48f38d628289e03).
Do not shadow built-in names.
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
You must use `default` if there is a fallback after it.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Use `utils.xpath_text` instead, again with `fatal=False`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `compat.compat_str` instead.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
This is never reachable.
This is default.
Must not be fatal.
Simplify: ```suggestion series_id = self._match_id(url) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Bitrate should go to corresponding format meta field.
`self._parse_html5_media_entries` for formats extraction.
This should be a ```list```.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Breaks if no `rate` key in `stream`.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Query should be passed as `query` parameter.
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
Uppercase is not honored.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
Should not be greedy.
`{}` doesn't work in python 2.6.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
If you accept this to be missing then it must be `default=` not `fatal`.
This is never reached cause sort formats will throw on `not formats`.
All of these will break extraction on unexpected data.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
should not fail if the extraction of one set or item is not possible.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
Read and follow code conventions. Check code with flake8.
What's the point of this? `canonical_url` is the same as `url`.
Breaks if no `name`.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
No. What's the point of video id here? `settings` part provides enough uniqueness.
Should not be greedy.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Do not use leading underscore for locals.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Constant names should be in uppercase.
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
You could make the regex more robust: ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*(.*?)\s*</script' ``` * `\s*` for all the places where JS/HTML allow whitespace * `.*?` to avoid capturing "target_json</script>...<script...>...</script>", etc * `</` don't need to be escaped. Also, as you're presumably hoping to get some brace expression, maybe this? ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*({.*?});?\s*</script' ```
There's still a loop on `format_id` which doesn't seem right: ``` for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): if not isinstance(server_json, dict): continue for format_id in ('hls', 'dash'): if format_id not in server_json.keys(): continue if format_id == 'hls': formats.extend(self._extract_m3u8_formats( server_json[format_id], lecture_id, 'mp4', entry_protocol='m3u8_native', m3u8_id=format_id, note='Downloading %s m3u8 information' % server_json.get('id', '?') , elif format_id == 'dash': formats.extend(self._extract_mpd_formats( server_json[format_id], lecture_id, mpd_id=format_id, note='Downloading %s MPD manifest' % server_json.get('id', '?'), fatal=False)) self._sort_formats(formats) ... ```
Uppercase is used for const.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
`.*` at the end does not make any sense.
This is already fatal.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
Either **do** or remove.
Carry long lines. Bother to finally read coding conventions.
This will process the same URL twice overwriting the previous results.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
should be in the `else` block of the `for` loop.
the same for `streaming` key.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
`id` must not contain any irrelevant parts.
Formats not sorted.
Should be tolerate to missing keys in `media`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`if mediatype == u'video':` is idiomatic Python.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
`<h3>` is intentional.
It should not match `h|`.
87-90 code duplication.
Query to `query`,
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_search_regex` is enough here.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
cookie => cookies. There are 3 items.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
default and fatal are not used together. `True` is default.
`if not content_url:`.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
`fatal` must be added to `_extract_info`. No changes to core code.
Add `fatal` flag.
Do not change the order of extraction.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
Use `compat_urlparse.urljoin` instead.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
`compat_str()`, here and in l.96.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Parenthesis are superfluous.
None of the optional metadata should break the extraction when missing.
All these regexes should be relaxed.
Must be int.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This test is pointless. You must test iframe extraction not regex match.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
This matches multiple videos.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
EntryId must be extracted the very first.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
Side note: Wow, these guys are military, but don't support https? Oh my...
To get regex highlighting, this string should be prefixed with `r`. That's just convention, but plain neat.
This is a tuple, that can't be right. Also, a test is only really useful with some things to test against, like title and md5sum of the image. You can run it with `python tests/test_download TestDownload.test_DefenseGouvFr`.
`lxml` is not part of the standard library, you'll need to use regular expresions.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
This will break extraction if no `id` present.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
Instead of this extractor you should add an extractor for `https://20.detik.com/embed/...` URLs and add detection of such embeds in generic extractor.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Query to `query`.
Just copy paste the whole line I've posted.
It's better to keep the original ID for existing patterns, or --download-archive will be broken.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
No trailing $, override suitable.
This should be split into building url and extracting formats.
Update `video_info` and return it directly.
It won't be longer.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
`{}` won't work in python 2.6.
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
The `list` call is superfluous here and can be safely removed.
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
This should actually be just `self.url_result(embedded_url)`.
`else` is superfluous.
No need to escape `]` is character set.
`--no-playlist` is not respected.
Use bare `re.match`.
Both test the same extraction scenario. The rule is one test per extraction scenario.
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
First group is superfluous.
No. You should not shadow the original explicitly provided password.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
Don't use bare `except:`
There are lots of Content-Type calls. Please merge them together
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Use compat_HTTPError instead
Never ignore generic exceptions
This is equivalent to InfoExtractor._match_id
default and fatal are not used together. `True` is default.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`if not content_url:`.
`'id'` is required.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
It should be robust in case of some missing fields.
For tv/se `og:title` contains unnecessary suffix.
Just cut it with `utils.remove_end`.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
still fails if `uploader_data` not available.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This should be just `return info`
it whould be better to iterate once and extract the needed information.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
`title` is mandatory. Move flags into regex itself.
at this point `sub_lang` is guaranteed to be in the subtitles dict.
keep the old fallback code.
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
It's a field name not a step name.
Again: relax regex.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
I've already suggested using `Downloading` as idiomatic wording.
I think it's safe to use `avi` here since we use extensions for this option. Moreover, it will simplify [this code](https://github.com/aurium/youtube-dl/blob/master/youtube_dl/postprocessor/ffmpeg.py#L297-L301) a bit.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
Don't use bare `except:`
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
`<display_id> is not a video`.
First group is superfluous.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
No such meta field.
Not used with formats.
url and formats are not used together.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
`unescapeHTML(n[1])`? (`utils.py`) See also line 59 below.
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
All tests that test similar extraction scenario should be `only_matching`.
Will never happen.
Never use bare except.
Read coding conventions on optional and mandatory data extraction.
Please remove debugging lines.
You don't need to call `report_extraction` here and above, since the extraction is over already.
Oh, alright, leave it in. I was suprised to find a `\x` instead of a `\u`, but specifically for the non-breaking space, that should be fine.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
Need fatal=False or default=None
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
This code duplication may be eliminated.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
the same for `streaming` key.
surround only the part that will threw the exception.
Add `fatal` flag.
Do not change the order of extraction.
`fatal` must be added to `_extract_info`. No changes to core code.
Delegate to generic always when original path found nothing.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
Use `video_id`. Remove `.replace('\n', '')`.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
It's already extracted as video_id.
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
Must not be fatal.
Breaks on None.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Read coding conventions on how mandatory data should be accessed.
`[]` is superfluous in group with single character.
No need to escape `]` is character set.
`--no-playlist` is not respected.
Use bare `re.match`.
`else` is superfluous.
Both test the same extraction scenario. The rule is one test per extraction scenario.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
You have some unmerged lines here
There should be an `id` group.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Use single quotes consistently.
m3u8 is also available.
Carry to the indented beginning of the line.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
Possibly referenced before assignment.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Possibly referenced before assignment.
`compat_str()`, here and in l.96.
This code looks similar to `sd` format and can be extracted to a function.
This regex does not look like generic embed. Provide several examples that use this embedding.
url and formats are not used together.
No trailing $, override suitable.
Use `\s*` instead.
All these regexes should be relaxed.
Don't capture groups you don't use. Use proper regex to match all country codes.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
No need to escape `/`.
Carry long lines.
No direct URLs in tests.
Extract id once before the loop.
If nothing matches `None` will be returned.
Playlist title is optional.
This should not be here as done by downloader.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
That's very brittle.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
and the function that normally used to encode postdata is `urlencode_postdata`.
This regex does not make any sense.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
JSON layout has changed. Error is not captured properly anymore.
Error handling broken.
This does not necessarily mean that. There is a clear captured error message that should be output.
This branch is never reached.
`/?` is senseless at the end.
There's no need to use `u` prefix given `unicode_literals` is declared.
This will process the same URL twice overwriting the previous results.
This line is unnecessary, webpage is never used.
Unnumbered placeholders are not supported in Python 2.6.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
breaks the extraction if `clips` is `None`.
will break the extraction if `profiles` is empty or `None`.
same if one of the values is `None`.
`<script>.*` this part does not play a roll in the regex, may be change to `<script[^>]*>\s*(var\s*)?`. Split Long lines(longer than 80 columns). ```suggestion hydration_data = self._search_regex( r'<script>.*hydrationData\s*=\s*({.+?})\s*</script>', webpage, 'hydration data', default='{}') ```
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
Correct field name is `format_id`.
`ext` here makes no sense.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Matched data-video should not be empty.
`/?` is senseless at the end.
Use display id.
`default` is not used with `fatal`.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
Re-read my post with eyes please.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
A minor bit: don't use a capturing group if it's not used.
This line can just be removed.
Upps, that's wrong, the results are indeed tuples.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
Extracting duplicate code into a function obviously.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
EntryId must be extracted the very first.
This is in the test suite, and in the test suite, warnings are an error of us, aren't they? So why isn't the implementation ``` raise Exception(message) ```
We should really provide a better interface to test against, something along the lines of `download(url)`.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
You can import `try_rm` from helper
Instead of adding new parameters put them into `ctx`.
Remove useless code.
Formats in webpage are still available and should be extracted.
This is pointless.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks if not arr.
This doc string does not match the function now.
It should always return a list.
It worth adding a doc string.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
Must not be fatal. Read coding conventions on optional/mandatory fields.
All debug garbage must be removed.
Breaks on missing key, breaks on download failure.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
No. _extract_urls of youtube extractor.
This line can just be removed.
Don't capture groups you don't use.
This breaks the extraction of prochan embeds. `[^"]+` is only applied for youtube group.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
Use `(?i)` in regex itself if you want case insensitivity.
Make more relaxed and add title regex as fallback.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
According to pep8, there's a missing space after `+` in here.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
From what I've seen there is always only one video on the page thus no need in playlist.
This is too broad and detects the same video twice.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
The description is always optional, so there should be a `fatal=False` in here.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
87-90 code duplication.
Query to `query`,
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks if not arr.
If they switch to strings this will break formats' sorting. Same should be done for width & height.
Change to `config_files.get('hls', {}).get('all')`.
Make it non fatal, provide correct `ext`, consider forcing to native hls by default, and set same `preference` as for direct links since for some videos hd is only available via hls (e.g. https://player.vimeo.com/video/98044508).
This can now be omitted.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
230-264 no copy pastes.
No, use a value that matches the user agent used by youtube-dl(`chrome`).
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
`int_or_none` for all int fields.
```python for path, format_id in (('', 'audio'), ('video', 'sd'), ('videohd', 'hd')): self._download_xml( 'https://www.heise.de/ct/uplink/ctuplink%s.rss' % path, video_id, 'Downloading %s XML' % format_id, fatal=False) ```
Must not be fatal.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
No escape for `/`.
Do not capture empty strings.
Avoid unrelated changes.
Make more relaxed and add title regex as fallback.
All these regexes should be relaxed.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Usually display_id is used before the actual video_id is extracted.
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
The description is always optional, so there should be a `fatal=False` in here.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
Use `self._search_regex` and `utils.unified_strdate` instead.
Must only contain description.
Carry long lines. Read coding conventions.
Must only contain title.
Should be tolerate to missing keys in `media`.
Query to `query=`.
Use `self._parse_html5_media_entries` instead.
This should be a ```list```.
I would always return a `multi_video` result.
If `_search_regex` fails `None` will be passed to `_parse_json`.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
Nothing to do with RFC 3986.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Extracting duplicate code into a function obviously.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
None of the optional fields should break extraction if missing.
`try_get`, single quotes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
cookie => cookies. There are 3 items.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
You have some unmerged lines here
``` for i, video_url in enumerate(video_urls): ```
`acodec == 'none'`.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This is not matched by `_VALID_URL`.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Upper case is idiomatic for constants.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
It should be robust in case of some missing fields.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
End users do not read source codes thus will never find this advice.
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
`ext` here makes no sense.
Correct field name is `format_id`.
Also pass `m3u8_id='hls'`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Matched data-video should not be empty.
Do not capture groups you don't use.
No exact URLs here.
Query to `query`.
`vcodec` to 'none'.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
Before these changes there were 4 references and extraction followed them strictly in order of reference declaration.
At least move iframe extraction after video tag extraction.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
`ref:` should not be removed from video id.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
Style: ```suggestion return '/'.join(urlparts) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
Read coding conventions on optional and mandatory data extraction.
No, provide subtitles as URL.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
```suggestion new = '' ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
I suggest `default=video_id` in the `_html_search_regex` call.
will be extracted from the URL.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `fatal=False`
Again: float_or_none, not parse_duration.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This code looks similar to `sd` format and can be extracted to a function.
`int_or_none` for all int fields.
There should be fallbacks for these values since they are more or less static.
This should not be fatal.
It's already done at L151.
Use `query` for query.
`acodec == 'none'`.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
Read coding conventions on how mandatory data should be accessed.
`[]` is superfluous in group with single character.
Must not be fatal.
Breaks on unexpected data.
Breaks on unexpected data.
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
`only_once` parameter isn't in yt-dl, yet.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
All integral numeric metafields should be wrapped in `int_or_none`.
Title is mandatory.
This may change as well. Add a fallback that just processes all videos without differentiation.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
This may change as well. Add a fallback that just processes all videos without differentiation.
Breaks if no `name`.
Dots should be escaped
This should actually be just `self.url_result(embedded_url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
`if mediatype == u'video':` is idiomatic Python.
Separate variable is superfluous.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
`enumerate` on for range.
Use `_sort_formats` instead.
`[]` is superfluous in group with single character.
Too broad regex.
87-90 code duplication.
No trailing markers - override `suitable`.
Must not return `None`.
`default=None` for the first.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
`self._search_regex` is enough here.
Also pass `m3u8_id='hls'`.
You still duplicate the URL and unnecessary `if/elif` branches.
You should make extraction tolerate to these fields missing not remove them.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
Title is mandatory field thus it should be `data['roominfo']['name']`.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
I guess this was deleted by a mistake.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
use the extension extracted from `determine_ext`.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
instead of this it's better to just put result of `determine_ext` in a variable and use it.
at this point `sub_lang` is guaranteed to be in the subtitles dict.
What's the point of this? `canonical_url` is the same as `url`.
Should be `flv`.
No need to specify this.
Read coding conventions and fix all optional meta fields.
Title is mandatory.
No such meta field.
will easily match outside the element.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
I suggest `default=video_id` in the `_html_search_regex` call.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
Don't use bare `except:`
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. You should not shadow the original explicitly provided password.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
Network connections in your browser.
`{}` does not work in python 2.6.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
The `return` is not needed anymore.
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
Move flags into regex.
Relax regex, make group unnamed, don't capture empty dict.
It's a field name not a step name.
This check is not necessary now as you test pathconf anyway.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
I would prefer testing with `hasattr` instead to avoid [possible breakages](https://github.com/rg3/youtube-dl/commit/22603348aa0b3e02c520589dea092507a04ab06a) I've learned about the hard way.
`os.pathconf` and `os.pathconf_names`. You should also test whether `os.pathconf_names` contains `PC_NAME_MAX`. And probably catch an `OSError`.
Use `utils.get_filesystem_encoding` instead.
Use bare `re.match`.
`else` is superfluous.
`--no-playlist` is not respected.
No need to escape `]` is character set.
Both test the same extraction scenario. The rule is one test per extraction scenario.
Re-read my post with eyes please.
A minor bit: don't use a capturing group if it's not used.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
Instead of such hacks you can name group differently and capture it without any issue.
This line can just be removed.
Should not break if missing.
Python 3.2 doesn't like u-literals.
Optional fields should not break extraction if missing.
Do not shadow existing variables.
Read: coding conventions, optional fields.
Removing useless noise.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
Won't work for `info = {'title': None}`.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
Breaks downloading of videos that does not require authentication.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
Note `video_data` may be `None`.
No such meta field.
Breaks. Read coding conventions.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Code duplication 173, 213. There is no sense to extract fields explicitly.
```suggestion if not (season_id and video_id): ```
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
For rtmp it's always flv despite of the extension.
`{}` doen't work in python 2.6.
Name an example URL where `og:description` has HTML tags.
Use `self._search_regex` and `utils.unified_strdate` instead.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Instead of escaping the inner double quotes you could single-quote the string.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
`{}` does not work in python 2.6
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
This doc should be updated.
Just modify mimetype2ext rather than introducing hacks in individual extractors
`.*/?` is pointless at the end.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
works also for me :+1:
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
This worked for me.
Video id length is 8.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
No need in another test.
It's obvious from `'is_live': True`.
It's a field name not a step name.
Carry to the indented beginning of the line.
Again: relax regex.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
Because all of them use the same extraction scenario.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
According to [PEP8](http://www.python.org/dev/peps/pep-0008/), there should be a space after the comma. But again, better just move the contents of this function to the bottom.
What if the title does not start exactly 30 characters after this? Also, what if this string is not found.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Dot is pointless here.
It's not an album id.
Not a video id.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
Simplify: ```suggestion series_id = self._match_id(url) ```
This does not mean it should not be included.
This is done automatically.
Breaks. Read coding conventions.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Default note is enough.
Capturing empty string is senseless. If you expect a dict then capture group must contain `{}`.
This should be extracted from `_VALID_URL` with `self._match_id`.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
All video formats must be extracted.
Default part should be removed since there is no default.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
try_get is pointless here. Read coding conventions.
`get` is pointless since availability of result key is mandatory.
Simple string concatenation is enough here.
try_get is pointless here.
Here you must use `urljoin`.
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
What's the point of this? Use `url` as base.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Use `self.url_result(inner_url, 'Generic')` instead.
Not a video id.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Actually, it's an opposite. It's a check for successful login.
Code duplication. This is already implemented in `CeskaTelevizeIE`.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
Either `text` or `html` is mandatory.
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
no longer needed.
there is not need for excess verbosity.
will return invalid URL if `search_url` is `null`.
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Query to `query=`.
Should not be fatal.
Should be tolerate to missing keys in `media`.
Carry long lines. Read coding conventions.
Must not be fatal.
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
still fails if `uploader_data` not available.
`creator` should be preserved for backward compatibility.
Redundant or not it should be preserved either by providing manually or by copying from `artist` automatically when not present (that is not implemented) since it can be used by someone already.
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
`lxml` is not part of the standard library, you'll need to use regular expresions.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
Network connections in your browser.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
All video formats must be extracted.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
The description is always optional, so there should be a `fatal=False` in here.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`id` and `display_id` should be tested as well.
Use quotes consistently - only a single or only a double. Use different kind of quotes only if there is no way to use same kind of quotes.
As already mentioned `id` and `display_id` should be extracted.
python's `json` use escape sequences, but both are supported. I prefer to directly use the character (imaging having to use escape sequences for titles in chines, arabic ...).
I don't see any reason to use unicode escape sequences here, you can directly use `'Ã­'`.
There is no need to test URLs.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
I've already pointed out: API URLs should be used.
API URLs should be used.
Formats should have `quality`.
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
should not break the extraction if the field is not available.
use `query` argument.
Do not escape quotes inside triple quotes.
this should be done once(in `_real_initialize`).
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
`compat_str()`, here and in l.96.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Use `sanitize_url()`, or let the core code, which does so, fix it.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
`else` is superfluous.
This does not mean it should not be included.
This is done automatically.
Use more python idiomatic `.get('title')` and `if alt_title:` instead.
What's the point of this? `canonical_url` is the same as `url`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
> Also append_url_to_file may be renamed to something better reflecting it's actual content. the function can be removed completely and move the code directly to the for loop.
121, 124 - DRY.
These are not used.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
As above, you can omit this and let the core set it from `release_timestamp`: ```suggestion ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
No `upload_date` is guaranteed to be present.
Breaks on `-F`.
I've already pointed out this does not work.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
No, it's not the point. It **must not** skip items than belong to the range specified.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Should not be fatal.
This is already fatal.
No such meta field.
Not used with formats.
This is superfluous since you provide `formats`.
This does not work in python 2.6.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
I guess `filename` can also be included.
This will fail if neither mplayer nor mpv is available.
34-35 can be easily moved into `for`.
1. Don't shadow outer names. 2. `url_or_none`.
Real id is in widget dict.
There should be a hardcoded fallback since it's always the same.
`.*` at the end does not make any sense.
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
Use `{}` dicts.
`title` must never be `None`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks extraction if `release_date[0:4]` is not `int`.
Code duplication in 70-102 and 104-137.
I've already pointed out: I won't accept this.
Must be list, not a string.
Use `_parse_json` instead.
`id` should be extracted via `_VALID_URL` when present.
Sholdn not break the extraction if missing.
Must be numeric.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Use `compat_urlparse.urljoin` instead.
This code looks similar to `sd` format and can be extracted to a function.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
Replace with `'md5:'`.
It's already extracted as video_id.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
Don't capture groups you don't use.
What's the point of this? Use `url` as base.
Some `display_id` should be extracted from `url` and shown instead of `None`.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
Breaks downloading of videos that does not require authentication.
There are two scenarios: video without auth and video with auth. The rest are matching only.
Automatic captions are also available.
Caps is used for constants.
Should not break extraction is download fails.
Query should go in `query=` .
Don't lookup `lang_code` twice.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
You're not guaranteed `thumbnails` is a list.
Should be better with ```isinstanceof(filename, BytesIO)```
There is no point to use `remove_start` since line is always a string.
Consistently use single quotes.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
you can get json output by appending `&format=json` to the api request url.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
Should not break if there is no `resolution` key.
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
Optional data should not break extraction if missing. Read coding conventions.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
`url` should not be `None`.
Rename to something else.
You must delegate with `url_result` instead.
`show = data.get('show') or {}`
Lack of information is denoted by `None`.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
`.*/?` is pointless at the end.
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
All these import are unused, check your code with flake8.
**Never ever** use `eval` on data you don't control.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Breaks on None.
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
Right, that's fine for `display_id`.
Avoid unrelated changes.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
You should use `self._request_webpage`, preferably with a HEAD request
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
```XimilayaIE.ie_key()``` is better
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
Relax `id` group.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
`flashvars[k]` is `v`.
This is pointless.
Formats in webpage are still available and should be extracted.
This code looks similar to `sd` format and can be extracted to a function.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This does not make any sense, you already have `url`.
Relax `id` group.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Do not remove the old pattern.
No escape for `/`.
Must not be fatal. Do not capture empty string.
All formats should be extracted.
Use `\s*` instead.
Use `self.url_result(inner_url, 'Generic')` instead.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Filter invalid URLs.
You technically can't login with this extractor apart from using cookies.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
```suggestion 'noplaylist': True, ```
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
Don't lookup twice.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
I did not suggest moving `subtitle_original_lang_code` into loop.
Move into loop.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Use ```query``` parameter of ```_download_webpage``` instead.
No escapes for slash.
Breaks on None.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Nothing changed. Also there is a video id available in JSON.
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
If a method is used by at least two extractors, it can be moved to ```common.py```
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
You don't need to call `report_extraction` here and above, since the extraction is over already.
`title` must be mandatory I've already told about this.
As we're always going to want `type` later (but that's a Python keyword, so rename it): ```suggestion main_id, type_ = re.match(self._VALID_URL, url).group('id', 'type') ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
I think it's customary to use `_VALID_URL` for id matching if possible.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
We should really provide a better interface to test against, something along the lines of `download(url)`.
This will break unicode strings under python 2.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
You can import `try_rm` from helper
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
Must be `int`.
Must not be `None`.
First group is superfluous.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
There should be a hardcoded fallback since it's always the same.
This should not be fatal.
This matches multiple videos.
There should be an `id` group.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion urlparts = video_url.split('/') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
It may be interesting to allow to add new items just as strings, for example : ``` IEs = [ ('Youtube', ['YoutubePlaylistIE', 'YoutubeChannelIE', 'YoutubeUserIE', 'YoutubeSearchIE', 'YoutubeIE']), 'Generic', ] ``` It would make easier to add simple IEs.
This doc should be updated.
will be extracted from the URL.
I suggest `fatal=False`
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
This TODO needs work
We may use proper XML parsing here and simply call `self._download_xml`
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
`_search_regex` is enough here.
No need for this check, this is already checked in `_sort_formats`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
This must be asserted.
This must be checked **before** any processing.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
This must be checked **before** any processing.
No need for this check, this is already checked in `_sort_formats`.
Request wrapping code can be moved to the base class.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Plain `for x in l`.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Extract `height` for each format.
Inline everything used only once.
Do not match by plain text.
All formats must be extracted.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Read code conventions on optional fields.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
Do not remove the old pattern.
Video URL is mandatory. Read coding conventions.
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
Remove all garbage.
This is too broad and detects the same video twice.
From what I've seen there is always only one video on the page thus no need in playlist.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Uppercase is used for const.
Again: remove. Any download attempt must have clear explicit message.
Read coding conventions.
Breaks on `None`.
This is already fatal.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This is fatal.
`note` and `errnote` of `_download_json` instead.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Query should go in `query=` .
Should not break extraction is download fails.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
Don't lookup `lang_code` twice.
Don't lookup twice.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Don't shadow built-in names.
No need to escape `]` is character set.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
No escape for `/`.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Breaks if no URLs extracted.
Never use bare except.
You must make it **not break** as you delegate.
[`determine_ext`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L901) is more robust. However, usually you don't need to specify `ext` in formats dictionary.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
[Correct way](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/pluralsight.py#L95-L100) to implement order independent argument matching.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
Part after `\?` should be removed since it's not used anymore.
`'%s'` is very unlikely to be a helpful error message.
Indenting is messed up here.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Must be extracted first.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion if episodes: ```
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
Use `self._html_search_meta` instead.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
There is no such field, must be `uploader`.
This should actually be just `self.url_result(embedded_url)`.
All formats must be extracted.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
Code in bracket is a set of matching characters. `or`ing won't work.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Breaks on `None`.
Breaks on `default=False`. `title` must be fatal.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
Use whitespace characters consistently.
Must be fatal.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Original regexes should be tested first.
**Do not remove** `_search_regex` part.
>Adding another fallback here does not make much sense since title is not used when delegated to kaltura anyway.
These will be needed later: ```suggestion from ..utils import ( get_element_by_id, get_element_by_class, int_or_none, js_to_json, MONTH_NAMES, qualities, unified_strdate, ) ```
Still does not handle aforementioned URLs.
Don't capture groups you don't use.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
No. It's a job if the video extractor.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
Breaks if no `name`.
Do not use leading underscore for locals.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
`default=None` for the first.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
Both used only once, move to the place where used. Also relax both regexes.
All these regexes should be relaxed.
Empty string capture does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This may change as well. Add a fallback that just processes all videos without differentiation.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, optional fields.
All formats must be extracted.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Code in bracket is a set of matching characters. `or`ing won't work.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
It's already extracted as video_id.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Wrong fallback value in `thumbnail`, `description` and `creator`.
All these regexes should be relaxed.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
JSON should be parsed as JSON.
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
Remove unnecessary verbosity.
This must be an id of the media.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
Just output complete stringified flashvars and consume in python code as JSON.
Parse from flashvars JSON.
What's the point of this? Remove.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
This is fatal.
`note` and `errnote` of `_download_json` instead.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
`int_or_none` for all int fields.
Use `self._download_json` instead.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Request wrapping code can be moved to the base class.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
There's no need to name a group if not used.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
Breaks if no `rate` key in `stream`.
This is bitrate, not quality.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
Field name is supposed to be `key` not `long_video_id`.
`_search_regex` is enough here.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
No need for this check, this is already checked in `_sort_formats`.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All methods only used once should be explicitly inlined.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
That's very brittle.
Should handle `src` with `https?:` also.
A minor bit: don't use a capturing group if it's not used.
Code duplication with base class method.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
_og_search_description is a youtube-dl function
Use ```_og_search_description()``` as a fallback instead.
This line is unnecessary as there's already self._og_search_description
This may match across several `meta`s.
Must not be fatal. Do not capture empty string.
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
accept `audio` `mediaType`.
should not break the extraction if the field is not available.
Does not work as expected in all cases.
Do not change the order of extraction scenarios.
`{}` won't on python 2.6.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
Do not touch existing tests.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Capture as `id` obviously.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
This doc should be updated.
I think you don't need `locals()` here.
But it would need to modify `gen_extractors`, it's not a great deal.
I did this so I can deprecate `--get-url` (along with all other --get options). Unless you want to do that too, `urls` field isn't really needed
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
Playlist metadata must not be fatal.
you already have `pgm_id` and `pgm_no` variables now.
Course extraction must be in a separate extractor.
Read coding conventions on optional and mandatory data extraction.
Playlist title is optional, description breaks.
Breaks if no URLs extracted.
No `ExtractorError` is raised here, `except` will never trigger.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Matching empty data is senseless.
Must not be `None`.
Never use bare except.
Instead of `resolution` and `preference` it should be extracted as `height`.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Use single quotes consistently.
Ok, It's up to you.
`formats` can be filtered out completely. ``` python -m youtube_dl nVlEpd92MJo -f "best[height<40]" [youtube] nVlEpd92MJo: Downloading webpage [youtube] nVlEpd92MJo: Downloading video info webpage [youtube] nVlEpd92MJo: Extracting video information [youtube] nVlEpd92MJo: Downloading DASH manifest [youtube] nVlEpd92MJo: Downloading DASH manifest Traceback (most recent call last): File "c:\Python\Python279\lib\runpy.py", line 162, in _run_module_as_main "__main__", fname, loader, pkg_name) File "c:\Python\Python279\lib\runpy.py", line 72, in _run_code exec code in run_globals File "C:\Dev\git\youtube-dl\master\youtube_dl\__main__.py", line 19, in <modul e> youtube_dl.main() File "youtube_dl\__init__.py", line 406, in main _real_main(argv) File "youtube_dl\__init__.py", line 396, in _real_main retcode = ydl.download(all_urls) File "youtube_dl\YoutubeDL.py", line 1614, in download url, force_generic_extractor=self.params.get('force_generic_extractor', Fals e)) File "youtube_dl\YoutubeDL.py", line 667, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "youtube_dl\YoutubeDL.py", line 713, in process_ie_result return self.process_video_result(ie_result, download=download) File "youtube_dl\YoutubeDL.py", line 1273, in process_video_result formats_to_download = list(format_selector(formats)) File "youtube_dl\YoutubeDL.py", line 991, in selector_function for format in f(formats): File "youtube_dl\YoutubeDL.py", line 1022, in selector_function yield formats[format_idx] IndexError: list index out of range ```
And `audio_selector` as well to catch `-f 'bestvideo+'`.
I would add an additional check here to catch syntax errors like `-f 'bestvideo,,best'`. ``` python if not current_selector: raise syntax_error('Expected a selector', start) ```
Breaks on unexpected data.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Same, no such key possible.
Breaks if no URLs extracted.
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
Will never happen.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
I've already pointed out: use `display_id` until you get real id.
No `id` extracted.
This is checked by `_search_regex`.
1. Provide clear evidence this field is supposed to store the video URL. According to http://atomicparsley.sourceforge.net/mpeg-4files.html `tvnn` is a TV Network Name and obviously has nothing to do with video URL at all. 2. This does not guarantee the actual container is `mp4`. 3. This won't work for audio.
This can be moved inside `if chapters:` condition.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
1. Single quotes. 2. `expected`.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
This does not make any sense, you already have `url`.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
will easily match outside the element.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
No such meta field.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
- - [ ]
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
This must be assert not exception.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
`True` is default.
This is pointless, you don't have any fallback.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
Query should be passed as `query`.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
If you accept this to be missing then it must be `default=` not `fatal`.
All of these will break extraction on unexpected data.
This is never reached cause sort formats will throw on `not formats`.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
Trailing /? is not necessary here
Same question for 'contains'
Same for re.search
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
then you should use `_remove_duplicate_formats` method.
i'm not sure why you're adding this step.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
again, extract all formats, there is no guarentee that those formats are available in all videos.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Request wrapping code can be moved to the base class.
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
Eliminate code duplication.
Use `_sort_formats` instead.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
This is bitrate, not quality.
Breaks if no `rate` key in `stream`.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Also pass `m3u8_id='hls'`.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Correct field name is `format_id`.
Read coding conventions and fix all optional meta fields.
No need to specify this.
Should be `flv`.
Title is mandatory.
No such meta field.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
You only ever call these functions once, so you can write them directly.
No hardcodes. Use API.
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
Dots should be escaped.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Code duplication at 58-60 and 68-70.
This check does not make any sense. If there are no formats extraction should stop immediately.
This video is georestricted.
Use `self._match_id` is better.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`self._search_regex` is enough here.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
All debug garbage must be removed.
It should match all non empty domain names.
Do not capture a group if you don't use it.
Capture with /album. Capture non greedy.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
This condition is not needed, t is always None here.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Do not capture empty strings.
The argument will already be a character string, no need to decode it.
Please use `print()` syntax, as we support also Python 3(.3+)
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
You have some unmerged lines here
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Relying on `userMayViewClip` is probably a [bad idea](https://github.com/rg3/youtube-dl/commit/2b6bda1ed86e1b64242b33c032286dc315d541ae#diff-0b85b33765d2939b773726fda5a55b06).
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Better to use `unified_strdate()`.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
No. Must be fatal. Read coding conventions on optional fields.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
There is no need in this method.
Same as in some previous PR.
I would always return a `multi_video` result.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
For 2.6 (yes, I know) compatibiility, `{0}` rather than `{}`, apparently. And number other formats elsewhere.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
All similar tests should be `only_matching`.
Of course when it appears inside `script`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
You don't need to call `report_extraction` here and above, since the extraction is over already.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
1. Don't shadow outer names. 2. `url_or_none`.
34-35 can be easily moved into `for`.
Real id is in widget dict.
Relax `id` group.
This does not make any sense, you already have `url`.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Fix test: ```suggestion 'ext': 'mp4', ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
This is never reached if Content-length is not set.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Just copy paste the whole line I've posted.
Also pass `m3u8_id='hls'`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
it's about the way you're setting `playlist_title`. `playlist_title` should not be set directly, it should be set in the playlist result title(by using the `playlist_result` method or using the playlist result type).
Playlist metadata must not be fatal.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
fallback to other available values.
Must be extracted first.
None is default.
Lack of information is denoted by `None` not `0`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Everything after `id` may be skipped and not matched since http://www.rte.ie/radio/utils/radioplayer/rteradioweb.html#!rii=16:10507902 works just fine.
Extraction should be tolerate to missing fields.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
Empty string capture does not make any sense.
If audio_info does not have category_name, categories will become [None, 'xxx']. It should be only ['xxx']
errnote still needs a fix. "try to parse web page" sounds wrong
group=1 is equivalent to the default behavior of _html_search_regex; just drop that.
(?:[^>]+)? can be simplified as [^>]*
Trailing /? is not necessary here
Just use `'thumbnail'` to match the default image URL.
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
There is no need in this method.
All these regexes should be relaxed.
Again: float_or_none, not parse_duration.
This code looks similar to `sd` format and can be extracted to a function.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
Use whitespace characters consistently.
Must be fatal.
Breaks on `None`.
This is default.
This is never reachable.
Playlist title is optional.
This must be done right after title extraction.
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
Note `dl_data` may be `None`.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `video_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Must not be fatal. Read coding conventions.
Use `re.sub` instead.
Read code conventions on optional fields.
Nothing changed. Also there is a video id available in JSON.
Read coding conventions on optional fields.
All formats must be extracted.
What I actually meant is to put them in a tuple or a list.
For tv/se `og:title` contains unnecessary suffix.
Just cut it with `utils.remove_end`.
If `_search_regex` fails `None` will be passed to `_parse_json`.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
All formats should be extracted.
`.*` at the end does not make any sense.
Avoid unrelated changes.
Must be extracted first.
url and formats are not used together.
Consistently use flags inside regex.
This is not true at the moment.
Plays fine without any authentication in browser.
`title` is mandatory. Move flags into regex itself.
Should be delegated via `url_result`.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
This code duplication may be eliminated.
Must be int.
Check code with flake8.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Instead of such hacks you can name group differently and capture it without any issue.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
Don't capture groups you don't use.
```suggestion 'url': 'http://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/', ``` This URL redirects to https://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/. This corresponds with the previous test L44 where we are using `play`.
There are two unrelated flags `_logged_in` and `logged_in`.
This is not true either. Login may be achieved via authorized cookies.
This is not necessarily true. Login errors should be detected and output.
You should **capture** error message and **output** it.
Insert this alphabetically (after veehd and before veoh).
Don't touch unrelated stuff.
Keep this test as `only_matching`.
https://www.3sat.de/wissen/nano/190822-sendung-100.html for 3SAT https://www.zdf.de/kinder/wickie-und-die-starken-maenner/der-wegezoll-102.html for ZDF both running with the patch above download klappt mit dem Patch
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
Empty string capture does not make any sense.
DRY with `_limelight_result`.
Extract `media_data['streamInfo']['sourceId']` into variable.
Some tests for these embeds could be useful, at least for reference.
I see. By the way, `cinematique` branch makes no sense since there is no such extractor.
Avoid shadowing built-in names, `id` in this case.
No point checking this.
There is no point checking `url`.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Carry long lines. Bother to finally read coding conventions.
`\s*` make no sense at the end.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
Use display id.
This is checked by `_search_regex`.
All formats should be extracted.
`int_or_none` for all int fields.
`note` and `errnote` of `_download_json` instead.
This is fatal.
No bare except.
I've already pointed out: API URLs should be used.
API URLs should be used.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
Do not shadow existing variables.
`'%s'` is very unlikely to be a helpful error message.
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
No need to escape `{}`.
Id from URL is not always a video id. Correct id is in JSON.
Field name is supposed to be `key` not `long_video_id`.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
`_search_regex` is enough here.
Should extract chunklists via `self._extract_m3u8_formats` here.
No. Override `suitable`.
This should not match playlist URLs.
`for key, value in media.get('images', {}).items():`
If `video` not in `media` empty formats will be returned that does not make any sense.
Read and follow code conventions. Check code with flake8.
Right, that's fine for `display_id`.
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
The most simple way is to use `utils.extract_attributes`.
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
Else branch is useless.
Direct URLs should also be extracted.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
Extract human readable title from the `webpage`.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
Move flags into regex.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
EntryId must be extracted the very first.
You must use `default` if there is a fallback after it.
Move to the place of usage.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
This is already imported and (in general) you should only use `import`s at the top level.
Single quotes. `item` is already a string.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
Breaks. Read coding conventions.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Read coding conventions on optional fields.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
`show = data.get('show') or {}`
Lack of information is denoted by `None`.
Should not break if `published_at` is missing.
Rename to something else.
`url` should not be `None`.
Any test case using this approach? I can't find it.
Also pass `m3u8_id='hls'`.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Unite in single list comprehension.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
`{}` won't work in python 2.6.
Won't work. See how this is done for output template.
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
This must be calculated once.
`vcodec` of format should be set to `'none'`.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
Formats not sorted.
Carry long lines.
``` python course_id = self._search_regex( (r'data-course-id=["\'](\d+)', r'&quot;id&quot;: (\d+)'), webpage, 'course id') ```
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
`/?` is senseless at the end.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
Breaks when `get_element_by_class` returns `None`.
Must be fatal.
`default` and `fatal` are not used together.
Pass as list of regexes, don't bulk.
No. Must be fatal. Read coding conventions on optional fields.
Master m3u8 should be extracted with corresponding method.
Missing `src` should not break extraction completely.
Must be optional.
both are know beforehand, so there is no need to use `urljoin`.
Also `_valueless_option` is probably a better name for it.
Second check should be removed.
Yes, argumentless options handling should be moved in separate method as well: ``` python def _argless_option(self, command_option, param, expected_value=True): ... return [command_option] if param == expected_value else [] ```
121, 124 - DRY.
This does not look to be possible on a clean session.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Carry long lines. Bother to finally read coding conventions.
You are **delegating** to brightcove that provides the id.
You must make it **not break** as you delegate.
Playlist title is optional, description breaks.
`.get()` idiom is used for optional fields only.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Don't do this manually use `_download_webpage_handle`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Can we resolve these IDs? There may also be a time encoded in there.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
`title` must be mandatory I've already told about this.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
```suggestion if not (playlist_files and isinstance(playlist_files, list)): ```
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
This can now be omitted.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
There are more video formats available that should be extracted as well.
This should be removed. `player_url` is used for RTMP.
Either **do** or remove.
There should be a hardcoded fallback since it's always the same.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
Everything apart from `url` is optional.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
I'm not talking about capturing upload date. Do not capture AMPM.
Breaks if not arr.
Move flags into regex.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
`try_get`, single quotes.
None of the optional fields should break extraction if missing.
Extracting duplicate code into a function obviously.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Does not work as expected in all cases.
If `_search_regex` fails `None` will be passed to `_parse_json`.
All these regexes should be relaxed.
Empty string capture does not make any sense.
Mandatory data must be accessed with `[]` not `get`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
both are know beforehand, so there is no need to use `urljoin`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
falback to a static URL.
There is no point in that.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
No newline at the end of file.
Shouldn't be fatal
```suggestion new = '' ```
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
I've already suggested using `Downloading` as idiomatic wording.
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
230-264 no copy pastes.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
Use `utils.xpath_text` instead, again with `fatal=False`.
Everything apart from `url` is optional.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
Remove all garbage.
there is not need for excess verbosity.
no longer needed.
incorrect URLs for Cook's Country.
extraction should not break if an episode or all episodes couldn't be extracted.
will return invalid URL if `search_url` is `null`.
Above, it's still possible that the JSON download works but doesn't result in a `dict`. So this would be better (setting `None` on error to help with the second point below): ``` status = try_get(info, lambda x: x['status']) ``` Then, if the API makes a breaking change without us noticing, is that `expected` or not? As the site is unlikely to revert the change, it becomes our bug and so not `expected`. I suggest `expected` should correspond to the API returning an actual status that is not OK, and nothing else, like so: ``` raise ExtractorError(status or 'something went wrong', expected=status not in ('ok', None)) ``` But you are obviously familiar with the API and I'm not ...
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
Actually I think the line I fingered was fine, but the replacement's fine too.
``` content_el = itemdoc.find(self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/')) duration = float_or_none(content_el.attrib.get('duration')) if content_el is not None else None ``` or ``` content_el = find_xpath_attr(itemdoc, self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/'), 'duration') duration = float_or_none(content_el.attrib['duration']) if content_el is not None else None ```
Use `compat_urllib_parse_unquote_plus` instead.
Use `self._match_id` instead.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
Network connections in your browser.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Should not be fatal.
As already said: parse as JSON not with regexes.
Read coding conventions on mandatory metadata.
Using preferences causes invalid sorting.
do you have an example with TTML subtitles? all the videos that i've tested with has only VTT subtitles.
Read coding conventions and fix optional meta fields.
Use `{}` dicts.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
This should be split into building url and extracting formats.
All of these will break extraction on unexpected data.
It does not necessarily mean that.
no need to create a method when it will be used once.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
Don't lookup `lang_code` twice.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
Avoid unrelated changes.
this is basically the same code repeated twice. It can be generalized
will easily match outside the element.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use single quotes consistently.
m3u8 is also available.
What's the point of `# match self._live_title` here? Remove.
It's obvious from `'is_live': True`.
Carry to the indented beginning of the line.
Breaks on `None` title.
Don't capture groups you don't use.
Query to `query`,
What's the point of this? Use `url` as base.
Use the original scheme and host.
Instead of such hacks you can name group differently and capture it without any issue.
Don't capture groups you don't use.
All debug garbage must be removed.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
` - Servus TV` should not be in title.
```suggestion get_element_by_class, int_or_none, ```
* the first element of the path can have more than 1 digit * the tail `/player\.html` isn't needed (unless there are other tails that should find different media with the same id): ```suggestion _VALID_URL = r'https?://(?:www\.)?embed\.vidello\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)' ```
The playlists are public: there should be tests for them. Watch this space.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
No escape for `/`.
If these `,`s are thousands separators, might they be `.`s for some locales (plainly not a decimal point for a count)? ```suggestion view_count = str_to_int(self._html_search_regex( (r'<strong>([\d,.]+)</strong> views', r'Views\s*:\s*<strong>([\d,.]+)</strong>'), webpage, 'view count', fatal=False)) ``` (and `from ..utils import str_to_int` at the top).
All these regexes should be relaxed.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
It's a field name not a step name.
Again: relax regex.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Then just keep this code and > create default fallbacks if extraction of these fails
Upper case is idiomatic for constants.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Same, no such key possible.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Use `\s*` instead.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Avoid shadowing built-in names.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
you can get json output by appending `&format=json` to the api request url.
Must be int.
Code duplication in 70-102 and 104-137.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
This is superfluous since you provide `formats`.
There's no need to name a group if not used.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
Do not shadow existing variables.
Carry to the indented beginning of the line.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Use _parse_json and js_to_json here
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
Do not capture empty strings.
Read coding conventions on mandatory metadata.
1. No `{}`. 2. Inline. 3. Query to `query`.
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This code looks similar to `sd` format and can be extracted to a function.
I think it's customary to use `_VALID_URL` for id matching if possible.
`self._parse_html5_media_entries` for formats extraction.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
This is already checked in `float_or_none`.
1. Do not remove fallback to previous duration value. 2. `scale` of `float_or_none` instead.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
Plays fine without any authentication in browser.
This is not true at the moment.
`id` should not be optional. No need in trailing `/`.
This is usually called `display_id` and included in info dict as well.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
3rd argument should be the name of field you search for, i.e. `'video id'`.
`http://api.nowness.com/api/` part can also be moved to `api_request`.
See how browser calls it.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
No need in these variables.
`video_id` is already a string.
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Playlist title is optional, description breaks.
You must make it **not break** as you delegate.
You are **delegating** to brightcove that provides the id.
`/?` is senseless at the end.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
yes, this is correct.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Breaks extraction if `release_date[0:4]` is not `int`.
`title` must never be `None`.
Breaks extraction if `json.loads` fails.
No trailing $, override suitable.
`parse_duration()` should work.
Same as in some previous PR.
You should have only one single method for extracting info.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Move to initial title assignment.
This is default.
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Don't do this manually use `_download_webpage_handle`
This is superfluous, the extension can be extracted automatically.
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
strip_jsonp should work here
Remove all unused code.
Regex should be relaxed. Dots should be escaped.
`video_id` may be `None`.
Remove all debug output.
Doesn't work in python 2.6.
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
`'thumb`' may not be present producing invalid thumbnail url.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
surround only the part that will threw the exception.
the same for `streaming` key.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Use `compat_urlparse.urljoin` instead.
This code looks similar to `sd` format and can be extracted to a function.
Don't use `;` unless you need to write more than 1 statement per line (personally, I would also avoid doing that)
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
`int_or_none` and `float_or_none` for all numeric fields.
This change is unrelated, you must open a new PR for it. That's one of the reasons why it's a good idea to use a new branch for each PR.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
This will process the same URL twice overwriting the previous results.
There should be a hardcoded fallback since it's always the same.
Relax `id` group.
The description is always optional, so there should be a `fatal=False` in here.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Just rethrow active exception.
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
This line is unnecessary.
Read and follow code conventions. Check code with flake8.
If `video` not in `media` empty formats will be returned that does not make any sense.
`for key, value in media.get('images', {}).items():`
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
What's the point of this? `canonical_url` is the same as `url`.
No need to escape `]` is character set.
`'id'` is required.
Should not be fatal.
Query to `query=`.
Should be tolerate to missing keys in `media`.
Never use bare except.
JSON should be parsed as JSON.
`_search_regex` per each field. Add fallback.
No need to use named group when there is only one group.
`player_id` is not extracted in this fallback but used at 71.
Upps, that's wrong, the results are indeed tuples.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
dict comprehensions don't work in python 2.6.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
I'm not talking about capturing upload date. Do not capture AMPM.
I've already pointed out: `.*$` is pointless at the end.
Capture as `id` obviously.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
This will break extraction if no `id` present.
Must be `list`.
It does since there may be no postprocessing at all.
Opening message is already applied for `multi_video`, I don't see any reason for closing message not to be applied for it as well.
No `upload_date` is guaranteed to be present.
I've already pointed out this does not work.
Read coding conventions on optional fields.
This breaks all non ks embeds. ks part must be optional.
Modify existing regex instead.
Just leave a link to kaltura embedding page.
That's incorrect. `\1` must be a number of capture group.
This should include `iframe` part.
This is always true.
Read coding conventions and fix code failsafeness.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Has no effect for url_transparent.
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
Should extract chunklists via `self._extract_m3u8_formats` here.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Superfluous. Move directly into the method call.
Does not match `var IDEC='`.
Read coding convention on optional fields and fix all issues.
Playlist id and title should not be fatal.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
Breaks. Read coding conventions.
Breaks if no `name`.
Either sloppy code or an anti-scraping measure.
There is no point in that.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Audio is not 128.
`width` and `height` instead.
- - [ ]
All formats should be extracted.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
You don't need list here. Just return it directly.
Outer parentheses are not idiomatic in python.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Not a video id.
It's not an album id.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
These looks like mandatory fields.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Remove superfluous whitespace.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
1. Extract dict if you expect dict. 2. Relax regex. 3. Escape dots.
Again: float_or_none, not parse_duration.
DRY. url is mandatory.
No such meta field.
Depends on what does it mean.
Do not shadow built-in names.
Capturing empty string does not make any sense. What's the point capturing this at all? id and path occur only once in webpage.
Should not be fatal.
No such meta field.
Not used with formats.
`video_id` is already a string.
No need in these variables.
See how browser calls it.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
No exact URLs, use `re:`.
No exact URLs.
Use whitespace characters consistently.
Must be fatal.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
The `list` call is superfluous here and can be safely removed.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
```not (foo is None)``` => ```foo is not None```
Everything apart from `url` is optional.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
No. It must be a soft dependency. Message requesting installation of `AtomicParsley` or `mutagen` should only be output when thumbnail embedding is requested and neither of these dependencies is found.
Some extractors use the domain name as `IE_DESC`, so I guess it's OK.
Geo restricted. Test will always fail.
Must only contain title.
Removing useless noise.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
This does not matter, if you open https://youtu.be/BaW_jenozKc there is a video embedded. If you open https://redirect.invidious.io/watch?v=BaW_jenozKc there is no video embedded. >used for URLs floating over the net to share youtube videos. Prove that.
There is no video on this page.
`{}` does not work in python 2.6.
Extraction should not break if one of the formats is missing.
You should use `self._request_webpage`, preferably with a HEAD request
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
Uppercase is not honored.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Oh, I see. Thanks!
Use default. Read coding conventions and fix code.
Audio must have proper `vcodec` set.
Doesn't work in python 2.6.
Why did you remove this test? It does not work with your changes.
This should be extracted right from `_VALID_URL`.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
You should extract `partner_id` and `entry_id` and return `kaltura:...` shortcut.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
There should be an `id` group.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
This statement does not conform to PEP8.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
Must be extracted first.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This should actually be just `self.url_result(embedded_url)`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
`note` and `errnote` of `_download_json` instead.
This is fatal.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Breaks extraction if there is no `stream-labels` key.
What's the point of lines 104-108? `ext` is already flv.
hls and rtmp are available as well.
Code duplication should be eliminated.
Extraction should be tolerate to missing fields.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
`split` returns a list, video_id must be a string.
In python 3.X print is a function, for printing to screen use `self.to_screen`. But if it's a fatal error then `raise ExtractorError`
Can we resolve these IDs? There may also be a time encoded in there.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
If `_search_regex` fails `None` will be passed to `_parse_json`.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
`[]` is superfluous in group with single character.
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
No. Use fatal search regex instead.
Capturing empty URL is senseless.
Move before youtube-dl imports.
```suggestion timestamp = unified_timestamp(rights.get('validFrom')) ```
Don't touch the old test.
`,` is fine here.
use `datetime` class directly.
this should be done once(in `_real_initialize`).
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
Do not escape quotes inside triple quotes.
Must be numeric.
This field is height not quality.
Nothing changed. Breaks in case regex does not match.
Invalid syntax at all.
This should be extracted first.
It also failed in your previous commit: https://travis-ci.org/rg3/youtube-dl/jobs/8459585. b64encode returns bytes, you must decode to get a string : `base64.b64decode(googleString).decode('ascii')`, I'm not sure since I have little experience with base64.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
This should actually be just `self.url_result(embedded_url)`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This will process the same URL twice overwriting the previous results.
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
```suggestion 'ext': ext, ``` fix flake8 check
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
Read coding conventions on optional metadata.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
Won't work. See how this is done for output template.
This has no effect. Postprocessors work on info dict copy.
avconv does not support `-cookies`, use `-headers` instead. You should also pass all headers.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Do not pass `default=None` to `_html_search_regex` instead.
`title` must be mandatory I've already told about this.
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
Or (probably the same result): ``` title = self._generic_title(url) ```
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Breaks on `default=False`. `title` must be fatal.
Breaks on `None`.
Code duplication. Must be single call to search regex.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
No escapes for `/`.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Omit expected type.
Must not break extraction if missing.
If there's only one format, just use `'url'`.
All formats must be extracted.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
`_parse_json`. Read coding conventions.
Read: coding conventions, optional fields.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
No need to escape `/`.
`id` should be extracted via `_VALID_URL` when present.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Use `_parse_json` instead.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Don't capture groups you don't use. Use proper regex to match all country codes.
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
Don't capture unused groups
It's obvious from `'is_live': True`.
What's the point of `# match self._live_title` here? Remove.
`<span[^>]+class="name">...` is better.
Use bare `re.match`.
No need to escape `]` is character set.
`--no-playlist` is not respected.
`else` is superfluous.
- use single quotes consistently. - i think it would be better to keep errnote closer to note.
```suggestion self._API_BASE_URL + 'authentication/login', None, 'Logging in', data=urlencode_postdata({ ```
Inline everything used only once.
Do not match by plain text.
`ext` should be mp4.
This should be split into building url and extracting formats.
Should check for a list.
Breaks if no videos in season.
`default=None` for the first.
`_` is idiomatic way to denote unused variables.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Breaks if no `rate` key in `stream`.
`user_info` may be `None`.
`created_at_i` as `timestamp`.
No need to escape `/`.
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
No need to escape `\`.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
There is no need in this method.
Again: float_or_none, not parse_duration.
All these regexes should be relaxed.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Breaks if not arr.
This code looks similar to `sd` format and can be extracted to a function.
Just copy paste the whole line I've posted.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Matched data-video should not be empty.
`self._search_regex` is enough here.
Also pass `m3u8_id='hls'`.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
surround only the part that will threw the exception.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
You must delegate with `url_result` instead.
That's very brittle.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
All formats must be extracted.
Read code conventions on optional fields.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
_ is not a special character in Python's regular expressions. There's no need to escape it.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
Allow arbitrary whitespace and both quote types.
Move everything into `_download_webpage`.
`expected_status` to `_download_json` instead.
eg find type by URL ext
This should not be fatal.
What's the point of this extractor? It's covered by album extractor. Remove.
This should be removed.
Yes, if it tests the same extraction scenario. There is only one extraction scenario in this code.
All duplicate tests should be set `only_matching`.
Use `self.playlist_result` instead.
will break if `item` is `None`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
More ways to get `title`: ```suggestion title = ( self._og_search_title(webpage, default=None) or get_element_by_class('my_video_title', webpage) or self._html_search_regex(r'<title\b[^>]*>([^<]+)</title\b', webpage, 'title')) ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Inline all these.
This will result is `[None]` is no category extracted.
This will break extraction if no `id` present.
Replace 245-257 with `entry.update({ ... })`.
Must be `list`.
`.*/?` is pointless at the end.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
I think it's better to just make DailymotionIE and YoutubeIE a sublcass of the subtitles IE class
```suggestion # coding: utf-8 from __future__ import unicode_literals ```
Coding cookie is only required for non-ASCII sources.
```suggestion try_get, url_or_none, ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
```suggestion get_element_by_class, int_or_none, ```
It does not necessarily mean that.
Inline to actual call place.
No such meta field.
No such meta field.
Read coding conventions on optional metadata and fix code appropriately.
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
Use ```query``` parameter of ```_download_webpage``` instead.
strip_jsonp should work here
`not json_lds` already does the second part.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
No, it won't. Bother to read it carefully. `(["\']?)` is idiomatic and correct way to say `(["\']|)`.
I think that this line would produce unexpected results with multiple urls. The first one would use the generic extractor and the rest would use the normal extractors. (I may have misunderstood it)
No `upload_date` is guaranteed to be present.
I've already pointed out this does not work.
It does since there may be no postprocessing at all.
Never ever use bare except.
`id` is of arbitrary length.
1. `id` is **not necessarily** 3 digits. 2. `id` must not contain irrelevant words.
All debug code must be removed.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
This matches multiple videos.
If either of these attrs is missing whole playlist extraction is broken.
This should not be here as done by downloader.
No direct URLs in tests.
Extract id once before the loop.
If nothing matches `None` will be returned.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Matching empty data is senseless.
Must not be `None`.
Breaks on `None`.
Don't shadow built-ins.
Nothing changed. Also there is a video id available in JSON.
Move into `_download_json`.
Do not use `sanitized_Request`.
Do not use sanitized_Request.
1. This must be downloaded after JSON. 2. This must not be fatal.
Move flags into regex.
Title part should be optional.
Code duplication 173, 213. There is no sense to extract fields explicitly.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Playlist id and title should not be fatal.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
This condition is not needed, t is always None here.
there is not need for excess verbosity.
no longer needed.
will return invalid URL if `search_url` is `null`.
incorrect URLs for Cook's Country.
extraction should not break if an episode or all episodes couldn't be extracted.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All methods only used once should be explicitly inlined.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
`note` and `errnote` of `_download_json` instead.
This is fatal.
The semantics of tests should be kept. This test should test `*-videoplayer_size-[LMS].html` URL. Same for others: `*-videoplayer.html`, `*-audioplayer.html` (on two different domains).
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
`self._parse_html5_media_entries` for formats extraction.
Bitrate should go to corresponding format meta field.
All debug output should be removed. `{}` does not work in python 2.6.
Should not be fatal.
`True` is default.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
This won't skip empty strings.
Avoid shadowing built-in names.
Couldn't we simply look at the extension (below) to get the format map? Then we don't have to update this list.
160 is a video format.
and 139 is a audio format
`_sort_formats` should always be called for non youtube videos(to break the extraction with a proper message when no formats has been extracted).
This URL is available from JSON.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
Use `compat_urllib_parse_unquote_plus` instead.
Code duplication in 70-102 and 104-137.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
This field is added automatically no need to add it by hand.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
This TODO needs work
you can get json output by appending `&format=json` to the api request url.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
PEP8 mandates two empty lines here.
This line does not help and is not necessary.
Side note: Wow, these guys are military, but don't support https? Oh my...
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
Don't capture groups you don't use.
Don't capture groups if you are not going to use them.
`/?` does not make any sense.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Don't capture unused groups
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
All these regexes should be relaxed.
Must be int.
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Capturing empty URL is senseless.
eg find type by URL ext
`expected_status` to `_download_json` instead.
Read coding conventions on optional/mandatory meta fields.
`expected_status` to `_download_json` instead.
Use display id.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
Not a video id.
Use `self.playlist_result` instead.
This won't skip empty strings.
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
No such meta field.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Debug code must be removed.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
All formats must be extracted.
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
Read code conventions on optional fields.
The code should match the content. If it's not possible to figure out the lang code there is no point placing it in subtitles. Moreover it's not time bound so can't even barely be treated as subtitles.
It's not always an .lrc since it does not always follow the format and not bound to time tags.
This is not always the case. Here is the `en` lyrics http://y.qq.com/#type=song&mid=001JyApY11tIp6.
As well as this.
Just update the dictionary with `subtitles` key when `.lrc` detected. There is no need in code duplication. Also replace `zh-CN` with something more generic like `origin`.
Do not touch the old patterns.
This test is identical to the first. Revert.
Lack of data must be expressed by `None` not empty string.
JSON should be parsed as JSON.
Unite in single list comprehension.
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
This should go into `YoutubeDL.py`
-`images that appear when hovering the cursor over a video timeline` This may not apply to other sites.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
This will break unicode strings under python 2.
Don't capture groups you don't use.
`.*/?` is pointless at the end.
Don't capture unused groups
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
No need to use `sanitized_Request` request here, pass url directly to download method.
There is already an extractor with such `IE_NAME`.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
No `(?i)`, no `$`. Dots must be escaped.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
Breaks on `None`.
Don't shadow built-ins.
Unite in single list comprehension.
None is default.
Nothing changed. Also there is a video id available in JSON.
Same issue for urlh
Dots should be escaped
Better to use integers for supported_resolutions and use str() here
Use ```query``` parameter of ```_download_webpage``` instead.
strip_jsonp should work here
`default` is already not fatal.
Allow arbitrary whitespace and both quote types.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
It seems you aren't using the module, remove this line.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
Doesn't match https://www.servus.com/de/p/Die-Gr%C3%BCnen-aus-Sicht-des-Volkes/AA-1T6VBU5PW1W12/?foo=bar. Should not match https://www.servus.com/de/p/Kleines-Geschenk-Set-Anti-Stress/SM129658/.
If it can be downloaded with native hls without any problem it should be forced in `_extract_m3u8_formats` and removed from test.
If you want to force this just pass `entry_protocol='m3u8_native'` to `_extract_m3u8_formats`.
What are you even trying to do? `'1'` that's all.
Move to base class.
Do not match by plain text.
Inline everything used only once.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Any test case using this approach? I can't find it.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Also pass `m3u8_id='hls'`.
fallback to other available values.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Harmonise with yt-dlp pt4: ```suggestion }] ```
Harmonise with yt-dlp pt3: ```suggestion ```
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Just rethrow active exception.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
The argument will already be a character string, no need to decode it.
`int_or_none` for all int fields.
No `id` extracted.
Since you now skip already downloaded segments the total fragment message displays wrong value after restarting: `[hlsnative] Total fragments: ...`. As a result - wrong download progress data.
121, 124 - DRY.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
These are not used.
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
do not use names of Python built-in functions(https://docs.python.org/3/library/functions.html#format).
`formats` should be sorted.
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
it's either one of two cases: they are identical -> keep them as they are(formats of the same entry). they are different -> separate them into a playlist with two entries.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
All these regexes are only used once thus make no sense as separate variables.
It should not. See the description of the field.
If `_search_regex` fails `None` will be passed to `_parse_json`.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
This produces invalid results when being called in a non-english speaking country. At least for me, in germany, titles will have `, - Anschauen auf Crunchyroll` appended (which is the same phrase being cut off here, just in german) This was not the case before
Again: relax regex.
It's a field name not a step name.
Lack of information is denoted by `None` not `0`.
Relax regex, make group unnamed, don't capture empty dict.
No need for escapes inside a brace group, all dots outside must be escaped.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
Bitrate should go to corresponding format meta field.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
You should add support for this playlist-alike 3qsdn URLs in any non-breaking way.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
Use `query` for query.
Non fatal, proper prefix id.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Since this extractor by itself can't provide too much info, maybe it would be better to remove the _real_extract and _VALID_URL, don't import it in `__init__`and do something similar to`MTVServicesInfoExtractor`, which is the base class for the extractors that need it.
`formats` is always a list of dictionaries.
Better to use `determine_ext` instead of `.endswith`
Inline all these.
You could consider using the library routine `parse_resolution()` (`utils.py`) ``` thumb_info = parse_resolution(res) thumb_info['url'] = base_url.replace(replace, res) thumb_info['preference'] = n # as above thumbnails.append(thumb_info) ```
Breaks if no `rate` key in `stream`.
With this: ``` for n, res in enumerate(common_res): ``` you could replace `len(thumbnails)` by `n` as the value of `preference`.
This should be split into building url and extracting formats.
There is no point to use `get` here.
`if not videos:`.
`ad_free_formats` is never empty here since `_sort_formats` will throw if's empty.
If such reconstructed URL is already processed it should be skipped.
This must be assert not exception.
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
Don't change extractor name.
Using preferences causes invalid sorting.
Inline to actual call place.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Pass `default` to `_og_search_title` instead.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
default and fatal are not used together.
Will never happen since it does not throw if not fatal.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
Do not touch this.
Do not touch this.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
Should be non fatal.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
All these regexes should be relaxed.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Already pointed out: must be `ExtractorError`.
No. It's a job if the video extractor.
Again: video URLs are already available on playlist page.
Don't shadow built-in names.
Network connections in your browser.
Query should be passed as `query` parameter.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
Rename to `KanalDIE`.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
The same. Should use https if ```url``` use https.
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
Same issue for re.search
Trailing /? is not necessary here
This intermediate dict is completely pointless. Build formats directly.
Must be int.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Add a rationale for that.
Consistently use single quotes.
There is no point in that.
This breaks streaming to stdout.
Usually display_id is used before the actual video_id is extracted.
Must only contain description.
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Use _parse_json and js_to_json here
Rename to something else.
`url` should not be `None`.
You must delegate with `url_result` instead.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
`compat_str()`, here and in l.96.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
The order of dictionaries is not deterministic before Python 3.6. For example: ``` $ PYTHONHASHSEED=0 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: ok ---------------------------------------------------------------------- Ran 1 test in 0.004s OK $ PYTHONHASHSEED=1 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: FAIL ====================================================================== FAIL: test_dfxp2srt (__main__.TestUtil) ---------------------------------------------------------------------- Traceback (most recent call last): File "test/test_utils.py", line 1093, in test_dfxp2srt self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data) AssertionError: u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... != u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... Diff is 663 characters long. Set self.maxDiff to None to see it. ---------------------------------------------------------------------- Ran 1 test in 0.005s FAILED (failures=1) ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
All debug output should be removed. `{}` does not work in python 2.6.
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
will break if `item` is `None`.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
Same as in some previous PR.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Always return a playlist.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
Don't capture empty list. `_search_regex`, `_parse_json`. Read coding conventions.
No need for that.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Don't add list items if 'url' is None.
It's better to fix thumbnail extraction instead of remove the test.
Put it in this one is OK.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
This is not supported in python 2.6.
Formats not sorted.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
`_search_regex` is enough here.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
`--no-playlist` is not respected.
Prefer `post.get()` for these two.
I would always return a `multi_video` result.
This does not mean it should not be included.
This is done automatically.
Incorrect. find returns -1 on failure that is Trueish value.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
Must not be fatal.
The idiomatic way to extract `id` is to use a group in `_VALID_URL`.
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
What's the point of this? `canonical_url` is the same as `url`.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
This is pointless. If no formats can be extracted extraction should stop immediately.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`for key, value in media.get('images', {}).items():`
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
`(server_json.get('id') or '?')` (or some other default value). Or server_json.get('id', '?') Similarly l.128.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
Shouldn't be fatal
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_id = self._match_id(url) ```
There is no need in this method.
All these regexes should be relaxed.
Playlist title is optional, description breaks.
Do not shadow existing variables.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
This check is pointless.
Don't shadow built-in names.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
falback to a static URL.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
Remove unused codes.
Remove debugging codes.
Use `self._sort_formats(formats)` instead.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Upper case is idiomatic for constants.
46-47 code duplication.
`self._search_json_ld` should be improved instead.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
Rename to `KanalDIE`.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
End users do not read source codes thus will never find this advice.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
`objectID` does not match the id from `AmericasTestKitchenIE`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
Should be `fatal=False`.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
As said duration must be int and in seconds.
There should be a hardcoded fallback since it's always the same.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
This should be just `return info`
Same as above, we should create a `formats` array here.
We now have a fully-fledged format system which can be used.
`if mediatype == u'video':` is idiomatic Python.
We may use proper XML parsing here and simply call `self._download_xml`
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This is determined automatically.
You should actually keep this line and change https://github.com/costypetrisor/youtube-dl/blob/autonumber_start/youtube_dl/YoutubeDL.py#L296, otherwise it won't be incremented.
You'll have to provide a default value for `autonumber_start` (like 1), because when using the youtube_dl module from python it will usually be missing.
This will break `--max-download`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
```not (foo is None)``` => ```foo is not None```
Not quite sure. Currently it redirects to `pornhub.com`. Possibly this was not the case in the past.
Should also match `pornhubpremium.net`. Or extractor should not match `pornhubpremium.net`.
You must provide account credentials/cookies for testing.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Will break if `episode_title` is `None`.
should not break the extraction here if a request fails or the `video` field is not accessible.
use the already extracted value(`video_url`).
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
`int_or_none` for all int fields.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
Use `self.playlist_result` instead.
This should be removed.
`{}` won't work in python 2.6.
They are actually different :`MIGcBg` vs. `MIGmBg`
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
This is determined automatically.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Do not touch `only_matching` tests.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
should not return empty formats.
`formats` should be sorted.
`secondaryurl` is `False` if downloading fails.
The indentation is messed up here, it should be 4 instead of 2 spaces. You may want to get a better editor - a modern editor should take care of indentation automatically.
The indentation is messed up here, it should be 4 instead of 3 spaces.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
The variable name `shand` is non-descriptive
Not having archive != having dummy `Archive` object.
Do not capture empty strings.
`.*` at the end does not make any sense.
Must not be fatal. Read coding conventions.
`skip_download` is needed for the test to pass similar to the first test.
No trailing $, override suitable.
Has no effect for url_transparent.
Must ensure numeric.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
Consts should be in uppercase.
`try_get` is useless here.
Trailing /? is not necessary here
Same question for 'contains'
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
Same for re.search
The same. Should use https if ```url``` use https.
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
This will process the same URL twice overwriting the previous results.
Use _parse_json and js_to_json here
extract mandatory information(title and formats) first, and sort formats.
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
try to parse multiple formats, set `vcodec` to `none`.
should not break the extraction if the field is not available.
use `query` argument.
Of course when it appears inside `script`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
You must use `default` if there is a fallback after it.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Request wrapping code can be moved to the base class.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
You can use self._download_json() here
fallback to other available values.
Lack of data is denoted by `None` not `0`.
This should be split into building url and extracting formats.
Same as in some previous PR.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Query should be passed as `query` parameter.
Should be `mp4`.
Sorting is incorrect. Best format is not really the best.
Use `query` for query.
`_live_title` should be used.
Breaks if no `rate` key in `stream`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
This TODO needs work
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
fatal=True is already the default
`quality` must be used for quality.
Breaks if no such key.
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Put `raise` after `if` section at the same indent.
This line is unnecessary.
You should consult some git manual.
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This does not make any sense, you already have `url`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
You've forgot to pass `info_dict` to `supports()`.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Remove all garbage.
**Do not remove** `_search_regex` part.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Remove useless code.
If `_search_regex` fails `None` will be passed to `_parse_json`.
To access both groups from the URL match, use `mobj = re.match(self._VALID_URL, url)` (`import re`) and then access the matches as `video_id = mobj.group('id')`. In this case the fact that the extractor is running means that the `id` and `display_id` groups matched. If you had an optional group, like `(?P<display_id>.+)?`, something like `display_id = mobj.groupdict().get('display_id')` would be appropriate.
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
Better to use unified_strdate for parsing dates.
Breaks on `None`.
Don't shadow built-ins.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Read coding conventions on optional/mandatory meta fields.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
This is already imported and (in general) you should only use `import`s at the top level.
`{}` doesn't work in python 2.6.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
It shouldn't fail if `user` or `username` is missing.
Single quotes. `item` is already a string.
This does not make any sense, you already have `url`.
Relax `id` group.
Do not capture empty strings.
Capture between tags.
No exact URLs here.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
You have some unmerged lines here
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Style: ```suggestion return '/'.join(urlparts) ```
Remove all garbage.
This does not necessarily mean that. There is a clear captured error message that should be output.
This makes no sense - you already have it in `url`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
All methods only used once should be explicitly inlined.
`int_or_none` for all int fields.
For now this should not be printed or only printed in `verbose` mode.
This could be moved several lines up.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
If you pass cookie you must not pass credentials.
This does not look to be possible on a clean session.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
Do not touch existing tests.
All these regexes should be relaxed.
Use `compat_urlparse.urljoin` instead.
Code duplication should be eliminated.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
There is no point in `or None` since `None` is already default.
Title is mandatory.
`int_or_none` and `float_or_none` for all numeric fields.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`id` by no means should be `None`.
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
Optional fields should not break extraction if missing.
No newline at the end of file.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Remove unnecessary verbosity.
`strip_or_none` no longer needed.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
Will never happen since it does not throw if not fatal.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
Add `fatal` flag.
`fatal` must be added to `_extract_info`. No changes to core code.
Do not change the order of extraction.
This line can just be removed.
extract all formats.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
still fails if `uploader_data` not available.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `self.url_result(inner_url, 'Generic')` instead.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Avoid unrelated changes.
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
All formats should be extracted.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Should not be fatal.
Should be tolerate to missing keys in `media`.
Query to `query=`.
Carry long lines. Read coding conventions.
Must not be fatal.
Must be separate extractor delegating to CNBCIE.
First group is superfluous.
Must not be `None`.
Despite being keyword arguments avoid changing the original order.
Shouldn't be fatal
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
Use _parse_json and js_to_json here
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
I'm **not talking about any particular test case**. I'm talking about a potential situation when this code: ``` <td class="key">Title:</td> <td class="value"></td> ``` **is not present on the webpage**. 46 will **fail** in this case because it has `fatal=True` and you will not reach a "fallback" in this case.
Dots should be escaped. Query may contain more arguments that will be incorrectly captured by this regex as path.
Title is mandatory.
None of the optional fields should break the extraction if missing. Read new extractor tutorial.
Has no effect on hls.
To be removed.
No need for escapes inside a brace group, all dots outside must be escaped.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
From what I've seen there is always only one video on the page thus no need in playlist.
Do not capture groups you don't use.
It should match all non empty domain names.
Should not allow empty 3rd level domain. Should not be greedy. Inner group is superfluous.
No captures for groups you don't use.
Must be separate extractor.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
m3u8 is also available.
Lack of information is denoted by `None` not `0`.
This is already fatal.
Should not be fatal.
Better to use `determine_ext` instead of `.endswith`
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
Breaks on missing file key.
Here you must use `urljoin`.
try_get is pointless here.
`tracks` is not guaranteed to be iterable.
`get` is pointless since availability of result key is mandatory.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
idk what the maintainers think about this, but I personally think this change is out of the scope of this PR. If this function is desired, it can be added separately. For now, you could just replace `self._match_valid_url(url)` with `re.match(self._VALID_URL, url)` as many other extractors already do.
This condition is not needed, t is always None here.
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
No unrelated changes.
Remove all unrelated changes.
This is never reached cause sort formats will throw on `not formats`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
`()` is a better 0-length iterable than `""`, which implies text.
Actually I think the line I fingered was fine, but the replacement's fine too.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
`info.get('description')` ? Not a mandatory item. Similarly with `info['uploaded_at']` below.
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
I've already suggested how to cover both scenarios without getting any error. > Looks like old videos with **5 digit length video id** are still available via xstream in much better quality than vgtv. New video ids are 6 digit length.
Looks like old videos with 5 digit length video id are still available via xstream in much better quality than vgtv.
Use formatted strings.
Remove all useless noise.
All methods only used once should be explicitly inlined.
Do not shadow existing variables.
According to the [W3C HTML5 syntax spec](http://www.w3.org/TR/html5/syntax.html), using `'\s'` to match whitespaces is better here.
Don't add list items if 'url' is None.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
`args` may be `None` here.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
Something like this (not tested): ```python args = self._downloader.params.get('postprocessor_args') if args is None: return default if isinstance(args, (list, tuple)): # for backward compatibility return args assert isinstance(args, dict) pp_args = args.get(pp_key) if pp_args is not None: return pp_args pp_args = args.get('default') # for backward compatibility if pp_args is not None: return pp_args return default ```
`cli_configuration_args` is supposed to be used with `params` dict, if you are working with `postprocessor_args` on your own you should not use it.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
Direct URLs should also be extracted.
Extract human readable title from the `webpage`.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
This check does not make any sense. If there are no formats extraction should stop immediately.
These formats should not be removed.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
```dict_get``` makes codes even shorter
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
TODO: do ;)
This value looks an awful lot like a `display_id`
I meant a _webpage that uses embed.ly to embed some non-video content and also contains a video that is detected by code after this_. I concur that the best action would be to wait for a test case, and then decide how we can exclude it or improve some other code.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Broken python 3. Must be bytes. `urlencode_postdata`.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
`try_get` is useless here.
Merge in single list comprehension.
Consts should be in uppercase.
This fails on Python 2. (duh!) Instead, we should fix SSL support in general.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
Should not be greedy.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
That's very brittle.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
do you have an example with subtitles.
if the `ext` can't be detected than fallback to `vtt` as it's the `ext` that most likely to be.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
`enumerate` on for range.
Should contain `quality` key.
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
`{}` won't work in python 2.6.
It's better to fail instead of fallback.
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
Should be more relaxed.
Pass video id.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
Because all of them use the same extraction scenario.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
API URLs should be used.
Move data and query into `_download_webpage` call.
`False` is not a valid note.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
This does not necessarily mean that. There is a clear captured error message that should be output.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
`\s*` make no sense at the end.
There is no point checking `url`.
No point checking this.
This is determined automatically.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Never use bare except.
Breaks on None.
No escapes for slash.
This is never reached cause sort formats will throw on `not formats`.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Formats in webpage are still available and should be extracted.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Read coding conventions on how mandatory data should be accessed.
Breaks on unexpected data.
Must not be fatal.
What's the point? It's not alphabetic altogether anyway.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
Actually, it's an opposite. It's a check for successful login.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
No newline at the end of file.
All video formats must be extracted.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
This should be extracted from `_VALID_URL` with `self._match_id`.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
`ext` should be mp4.
Use single quotes consistently.
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
Duration calculation is incorrect.
`{}` won't work in python 2.6.
This has no effect. Postprocessors work on info dict copy.
You should **capture** error message and **output** it.
This is not necessarily true. Login errors should be detected and output.
`default` implies non `fatal`.
This is not true either. Login may be achieved via authorized cookies.
There are two unrelated flags `_logged_in` and `logged_in`.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Just `video_url = urljoin('https://ndtvod.bc-ssl.cdn.bitgravity.com/23372/ndtv/', filename)`.
1. This will never be reached. 2. Don't change the order.
Each test the same scenario = duplicate.
All duplicate tests must be `only_matching`.
Must not be fatal. Read coding conventions.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
Use `self._search_regex` and `utils.unified_strdate` instead.
Name an example URL where `og:description` has HTML tags.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
`default=None` for the first.
Both used only once, move to the place where used. Also relax both regexes.
Should be delegated via `url_result`.
Must be int.
it better to extract all the urls in the `mpath` array.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
you can get json output by appending `&format=json` to the api request url.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
`ExtractorError` is not raised when `fatal=False`.
Breaks if not arr.
No need to escape whitespace.
Formats in webpage are still available and should be extracted.
This is pointless.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This should be just `return info`
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
`{}` doesn't work in python 2.6.
There's no need to use `u` prefix given `unicode_literals` is declared.
This should actually be just `self.url_result(embedded_url)`.
There should be spaces between `%`.
`/?` does not make any sense.
Don't capture unused groups
Do not carry dict values.
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Do not capture empty strings.
Breaks on None.
No escapes for slash.
Nothing changed. Also there is a video id available in JSON.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Breaks on `None`.
It should match at least one character.
extraction must be tolerate to missing fields.
This should be extracted first.
Sholdn not break the extraction if missing.
Use `compat_urllib_parse_unquote_plus` instead.
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
first_three_chars = int(float(ol_id[:3])) is cleaner
The current working directory is not always writable.
Sorry - dismiss that
Temp file is not removed in this case.
No hardcodes. Use API.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Code in bracket is a set of matching characters. `or`ing won't work.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
All formats must be extracted.
`'id'` is required.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Use `self._parse_html5_media_entries` instead.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Don't shadow built-in names.
Already pointed out: must be `ExtractorError`.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
Request wrapping code can be moved to the base class.
Use `self._html_search_meta()` instead.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
All methods only used once should be explicitly inlined.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
Never ever use bare except.
Playlist title is optional, description breaks.
Extraction should be tolerate to missing fields.
fallback to other available values.
Use `self.playlist_result` instead.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
All debug garbage must be removed.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
This worked for me.
ðwork for me, modify the file by hand
Suffer. In addition to that they can install python and run this themselves quite fine.
works also for me :+1:
Video id length is 8.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Breaks. Read coding conventions.
Subtitles requiring additional network requests should only be extracted when explicitly requested.
You have some unmerged lines here
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
1. Relax regex. 2. Do not capture empty dict.
Omit expected type.
Invalid arguments for 4-5.
Yes, it should accept any variation of whitespace.
Must not break extraction if missing.
this will fail if `type` is not present.
you can iterate here using `values` method and make it in a single line without checking if `Item` is present: ```python for metadata in video_data.get('__children', {}).get('Item', {}).values(): ```
no need to create a method when it will be used once.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
This is only used once.
You've forgot to pass `info_dict` to `supports()`.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
I'm pretty sure PEP8 mandates the `break` to be in a new line, but that's not that important.
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
`<h3>` is intentional.
It should not match `h|`.
Too broad regex.
87-90 code duplication.
Query to `query`,
`{}` won't work in python 2.6.
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
I've already pointed out: I won't accept this.
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
This has no effect. Postprocessors work on info dict copy.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
The URL should have been `compat_str` already, so the regex groups will be too.
- we don't directly use `str` in the project, instead we use `compat_str` to keep the code compatible with all support versions of Python. - when you call `str/compat` on a dict it won't give a good result, so, i think it should be ommited.
Empty string capture does not make any sense.
Do not escape quotes inside triple quotes.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
check the existence of the `contentUrl` before adding the format.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
Nothing really changed. You construct the same structure two times.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
Should contain `quality` key.
Must not break extraction if missing.
No such meta field.
For `url` type any metadata here have no effect.
Query should be passed as `query` parameter.
End users do not read source codes thus will never find this advice.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
Move flags into regex. Regex should match `runParams={`.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
`default=None` for the first.
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
```XimilayaIE.ie_key()``` is better
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Relax `id` group.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Note `dl_data` may be `None`.
Note `video_data` may be `None`.
No such meta field.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
Captures `?#&` ending as id.
`None` is not an id.
Use raw strings.
Capturing empty string is senseless. `\n` instead of `root[__env]`.
`_match_id`. Do not shadow built-in names.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
Use `sanitize_url()`, or let the core code, which does so, fix it.
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
This should not raise a generic Exception, but an `ExtractorError`.
nitpick: `url not in api_response` is somewhat nicer to read.
This looks like a really complicated way of writing `time.time()`
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Why have you changed the quotes here? It's not that important, but we strive to use `'` when possible, and have when possible a consistent quote character in a file.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
should be in the `else` block of the `for` loop.
surround only the part that will threw the exception.
the same for `streaming` key.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Don't try logging in when `username` is `None` obviously.
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Same for this test entry
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
fatal=True is already the default
Final bit, self._search_regex is better than re.search
This is not a generic embed.
This is too broad. It must not capture plain text URLs.
The other case was some legacy code. I don't see much sense in such messages since it's clear what extractor is delegated to since all messages from the final extractor are prefixed with `IE_NAME`.
To be removed.
This regex does not look like generic embed. Provide several examples that use this embedding.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
These 2 lines could be replaced by: ```python uploader_url, creator = creator_data[0][0:2] ```
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
Also looks like they redesigned the site so that extractor does not work any longer.
Matching empty id is senseless.
Although I think you may have taken "specific" more literally than I intended!
No direct URLs here.
No need to escape `/`.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
Use `self.url_result(inner_url, 'Generic')` instead.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
`{}` won't work in python 2.6.
Dot is pointless here.
Not a video id.
Use `self.playlist_result` instead.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
54-58, 71-75 code duplication.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
Final bit, self._search_regex is better than re.search
Same issue for re.search
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
The same. Should use https if ```url``` use https.
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
> Does that seem more helpful? If this error was to actually happen, it would mean that what the API returns has changed beyond what we expect. When the happens, I think all the user can do is report an issue here. there is a generic option to do this(`--dump-pages`), and we would ask to the user to use this option if needed. > Is it typical practice to say in error messages "report this error to youtube-dl"? If so I can add that as well. you can do that, by removing `expected=True` from the options passed to `ExtractorError`,
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
By the way, the pythonic way is to just evaluate `thumb_list`
Remove useless code.
This is pointless.
Formats in webpage are still available and should be extracted.
This line is unnecessary, webpage is never used.
This should actually be just `self.url_result(embedded_url)`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
We may use proper XML parsing here and simply call `self._download_xml`
metavar should be `FILE`. `type` is already string by default.
Just `File to read configuration from`.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Should be extracted from `ytplayer_config`.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
Extractor should not return `None`.
Relax `id` group.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
This does not make any sense, you already have `url`.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Escape dot. No need to split URL.
Do not match by plain text.
Place on a single line.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
I'm addressing this concrete line of code.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
```dict_get``` makes codes even shorter
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
Single loop for all sources without any unnecessary intermediates.
This should be fixed in `js_to_json`.
Optional data should not break extraction if missing. Read coding conventions.
There is no point in that.
`enumerate` on for range.
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
```XimilayaIE.ie_key()``` is better
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Same for re.search
87-90 code duplication.
Too broad regex.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
will be extracted from the URL.
I suggest `fatal=False`
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
yes, remove duplicate formats if the qualities are not available for all programs.
from first test of `SverigesRadioPublicationIE`: https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=low&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a32.m4a", "duration": 132, "codingFormat": 12, "state": 0, "isGeoblockEnabled": false } ``` https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=medium&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a96.m4a", "duration": 132, "codingFormat": 13, "state": 0, "isGeoblockEnabled": false } ```
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
should not return empty formats.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
try put value that are used multiple times in a variable(ex: `author_info.get('id')`). i think the `{}` format is not supported in python 2.6.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
Right, that's fine for `display_id`.
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
Playlist title is optional, description breaks.
No, it's not the point. It **must not** skip items than belong to the range specified.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
I've already pointed out this does not work.
Uppercase is not honored.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
That's completely different videos.
Ids must stay intact.
Using preferences causes invalid sorting.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
the same for `streaming` key.
should be in the `else` block of the `for` loop.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Usually display_id is used before the actual video_id is extracted.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Carry long lines.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
You could just write this like so, as other extractors seem to do: ``` info_dict = { 'id': video_id, ... ```
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
Instead of such hacks you can name group differently and capture it without any issue.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
Groups around `video` and `sptv/spiegeltv` are superfluous.
All methods only used once should be explicitly inlined.
`if mediatype == u'video':` is idiomatic Python.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`.*/?` is pointless at the end.
The URL should have been `compat_str` already, so the regex groups will be too.
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
1. No umask respected. 2. There is no `os.chmod` in python 3.2 according to python [docs](https://docs.python.org/3/library/os.html#os.chmod). 3. flake8.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
Won't work. See how this is done for output template.
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
`--ap-mso` should not be touched since it's a separate stand-alone mechanism that has stable unique MSO identifiers for TV providers across all extractors while this options is not.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
`autonumber` is not reset to zero in the first place.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is not reachable when title extraction fails.
I'm **not talking about any particular test case**. I'm talking about a potential situation when this code: ``` <td class="key">Title:</td> <td class="value"></td> ``` **is not present on the webpage**. 46 will **fail** in this case because it has `fatal=True` and you will not reach a "fallback" in this case.
Dots should be escaped. Query may contain more arguments that will be incorrectly captured by this regex as path.
This is pointless, you don't have any fallback.
56-71 code duplication.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
`True` is default.
Default sorting is just fine. Remove `field_preference`.
Read coding conventions.
Again: remove. Any download attempt must have clear explicit message.
Uppercase is used for const.
`.*` at the end does not make any sense.
This is already fatal.
will return invalid URL if `search_url` is `null`.
extraction should not break if an episode or all episodes couldn't be extracted.
incorrect URLs for Cook's Country.
no longer needed.
there is not need for excess verbosity.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
is there a URL where the `duration` is available.
We should use the various `compat_*` down below and delete these imports, so that the code also runs on Python 3.
Do not capture groups you don't use.
`https?://` is better in this case. openload supports both HTTP and HTTPS.
This doc should be updated.
You can import `try_rm` from helper
Do not shadow existing variables.
You have some unmerged lines here
`{}` doesn't work in python 2.6.
It shouldn't fail if `user` or `username` is missing.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
The class name should ends with FD
The number 25 should be configurable
Don't capture groups if you are not going to use them.
You should capture a part of URL that represents a video in unique way...
Use `_request_webpage` instead.
Again: relax regex.
It's a field name not a step name.
Relax regex, make group unnamed, don't capture empty dict.
Move flags into regex.
Lack of information is denoted by `None` not `0`.
Same for re.search
Same question for 'contains'
Trailing /? is not necessary here
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
Not all URLs contain video id.
Should be `display_id`.
No, it's not. If video_id extraction from page fails whole extraction fails.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
```suggestion if episodes: ```
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
Usually display_id is used before the actual video_id is extracted.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
No way. Return value type must not change.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
From what I've seen there is always only one video on the page thus no need in playlist.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
This introduces an ambiguity in case of several mso available. That's why it won't be accepted.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
`ext` should be mp4.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Never use bare except.
No need to escape forward slash.
Instead of `resolution` and `preference` it should be extracted as `height`.
This will break the entire extraction if there is no match for some format.
Use single quotes consistently.
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
All these regexes should be relaxed.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
`'thumb`' may not be present producing invalid thumbnail url.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
`_search_regex` is enough.
`_search_regex` is enough.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
Empty string capture does not make any sense.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
`if playlist_size:` is enough.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`default=None` for the first.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Extraction is broken if any these key is missing.
Query should be passed as `query`.
`True` is default.
This is pointless, you don't have any fallback.
Looks like, it's off then.
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
`_match_id`. Do not shadow built-in names.
`True` is default.
This is not supported in python 2.6.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
No trailing `$`, override `suitable`.
No escapes for slash.
Capture with /album. Capture non greedy.
Dot is pointless here.
`{}` won't work in python 2.6.
`for k, v in flashvars.items()`.
`flashvars[k]` is `v`.
Must be numeric.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
26-29, 32-37 code duplication.
Move `_match_id` into `_extract_audio`.
should not return empty formats.
Use self._report_warning instead
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
Actually, it's an opposite. It's a check for successful login.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
`ext` should be mp4.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
This prevents from authenticating with `--cookies`.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
It's obvious from `'is_live': True`.
What's the point of `# match self._live_title` here? Remove.
Again: relax regex.
It's a field name not a step name.
Relax regex, make group unnamed, don't capture empty dict.
If there's only one format, just use `'url'`.
All formats should be extracted.
Use `self._search_regex` and `utils.unified_strdate` instead.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
first_three_chars = int(float(ol_id[:3])) is cleaner
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
If `_search_regex` fails `None` will be passed to `_parse_json`.
We may use proper XML parsing here and simply call `self._download_xml`
Same as above, we should create a `formats` array here.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
- - [ ]
`width` and `height` instead.
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
surround only the part that will threw the exception.
Use display id.
This is checked by `_search_regex`.
`default` is not used with `fatal`.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
Sure, that would be fine.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
This is never reached if Content-length is not set.
Move data and query into `_download_webpage` call.
This condition is not needed, t is always None here.
Or at least, it's not `unicode`. In yt-dl `str` should almost always be `compat_str`, but as above it's not needed here.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
There are more video formats available that should be extracted as well.
This should be removed. `player_url` is used for RTMP.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
This fails on python 3 and doesn't look too good.
You can import `try_rm` from helper
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
Title is mandatory.
No need to use named group when there is only one group.
`player_id` is not extracted in this fallback but used at 71.
No `ExtractorError` is raised here, `except` will never trigger.
`_search_regex` per each field. Add fallback.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
is there a URL where the `duration` is available.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
I've already pointed out: API URLs should be used.
`try_get`, single quotes.
`if height and filesh:`
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Optional data should not break extraction if missing. Read coding conventions.
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
Read and follow code conventions. Check code with flake8.
`for key, value in media.get('images', {}).items():`
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Relax `id` group.
This does not make any sense, you already have `url`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Parse from flashvars JSON.
Breaks extraction if not available. Again read coding conventions.
What's the point of this? Remove.
Parse from flashvars JSON.
Just output complete stringified flashvars and consume in python code as JSON.
You should capture a part of URL that represents a video in unique way...
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
I think you don't need `locals()` here.
No exact URLs.
We should really provide a better interface to test against, something along the lines of `download(url)`.
This can be moved inside `if chapters:` condition.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
OK. I'll open an issue for discussing this. For now you can remove this line.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
These looks like candidates for generalization and extracting into a separate method.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Breaks if no `name`.
Title is mandatory.
No `url` and `formats` at the same time.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
Inline everything used only once.
Do not match by plain text.
Plain `for x in l`.
Extract `height` for each format.
surround only the part that will threw the exception.
falback to a static URL.
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
This will break extraction if no `id` present.
Must be `list`.
This will result is `[None]` is no category extracted.
Read coding conventions on optional fields.
Replace 245-257 with `entry.update({ ... })`.
Again: no, do not touch token extraction code. Just eliminate duplication.
Again: you must eliminate code duplication. In both places.
Can't be None.
no need to create a method when it will be used once.
All formats should be extracted.
just use a hardcoded value for now.
accept `audio` `mediaType`.
use `query` argument.
should not break the extraction if the field is not available.
extract mandatory information(title and formats) first, and sort formats.
Just put original texts: æ§ããã«è¨ã£ã¦é å¼µã£ã¦ã
url and formats are not used together.
No trailing $, override suitable.
`.*` at the end does not make any sense.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
No trailing $, override suitable.
All these regexes should be relaxed.
Must be int.
```suggestion new = '' ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Bitrate should go to corresponding format meta field.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
No trailing $, override suitable.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
I've already suggested using `Downloading` as idiomatic wording.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
Do not mix unrelated changes in single PR.
Remove unrelated changes.
Remove all unrelated changes.
No unrelated changes.
Remove all unrelated changes.
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
It may be interesting to allow to add new items just as strings, for example : ``` IEs = [ ('Youtube', ['YoutubePlaylistIE', 'YoutubeChannelIE', 'YoutubeUserIE', 'YoutubeSearchIE', 'YoutubeIE']), 'Generic', ] ``` It would make easier to add simple IEs.
This doc should be updated.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
This can be moved to station extractor regexes eliminating this extractor at all.
This is already embedded into extractors. DRY.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
Sorry I didn't check it, it should look into `<div class='more_info'>`
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
`title` must be mandatory I've already told about this.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
What's the point of this? Remove.
The whole code until here can be simplified to `page_id = self._match_id(url)`
This looks like `orderedSet(m.group(1) for m in re.finditer(r'href="/video([0-9_]+)"')`. Also, since it only gets called once, feel free to move it in the main function.
Why is the `m?` group in here? If it's optional anyways, you can just leave it out ;)
This line is superfluous, the youtube-dl core can guess the extension better than that already.
You technically can't login with this extractor apart from using cookies.
Again: it must contain `vbr or abr` if available.
The starting part that is already used for it.
This should be fixed as well.
This will skip `format_id` completely even if `media_type` is available.
Breaks on unexpected data.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Use `self._parse_html5_media_entries` instead.
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
Sorry, we actually request 10KiB, and `K` does stand for Kilobyte in head.
`K` in head stands for `Kibibyte`, the test uses Kilobytes.
`id` and `display_id` should be tested as well.
This looks as if it would be better to split the code into two extractors. You can easily pass from one into the other by returning a `url_result`.
Do not shadow built-in names.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Again: ```suggestion {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie}, separators=(',', ':')) message = 'nobody{0}use{1}md5forencrypt'.format( URL, request_text).encode('latin1') ```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Using preferences causes invalid sorting.
Breaks if no `rate` key in `stream`.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Course extraction must be in a separate extractor.
You have some unmerged lines here
`flashvars[k]` is `v`.
`for k, v in flashvars.items()`.
This field is height not quality.
Nothing changed. Breaks in case regex does not match.
Invalid syntax at all.
Again: video URLs are already available on playlist page.
No. It's a job if the video extractor.
Don't shadow built-in names.
This check is pointless.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
This should only be printed when sleep interval is provided. It's kind of misleading and unrelevant to see when you didn't want any sleep interval.
You have some unmerged lines here
`/?` does not make any sense.
Do not carry dict values.
Do not capture empty strings.
`.*` at the end does not make any sense.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
Optional fields should not break extraction if missing.
54-58, 71-75 code duplication.
`int_or_none` and `float_or_none` for all numeric fields.
`--rm-cache-dir` wipes the whole cache thus should never be suggested to use.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
This prevents from authenticating with `--cookies`.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
`ext` should be mp4.
This video is georestricted.
Use `self._match_id` is better.
There's no need to name a group if not used.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
Breaks if not arr.
If `_search_regex` fails `None` will be passed to `_parse_json`.
This code looks similar to `sd` format and can be extracted to a function.
Remove useless code.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
No need for this check, this is already checked in `_sort_formats`.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
Field name is supposed to be `key` not `long_video_id`.
`_search_regex` is enough here.
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
accept `audio` `mediaType`.
should not break the extraction if the field is not available.
You should have only one single method for extracting info.
170-172 - code duplication.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
These looks like mandatory fields.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
What's the point? It's not alphabetic altogether anyway.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Real id is in widget dict.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
1. Don't shadow outer names. 2. `url_or_none`.
34-35 can be easily moved into `for`.
Audio is not 128.
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
`if not formats` is enough.
Read coding conventions on optional and mandatory data extraction.
All these regexes should be relaxed.
The third parameter of `_html_search_regex` is name but not ID.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Should not be fatal.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
Must not be `None`.
Move to initial title assignment.
`note` and `errnote` of `_download_json` instead.
Part after `\?` should be removed since it's not used anymore.
Must be extracted first.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
m3u8 is also available.
Lack of information is denoted by `None` not `0`.
Must be int.
This intermediate dict is completely pointless. Build formats directly.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
There is no need in named group when it's the only one.
Do not shadow existing variables.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Lack of information is denoted by `None` not `0`.
No need for escapes inside a brace group, all dots outside must be escaped.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
`int_or_none` for all int fields.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
Use `query` for query.
`acodec == 'none'`.
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
If both test the same extraction scenario leave only one.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
EntryId must be extracted the very first.
Don't capture groups if you are not going to use them.
Don't capture unused groups
No need to escape `/`.
Should not break if `published_at` is missing.
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Request wrapping code can be moved to the base class.
26-29, 32-37 code duplication.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
Move `_match_id` into `_extract_audio`.
should not return empty formats.
Do not shadow `url` variable. `fatal` has no effect when `default` is provided.
Code duplication at 58-60 and 68-70.
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
- - [ ]
No need to escape `#`. No need to capture groups you don't use.
`int_or_none` and `float_or_none` for all numeric fields.
Use regular string format syntax instead.
`id` by no means should be `None`.
Title is mandatory.
`/?` makes no sense at the end.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
Set empty `vcodec`.
make one of the tests an `only_matching` test.
Now you break extraction if any of these keys is missing.
Code duplication. `url` must not be `None`.
youtube-dl does not use non-browser user agent.
`error.get('code')`, `error.get('message')` may be `None`.
Breaks if no error key.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
230-264 no copy pastes.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
should not fail if the extraction of one set or item is not possible.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
I guess something like `base_url` is a better name than `url`.
I guess using `content_type` instead of `ct` improves readability.
`r'(?s)(<(?P<tag>video|audio)[^>]*>)(.*?)</(?P=tag)>'` is better as numbers may change in the future.
`media_attributes` is somewhat misleading. It's the whole tag but not just attributes.
`(?s)` has no effects in regular expressions without `.`
It's better to fail instead of fallback.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
`{}` won't work in python 2.6.
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
No need to escape `/`.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
Do not touch `only_matching` tests.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Either sloppy code or an anti-scraping measure.
Single loop for all sources without any unnecessary intermediates.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
`'id'` is required.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
`id` by no means should be `None`.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Title is mandatory.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
Playlist title is optional.
`_parse_json`. Read coding conventions.
Will never happen.
Never use bare except.
Breaks if no URLs extracted.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
OK. I'll open an issue for discussing this. For now you can remove this line.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
`'id'` is required.
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
Breaks. Read coding conventions.
Breaks if no `name`.
Read coding conventions on mandatory data.
Do not touch existing tests.
All these regexes should be relaxed.
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
This field is added automatically no need to add it by hand.
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Breaks on missing key, breaks on download failure.
As already said: no trailing $. Override `suitable`.
`urls` is pointless. Build `entries` straightaway.
All debug garbage must be removed.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
`autonumber` is not reset to zero in the first place.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Read coding conventions on optional/mandatory meta fields.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Do not escape `/`.
Read coding conventions on optional metadata.
this will be done for you by just providing `width` and `height`
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
The most simple way is to use `utils.extract_attributes`.
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
I'm not talking about capturing upload date. Do not capture AMPM.
Unless it supports videos from other sites, the IE name already says for what it is.
They are actually different :`MIGcBg` vs. `MIGmBg`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Don't do this manually use `_download_webpage_handle`
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Use `(?i)` in regex itself if you want case insensitivity.
Use `video_id`. Remove `.replace('\n', '')`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Breaks extraction if no `followBar`.
Strings in JSON may contain `<`.
Move flags into regex. Regex should match `runParams={`.
Use default. Read coding conventions and fix code.
Oh, I see. Thanks!
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
Audio must have proper `vcodec` set.
Doesn't work in python 2.6.
`default` implies non fatal.
Place right here in the following order: 1. If no title extract with `_og_search_title`. 2. If still no title extract from `<title>`. 3. Extract `playlist_description`.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
Also do not use double quotes for string literals.
Don't shadow outer names. No need for bracket when using single character.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
Only whitespace is allowed between `videoInfo` and `=`.
Do not remove existing tests.
1. This will never be reached. 2. Don't change the order.
All these regexes should be relaxed.
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
Weird. Yesterday it was working: ``` python -m youtube_dl http://mobile-ondemand.wdr.de/CMS2010/mdb/ondemand/weltweit/fsk0/75/752868/752868_8108527.mp4 [download] Destination: 8108527-752868.mp4 [download] 1.4% of 291.34MiB at 8.53MiB/s ETA 00:33 ERROR: Interrupted by user ``` But not now.
[Use `utils.qualities` instead](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/npo.py#L123).
still the check for element class is missing.
the `class` attribute of the `a` HTML element.
keep similar checks for element class and `get_element_by_class` value.
The third parameter of `_html_search_regex` is name but not ID.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Use `self._match_id` is better.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
I suggest extracting from `mediaLink` element instead of matching the JS URL as it may change in the future. For example: ``` Python media_link_obj = self._parse_json(self._html_search_regex( r'class="mediaLink\b[^"]*"[^>]+data-extension="([^"]+)"', webpage, 'media link'), display_id, transform_source=js_to_json) js_url = media_link_obj['mediaObj']['url'] ```
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
This will result in reference to unassigned variable when time fields are missing.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
Consistently use single quotes.
This breaks streaming to stdout.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
These are not used.
Instead of adding new parameters put them into `ctx`.
1. This must only take place when it's not available from player JSON. 2. Query must be passed as `query`.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Nothing changed. Also there is a video id available in JSON.
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
As already pointed out: you must delegate to `SBSIE` extractor not inherit from it.
Not acceptable. Search `url_result`.
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
Must not return `None`.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
`playlist_description` can be extracted from the same data.
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
This should be recursively delegated to pbs extractor instead.
Breaks when `player` is `False`.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
`show = data.get('show') or {}`
You must delegate with `url_result` instead.
Rename to something else.
No exact URLs here.
Do not capture groups you don't use.
Query to `query`.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
Move `_match_id` into `_extract_audio`.
Don't carry URLs. Read coding conventions.
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
All these regexes should be relaxed.
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
If `_search_regex` fails `None` will be passed to `_parse_json`.
Name an example URL where `og:description` has HTML tags.
`player_id` is not extracted in this fallback but used at 71.
No `ExtractorError` is raised here, `except` will never trigger.
Extraction should be tolerate to missing fields.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
Use display id.
This is checked by `_search_regex`.
`(byte_counter / rate_limit) - elapsed` sometimes takes negative values (tested on python2). Negative delay to `sleep` results in `IOError` and failed download. There should be at least a check for that.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
That's all correct code [53-93], do not remove it. Fix `add_m3u8_format` instead.
Duration calculation is incorrect.
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
Should not be fatal.
This is already fatal.
No such meta field.
Not used with formats.
This is superfluous since you provide `formats`.
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
no need to set `fatal=True`, this is the default.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
breaks the extraction if `clips` is `None`.
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
Do not remove the old pattern.
No escape for `/`.
Use `\s*` instead.
All formats should be extracted.
All these regexes should be relaxed.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Same here. And use `utils.int_or_none` instead.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Use `utils.xpath_text` instead, again with `fatal=False`.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
As already said: no trailing $. Override `suitable`.
All debug garbage must be removed.
Must be separate extractor.
It `md5` flag should not go here as an argument. Instead it should be extracted in particular downloader from `self.params`.
Calculate it in chunks after the final file is on disk. As said it makes no sense to calculate it immediately since the final file may be modified by postprocessor.
This is missing a `cwd=` spec at the latest. If we need a git revision number, we should really think about releasing more often instead.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Don't use print() directly as -j will be broken. Use to_screen or related functions instead.
```suggestion if not (season_id and video_id): ```
would still fail if `episodes` isn't available for a perticular `season`.
`playlist_description` can be extracted from the same data.
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
This is already fatal.
Should not be fatal.
Capturing empty URL is senseless.
No. Use fatal search regex instead.
No such meta field.
parentheses not needed.
fallback to other available values.
use single quotes consistently.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
`strip_or_none` no longer needed.
As already said: no trailing $. Override `suitable`.
Must be separate extractor.
All debug garbage must be removed.
Breaks on missing key, breaks on download failure.
`urls` is pointless. Build `entries` straightaway.
You have some unmerged lines here
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
There should be an `id` group.
Use `self._search_regex` and `utils.unified_strdate` instead.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
Use the original scheme and host.
Don't shadow built-in names.
Breaks on `None` title.
What's the point of this? Use `url` as base.
Don't capture groups you don't use.
Inline everything used only once.
Do not match by plain text.
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
I guess it a typo? now -> not
There's no need to `return` after `raise`.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
It won't be longer.
Revert. >Checking download_**url** video format **URL** makes even less sense.
`{}` won't work in python 2.6.
None of the optional fields should break extraction if missing.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
`utils.int_or_none` should be used instead.
Move it right after `title = info['title']`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
Plain `for` is enough.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
use the extension extracted from `determine_ext`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Code duplication at 58-60 and 68-70.
This check does not make any sense. If there are no formats extraction should stop immediately.
- - [ ]
Do not match exact URL.
No exact matches for URLs.
`width` and `height` instead.
Audio is not 128.
No such field.
No such field.
This should be split into building url and extracting formats.
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
This must be assert not exception.
Python 3.2 doesn't like u-literals.
Should not break if missing.
Optional fields should not break extraction if missing.
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Same for re.search
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
```suggestion new = '' ```
This code looks similar to `sd` format and can be extracted to a function.
Capturing empty string is senseless. `\n` instead of `root[__env]`.
Use raw strings.
Breaks if no videos in season.
Eliminate excessive verbosity.
`None` is not an id.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
yes, this is correct.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
`/?` is senseless at the end.
This branch is never reached.
There are also vtt subtitles available.
Check code with flake8.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
You have some unmerged lines here
``` for i, video_url in enumerate(video_urls): ```
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
It's not used inside FileDownloader.py so you probably don't need it.
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion new = '' ```
`compat_str()`, here and in l.96.
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
This line can just be removed.
170-172 - code duplication.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Don't shadow built-ins.
The whole code until here can be simplified to `page_id = self._match_id(url)`
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
fatal=True is default.
What's the point of this? Use `url` as base.
Query to `query`,
Use the original scheme and host.
Breaks on `None` title.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
1. Single quotes. 2. `expected`.
Move data and query into `_download_webpage` call.
`False` is not a valid note.
`expected_status` to `_download_json` instead.
Any test case using this approach? I can't find it.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Also pass `m3u8_id='hls'`.
Unite in single list comprehension.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
It should be robust in case of some missing fields.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
No bare except.
All methods only used once should be explicitly inlined.
`int_or_none` for all int fields.
It's better to use `self._download_webpage(url, video_id)`
The argument will already be a character string, no need to decode it.
This line is superfluous, the youtube-dl core can guess the extension better than that already.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Request wrapping code can be moved to the base class.
Allow arbitrary whitespace and both quote types.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
If `_search_regex` fails `None` will be passed to `_parse_json`.
Extracting duplicate code into a function obviously.
160 is a video format.
Everything except title and the actual video should be optional.
Raise `ExtractorError` instead.
This is not matched by `_VALID_URL`.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
It's already extracted as video_id.
Request wrapping code can be moved to the base class.
1. `v` may not be dict. 2. `v.get('slug')`.
As already said: parse as JSON not with regexes.
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
Don't change extractor name.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
Using preferences causes invalid sorting.
Inline to actual call place.
There should be spaces between `%`.
`.get()` idiom is used for optional fields only.
Carry long lines.
Better to use `determine_ext` instead of `.endswith`
`formats` is always a list of dictionaries.
Conversion between different date formats is redundant. Just return Unix timestamps.
Whats the point reconstructing the URL? You already have it in `url`.
It does not necessarily mean that.
DRY. url is mandatory.
Depends on what does it mean.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
This is pointless, you don't have any fallback.
`True` is default.
Query should be passed as `query`.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
You should not call this yourself instead you should define `_GEO_COUNTRIES`.
Playlist extraction should only take place after no video formats found.
This change breaks MTVServicesEmbeddedIE
cookie => cookies. There are 3 items.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
Extraction should not break if one of the formats is missing.
Use `_parse_json` instead.
You should use `self._request_webpage`, preferably with a HEAD request
this is basically the same code repeated twice. It can be generalized
Do not remove existing tests.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
Request wrapping code can be moved to the base class.
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
Change double quotes to single quotes
OK. How about creating one base class, define the outside functions there, and make derivative classes for playlists that return `playlist_result()`. This way you'll reduce the boilerplate code
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
Part after `\?` should be removed since it's not used anymore.
This should actually be just `self.url_result(embedded_url)`.
`'%s'` is very unlikely to be a helpful error message.
`'id'` is required.
Indenting is messed up here.
By the way, the pythonic way is to just evaluate `thumb_list`
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This is pointless.
Use `\s*` instead.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Don't capture unused groups./
Must be separate extractor.
Instead of such hacks you can name group differently and capture it without any issue.
All debug garbage must be removed.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
This is checked by `_search_regex`.
Use display id.
`default` is not used with `fatal`.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
This should be in `_real_initialize`. Same for all other occurrences.
You must provide account credentials/cookies for testing.
`acodec == 'none'`.
This should not be fatal.
There should be fallbacks for these values since they are more or less static.
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
will break if `item` is `None`.
I would prefer hiding all phantomjs related code in a separate wrapper class.
Parse from flashvars JSON.
Parse from flashvars JSON.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
This must be an id of the media.
Pattern should include `<iframe` part.
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
Also pass `video_id` since it's known beforehand.
Extraction should be tolerate to missing fields.
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
Sorry for the late response. This check may give a false alert if someday afreeca.tv decides to use a different name than `./track/flag`. `./track/video/file` is more reliable. The overall extraction workflow should be: 1. Check `./track/video/file` 2. Raise an error if no entries 3. Check other fields
If there's only one format, just use `'url'`.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
then I think it would be better to use `fatal=False` instead of `default=None`.
looking again at this, there are multiple problems with the `og:published_time`, the timezone is important to calculate the correct timestamp so the `og:published_time` value for embeds is malformed, and also there is a discrepancy between the values from the video page and the values from the embed page, so unless there is a way to determine the correct value, it might be better to drop it.
> so I guess I'll remove it at that's it. right? yes, it would be better to remove it.
don't use `modified_time` as publish timestamp.
don't use the image that has the play icon(`image_full_play`).
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
```suggestion 'language': compat_str(lang), ```
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
`only_once` parameter isn't in yt-dl, yet.
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
Again: it's not part of the video's title and must not be in the title.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
` - Servus TV` should not be in title.
Must only contain description.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
This does not match anything.
I've already pointed out: `.*$` is pointless at the end.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Capture as `id` obviously.
No need to escape forward slash. Should be `https?`. Part after `(?P<id>\d+)` should be optional since `id` is only used for extraction.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
The keys of the `dash_formats` dict are the `format_id` fields themselves, so you could directly `set(dash_formats)` or `set(dash_formats.keys())` to make it clearer.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Why did you remove this test? It does not work with your changes.
Don't touch the old test.
Must be separate extractor.
There is no need to test URLs.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
Again: you should not bother with that. It will be automatically extracted from the download URL.
1. This will duplicate source format when multiple non source formats are available. 2. Source format must be named `source`.
Just replace the part appended to non source formats with empty string.
This is pointless. You already have `source_url` as marker.
`'id'` is required.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
No need to escape `]` is character set.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
Query to `query`,
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
No need to escape `{}`.
Direct URLs should also be extracted.
should not return empty formats.
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
26-29, 32-37 code duplication.
`audioUrl` may be missing.
Such cases should be handled, too. I guess a possible approach is creating a table with common video and audio codecs. If given codecs are not on the table, fallback to `video,audio` order.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
And this as well.
`.*/?` is pointless at the end.
```suggestion get_element_by_class, int_or_none, ```
Don't capture groups if you are not going to use them.
Use `_request_webpage` instead.
This check is not necessary now as you test pathconf anyway.
Use `utils.get_filesystem_encoding` instead.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
All similar tests should be `only_matching`.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
Lack of data must be expressed by `None` not empty string.
Unite in single list comprehension.
Direct URLs should also be extracted.
Read coding conventions on mandatory data.
Breaks. Read coding conventions.
Breaks if no `name`.
Do not touch existing tests.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
Better to have some URLs in _TESTS with ```'only_matching': True``` for changes to _VALID_URL
create a seperate extractor.
It should not match `h|`.
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
`[]` is useless.
1. Breaks if div is not found. 2. `re.finall`.
Else branch is useless.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Use `\s*` instead.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
No need to escape `/`.
- - [ ]
`width` and `height` instead.
Use `\s*` instead.
Else branch is useless.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
All these fields should be `fatal=False`.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Must be `int`.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
Remove all unrelated changes.
```suggestion class NhkBaseIE(InfoExtractor): ```
_ is not a special character in Python's regular expressions. There's no need to escape it.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
`title` must be mandatory I've already told about this.
This regex does not make any sense.
Network connections in your browser.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Query should be passed as `query` parameter.
`objectID` does not match the id from `AmericasTestKitchenIE`.
Use `compat_urllib_parse_unquote_plus` instead.
Unnumbered placeholders are not supported in Python 2.6.
Avoid shadowing built-in names.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
It should match at least one character.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
`acodec == 'none'`.
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Move it right after `title = info['title']`.
`utils.int_or_none` should be used instead.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
no, as i said you would extract the metadata and return immediately.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
No `ExtractorError` is raised here, `except` will never trigger.
All formats should be extracted.
`self._search_regex` is enough here.
Matched data-video should not be empty.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
All these regexes should be relaxed.
If `_search_regex` fails `None` will be passed to `_parse_json`.
It should not. See the description of the field.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
This code looks similar to `sd` format and can be extracted to a function.
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
hls and rtmp are available as well.
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
do not capture groups that you're not using.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
230-264 no copy pastes.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
54-58, 71-75 code duplication.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
`None` is default.
Optional fields should not break extraction if missing.
JSON should be parsed as JSON.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
All formats should be extracted not only mp4.
could you add description from perex key
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
All methods only used once should be explicitly inlined.
This may change as well. Add a fallback that just processes all videos without differentiation.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
This code looks similar to `sd` format and can be extracted to a function.
**Do not remove** `_search_regex` part.
Move flags into regex.
I'm not talking about capturing upload date. Do not capture AMPM.
Remove debugging codes.
This should actually be just `self.url_result(embedded_url)`.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
You should **capture** error message and **output** it.
`default` implies non `fatal`.
Does not match https://narando.com/r/b2t4t789kxgy9g7ms4rwjvvw.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
No exact URLs here.
Don't capture unused groups
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
This must be checked **before** any processing.
This must be checked **before** any processing.
Also pass `m3u8_id='hls'`.
`ext` here makes no sense.
Correct field name is `format_id`.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
`.*` on both ends of regex make no sense. Also when capturing at least one character is required - there is no sense capturing empty video id.
...and output it here instead of const string `video_id` here that makes no sense.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
All formats should be extracted.
This is fatal.
What's the point of this base class? It's only inherited once.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
`update()` one dict with another instead.
it whould be better to iterate once and extract the needed information.
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
Breaks if no URLs extracted.
Escape dot. No need to split URL.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
both are know beforehand, so there is no need to use `urljoin`.
surround only the part that will threw the exception.
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
`break` as soon as the `links_data` has been obtained.
falback to a static URL.
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
Query to `query`.
should not return empty formats.
Do not capture groups you don't use.
`secondaryurl` is `False` if downloading fails.
`audioUrl` may be missing.
Let's do it, then, if you like.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
As in, me personally? I think I knew that, but protocol.
This is already imported and (in general) you should only use `import`s at the top level.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
resolution is defined as "Textual description of width and height". "medium" does not fit.
This should be a ```list```.
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Usually display_id is used before the actual video_id is extracted.
Prefer consistent using of single quotes when possible.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
Pass `default` to `_og_search_title` instead.
I guess it a typo? now -> not
Read coding conventions on optional and mandatory data extraction.
Course extraction must be in a separate extractor.
`/?` is senseless at the end.
I've already pointed out: this must be removed.
No, provide subtitles as URL.
Breaks. Read coding conventions.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
No, it's not. If video_id extraction from page fails whole extraction fails.
Not all URLs contain video id.
Should be `display_id`.
`default` implies non `fatal`.
You should **capture** error message and **output** it.
Does not match `var IDEC='`.
Also pass `video_id` since it's known beforehand.
Use bare `re.match`.
`--no-playlist` is not respected.
You can use self._download_json() here
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
Matching empty data is senseless.
Must not be `None`.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
This regex does not make any sense.
This may change as well. Add a fallback that just processes all videos without differentiation.
Must be `int`.
Use single quotes consistently.
No need to escape forward slash.
This will break the entire extraction if there is no match for some format.
Instead of `resolution` and `preference` it should be extracted as `height`.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Default part should be removed since there is no default.
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
Single quotes. `item` is already a string.
Request wrapping code can be moved to the base class.
Recursion should be replaced with plain loop.
From what I've seen there is always only one video on the page thus no need in playlist.
Course extraction must be in a separate extractor.
It worth adding a doc string.
This doc string does not match the function now.
It should always return a list.
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Fix test: ```suggestion 'ext': 'mp4', ```
No trailing `$`, override `suitable`.
Capture with /album. Capture non greedy.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
Dot is pointless here.
`{}` won't work in python 2.6.
We use single quotes as much as possible.
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
Capture as `id` obviously.
It's not an album id.
Not a video id.
Dot is pointless here.
Capture with /album. Capture non greedy.
`{}` won't work in python 2.6.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
None of the optional fields should break extraction if missing.
`vcodec` to 'none'.
Put `raise` after `if` section at the same indent.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
You should not silence all another exceptions but re-raise.
This line is unnecessary.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Original regexes should be tested first.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
Keep the old way also.
Code duplication at 58-60 and 68-70.
`parse_duration()` should work.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
Possibly referenced before assignment.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
is there a URL where the `duration` is available.
This should be removed.
Use `self.playlist_result` instead.
That should work. Open a new PR with two commits: - `[myspace] Use play_path for faster download` - `[myspace] Add extractor for albums` (it could be done in this PR but you would need to use `git rebase` and `git push --force`)
They are actually different :`MIGcBg` vs. `MIGmBg`
Can we resolve these IDs? There may also be a time encoded in there.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
230-264 no copy pastes.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
I've already pointed out - title **must be fatal**.
Pass as list of regexes, don't bulk.
`default` and `fatal` are not used together.
Must be fatal.
Must be fatal.
Breaks extraction if no `followBar`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
Use `video_id`. Remove `.replace('\n', '')`.
Strings in JSON may contain `<`.
I've already pointed out: use `float_or_none`.
`{}` does not work in python 2.6
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Of course when it appears inside `script`.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Instead of escaping the inner double quotes you could single-quote the string.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`skip_download` is needed for the test to pass similar to the first test.
Must be extracted first.
Should be non fatal.
Do not touch this.
Should not break if there is no `resolution` key.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Code duplication in 155-163 and 165-172.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
In general we use single quotes unless there's a good reason. (single quotes in strings, etc.)
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
`<span[^>]+class="name">...` is better.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
This is not true at the moment.
`title` is mandatory. Move flags into regex itself.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
Option should be called something like `--date-playlist-order` and should accept `asc`, `desc` or `none`. Code should process playlist according to it.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
Post-processors are already identified by `key` in API same should be used here.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
Invalid syntax at all.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
`for k, v in flashvars.items()`.
Must be numeric.
Use `\s*` instead.
All these fields should be `fatal=False`.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
Just remove the `try:` and `except:` lines.
`self._html_search_meta` is better here.
Code duplication should be eliminated.
Use `compat_urlparse.urljoin` instead.
This should be extracted right from `_VALID_URL`.
Why did you remove this test? It does not work with your changes.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
No such key `thumbnailUrl` possible in `info`.
Real id is in widget dict.
Should not break if there is no `resolution` key.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
`width` and `height` instead.
I suggest `default=video_id` in the `_html_search_regex` call.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
will easily match outside the element.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
No brackets needed.
This is no longer actual.
This is useless at the end.
Referring `url` from `url` looks like nonsense. Provide rationale.
Breaks. Read coding conventions.
Do not capture groups you don't use.
Groups around `video` and `sptv/spiegeltv` are superfluous.
only_matching, move to the end.
Don't capture groups you don't use.
Instead of such hacks you can name group differently and capture it without any issue.
Make it match URLs from the first post obviously.
This no longer matches http://v.youku.com/player.php/sid/XNDgyMDQ2NTQw/v.swf and http://player.youku.com/v_show/id_XMTc1ODE5Njcy.html.
Don't capture groups you don't use.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
Not having archive != having dummy `Archive` object.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
This doc should be updated.
I think you don't need `locals()` here.
1. `v` may not be dict. 2. `v.get('slug')`.
`video_data` is totally useless. Write directly to id variable when found.
Extraction should be tolerate to missing fields.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
All formats should be extracted.
Wrong fallback value in `thumbnail`, `description` and `creator`.
Optional data should not break extraction if missing. Read coding conventions.
try_get is pointless here.
There is no need in this method.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
Mandatory. Read coding conventions.
Id from URL is not always a video id. Correct id is in JSON.
No need to escape `{}`.
All these regexes should be relaxed.
This does not make any sense, you already have `url`.
It should be robust in case of some missing fields.
Matching empty data is senseless.
Must not be `None`.
Move to initial title assignment.
use single quotes consistently.
`id` by no means should be `None`.
Title is mandatory.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
There is no point in `or None` since `None` is already default.
`int_or_none` and `float_or_none` for all numeric fields.
Regex should be relaxed. Dots should be escaped.
Remove all debug output.
`video_id` may be `None`.
Doesn't work in python 2.6.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
`note` and `errnote` of `_download_json` instead.
This is fatal.
What's the point of lines 104-108? `ext` is already flv.
Extraction should be tolerate to missing fields.
All methods only used once should be explicitly inlined.
Breaks if no URLs extracted.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
JSON should be parsed as JSON.
Capturing empty URL is senseless.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
Just modify mimetype2ext rather than introducing hacks in individual extractors
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Don't capture unused groups
This is equivalent to InfoExtractor._match_id
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
Breaks if not a list.
Outer parentheses are not idiomatic in python.
You don't need list here. Just return it directly.
Should be tolerate to missing keys.
Iteration over dict not the hardcoded list.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
Should not break if there is no `resolution` key.
No escape for `/`.
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
`video_data` is totally useless. Write directly to id variable when found.
1. `v` may not be dict. 2. `v.get('slug')`.
1. Do not remove the old pattern. 2. Relax regex.
It's already extracted as video_id.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
`_match_id`. Do not shadow built-in names.
`True` is default.
Breaks if no videos in season.
default and fatal are not used together. `True` is default.
`if not content_url:`.
don't use both `fatal` and `default`.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
no, as i said you would extract the metadata and return immediately.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
This should be recursively delegated to pbs extractor instead.
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
check the existence of the `contentUrl` before adding the format.
same if one of the values is `None`.
this does not handle the case where `contentUrl` value is `None`.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
Using preferences causes invalid sorting.
That's completely different videos.
Ids must stay intact.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
Better to use integers for supported_resolutions and use str() here
Dots should be escaped
Use ```query``` parameter of ```_download_webpage``` instead.
Same issue for urlh
strip_jsonp should work here
Whether it has changed or not does not mean there should be a format with invalid URL.
Must not be `None`.
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
This is superfluous since you provide `formats`.
Well, the helper method can be smart about when to call `cleanHTML`
Same for re.search
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
This should be just `return info`
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
You should be able to replace those 2 lines with ```python3 subtitles[s['language']].extend({'url': s['url'], 'ext': s['category']} for s in ysubs) ``` . Untested but it should become a generator which gets implicitly iterated, which should be faster than appending 1 by 1.
Doh, somehow I didn't realize the ref to `s` didn't exist if switching to that. My bad. Will need to use PyCharm 1st ...
Is it really required for a single video URL? All tests you provided end up with empty `data` array and construct `playlist` it from [the original `url`](https://github.com/hlintala/youtube-dl/blob/yle/youtube_dl/extractor/yle.py#L109).
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
This breaks streaming to stdout.
Add a rationale for that.
These are not used.
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
This could be moved several lines up.
Oh, ok. That makes sense. Shoulda read the statement above it.
Here `if episode_json is False` can/should be replaced with `if not episode_json`, which is also the way you're doing it later.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
Must be `int`.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
Must be `int`.
Must not be `None`.
First group is superfluous.
This must be done right after title extraction.
This is never reachable.
Playlist title is optional.
This is default.
Carry long lines. Read coding conventions.
`{}` won't on python 2.6.
Read coding conventions on how mandatory data should be accessed.
Must not be fatal.
will be extracted from the URL.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
If ```options[0]``` _is_ ```{```, options should be returned.
1. ```options``` should be escaped - ```re.escape``` 2. Nested variables won't be detected. For example: (from https://developer.jwplayer.com/jw-player/docs/developer-guide/customization/configuration-reference/) ``` var config = { "playlist": [{ "file": "/assets/sintel.mp4", "image": "/assets/sintel.jpg", "title": "Sintel Trailer", "mediaid": "ddra573" },{ "file": "/assets/bigbuckbunny.mp4", "image": "/assets/bigbuckbunny.jpg", "title": "Big Buck Bunny Trailer", "mediaid": "ddrx3v2" }] }; ```
Sorry for previous noises. I just actually checked the HTML source of your example and found that this PR doesn't work as expected. ("resolving the variable to the JSON string") Try to print the value of ```mobj``` after this line.
Use `self._search_regex` and `utils.unified_strdate` instead.
This statement does not conform to PEP8.
Make `www\.` part optional.
Must only contain title.
Do not escape `/`.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
The playlists are public: there should be tests for them. Watch this space.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
`title` is mandatory. Move flags into regex itself.
parentheses not needed.
fallback to other available values.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
Empty string capture does not make any sense.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`_search_regex` is enough.
Empty string capture does not make any sense.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
This is not matched by `_VALID_URL`.
Everything except title and the actual video should be optional.
Upper case is idiomatic for constants.
Then just keep this code and > create default fallbacks if extraction of these fails
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
Breaks if not arr.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
No such meta field.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Playlist metadata must not be fatal.
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _ (underline) instead of webpage if the value is not used.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
Use ... instead of â¦ here. Non-ascii outputs may break random things.
All formats should be extracted.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
No need for this check, this is already checked in `_sort_formats`.
`'thumb`' may not be present producing invalid thumbnail url.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
use the extension extracted from `determine_ext`.
What's the point of this? `canonical_url` is the same as `url`.
"" -> ''
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
Alternatively you can just restore it after this PR is merged.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
I guess `filename` can also be included.
This will fail if neither mplayer nor mpv is available.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
All these import are unused, check your code with flake8.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
All formats should be extracted.
Use `_search_regex`, it reports an error message if the regex doesn't match.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Yes, it should accept any variation of whitespace.
Invalid arguments for 4-5.
Omit expected type.
`vardata` makes no sense.
Must not break extraction if missing.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
i'm not sure why you're adding this step.
then you should use `_remove_duplicate_formats` method.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
again, extract all formats, there is no guarentee that those formats are available in all videos.
Single quotes. `item` is already a string.
Recursion should be replaced with plain loop.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
`{}` doesn't work in python 2.6.
`xrange` is not defined in Python 3.x
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
Unnumbered placeholders are not supported in Python 2.6.
Use `compat_urllib_parse_unquote_plus` instead.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Avoid shadowing built-in names.
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
don't use both `fatal` and `default`.
no, as i said you would extract the metadata and return immediately.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Breaks when `player` is `False`.
Title is mandatory.
There is no point in `or None` since `None` is already default.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`int_or_none` and `float_or_none` for all numeric fields.
`id` by no means should be `None`.
