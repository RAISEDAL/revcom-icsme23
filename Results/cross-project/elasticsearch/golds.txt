This should accept XContentFilterBuilder, and then use toXContent to transform it to bytes.
Lets rename this to filter, and in all other places as well.
When we rename to filter instead of source, make sure we rename it here as well.
Right, then parsedFilter is also Nullable :), also, for completeness, you can mark the filter() and parsedFilter() methods as Nullable
We don't really need the seenAliases logic here. Its there for mappings since an index doc API can generate new mappings, and its not really the case for aliases.
Don't forget to close the parser here in the finally clause.
Use XBooleanFilter, optimized for different doc sets
We need to think about optimizing here for the most common case. Which is a single alias. In this case, we don't really need to allocate a list. Might be an overkill, but stil.. :)
Why not public? Will make reflection faster for guice.
The serialization here should use the versioning header to see if it needs to happen or not (see create/index for samples) so we maintain backward copm on the translog format.
I suggest trace logging here
trace logging here as well
We need to check here if `ttl` read from translog is lower than 0, if so, then we actually don't have a value...
You can't create the thread here, it should be created in `start` (you can't start again a closed thread). It should also be set to be daemon, and it should have a name as well. Check EstimatedTimeThread in ThreadPool for an example.
Small optimization: Use `UidFieldMapper.TERM_FACTORY.createTerm`
When getting a `searcher`, you need to keep it around, and then call `release` on it on the `finally` clause.
This default of 1 second is too low, it should be 60 seconds by default.
Once we have a `running` flag, we can check on it and not on `isAlive`.
I suggest adding a volatile flag called `running`, add a method called `stop` that sets it to `false` and interrupts the thread.
We should also have a different `valueForSearch` only for this case, so we won't need to create a Field just to get the value... .
I think we can go with a higher bulk size, because we only do deletes. Something like 5000 or even 10000.
do you think it makes sense to support the other options as URI parameters as well? will simplify curl executions.
we should support the preference and parent flags similar to the get API.
need to wrap in try ... finally block and close the parser
the printStackTrace should go away here
This iteration over the leaves can be replaced with `Versions.DocIdAndVersion docIdAndVersion = Versions.loadDocIdAndVersion(topLevelReader, uidTerm);`
I think we are still missing preference? Should be similar to the get API.
Can we have `field_statistics` as an XContentStringBuilderString as well? if it applies to other places, would be great
I think we can just pass the bytes directly to the json generator it will do the right thing with it.
should we use a native trove collection here from String to long
Not sure if we need to do that it's just one entry per field though.
can you remove the pringStackTrace
see below.. remove the catch block
I am not a huge fan of base64 but I guess you are right.
isCreated instead of getCreated
formatting - 1 line instead of 2
formatting, 1 line instead of 2
formatting, 1 line instead of 2
doesn't this need to be set somewhere? i.e. back to the to the geohash. Also, I think that the GeoPoint itself needs to be used as a varialbe and not the geohash, since generating the geohash will be based on the precision specified? For example, someone might define a very high precision, and then the geohash method here uses the default precision which might not be good enough... I think that we can simply call GeoPoint.parse(parsers) without the check on VALUE_STRING, and then use the GeoPoint to generate the has based on the precision later outside of the parsing loop.
should be #clearAndRelease() instead of #release()
I think this is fine.
Maybe use Object float? We do the same thing on SearchContext
Instead of having a reserved null value (-1 in this case), use an Object float for `minScore` in ShardTermsByQueryRequest and TermsByQueryRequest. Then we can just do a null check: ``` java if (request.minScore() != null) { ```
can we use "script_factor" I think it's nicer than the came case
I think we should use this notation `[0..1]`
can we have braces around that ie: ``` if (newQ == subQuery) { return this; } ```
Can we get away with RAMDirectory or something like that? it's a shame to have to create something on disk. Especially if this test is running on some server somewhere in a build system server.
Move to the new randomized testing.
Change to Throwable.
Move to the new randomized testing. Important for reproducibility
Maybe call the validate method on the TermVectorRequest? it will check for things we check here and more... It is also more future proof.
you could remove the null check by changing the second check to `Defaults.NAME.equals(parser.currentName())`
I think the first check should be: ``` Java if (parser.currentName() != null && parser.currentName().equals(Defaults.NAME)) { if (parser.currentToken().isValue() == false) { throw new MapperParsingException("Expected a value as Content id but got " + parser.currentToken()); } /.... } ``` that way we also fail for arrays etc.
Not too happy with the name `no_results`, but can't come up with a better one :)
Yea, I can see that... . The problem though is that it might create a lot of warn messages... . People can always decide to dynamically enable debug logging here to see whats going on. We do not allocate shards on other deciders, and never warn because of it... One of the things we were thinking about is that potentially part of the reroute call, we will give back a full explanation on what was allocated and what was not, so people can see it as part of an API call about why things were rejected.
I don't think this should be warn, its ok not to allow, but is it really a warning? I think debug should be good? Otherwise, our logs will be filled with it with each reroute and each check for shard on that node per reroute. That warn note also applies to ones below.
the stats request should execute only on data nodes? in that case, we can pass in the list of nodes ids: `data:true` and it will only execute on data nodes.
Can we use `in.getVersion()` to check in the read (and write) for serializing this as a single string for old versions, and array for new ones? Then we can backaport it to 0.90.
sorry, missed the loop
Looks like you fixed a bug here!
Looks like the '--' is not necessary since the variable is not used anymore later on.
Why? We don't typically have a method name starting with upper case...
I think we can add here the loading for the simple id cache, if that is used. (gettable from: indexService.cache().idCache())
maybe use `atLeast(250)`
This can be private to the Try context and final - no need to declare it outside
can you do that with a try / finally block? this throws an Exception!
As Boa mentioned before we would need both a default to print out, and a boolean that tells whether the current value is default or not, as the `currentValue != defaultValue` is not enough. Something like the following should help in most cases I think? ``` public static void maybeAdd(XContentBuilder builder, String key, Object value, Object defValue, boolean isDefault, boolean includeDefault) { if (value != null || !isDefault) { builder.field(key, value); } else if (includeDefault) { builder.field(key, defValue); } } ``` That said, maybe it doesn't cover 100% but 90% of the cases, and for the 10% left we can still have the custom if? In my opinion it doesn't need to be perfect but still better than copy pasting that `if` so many times.
/me is wondering why this one didn't get an @Override annotation while other methods did, is it Intellij's fault? :)
We will need an AssertingBytesValues. :-) (Can be done in another commit)
Instead of exposing it as a shared scratch, maybe it could be exposed as the current value? Meaning that instead of eg. doing ``` java BytesRef scratch = bytesValues.getSharedScratch(); bytesValues.setDocId(42); int hash = bytesValues.nextValueHashed(scratch); // the value is now in scratch ``` Consumers would do ``` java bytesValues.setDocId(42); int nextHash = bytesValues.nextValueHashed(); BytesRef value = bytesValues.current(); ``` (The name is maybe not the best candidate, but the idea is to do something like `TermsEnum.term()` or `DocIdSetIterator.docId()`.)
The main advantage to me is that implementers of nextValueHashed wouldn't have to be concerned anymore about whether they can fill the bytes or are just allowed to change the pointers, since they would know for a fact they are always operating on a private BytesRef instance, they are allowed to do whatever they want to.
for readability I'd use this. as well
can we use a limit variable that looks like `int limit = indexMetaData.numberOfReplicas() + 1; // one for the primary` and then bound the loop with `<` instead of `<=` it took me a while :)
really this would look nicer if we counld just do: ``` indexShardRepository.lock() try { .... } finally { indexShardRepository.unlock() } ```
any change we can use `URI` here instead? `URL` is pretty tricky though has odd equals impls etc.
I'd really like to see this to be positive logic. ie. x || y || ... this is much easier to read and explicit since in the current case I don't the the invalid states
`numberOfFiles++` can be moved out? minor nit pick but it reads easier IMO
maybe just `return totalSize == fileInfo.length();`
maybe just `return blobMetaData.length() == fileInfo.length();`
err I guess you need to have failures added first so second...
I'd feel better if the `latch.countDown()` would be the first line in the catch block
maybe s/INDEX/COPY and s/NONE/NOT_STARTED to be consistent you should also s/FAILURE/FAILED
ok fair enough
I really don't think we should add more of those double negations is should `CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED`
so to me the biggest issue is that they tend to cause NPEs so I kind of like to have a sentinel rather than null where we check instance == SENTINEL instead of instance == null. Yet, this might come from my background and where safety was king :)
URL should never used IMO you should always use URI. Generally, if the URL describes both the location and name of a resource, the term to use is URI. URI also doesn't take care of encoding of the characters which might come in handy if there are spaces in the path etc. Yet unless you need to have a URL I'd always use URI (ie. if you need to have support for credentials which I'd discourage anyways here)
trash the @param and @return
given that this is a private class we can just make all the members package private to prevent syntetic access and remove the getters? Prevents unnecessary code IMO - I hate that java has no structs yet :)
hmm this has an empty impl? Not sure if we need the `Injectors.close()` if we need it, it should deal with null values!
I think we have this logic in the settings already maybe we can factor it out and reuse? seems scary and it would be good to be consistent.
can cause and name be final
just for kicks should we `assert currentGen >= 0;`
Gotcha, thanks for the explanation!
ok cool then we need to fix this place? https://github.com/elasticsearch/elasticsearch/pull/3953/files#diff-79371c2235df5580ddc99db30932bea5R89
should the countDown be called anyway? this logic seems odd
no file? maybe IOException
you can just use IOUtils here too it accepts null etc. and ignores the IOException if you do closeWhileCatchi...
yeah use READONCE
I guess that is ok! I think you can also add SnapshotInfo.readSnapshotInfoOptional(in) ? I really don't care if it's public to be honest!
this should not be `AnalyzerScope.INDEX`, cause its cached on the enum level, so the scope should probably `AnalyzerScope.INDICES`. I would simply use the scope provided in the constructor (which will be the INDICES one).
You don't need these `text-align`s. The one in the header is enough.
Consider checking for `null` somewhere for `primarySize` before the division below.
Let's add a `"text-align:right;"` to these last three numeric fields.
This is great @s1monw ! Unfortunately it covers only the cases where we submit the task in the transport action, using directly the Java API request class (extends AcknowledgedRequest). In a lot of cases we use a MetaDataService and we translate the request to the internal one (just a POJO, no support for serialization) which extends ClusterStateUpdateRequest. If we either always use a metadataservice or never use it, we can apply this approach to all cluster state updates that support ack. I vote for the internal layer that uses the metadataservice all the time (`<T extends ClusterStateUpdateRequest, R extends ClusterStateUpdateResponse>`)
Maybe use a simple for loop here like: ``` Java if (!contextMapping.isEmpty()) { builder.startArray(Fields.CONTEXT); for (ContextMapping c : contextMapping) { builder.value(c); } builder.endArray(); } ```
Maybe use a simple for loop here like: ``` Java if (!contextMapping.isEmpty()) { builder.startArray(Fields.CONTEXT); for (ContextMapping c : contextMapping) { builder.value(c); } builder.endArray(); } ```
this one as well.
this should throw `UnsupportedOperationException`
`this()` is obsolete
hmm we have 4 spaces indent
Based on our last conversation we still use `|` I thought we wanted to go to a less prominent char like `\u001F`
I think `analyzingSuggestHolder` should have a getter for `preserveSep`
so I think we need to somehow extend the `fuzzyPrefixLength` with the length of our context otherwise we will apply LD to the prefix as well? Also we need a test for this I guess
rather use `logger.debug` than `System.out`
this is really just nitpicking but I don't undestand the generics here... Can this just be ``` public static final Collection<String> addNeighbors(String geoHash, Collection<String> neighbors) { ... } ```
maybe also use `Lists.newArrayListWithExpectedSize(4);` for consistency
if think you can throw a hard exception here ie. `ElasticsearchIllegalArgumentException`
guava has a `Iterables.elementsEqual()` - which works slightly different, maybe your implementation is a bit faster
lower cased now...
so I think the issue here is that `\u001F` is a reserved char in the suggester (we move to it lately). What about `\u001D` instead? (http://www.fileformat.info/info/unicode/char/001d/index.htm) once you do this you need to add it to `CompletionFieldMapper#isReservedChar(char character)`
I think we can just do `prefix != null` here
I think that license header is broken - check the other for details... if you have java 7 running you can run `mvn license:check` it will tell you were you have problems
this header is broken too
maybe make if final
this entire thing looks much better now! cool stuff
where is this method used? I can't find it
lower cased exception name
code style nitpicking here: do we usually do a space after the if? if so, there have to be several corrections in the next lines..
must be param prefixes now
I am not sure if we should keep this analyzer - I think we can just use the `PrefixTokenFilter` by itself and move it somewhere in the completion namespace. Yet I think we should have some dedicated tests for this that use `ElasticsearchTokenStreamTestCase` there are some awesome helper methods that can find a lot of stuff that is not correctly set in the `TokenStream`.
ah cool I like that!
not a big deal but maybe phrase it `remove() is not supported on GeoHashPathIterator`
please make either this class or `resetFromString` final otherwise this might have odd side-effects if somebody subclasses this.
can you put a // FLORIAN EDIT here as well please
The problem with adding this is that the idea of the non sniff transport client option is to use the address of the listed node to talk to the cluster, and not the (publish) address of the node within the cluster. What we can do is create a new `DiscoveryNode` instance, that uses the transport address from the listed node, and the rest of the data can come from the node in the nodes info. Note, if we do that, then we need to use regular `connectToNode` above....
I'm not sure about this test. I think it's confusing if ``` { "obj" : { "f": 1 } } ``` returns `{}` but ``` { "obj" : { } } ``` returns `{ "obj": {}}`
Btw, you don't necessarily need to use a jsonBuilder here, you can just do `setSource("text","parent")`
Also, you dont necessarily have to change this but you can now replace `.execute.actionGet();` with just `.get();`
Indeed, it happens because all shards fail. I guess you want to verify that a normal query would fail but since we parse it only for the right indices we don't get back the failure using indices filter/query. I would then try and catch the exception that's thrown.
The way I see it we call `concreteIndices` as soon as we found `indices` (or `index`), and at the same time we could verify if those indices match the current one and store the result, no need to call `matchesIndices` in multiple places.
This line is called multiple times as we keep adding indices. It could be called once at the end of the loop I guess.
you can replace both lines (check for failed shards and total hits) with: `assertHitCount(response, 2l);`. I would it everywhere in new tests as it makes them less verbose.
if you use here `refresh();` from the base class we also make sure we get back no failures.
can you name this label differently we already have a variable that is named `res`
I think this would read much simpler if you would do `if (it != null)`
can this be `else if (res == null) {...`
I really don't think we should put methods that have side-effects into a short circuit logic. Can we use a traditional if statement here like ``` if ( docIdSetIterator.docID() == target) { return true; } else { return docIdSetIterator.advance(target) == target; } ```
we should have the same logic as DoubleFieldMapper#parseValue. Maybe have a NumberFieldMapper#parseDoubleValue, that the double field mapper can call as well.
Same as the double note above, just for long.
this feels too magical? I would go with if the value is `-1`, then call the optimal, otherwise, expect the value to be the number of hash functions, and throw a failure if out of range.
I would debug log it, not warn, this is not always caused by user permission (getting null when listing files)
I think all three implementations of `PreBuiltCache` could potentially be `private`
I would consider reducing the visibility of all these details and methods. I think most of them only need package visibility. Some of them might be needed only in tests, which most likely means that the tests are in the wrong package :)
after the change to `PreBuiltCacheFactory.getCache` you can remove `<Analyzer>` from here, it's redundant.
This should be `public static <T> PreBuiltCache<T> getCache(CachingStrategy cachingStrategy)`. As a result a few warnings on the caller methods are gone.
same as above here as well
yeah with this you can just go an push it IMO
this on should restore the interrupt status
Thanks for removing this. I had hesitated to do it when importing the fixed CSQ.
strike that, it looks good, confused by the changes view on github...
cache flag might come after / before execution, so you might be setting the flag when it has been set by the user..., I would check it after the parsing
make sure you fix the codestyle here
so if the manager would be folded into this class we could make this just one call? I think we should do it and fold it all into this class
ok fair enough!
well if you put in into an `assert` it's not called in production that is the purpose of `assertions`? I think we should call it at the bottom of the constructor and maybe in `iterator()` as well as `shards()`
I usually have one assert statement per comparison in here and just return `true` on the bottom. Easier to figure out what's going on. ;)
In general can you please make sure you are not using your own code style. Please adopt to the codestyle which is mainly default SUN except of 4 spaces indent.
same here this is a left over!
I think for now that is fine but I'd really like to move move the mutator out of this class somehow. I also think we should remove the `#nodes()` getter altogether since the class already implements `Iterable<MSR>`. but for now that is find. Maybe add a TODO here
I think we should make all of the mutating methods in here package private to be consistent like `moveToPrimary` etc.
if it is unused remove it!
no matter what I really want to have the manger go away here. I think it bloats the code we can just fold it in.
yeah make it an assert
I like these simplifications ;)
a general remark. I'd like to have a method like: `private boolean assertShardStats()` that calculates all the statistics from the shards we have to make sure they are identical. We can then just use this method in statements like `assert assertShardStats()` to make sure the tests fail if we miss something!
can we maybe make this an empty list instead. Unless this has a special meaning I'd like to prevent `null` invariants
please add `{}` even if it is only one line
please add `{}` around this
here is one more code style problem :)
move the else if one line above
please put the else on the same line as the closing `}`
please fix the codestyle here! we don't have spaces before and after the braces.
please put a TODO here that I generalize this class and add it as a util! :)
I guess it would be better to have a random unicode string here instead of the English? We don't need hits here really
Awesome! This is what I was missing! :+1: :heart:
I've never really liked references to github issues. I'm of the mind that if the test is well enough named there isn't really a point and if you really need to trace it back to an issue you can `git blame` the file.
Could we have an assertion that the Lucene version is 4.6 so that we don't forget to remove this code when Lucene gets fixed? Something like `assert Lucene.VERSION == Version.LUCENE_46 : "LUCENE-5361"`
Impl. `Closeable` here as well then you can use `IOUtils` from lucene to supress exceptions etc.
remove the super call
maybe just `esVersion()`
maybe: ``` Java for (int i = 0; i < params.length; i++) { paramsMap.put(params[i++], params[i}); } ```
remove the super call
maybe just use `IOUtils` here they handle null values as well
should be removed, `PluginInfo#DEFAULT_VERSION` should be made public and used here instead
just use `IOUtils.closeWhileHandlingException(is)` instead of the 6 lines in the finally block
ah I mean't Throwable.... sorry
add a a `logger.isTraceEnabled()` check
please reformat to: ``` java if (logger.isTraceEnabled()) { logger.trace("adding jvm plugin [{}]", plugin.v1()); } ```
please reformat to: ``` java if (logger.isTraceEnabled()) { logger.trace("adding site plugin [{}]", plugin.v1()); } ```
the exception is completely swallowed... we should at least log something
Using `ElasticsearchIllegalArgumentException` instead would help set the status code of the response to "bad request" instead of "internal error".
If users start using eg. "content" because it works and if we want to use the "content" parameter in the future for another use, it will feel like a backward compatibility break to those users. So I would rather throw an exception.
I'd move this to the end of InternalIndexShard constructor. This would help keep the calls next to the places where the state variable is actually change.
minor detail: I think currentState instead of newState will better communicate that the state was already applied.
I'm on the fence regarding this one - on one hand it's a public api and people can do whatever with it so you don't want them to slow down the release of the lock. On the other hand as an API it is really weird to have `POST_RECOVERY` come in after `STARTED` (which may happen if the shard is allocated on master - though the chance is still very small)... will think more.
Fair enough, good to know.
My personal preference is `<object>`, `<metadata>`, so I'd prefer the IndexShard first and the state second, but that's just my opinion :)
Why not use the dedicated get aliases api? IndicesAdminClient#getAliases()
cool stuff - I wonder if we should only add fields to the multimap that are explicitly added like via `add(IndexableField field, String key)` for the most of the field this is not needed at all though.
I like the fact that any `AbstractBigArray` can use differently typed pages! that's pretty cool though!
Okay, I am going to commit #4613 for now and I will update it to use this with #4616 once this has been merged.
`o1 +=8;` <== format this file again :)
or rather the `Settings#getAsMemory` you added...
do we have plans for a better hash function? I could think of a couple of ways to make this work ;)
I guess you should rebase since I added a section that sometimes uses the actual module not the mock one...
I try to use the ES exceptions since they have: ``` public RestStatus status(); ```
I added this here https://github.com/elasticsearch/elasticsearch/commit/602c63d2aa3936a766e4e3432721812096ed54f5
cool thanks :)
maybe implement `Releasable`
can we use the ThreadPool#info API here, so we don't have to cast to `ThreadPoolExecutor`, and be able to get the max back through the info class.
just a style question but this loop looks more like a `do/while` would be easier to read IMO
can this be called `assertConsistent` makes it clearer that it is used for assertions
`try / finally`
there is a bunch of duplication here so I guess we could at least put the `recycler != null` case in a a single method like ``` Java private <T> T registerNewPage(Recycler.V<T> v, int expectedSize) { cache = grow(cache, page + 1); assert cache[page] == null; cache[page] = v; assert v.v().length == expectedSize return v.v(); } ```
also, the setting should be using `componentSettings.get("page.limit", ...)`, so it will resolve to `cache.recycler.page.limit` (and I think the in_bytes usage here is not needed).
this setting should probable `componentSettings.get("page.type", ..)`, which resolves to the full setting of `cache.recycler.page.type`.
not related to your patch but I think `autoBoost` and `similarity` are missing here
I guess we should be consistent and use top/bottom across the board for now!!
that @return statement seems wrong now
so this means we don't support `topLeft` anymore? I think we have to to be honest. Also we have to support `northWest`
can we have constants for these instead of building them dynamically. I am not sure if it makes a difference perf wise but I think it's more consistent with all the other parsers.
this is wrong we don't support `topleft` we support `topLeft`
we should never have an iterable here, right? it should all be "raw" values, I think calling extractRawValues in fetchPhase when from source will fix it.
nice all the branches are gone!!
if we'd make the `2` a double I would sleep much better at night ;) same below :)
here is a space missing before `EMPTY_FLAGS`
space missing before `"max_input_len"`
I think a better way to do it would be: ``` if (success) { IOUtils.close(is, os); } else { IOUtils.closeWhileHandlingException(is, os); if (dest != null && dest.exists()) { dest.delete(); } } ```
Not related to your commit but I think we should return false immediately in case of an InterruptedException.
You can do IOUtils.closeWhileHandlingException to close these streams only if they are not null and silently
would be nice to allow to configure it to a percentage of the heap size
should also be `trackScores`...
Right, the rest code only produces an empty array.
we try to be practical when it comes to code style... some things we're strict about, other less. I think with today's idea's the import styles are quite meaningless (I personally have time folded all the time and never find the need to look at them), so whether one uses \* or use explicit imports... I think it's really not a big deal (I think I personally left the default intellij behaviour where imports are collapsed above a certain threshold)
I've noticed these flip flopping some. I think someone had "use star if >3 imports per package" and I ended up setting that. I see you do the more normal "never use star". This isn't a big deal and I dunno if it is worth having a standard. The only real way to enforce these standards is with something like checkstyle because reviewers forget.
When I first started contributing that was something I thought about. I tried to puzzle out what setting would cause the least churn and still let Eclipse handle all the imports for me. I set the setting and then pretty much ignored it.
I ran it and it took ~2-4 seconds, I was checking to see whether the `@Slow` tag was needed :)
can we remove this one, so it won't be called by mistake? I think only the benchmark calls it, in which case, the benchmark can call the one with the concurrencyLevel parameter using the available processors
I think we can now revert this change, `for` instead of `while`, 2 lines less? ;)
Looks good now!
Looking at my previous explanation, it wasn't that clear :) When I said: save whether we found at least one _meta not found and schedule a retry at the end of the loop. I wanted to say: save whether we _didn't find_ at least one `_meta` not found and schedule a retry at the end of the loop. Sorry about that. The difference is that if you don't schedule a retry when you found at least a `_meta`, you might be loosing other rivers I'm afraid? There might be some that need a retry? But wait before changing this, I want to have somebody else looking at this too and make sure this is correct :)
The fix apparently works, but what the diff here doesn't clearly show is that it relies on the order of the types. Imagine the case where we get three types for the `_river` index e.g. `_default_`, `dummy_river` and `atype`. Even though the _meta doc for the dummy river is found (second one), the _meta doc for `atype` (last one) is not found, thus a retry is scheduled, but more importantly the `currentState` is returned (without the dummy river!). That return (within the `if !getResponse.isExists()` block) needs to be removed.
My bad, looking again at it, I think it is not needed due to the way the method is called, always submitting a state update task, which makes sure there are no concurrent calls.
Just about code style, I would prefer ``` if (iterator.hasNext()) { continue; } ``` rather than the short form. But I would indeed keep the `continue` to avoid starting one retry thread per type not found. One is enough.
why is this method synced? I don't see a reason though...
Oh I see, the +64 was specifically in case of wrapping buffered IndexOutputs...
can't we do this here: ``` Java void ensureNumberConversion(boolean coerce, long result) { if (!coerce) { double fullVal = doDoubleValue(); if (result != fullVal) { handleNumberConversionError(fullVal, result); } } } ```
space missing before `Explicit`
space missing before `Explicit<Boolean> coerce`
space missing before `new Named...`
space missing before `new Named...`
can you format this, there is a space missing after `=`
can we make this a method somewhere ie. ``` Java private static void checkCoerce(boolean coerce, Class<? extends Number> clazz) { if (!coerce) { throw new ElasticsearchIllegalArgumentException(clazz.getSimpleName() + " value passed as string"); } } ```
I also think we can add a method that checks for precision to simplify this! please
here is a space missing before `coerce(context`
here is a space missing before `Explicit`
here is a space missing before `postingsProvider`
here is a space missing before `new NamedAnalyzer`
here is a space missing before `new NamedAnalyzert`
one line too much
these replacements seem to be wrong the if / else logic is obsolete now
can `aliases` be final as well
if you make the `AliasActions` class static you can remove this odd looking `request.new` :)
please replace `assert false` with `fail()` if we run without assertions this test fails :)
please add one space after `,`
this is a java 7 feature you need to do `new String[] {alias};`
I think there is a missunderstanding here. this line fails by default if you have a destructive operation like a wildcard. But you can change this via a settiing in Realtime. this is new in 1.0 and we should keep it. also the test seems to be important
maybe add this to the `Strings` class like: ``` Java public boolean hasEmptyString(String[] array) { //... } ```
we have `Strings#EMPTY_ARRAY` for this but IMO we should keep the `null` and validate it. The `null` is very likely a bug and we should fail fast in that case" - afaik britta already added that
should be action
should be aliases
parameters should be indices and aliases
The multi-plural-ness of `aliasActions` is kind of confusing. For example, we have `List<AliasActions> aliasActions`, which is actually a list of multiple alias actions, I think it would be clearer to rename the main list to `allAliasActions` to signify it's a plural set of plural operations. Without it I read this and my first instinct is "Oh, that should be `aliasActions.addAll(readAliasActions(in))`" (which I know isn't correct) because of the naming.
Maybe replace these with writeOptionalString as well while you are at it
we now have `CollectionsUtils#isEmtpy(Object[])` might be interesting to use here
It was deprecated prematurely. It has functionality that is used in Kibana and which we need to replace before we can deprecate or change this.
cool looks good!
what happens if there is no mapper? I would tend to simply ignore (to match the logic we have for stored fields now)
I suggest we remove _source. handling as well..., including in get.
don't ignore it? Just call `parser.text()` without the if, should work.
just nitpicking, maybe we could change the name of the test method that's left, or even remove it? I don't think we need to explicitly test the serialization here anymore...
Should it be `setRescorer(Rescorer...)` instead of set/add/clear? (just wondering)
ok let's move on I don't know if I like it it seems not consistently used across the board.
why do we return `this` here this is not a builder and I don't think we need to do this though!
use `assertNoShardFailures` here please
use `assertNoShardFailures` here please
yeah that is odd - I will try to investigate!
this is the reason; https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java#L127 I guess that's fine here we don't support DFS for rescores
oh nevermind I just saw that this is the object path
do we also support `copyTo` I guess not
yeah that is what I figured!
I wonder if we should barf already if the field is not mapped or automatically create it in the mapping? I don't think we should throw and exception at this point to be honest!
can we change the method name to `valueForKey(long key)` (better indicates that the passed in value is the key... and differentiates it form `round(long value)`
can we maybe only enter this logic if the if the list is not empty? ``` JAVA if (!entries.isEmpty()) { // do the magic } return 1.0f; ```
try to use `AbstractRestResponseActionListener` instead of `ActionListener` the `onFailure` method is already implemented there
Nitpicking but I wonder if we should just use the File's constructor for this use case: http://docs.oracle.com/javase/7/docs/api/java/io/File.html#File(java.io.File, java.lang.String)
I would also test the direct retrieval of a file (not relevant to this change but we miss that one now and it will be good to have)
watch out for manual path concatenations - / doesn't work on windows :)
I would check wether the request file as a directory before going into this mode
So maybe it would be easier to just implement `AtomicFieldData` here (not AtomicFieldData.WithOrdinals)? (however `getBytesValues(type)` would still return BytesValues.WithOrdinals)
this license change looks wrong
You need to use `BytesValues.scratch` instead of allocating a spare here in order for `copyShared` or `currentValueHash` to work correctly.
I think you also need to implement `getOrder()` here in order to return `AtomicFieldData.Order.NONE`.
maybe you can just delegate to `read(byte[],int,int)`
I meant something like this: ``` long pause = rateLimiter.pause(1); if (pause > 0) { listerner.onPause(pause); } ```
formatting should be fixed like the rest in these three lines
maybe return to the two allowed values here to give the user a hint
Not sure here... some of these options require the related plugin to be installed.
YEAH so I'd put an assert in there just for kicks
do we wanna call it `regex` or `regexp` I don't care too much...
Similar concern wrt `useClassicScroll` and `executeQueryPhase`
maybe we should have a helper method for this? this looks used in several places
Maybe it would be simpler by iterating forward as this would remove the need for a null check: ``` for (int i = 0; i < sortedShardList.length; ++i) { ScoreDoc scoreDoc = sortedShardList[i]; lastEmittedDocPerShard[scoreDoc.shardIndex] = scoreDoc; } ```
we never released that so I think we should really remove that bw compat!
Makes sense. Thanks for the explanation!
Should be `for (JvmPlugin plugin : jvmPlugins)`
can this be ``` if (pluginClass.getName().equals(plugin)) { luceneVersion = pluginProps.getProperty("lucene"); break; } logger.debug("skipping [{}]", pluginUrl); ``` I think taht is more clear
that is unnecessary - print that automcatically taht we start a new test
hmm you might want to be able to return a subclass of `T` even if T is an interface? `Class#isInstance` is not symmetric to `Class#isAssignableFrom` which also works for interfaces. For instance if I pass `Comparable` as `T` it won't work even if I can cast the class into `Comparable`
license header is broken here
can you make the signature like this ``` Java pubilc <T> T parseExternalValue(Class<T> clazz) ```
it's only as a safe guard for the future, or a warning for someone using totally the wrong classes. I think it's OK in these cases to fail the entire request? (unlink non-programmatic errors)
Nit picking: indices is only used in the creation test, maybe move it's building to be under the if with autoCreateIndex.needToCheck? (slightly faster execution when auto creation is disabled)
can you explain why we do this now only if `autoCreateIndex.needToCheck()`
I wonder if it could/should be adapted to work for fuzzy queries too. Conceptually it is the same thing - given a user's vague input you try a number of variations on their behalf and focus in on the most likely interpretation. With multi-match you vary the choice of field context and with fuzzy you vary the choice of word spelling but the objective is the same - how do I use the index's term stats to select the most likely interpretation of user input? Lucene's FuzzyQuery has been broken for many years because of the IDF bias. I tried to fix that in FuzzyLikeThisQuery but I think you have it right here in rewarding all variants based on max DF rather than what I did with averaged DF - you pick the reward based on the most likely interpretation. Here we also added a slight tie-breaker bias to improve focus. Admittedly "fuzzy" brings edit distance into the ranking equation but I feel it should share the same IDF blending policy implemented here.
Sure, I was just thinking out loud
How about this existing one? https://github.com/elasticsearch/elasticsearch/issues/3125
we indeed need to fix this **and** `TermsParser`, `scriptValuesUnique` needs to be supported
hmm general question, why do we not use `"script_values_unique".equalsIgnoreCase(currentFieldName)`
I am still not happy at all that you can't set `explain(randomBoolean())` on all the other tests. I think we really should allow this. I think it's very confusing if I set explain(true) and the actual action doesn't happen. We really have to do this I think.
this looks awesome! exactly what I expected can you also try to add prebuild TokenFitler and Tokenizer? just for kicks
hmm why did you remove the mapping from here? I think that was a good change? you should add the settings from from `public Settings indexSettings()` are only used if you use `prepareCreate` so you should add the settings to the versionSettings below. other than that it looks awesome
I think we should also name it `get` or `getOrDefault`
same as above and can you make it `public static` rather than `static public`
can we just delegate to `valueOf` and if it throws an exception we return the default? I don't think we should do the linear checks here
those license headers are old afaik
just call `parser.text()` instead of `parser.bytes().utf8ToString()` since it might be optimized under the hood
IMO key should be `null` and you should reset to null as well. Then when we do `vars.put(..` throw an exception if we see key to be null. I also think we should just to `vars.put(key, parser.text());` there
you could also use `ElasticsearchAssertions.assertHitCount` here
scratch that... after looking closer, copyBytes cannot be used, which means the other alternative would be to copy the array and assign it explicitly and indeed `deepCopy` is cleaner (creates a new instance, but we only do that for those buckets we return anyway)
I like this! :)
@javanna we do check this below already if the searcher is released twice. I think you can drop this
there is a whitespace too much here ;)
if we use a negative value here we can also just do this: ``` if (output.getVersion().before(Version.V_1_1_0)) { b = Math.max(0, b); } ```
can we please keep the ordering as its and maybe add it at the end. I don't think we should change the byte value its' just a code. We can also use a negative value btw.
I just want to remark that this is working differently to the `query_string` parser. In the `query_string` the default locale is used which I think should be an option on the query for both of them You might want to open a new issue to fix that or add it here already and then open a new one for the `query_string`
also 1.7 :/
this is also java 1.7
Same here. No need to continue
This can stay as a break - exactInclude is the highest form of checking, no need to check more
Same here. Can stay a break
This is the actual cause of the issue. +1
you need to be careful here you might have a filter that is wrapped but not a query than you get a NPE here
true for `XFilteredQuery` used to subclass `FilteredQuery` but not for Constant: ``` Java public class XConstantScoreQuery extends ConstantScoreQuery { ```
this needs to be `FilteredQuery` rather than `XFilteredQuery` same goes for the `XConstantScore`
spaces on each side of `=` would make it easier to read
we can make this backwards compatible by checking the version of the node we talk to. Assuming that this feature will be released with `1.1.0`, you can do the following in `readFrom`: ``` if (in.getVersion().onOrAfter(Version.1_1_0)) { //read the newly added bits } ``` and the following in `writeTo`: ``` if (out.getVersion().onOrAfter(Version.1_1_0)) { //write the newly added bits } ```
`tokenFilters` is not nullable anymore, as `charFilters`. I think we can use `writeStringArray` here, I checked and this doesn't seem to break backwards compatibility on the wire level.
yeah so I'd initialize the `String[]` with `Strings.EMPTY_ARRAY` and then make sure that it is never `null` but maybe that is an overkill. I just don't like that we use the `null` invariant use all over the place. Rather check for `null` and throw an `ElasticsearchIllegalArgument` on the setters. What do you think @brusic
see above we have a `writeStringArray` as well
given this I think we should add some checks to the place where this is created in `SearchService` ie. this: ``` Java SearchContext createAndPutContext(ShardSearchRequest request) throws ElasticsearchException { SearchContext context = createContext(request); activeContexts.put(context.id(), context); context.indexShard().searchService().onNewContext(context); return context; } ``` should look like: ``` Java SearchContext createAndPutContext(ShardSearchRequest request) throws ElasticsearchException { SearchContext context = createContext(request); boolean success = false; try { activeContexts.put(context.id(), context); context.indexShard().searchService().onNewContext(context); success = true; return context; } finally { if (!success) { freeContext(context); } } } ```
the below code still fails with an NPE... so maybe there is another null check for `pluginsFIle.listFiles()` needed? ``` File pluginsFile = new File("/tmp/does-not-existing"); if (pluginsFile != null) { for (File pluginFile : pluginsFile.listFiles()) { // NPE happens here, due to trying to iterate over null object } } ```
provided tips too fast.. you might want to use `file.isDirectory()`, `file.canRead()` etc.. might make more sense than simple null checks :-)
can we factor the lentient handling part out in a single mehtod? ``` private Query rethrowUlessLentient(RuntimeException e) { if (settings.lenient()) { return null; } throw e; } ``` man I with we had support for annonymous functions here or macros even :)
Can FieldOptions be really immutable? (final fields)
``` if (Double.isNaN(v1)) { return Double.isNaN(v2) ? 0 : 1; } ```
Maybe this should even be ``` java if (Double.isNaN(v1)) { if (Double.isNaN(v2)) { return 0; } else { return 1; } } ```
You can extend FilterInputStream instead of InputStream so you don't have to override methods that just delegate to the original stream.
why not native boolean type instead of Boolean object? Also, we use the package names as prefix for modules settings, so I would go with `plugins.isolation` compared to `plugin.isolation`.
code style can you put the else on the same line as the `}`
hehe :) nice one
indentation makes the `if` block a bit hard to read
(not as a float but due to the cast to int after `Math.ceil`)
I am actually +1 on moving this to a built in thread pool, so we can enjoy the stats for it and all.
but, I would do this in a different change
I mean, fail if the force flag is set and the instanceof check doesn't work.
Maybe as part of the hint itself like: map_less_128
I see, makes sense to me now.
just beware that Long.compare is Java 1.7 only, you might want to use Longs.compare from Guava instead when merging to 1.x
Maybe it could be something like: ``` if (bytes.size() > BigArrays.PAGE_SIZE_IN_BYTES) { bytes = bigarrays.resize(bytes, BigArrays.PAGE_SIZE_IN_BYTES); } ```
if the size was previously less than PAGE_SIZE_IN_BYTES (possible with the contructor that exposes a size), this will actually grow the array (potentially going from a simple heap-allocated byte[] wrapper to a recycling instance)
Then I'd rather remove this method, because if they start using it, it will have surprising behavior (I would never expect memory to be allocated in a method called `reset`)
Oh, I assumed so, thanks for fixing!
Even if it is more compact this way, I'd rather have different if statements for the version check and the `hasValueName` read so that future backward-incompatible changes will be less error-prone to add.
oh, multi-bucket comparisons are ready already? :)
maybe `Objects.equal` could make it easier to read
Double checking, there might be something I missed here...not sure we really need to use this variation of `prepareCreate`, that makes sure the index is going to be allocated on as many nodes as `nodeCount`. If we only start a single node, looks like we are going to use needless allocation exclusions.
All our tests currently use `RandomizedTest#atLeast` method, you can do the same here.
You can use `assertAcked` here and just wrap the above line... one line saved ;)
maybe have an `assertTotalShards()` and `assertSuccessfulShards()` or `allows assertAllSuccessful()` to provide an amount of shards as well
I think you also need to mark activeBenchmarks as volatile for this to work
By design, t-digest is supposed to return accurate values for minimum and maximum values (when quantile is 0 or 1) so it shouldn't be needed to track them separately.
please remove that blank line
(hint: ParseField makes it easy)
I think we should enable this by default and maybe say `5`
we try to not sync on this generally maybe you can just add a `private final Object lock = new Object()` and sync on that? it's better to hide the sync lock from the outside :)
at some point we should make this a helper method in `Strings` and use it across the board I bet it can safe hundreds of lines
I'd like to use a sentinel here rather than null like `BenchmarkMetaData.Entry.INVALID_ENTRY` or so.. I try to prevent `null` invariants they tend to throw NPEs :)
I think we should use `debug` for the logging here
hmm can't we use `State.values()` and if this is only used for `fromId` I think we can just do this: ``` Java switch(id) { case RUNNING.id: return RUNNING; //... } ```
no need to break here
+1 on making configurable as well (it may very well change from one use case to the other.. this is a good default set however)
state might be null here since the benchmark id might have been removed between the call to `containsKey` and get. I think it should rather be: ``` java BenchmarkState state = activeBenchmarks.get(benchmarkId); if (state == null) { throw new ElasticsearchException("Benchmark with id [" + benchmarkId + "] is missing"); } ```
The `activeBenchmarks` reference can change anytime, so I think there should be a ``` java final ImmutableOpenMap<String, BenchmarkState> activeBenchmarks = this.activeBenchmarks; ``` at the beginning of the method to make sure we are always talking to the same instance.
`newMean` & `newVariance` (we don't do underscores in var names)
how about guava's `return Longs.compare(o2.v2().maxTimeTaken(), o1.v2().maxTimeTaken());`
wouldn't be better to output `null` instead of a string? will be easier to handle on the client side
we can just use Long.compare now :) we are on java 7
I didn't check but unittests for this would be awesome!
also use new `writeDoubleArray` here
see above & there are some extra lines here
can we move to the new `write / readLongArray`
I'd just do `sum += Math.max(0, data[1])`
can we move this to the new `writeDoubleArray` methods
For static varialbles, `final` should indeed be used whenever possible.
the fully-qualified import should be unnecessary with the wildcard above
I think it should either be always "Benchmark" or "Bench"
no utils for this :( `out.writeLongArray()` maybe :)
;) do we have unit tests ? (I didn't check if so ignore me!)
yeah I think this one is ok!
maybe use `AbstractRestResponseActionListener` then you don't need the `onFailure`
can we make this a `if (!closed) {}`
can we init this with `1`
can we hide `shared.refcount` behind a method ie. decRef() / incRef() to be consistent with other stuff
well maybe you don't like the success pattern though... but I think it should be closed even on Throwable
why don't you use `finally` instead of `catch`? I guess that is legacy or copy/paste
if you make `MulticastChannel` generic and the listener as well you safe the hard cast in Shared... just like `Shared extends MulticastChannel<MultiListener>` ...just an idea...
argh! yeah I can see why you wanna do this.... I will open an issue for master to remove the gateways rightaway so we can at least move master to atomic update!
I think having friend Input/Output classes to ByteArray, as you suggested, would help make this easier.
BigByteArray materializes as soon as you ask for something that goes across several pages. So I think this should return`offset + length <= PAGE_SIZE` instead.
see above - I think you should add it though
I think I prefer the current logic of this method to relying on the return value of get().
we should use CompositeChannelBuffer, but then extra care need to be taken when writing to netty, and if it should be a gathering one or not. It might make sense for it to be gathering if under ~512k, and not if above.
IMO this is useless since it will throw an NPE anyway the check is obsolet
this check is obsolet - we can just remove it
IMO the entire class should be final
good that you added this assertion :)
totally +1 on having bulk operations, we already started discussing it with @hhoffstaette
This assumption is wrong if `offset >= N * PAGE_SIZE` and `offset + length < (N+1) * PAGE_SIZE`. I think the only way to fix it would be to make ByteArray.get return a boolean which can be used to know whether a copy has already been performed.
actually, if its a single page, then we can just reference the first page byte array, if not, then we should return false. same with `array` and `arrayOffset`.
+1 to do that and then fix `BigArrays.hashCode` to accept offsets in a separate change.
Indeed, I think two BytesReference instances should be considered equal if they have the same content (which is what the way other children of BytesReference are implemented suggests). My point was this path of the equals method ignores the offset of `other` (it starts comparing its bytes at `0` instead of `other.offset`.
is this correct? this will return a copied array if offset > 0, yet the `arrayOffset` method will return the offset into an array that has offset 0... .
hasArray API concept is that it is backed by a direct reference array, so calling `array()` is cheap (similar to netty `ChannelBuffer`. In this case its not, so this should return false, and `array()` should return null.
I'm a bit concerned that we have different code paths depending on the value of `offset` since this means that changing the code for `BigArrays.hashCode` might break this hashCode implementation.
I think it has the same issue as writeTo with lengths that are multiples of `PAGE_SIZE`.
I think we should return the BytesRef array we get from reading from byte array, and `arrayOffset` will use the same logic, and return the offset form the BytesRef
I think there is an issue here when `length` is a multiple of `PAGE_SIZE`: for example I think a length of `2*PAGE_SIZE` with `offset == 0` should use 2 buffers, but when I do the math here, this gives: `(2*PAGE_SIZE / PAGE_SIZE) + 1 = 3`.
@hhoffstaette I fixed the places where we didn't respect the `hasArray` contract in #5455, so now we can go ahead and return `true` when we have a single page, and false otherwise. Also, `array` and `arrayOffset` should throw an exception if its not a single page (i.e. hasArray is not true)
which asserts failed? the idea of hasArray is that if its cheap to get the array for it, so any code block that fails is potentially a bug for implementations that don't have support for it.
oh no I see, there is also a return. I think it's confusing that we can reach here because of either an assertion or a return statement
I can't wait for try-with-resources :)
The call to release should be indented
looks like a left-over
When I see this, I am now happy that wait is a forbidden API. :-)
(with a different success variable, I see there is already one for the higher-level try block)
Should it be instead? ``` java if (success) { // close } else { // closeWhileHandlingException } ```
Since we have this guard here, should `innerClose` be changed to have ``` java assert !closed; ``` instead of ``` java if (closed) { return; } ```
`i` should be a long since `size()` returns a long
so `round` should be called once per factory instead of once per aggregator
I think this doesn't work since rounding is not always idempotent: if this is used as a sub aggregation of a PER_BUCKET aggregation, an HistogramAggregator will be created several times and each instance might have different bounds.
is it fixing another issue? If yes, I think we should have a separate PR
I will provide you with a PR showing what I mean!
Actually I was not thinking of always applying deleted docs but rather to leave this responsibility to consumers of this class: if they just want to apply their filter, they pass it as-is and if they want to apply deleted docs, they can just wrap it with `ApplyAcceptedDocsFilter`.
+1 there is a good example in BlendedTermsQuery though
Different leaves might use different codecs, and some of them might support `totalTermFreq` while other leaves might not. So this should return `-1` if any of the leaves returned `-1`.
I think we should have a baseclass that only handles DocFreq and then subclass it if we need TTF that should remove a lot of branches here though. I don't like the long methods that basically repeat code because of the TTF / DF swtiches. I mean it makes sense do split it since they have a higher cost if TTF is needed though.
@s1monw if you're proposing we use inheritance and you assume the base class will always be caching DF then we could just remove all the "if(this.docFreq)" checks in the existing code as a simple way to clean things up? That would leave us with just the "if(this.totalTermFreq)" checks.
We have two modes - filtered and unfiltered. DocsEnum is only relevant in the less-common filtered mode and it looks like we already pass that choice of FLAG_FREQS/FLAG_NONE so this would just save the if statement around the call to docsEnum.freq(). > I wonder if we should get rid of all the if's and cache both by default. I'm not sure about the trade-off between removing ifs and caching counts unnecessarily? Signif-terms does a lot of DF lookups and I wouldn't want to add a cache of TTF count for each unique term when it is of no interest to me.
can you use try with resource here since we are on java 7 now ie: ``` Java try (CBORParser parser = CborXContent.cborFactory.createParser(content)) { parser.nextToken(); generator.copyCurrentStructure(parser); } ```
I see!, in that case, I would stay with what you suggested in the first part, and just rely on the first byte, it will work well in our context.
I think that we can check the second byte as well to make sure, same way we do in SMILE (by not only checking the first byte), check here: https://github.com/FasterXML/jackson-dataformat-cbor/blob/master/src/main/java/com/fasterxml/jackson/dataformat/cbor/CBORParserBootstrapper.java#L112 for the logic
I'm worried that by copying over the field type completely we get more stuff then what we tested for collisions. For example, indexOptions is not tested. Maybe there are other stuff.
ok so I try to think about situations where this doesn't work.... the missing / hasvalue filter can be expensive though but I guess we should just make it fast for that case and expose the filter on the field data... I like the idea... ok fair enough.
I'd rather have a different parameter there. However, that would add complexity. It might be better to not handle missing field or NaN and Inf at all and let the user sort it out with range filters.
I don't think you should catch this here! this must not be null at all just let it run into NPE
well we don't loop at all we return the first one no? you can also do `if (numValues>0)` though.. same thing :)
This method can be removed since it isn't used.
Instead of this line can add the following to the method being (includeInAll with the two booleans) invoked here? ``` java if (withinMultiFields) { return false; } ``` Then it will be consistent with `withinCopyTo`.
Vote for the distance to be configurable, which also justifies the second param being `DistanceUnit.DEFAULT`.
no worries I was just curious.
I think we should have a `clear` method on the cluster instead of calling them here. I also think you should remove the delegates and fix the uses of these methods to use `cluster().ensure...` and `cluster().wipe...`
I think it should either be an `else if` or the `if` should be on the next line.
grr nevermind I didn't see the last line pfff...
or N times
I don't get this part why do you change the way we read the `TranslogStats` here? can't this just be ``` Java translog = in.readOptionalStreamable(new TranslogStats()); suggest = new SuggestStats(); if (in.getVersion().onOrAfter(Version.V_1_2_0)) { suggest = in.readOptionalStreamable(suggest); } ```
can we invert this and say `if (suggestStats != null)`
`class` should itself be `static` if possible (hard to tell from diffs, but it looks like everything is passed into it)
Personally, I don't think that the `Plugin` should need to know about the `PluginManager`; the `environment` made sense to pass in, but I don't know if it's worth it to you all to refactor it to change the manager-to-plugin relationship. I was hoping that it would be an easy change.
I'd rather call this `queueSize` as we do in nodes info api.
`null != poolInfo`? The booleans are more terse, but not so much so that they're worth it IMO.
Even better you can use `try (BulkProcessor processor = BulkProcessor.builder(client(), listener).setBulkActions(6).setConcurrentRequests(1).setName("foo").build()) {` since we are on Java 7 :)
you can use `IOUtils.close(processor)` it deals with `null` values...
I'm not sure myself why this hasn't been done this way. :-) It's fine, I was just curious if you had tested calling super and if it introduced issues.
I am just wondering if we can make sure we never get a class taht does that but I guess the assert is ok
I your reply got busted.... lemme reread
here you can use a `scaledRandomIntBetween` something like ``` Java int length = randomIntBetween(1, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20)); ``` this will use the upper bound rather than the lower bound when we run with nightly=true.
just as a sidenote I think the arguments should be final to make sure we don't reassign them :)
ok lets open an issue for it and get this one in...
I still don't get why we have all these branches. For the `docFreq` we can just calculate is all the time. For the TTF we can just do what lucene does internally: ``` if (totalTermFreq == -1 || leafTotalTermFreq == -1) { totalTermFreq = -1; continue; } ``` then we don't need the `hasMissingTotalTermFreq` alltogether
I wonder if we need this - we could assign the `NOT_FOUND` in the beginning of this method and then assign `currentDocFreq` as well as `currentTotalTermFreq` each time we update their local corresponding variables ie. as the last statement in the `for (Holder anEnum : enums)` loop. Same is true for the `text` and that way we can just trash everything below the for loop and return `found` I also think it's ok to just use `-1` as `NOT_FOUND`
I think the message here should 1. be a static final constant and 2. say what this TermsEnum allows ie.: `"This TermsEnum only supports #seekExact(BytesRef) as well as #docFreq() and #totalTermFreq()"`
whitespace before and after `=`
I guess that's ok...
I'd just do the same thing as mentioned above to keep this simple though
May be we should change `String source` to `String templateSource`? Just "cosmetic" change though... :)
we decrease the ref count multiple times if we call close more than once - I think we must protect from this though
yes oui ! (sorry, couldn't resist...)
Please don't undo the migration to the diamond operator. :-)
Consider `value.isEmpty()` instead of `value.equals("")`. Also, would it be safer to do to avoid potential incosistencies (same for above): ``` return !(value.isEmpty() || isExplicitFalse(value)); ```
I also think we should default initialize the `mapingsToUpdate` and trash all the `null` checks just make if final and done...
Instead of doing the instanceof check here can we maybe wrap the body of the try / catch in another try catch and then rethrow? like this: ``` Java try { try { } catch (WriteFailure e) { // do all the things throw e.getCause(); // maybe check if e.getCause() can be null? } } catch (Throwable e) { // do all the other things } ```
there are... but I believe this is the most common one by far, hence my suggestion
I like that... we should probably move that to: ``` public interface Versioned { Version getVersion(); public static class Predicates { public static Predicate<Versioned> onOrAfter(final Version version) { return new Predicate<Versioned>() { @Override public boolean apply(Versioned versioned) { return versioned.getVersion().onOrAfter(version); } }; } ... } } ``` then have StreamOutput/Input implement `Versioned` and then: ``` public void writeTo(StreamOutput out) throws IOException { ... out. writeOptionalStreamable(streamable, onOrAfter(Version.V_1_2_0)); // static import for Versioned.Predicates } ``` clean & simple (the only downside, is that we're creating a bunch of temp objects... where we don't really need to, but there shouldn't be that many of those anyway)
I think we should keep it in the name even if it is verbose
I still don't like the verbosity of this... it can get really hairy... another option is to have a boolean overloaded version of the method instead: ``` public void writeOptionalStreamable(@Nullable Streamable streamable, boolean apply) throws IOException { ``` then have it used as: ``` public void writeTo(StreamOutput out) throws IOException { ... out. writeOptionalStreamable(streamable, out.getVersion().onOrAfter(Version.V_1_2_0)); } ```
this is a very cool first step. I think once we have this one in we should add support for NodeClients / MasterOnlyNodes etc!
oh I see - I was just suprised that it's unused...
ahh yeah in `assertAfterTest()` nevermind
make that dude final for now
+1 other than than LGTM
why is this needed? `publishNode(buildNode);` should be synced!
only the `#start()` call should be concurrent. The publish should be done in a sync manner and the `NodeAndClient buildNode = buildNode(settings, version);` should be done before in a sync manner. This is important since we want the same setup no matter of how threads are scheduled and `NodeAndClient buildNode = buildNode(settings, version);` uses random internally
we can randomly use a different cluster? or maybe downsize the global cluster to 1 node for this test? I also wonder if we should consider to run tests with one node as well? the minNode=2 was only convenience...
If you are asserting `indexService` is not null right after this, might as well remove the `@Nullable` from it here.
We can save object creations here by making the ByteArrayDataInput final and using `ByteArrayDataInput.reset`.
I think the length of individual byte[] values should also be encoded using vInt.
This should do an unsigned comparison: ``` java int a = left[i] & 0xFF; int b = right[i] & 0xFF; ```
Maybe we should sort the list of byte[] here? I'm thinking this might be useful if we decide to support sorting on binary values in the future.
Instead of creating a new BytesRef on every call to `nextValue`, it should modify in-place the `scratch` member that it inherits from `BytesValues` (potentially resizing the array if it is too small).
you can use `getRandom().nextBytes(bytes)` here
I think it would be better to use a [`vInt`](http://lucene.apache.org/core/4_7_0/core/org/apache/lucene/store/DataOutput.html#writeVInt%28int%29) to save space. Up to a length of 127, a vInt would require one single byte while an int always requires 4 of them. You can use a ByteArrayDataOutput in order to make it easier to fill the byte[]. In order to compute the size of the array to write into, you can just assume worst-case, which is 5 bytes for a vInt.
You can use a for-each syntax here: `for (ObjectCursor<byte[]> cursor : bytesList)`
ideally, you should read directly into `scrach.bytes` instead of allocating a `byte[]`
Good points. For consistency, I think it should indeed remove duplicates.
It'd be ever so slightly clearer if the two examples parsed to the same RaioValue. Like (eg. 73.5%) and (eg. .735).
Actually, now that you refactored LongHash, I think this can be made much simpler: we can just iterate with `i` from `0` to `bucketOrds.size()` (which is the number of entries in the hash table, instead of `bucketOrds.capacity()` which is the number of slots) and directly use `i` as a bucket ordinal. ``` patch --- a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java @@ -108,13 +108,7 @@ public class LongTermsAggregator extends BucketsAggregator { BucketPriorityQueue ordered = new BucketPriorityQueue(size, order.comparator(this)); LongTerms.Bucket spare = null; - for (long i = 0; i < bucketOrds.capacity(); ++i) { - final long ord = bucketOrds.id(i); - if (ord < 0) { - // slot is not allocated - continue; - } - + for (long ord = 0; ord < bucketOrds.size(); ++ord) { if (spare == null) { spare = new LongTerms.Bucket(0, 0, null); } ``` (The same change should apply to other aggs.)
I think this needs to be `@param` in order to be picked up correctly.
maybe a switch statement would make it easier to read? (now that we are on Java 7)
We are missing the license header here... build fails :(
given the fact that this is really unexpected we should do a `warn` logging IMO. I think it's an exceptional condition and we should log it as such.
(or maybe `ObjectArrays.concat` to make this more concise)
Even if the number of sub aggregation is expected to be small, I'm not too happy with the use of `ListIterator.add` which is linear on array lists.
(I'm all for it, but would just like to make sure we have a changes entry for it)
`originalTimestamp` can be null here, causing an NPE
`String.toString` just returns `this`, so you could remove the `instanceof` check and cast after you ensure it's non-`null`.
In this example, you can use `assert` directly in the script itself, so you could do ``` java .setScript("assert ctx._version == 1 : \"version should be 1\"" + ... etc ... ``` If you would like to (I think either way is fine, just depends on personal preference).
This empty `createIndex` causes the tests to fail, it should be `createIndex("test")`
I think this assert in particular (the timestamp one) could be greatly simplified by asserting inside of the script itself instead of setting a new field value.
we need to remember to update this when we add the reverse_nested aggregator :)
I think it is not OK because of the lack of symetry of the equals method: although `new ClosestNestedParentFilter(parentAggregator).equals(parentFilter)` would return true, `parentFilter.equals(new ClosestNestedParentFilter(parentAggregator))` would return false, which means we might pollute the filter cache with a new filter that will never be reused every time this aggregation is used. So I think we need to fix the root issue, that is that parent aggregators should be fully constructed before constructing sub aggregators, before fixing this one.
I think we need a better fix for this: it's bad that we pass the parent aggregator in but it's not fully constructed yet. Additionally, I'm concerned that the equals method of `ClosestNestedParentFilter` is not symetric, this could lead to duplicates in the filter cache.
the logger here should either be static, or passed in the constructor (better, since it will have much more info)
I think you need to do that with ClusterState & ClusterStateStatus as well otherwise you will see inconsistent state
fyi I think you can IOUtils.closeWhileHandlingException(files) since its Iterable...
this is not a warning level log, if we return the error over the API call, we debug log those
;) thanks :)
I think we might miss some responses in case of onFailure() because it will be using responses created [here](https://github.com/s1monw/elasticsearch/blob/issues/5766/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java#L94)
for style can we maybe invert the if statements here ie `if (!masterCopy.isEmpty)` and `if (!success)`? I like to have only one return statement at the end of the method
I wonder if we want to add and an else clause with a debug log here to make sure we see duplicate failures? might be helpful in debugging.
This will throw an EngineClosedException which is a different type and we do check for RecoveryEngineExceptions in some other place of the code. I'm not sure this matters, I think it's safer to keep as is? Also we loose the information about the recovery phase (1)
Typo... should be assertLockIsHel_d_
I think this missed a misses a maybeFailEngine as well
I think this missed a misses a maybeFailEngine
as far as I can tell this check previously could never be triggered as we used to hold a reader lock which means no one was allowed to change indexWriter. Now we do it out of the lock which means it can happen? I think we should remove it.
also applies to other places in the code below, if we decide to do it.
This is an excess to the indexWriter without a lock. I think this can lead to an NPE if the shard is closed. I realize it's not part of the change, but I think we should deal with it.
I think writer can be null if optimizeMutex is true when the method begins. It seems we have this recurrent pattern of calling ensureOpen and than getting the indexWriter to do something. Perhaps we can change ensureOpen to either throw an exception or to return a non-null writer (guaranteed) . This this can become `ensureOpen().waitForMerges()`
It seems `optimizeMutex` is there to avoid doing multiple optimizes but also not accumulate pending ones. Lock.tryLock could also work, I guess. Back to the merge policy. It is acquired from the indexWriter so is it OK to change it while not have a read lock? the indexWriter may have been changed/closed. I'm not sure just want to make sure it's considered.
I think the OOM reference was a copy paste error from another clause. This clause doesn't do `translog.newTransientTranslog(translogId);` so no need to revert it - although it seems to do no harm if there is no current transient translog.
super pedantic but the second param should be deleteRoot_s_
Awesome :) On 18 April 2014 21:36, Lee Hinman notifications@github.com wrote: > In > src/main/java/org/elasticsearch/index/query/support/XContentStructure.java: > > > +import org.elasticsearch.common.xcontent.XContentFactory; > > +import org.elasticsearch.common.xcontent.XContentHelper; > > +import org.elasticsearch.common.xcontent.XContentParser; > > +import org.elasticsearch.index.query.QueryParseContext; > > + > > +import java.io.IOException; > > + > > +/** > > - \* XContentStructure is a class used to capture a subset of query, to be parsed > > - \* at a later time when more information (in this case, types) is available. > > - \* Note that using this class requires copying the parser's data, which will > > - \* result in additional overhead versus parsing the inner query/filter > > - \* immediately, however, the extra overhead means that the type not be > > - \* extracted prior to query parsing (in the case of unordered JSON). > > - */ > > +public class XContentStructure { > > I think this is possible with some refactoring, I will work on adding it. > >  > Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/5838/files?utm_campaign=website&utm_source=sendgrid.com&utm_medium=email#r11773450 > . ## Met vriendelijke groet, Martijn van Groningen
Maybe we can remain streaming parsing if the type was parsed before the query? If the `XContentStructure` class has 2 set methods setTypes() and setQuery(), then the latter method can be smart, so that if the type is already know it would immediately create the Lucene query. If the type isn't know it would just keep the unparsed query around in a BytesReference field. I see that would change this class completely, but I really like to do streaming parsing if possible.
we can do this in a more optimize manner. We can create a builder, and then use `copyCurrentStructure` on the builder (passing it a parser), to just copy it over, compared with parsing into a map and then serializing the map. Also, since its internal, I would use smile builder, as its considerably more efficient than json.
this is not guaranteed to be smile! We randomize this in our tests I thing this should break if you run it multiple times.
Yeah, it's pretty new. :-)
Sorry, I missed that you need to track scores as well. Then indeed, a LongHash would make sense.
we end up double logging warnings, no? The first here, and the second when failing the engine. I think its enough to log a warning when failing the engine later.
you should count down the latch in a try / finally not in the exception block. Another thing which is nice about java 7 is that you can just rethrow even if you do catch throwable like in your case this would look like this: ``` } catch (Throwable t) { logger.warn("Failed to update master on updated mapping for {}", e, mappingRequest); throw t; } finally { latch.countDown(); } ``` since java 7 realizes that this must be an unchecked exception!
since we catch throwable I think you can scratch the `success` thing and just do the `latch.countDown();` in there? sorry I could have realised that earlier :)
+1 this should be a simple struct like Object all `public final`
as discussed over voice we should really try to disambiguate and barf if we can't so no index should trigger and exception asap
so I think that is an ok test but what you really want is to: - create an item with random values - serialise it to json with the toXContent() - deserialise the json back to an item - compare the items - they should be the same - repeat a couple of times...
I am still missing `_version`, `_version_type`, `fields`, and `_parent` here we should add them!
Maybe this should use a Boolean instead (the object wrapper) and only write it if not null. I know the other integers are not doing it, but I think this one is different since `true` is a valid value while `-1` is an invalid value for the other fields.
I think this assumption is pretty broken. What if the type is `null`? We don't define any order in the types when they are specified in the URL but this code assume that there is an order. I think we have to make this explicit which type should be used.
so what about the other settings you can set on the Item like source filtering? I think we should expose all of them here though.
I think we should add a marker class here like: ``` public static final Item extends MultiGetRequest.Item { public Item() { super(); } public Item(String index, @Nullable String type, String id) { super(index, type, id); } // pure delegate to multi get } ``` this way we can hide the MultiGet impl. detail :)
can we fix this and use `more_like_this` rather than `moreLikeThis`
I wonder if we can detect this but for now it's ok i guess
I don't think you need the List here you can just initialize the array directly? ``` BytesRef[] uids = new BytesRef[items.size()]; int idx = 0; for (MultiGetRequest.Item item : items) { uids[idx++] = creatUidAsBytes(item); } return uids; ```
Can you call `assertSearchResponse` on the DSL and API responses? If there are different hits, this will help make sure this is not because of failed shards.
Is it necessary? the aggregator factory should create it with the right size directly? (if it does not I think this is a bug)
I think we should work on this before we commit this change. The mutable fashion of this is to dangerous to miss something. I think this should be a single function that requires you to specify all the different parameters such that we don't have a chance to miss it. This is too risky for my taste here.
typo I guess `s/chanHaveDuplicates/canHaveDuplicates/`
I am sorry but this name is completely off :) I don't get what it means :) Can we find a really good name here? here are some suggestions: - `updateRequired` - `canHaveDuplicates` - `requiresUpdate` - `resolveDuplicates`
maybe we can utli method to the shardIter here? I'd prefer to keep this code short here it's large enough
ok, fair enough
can you remove that shared string and use `readString() / writeString()`
can you remove that shared string and use `readString() / writeString()`
I think we should now move this mode into a util class maybe in `org.elasticsearch.search.MultiValueMode`
then check for non null here...
I think this one should be `null` as well to keep defautls in one place
@javanna thank you that helps a lot.
Since scripting is disabled by default, we re-enable it back when we start each node in our test infrastructure, just because we have quite some tests that need it on. We are thinking about re-enabling it only for the tests that rely on it though...
This field is `static`, but it's set via a constructor. That seems odd.
Should probably make these `final` where possible.
Sure. Created #6090
Helper method is no longer needed now that `Logger.log` exists.
Helper method is no longer needed now that `Logger.debug` exists.
Can change this to the new autoclose functionality in Java 7 now that the codebase is on it: ``` try (ZipFile zipFile = new ZipFile(pluginFile)) { // ... } catch (Exception e) { // ... } ``` Thereby dropping the entire `zipFile`-related code from within the `finally` block.
This exception should probably be raised in the `PluginHandle.parse` method and/or constructor to avoid any issues. Otherwise, if `name` is `null`, then `PluginHandle.parse` will throw a `NullPointerException`.
This happens regardless in the next `if`-statement since `address` remains `null`.
I was thinking of something like: ``` java public static enum TestQueryType { MATCH_ALL { @Override public QueryBuilder buildQuery() { return QueryBuilders.matchAllQuery(); } }, MATCH { @Override public QueryBuilder buildQuery() { return QueryBuilders.matchQuery(TestIndexField.STRING_FIELD.toString(), randomAsciiOfLengthBetween(MIN_SMALL_INTERVAL, MAX_SMALL_INTERVAL)); } }, TERM { @Override public QueryBuilder buildQuery() { return QueryBuilders.termQuery(TestIndexField.STRING_FIELD.toString(), randomAsciiOfLengthBetween(MIN_SMALL_INTERVAL, MAX_SMALL_INTERVAL)); } }, QUERY_STRING { @Override public QueryBuilder buildQuery() { return QueryBuilders.wildcardQuery(TestIndexField.STRING_FIELD.toString(), randomBoolean() ? "*" : "?"); } }, WILDCARD { @Override public QueryBuilder buildQuery() { return QueryBuilders.wildcardQuery(TestIndexField.STRING_FIELD.toString(), randomBoolean() ? "*" : "?"); } }; public abstract QueryBuilder buildQuery(); } ``` Then to build a random query, you could do: ``` java randomFrom(TestQueryType.values()).buildQuery(); ```
60s is way too much for a single test in my opinion. I think we should either make the test faster or mark it as @Nightly.
you can use the `randomFrom` method
I think our other enums need this ids for backward compatibility of the streams, so that even if we add/remove entries to the enum they still get serialized with the same byte.
Similar problem when writing data: you need to handle stream versioning with something like: ``` java if (out.getVersion().onOrAfter(Version.V_1_2_0)) { out.writeBoolean(include); } ```
This breaks backward compatibility of the stream format, so you need to only read this boolean if the node that is sending this request to you has a recent enough version of Elasticsearch. It would look something like: ``` java if (in.getVersion().onOrAfter(Version.V_1_2_0)) { include = in.readBoolean(); } else { include = false; // hard-coded behavior until Elasticsearch 1.2 } ```
ok, didn't know that guice would complain about that. Indeed mocking can be useful.
The default case can be removed here.
It's frustrating that we cannot share this code with SearchRequestBuiler.
Can we make sure that doClose is called even if `Releasables.close(recordingWrapper)` throws an exception? For example: ``` java try (Releasable _ = recordingWrapper) { doClose(); } ```
:+1: I like how this method abstracts the way sub aggregators should be collected and how you can mix deferred aggregation with sorting.
spaces around '='
spaces after if and around '='
spaces after '//'
spaces after '//'
Let's maybe call it `preCollect`? (symetric with `postCollect`)
could we make this test somehow operations based rather than time based. The Problem with time-based tests is that you have a very very hard time to get anywhere near reproducability. I know this is multithreaded anyways so it won't really reproduces but we can nicely randomize the num ops per thread which make it a bit nicer :)
you should use the `awaitBusy` method here which doesn't have a fixed sleep but increase the sleep interval in quadratic steps...
I wonder if it would make sense to add a method that allows us to filter the snapshots here and only return the ones relevant to this method. This method is crazy :)
like a method that returns all started shards ie `public void Map<ShardId, ShardSnapshotStatus> getStarted(SnapshotMetaData meta)` this would make this method soo much simpler
there is a `request.hasParam()` shortcut...
if you do not supply the the content-type, you can just hand over the builder and do not need to create a string object here. Also I would just return `JSON is disabled` instead of mentioning the config option here. The shorter the better IMO.
From the top of my head I think you can leave this out as well
Minor nitpick 3: You could put the two if-statements into a long one making this diff really small
I'll leave this one
seems redundant indeed
if not needed +1 on removing
it's fine to remove it
> both good points... the reason why it's an assert is that it should never happen, as we pass in a single index and we say we don't resolve aliases to multiple indices. but an alias to a single index is allowed? If "yes", then it needs to be an exception, as it's not under the code control what input the user provides (the user can provide an alias that points to multiple indices... ) > As for removing the method entirely. I like the idea, but then we'd end up with these same two lines all over the place...not sure it's great, I thought it's better to share those lines (length check and array to single string conversion). fair point... if it's called all over the place, then perhaps the exception should be thrown here (with somewhat of a generic yet descriptive & user friendly message)
ah.. ok, gotcha... I'd leave the assert here but change it to ``` assert indices.length == 1 : "expected an exception to be thrown otherwise"; ``` for clarity
Those two code snippets are very similar.. I think we should go more generic here, maybe you can add a static `XContentHelper.toString(ToXContent foo)` which acts like in the `SearchSourceBuilder` and does not throw an exception, but returns an error JSON ``` java @Override public String toString() { try { XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint(); toXContent(builder, ToXContent.EMPTY_PARAMS); return builder.string(); } catch (Exception e) { return "{ \"error\" : \"" + e.getMessage() + "\"}"; } } ``` There should be another `XContentHelper.toString(ToXContent foo, boolean wrapInObject)` method which adds the needed `builder.startObject()` and `builder.endObject` calls, if specified. With this change, both of this calls, would basically be one-liners. Hope it makes sense...
you should assign a new list instance (or clear the `abortBenchmarkNodeStatuses` list) to make sure we are not reading an instance that already has some status in the list.
don't think we should do this for abort, i think this should stay like it is for abort and only work as described for READ ONLY operations. For abort I think the current behavior makes perfect sense.
I think this is cleaner to have inline? it is very short and not used anywhere else.
good catch :)
Perhaps it should just be `script.groovy.sandbox`? It would then serve as the root property of all the child properties of it in a readable way: ``` script.groovy.sandbox = true script.groovy.sandbox.class_whitelist = ... script.groovy.sandbox.package_whitelist = ... ``` Versus: ``` script.groovy.sandboxed = true script.groovy.sandbox.class_whitelist = ... script.groovy.sandbox.package_whitelist = ... ``` Or, to go back to enabled, even `script.groovy.sandbox.enabled`. Longer, but I think it makes it better because it lives in the same hierarchy as the other sandbox properties.
Feel like this kind of thing should be logged for debug purposes (realizing it was not done previously).
It might be worth logging it at warning, maybe. Its not "normal" not to have it but is "OK". Its just one line on startup and so long as the line clearly states that everything is ok, we're just disabling groovy, then I think it should be logged every startup.
It'd be nice to know that some more things we expect to work do - stuff like the current time, all the listed methods. Most of the the stuff you need to fake out scoring is in the IndexLookupTests so that is cool.
Search and executable, and execute look like they should delegate stuff to a single method. Orsomething.
Maybe merge all the groovy securing code into one place? It feels funky to have the default receiver whitelist here but method blacklist above.
+1 to that for the yaml configuration.
please use constants here
not sure if we need it here, it's a transport client, not node client...
good catch! not 100% sure we should fail the whole request though, as the actual cluster update settings went well, although the following cluster reroute failed. The settings have effectively been applied...maybe we should return them doing something like this: ``` listener.onResponse(new ClusterUpdateSettingsResponse(transientUpdates.build(), persistentUpdates.build())); ``` But then we would not return any error and only log it...
typo ;) s/unwarp/unwrap
The entire slow search can be removed, because we break BW there is no need to keep this code around any more.
same here with awaitBusy
I think you should implement this in an awaitBusy block ie repeat until you see eveictions but never wait longer than 10 sec by default.
This also might need an `ensureGreen`
This `if` is never false because `numberOfShards` is between 4 and 10
This `if` statement will always be run, so it could probably be removed
are those number randomizable ie `randomIntBetween(1,100)`
@colings86 Is it correct to use the printer to define how dates should be parsed, I would rather expect it to be the role of parsers? I think we need in this iteration to collect _parsers_ for all inputs formats as well as the _printer_ of the first one? (while master currently uses the first parser as a printer, and your patch collect all printers to know how to parse)
I think we should only proceed if the analyzer is not set yet? ie. ``` java if (context.analyzer() != null) { // already set return; } ```
I wonder if we should skip the cache entirely here. Those filters correspond to a term query IMO we should just skip caching them entirely? the will be very fast and we can safe memory here
Should it keep using `exclude` when serializing the request? Otherwise a 1.3 node couldn't talk to older nodes? Specifically, what I'm thinking about is doing: ``` java if (include != null) { builder.field("exclude", !include); } ```
I think the framework supports both: method names that start with `test` or `@Test` annotations.
nitpicking here... sorry :)
can we also step out if `matchedDocId < DocIdSetIterator.NO_MORE_DOC`
why don't we just bubble this exception up as an `ElasticsearchException`
we usually do check the lucene version in a static block ``` Java static { assert Version.CURRENT.luceneVersion == org.apache.lucene.utli.Version.LUCENE_48 : "Remove this class in Lucene 4.9"; } ```
why this opening bracket here and the closing one in line 186? Apart from that LGTM
I don't think we should do `__default__` we can pass the default separately...
I don't get it sorry :)
oh I see I think we can just use the mltQuery.getAnalyzer() instead since it will resolve the fieldname. so you can just do ``` Analyzer a = fieldsAnalyzer.get(fieldName); if (a == null) { a = mltQuery.getAnalyzer(); } mlt.setAnalyzer(a); ```
Can you rather call Releasables.close(tops, bottoms, posLefts, posRights, negLefts, negRights)? This way, it will try to close eg. negLefts even if closing posRights threw an exception
this assumes that the returned array will be of the same size as tops. This will be true in practice since they are both arrays of doubles, but I think the code would be more robust if it called `resize(bottoms, tops.size())` instead of `grow(bottoms, owningBucketOrdinal+1)`.
Can you reorganize imports? (several files seem to have that issue, please however only do that on files that you modified)
Guava has a thing called BiMap to make dealing with these two way maps a little cleaner. OTOH the implementation of it we'd use here is based on two hash tables any way.
this one becomes too dangerous IMO. It's the only place we use this class and we risk to miss passing on a parameter. IMO we should remove all the setters and make this a struct like immutalbe class taking all the values here as ctor args. The information what they are is already there via the getter and the compiler should tell us if we have to pass on any further information!
I think the method name is a bit confusing as it doesn't set the limit to a new value but rather returns a new BigArray instance. I think we need a better name or maybe have new constructor `BigArrays(BigArrays arrays, long limit)`
good catch on delta > 0
`limitedTo(long bytes)`? Clone is kinda non-specific.
you can use the common method `refresh()` here which checks also if there are failures in the response.
I know where it came from :)... can be removed
I was not sure where it came from. Would be nice to remove it, there is no execution hint anymore.
I think it'd be nice to also test some values that are out of range and check that we actually get back 0 or 100 when it's the case.
rename to `percentileRanks` (should be aligned with the rest api
does this work? it works for percentiles, but with percentiles rank it's reversed
you can call `randomFrom(twoBadNames)`
Got it...sounds weird though that you use your own `FetchSubPhase` which depends on a custom `HighlighterContext`, as the context is pretty much bound to our `HighlightPhase`. I wonder now if its constructor should have been package private from the beginning too :) Anyways, I think a `protected` constructor would do the trick for you instead of `public`, I'd prefer that so it's clear that people are not supposed to instantiate a `HighlightQuery` in the ordinary usecase.
I do see the reason why we need to make the `HighlightQuery` public, but I'm not sure about the constructor. The object gets created by the HighlightPhase and ideally the highlighters would just rely in the instance created there.
can you remove this additional line? ;)
closing bracket should be inlined
I would be +1 on forcing it to be a map
closing bracket should be inlined
setter doesn't belong here, should be removed
the way that we usually do this is by making toXContent final, write the name and metadata, and delegate to a `doXContentBody` method. This ensures that no aggregation can by-pass the serialization of the metadata.
Since this API is public, it should rather return the Map&lt;String, Object&gt;
+1 I think create should rather take the metadata as an argument
(neither the setter if the changes proposed on the factory are done as well)
style-wise we generally strive to make the scope of if/else blocks explicit (brackets)
yeah.. it should always be a map
so the semantics of `add` vs. `set` are different, with `add` one might expect that adding several values with the same key will end up with the key pointing to an array of values. `set` (IMO) more explicitly indicating that you're overriding previously set values.
argh... didn't read it properly... nevermind
I think node can't be null here.
I see your point, on the other hand I think we can have options that are part of `IndicesOptions` although not exposed to users, even if they change only depending on the api. That was the direction in #6169. It would be great if we can keep a single `concreteIndices` method that knows what to do based on the `IndicesOptions` provided as argument and move this `canFailClosed` flag to `IndicesOptions` without exposing it to users.
not sure if you wanted to do the following instead? ``` IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed(); ```
+1 to setTopReader instead of next (it makes more sense imo since there is a single top reader)
oh I thought that is an array from line 66 hmmm
`this` is obsolete
It's a common theme across the codebase, we don't create iterators (explicitly/implicitly) if we don't need to
hmm we do this twice? this looks odd
IMO we should not handle this in here... we should special case this in `ThreadPool.java` and maybe just use `null` as the value for the queue or move the `UNBOUNDED` sentinel there.
honestly I don't think we should allow negative values here! In such a case we should maybe use a sentinel or `null` as the negative invariant. Throw a hard exception if a negative value is passed!
this else clause is unnecessary or infact wrong :) no? we should not write `UNBOUNDED`
maybe rename those methods to assert\* as this is what they do... same with the next one
underscore case? :)
Its a bit silly that an instance of this will return true for instanceof ImmutableTestCluster - its certainly not immutable. Not a big deal, probably.
Thank you for using the `getX` naming style!!
since parent is an object, can you use .equals instead? (or Objects.equal like for type)
Ok, I see the problem. Yes, I think really all the stuff happening in the metadata index service for mappings should be moved into the mapper. So the if multiple types are being added, whether for index creation or mappings update, validation can happen within the mapper service. But I can handle this as a separate issue later.
this should probably be an array
I'd go the other way around ``` for (int i = 0; i < usefulHeaders.length; i++) { request.putHeader(userfulHeaders[i], restRequest.header(usefulHeader[i])); } ```
Can we keep the refs to these internal clients
you can use `assertAcked(prepareCreate(...))` here
If I understand the logic correctly, we get here if lucene **didn't** qualified this as a file corruption exception. It could be OOM or NPE, for example. So, something generic like `failure in [{}]` might be a better message here.
I think we should, yes
yea, I would at least debug log it..., it shouldn't happen
since we calculate the checksum lazily here, shouldn't we handle the case (in close()??? yuck but where else?) where a corrupted file is truncated? In such a situation we will never get to the checksum position, so we will never fail
I think this message might be misleading.
That seems to be pretty harsh. If I have a huge shard and this shard has one small file that is corrupted, that will add "corrupted_" prefix to all files and then replicate all files from another shard (including files that were perfectly fine locally).
Should this be `debug` instead of `info`? It generates a lot of message like this during replication: ``` [2014-07-01 22:30:36,127][INFO ][index.store ] [Mimic] [test][1] create legacy output for segments_1 ```
++ on debug message
you must drop this equals method unless you have a corresponding hashCode impl Yet since this is a singleton you can just drop it.
this class is also missing a hashCode impl.
Check nextToken is Token.END_OBJECT and throw appropriate error if not. Without this additional check the parser errors are somewhat confused if the JSON contains a parameter.
s/does you background/does your background
Ditto on the use of ParseField
Note that ParseField does the camel casing for you and the 2nd/3rd... args to its constructor are actually deprecated names so that in future we can run in "strict" mode and flag any client uses of deprecated APIs
you can just use `MultiFileds.getFields(index.createSearcher().getIndexReader());`
Beware that strings need to be compared with .equals, `!=` might return false even if both strings contain the same bytes
also do the same in `BufferingFsTranslogFile`
yeah but then please fix this in 1.2.x
oh I see yeah fine :)
what I meant here was instead of casting here to a `SegmentReader` at all you should do this: ``` Java Boolean previous = cache.seenReaders.putIfAbsent(context.reader().getCoreCacheKey(), Boolean.TRUE); if (previous == null) { // we add a core closed listener only, for non core IndexReaders we rely on clear being called (percolator for example) SegmentReaderUtils.registerCoreListener(context.getReader(), cache); } ``` we fixed `SegmentReaderUtils.registerCoreListener(AtomicReader reader, SegmentReader.CoreClosedListener listener)` in lucene 4 to work with atomic readers too so we get that upgrade for free
Since you had to check the log at the `DEBUG` level to see the error message, it implies that it must have been printed to the console, or else you should have seen it as part of the `logger.error(errorMessage)` printed in the log from line 243. I think the more appropriate change--if any--would be to remove the `else` block from line 242 and then keep the rest of this change. That way the exception is _always_ logged with the stacktrace (makes sense to have once you start jumping through logs), and `foreground` usage remains unchanged. This would also mean that line 238 can go into the `System.err.println` on line 240 rather than always being processed.
I think you set the wrong setting here? it needs to be `indices.recovery.max_bytes_per_sec` (there is a constant for it (`RecoverySettings#INDICES_RECOVERY_MAX_BYTES_PER_SEC`). Also, much nicer to set using the explicitly settings object I would say.
This could be `!Strings.hasLength(nodeLocal)` instead of an equality check against `false`.
you have a logger in the AbstractComponent, can you use that and not create a new one? It automatically makes sure to log the node name in top level component, and if its an index / shard service, log the index/shard as well
I suggest we trace log the failure as well, the stack trace might prove important if we are debugging more complex script failures
It'd be nice to be sure it contained that `not_found` wasn't found.
I think you forgot to add the name of the script here to the failure log
I think you forgot to add the name of the script here to the failure log
ok fair enough...
if you change the `.toString()` method to not use fileswitch this will need to be changed also.
This should be `default` now.
This should be `Type.FS.match(` instead of `Type.FS.name().equals`
unrelated to your issue: but maybe NRTCachingDir should be a FilterDirectory so you dont have to do this? I don't know why it extends Directory directly.
Just FYI this is a bit out of date with respect to FSDirectory.open. Actually we default to mmap on all 64-bit systems now, as its faster on the macos X too.
fileswitch -> default here
Nit picky: if we capture the node name from the start async we can do `internalCluster().getInstance(DiscoveryNode.class, blueNodeName).id()`
a bit of nitpicking here: can we use a constant? I see that we use the same string many times in this class. Same could be done for delete and the other string constants as well while we are at it (could be a separate change though)...
Minor, but both places that use this could avoid assigning a value by always using the contents of `else` block if this were initialized to `0`. ``` Int cnt = termFreqMap.get(term); if (cnt == null) { cnt = new Int(); termFreqMap.put(term, cnt); } cnt.x += freq; ```
I think its better to use the `currentState` here, no? Instead of going back to the cluster service to get the state, then you know you have it and such
Discussed via chat - we should not use the Version.CURRENT as a default to make sure the version is set. The part about a 1.2.0 master is not relevant as it will set the SETTING_VERSION_CREATED key as well.
I think it might make sense to do the same in `ExternalTestCluster`, so that we state that network mode is required there as well.
I'd really like us to start using `EnumSets` for this kind of stuff and then do ``` Java private static final EnumSet<IndexShardState> CAN_UPDATE_INDEX_BUFFER_STATES = EnumSet.of(....); if (CAN_UPDATE_INDEX_BUFFER_STATES.contains(state) == false) { continue; } ```
same concern about SearchPhaseExecutionException
Can you keep the formatting? I tend to find it easier to read when formatted
useless nitpick of the day.. everywhere else 'this builder' is returned with the exception of the two last methods and this one..
do we need busy waiting here? and we should probably wait on the node to join the cluster and have no relocation pending.
should say shard active
this can ne ThreadPool.Names.SAME, the handling is lightweight enough to not require forking to the unbounded generic TP
the logic here should be only if _all_ nodes responded with the shards active we should continue with the deletion process..., same check we do on the cluster state if a shard can be deleted
we need to check on the node version, and only call it on nodes that are version 1.3 and above, otherwise they won't have this API
We need to check for all active states (using an EnumSet<>( STARTED, RELOCATED ))
I think we should encapsulate the testing code from the `clusterChanged` method and just call it again to sanity check it's safe to delete
can we rename it to something like allNodeResponded ? we're not sure we're going to delete...
test for >= just to be safe? (and we can assert in `deleteShard` it is identical to expectedResponses
can we call this ShardActiveResponseHandler? (and the derived family should be renamed as well)
I don't think we need all these counters? we can have awaitingReponses which goes to 0 and a activeCopies one that goes up. We stop when awaitingResponses reaches 0 and delete when activeCopies is what we expect
I don't think we need to go to all the nodes in the cluster. Just the nodes that host a copy of this shard.
I think we should the state we get from the event, to make sure it's consistent with the test we just made.
I think we can check also randomly on a shard that relocates _to_ the local node
Same here, we need j<= numReplicas, which also makes me wonder if we want to validate in shardCanBeDeleted that the total number of shards in the routing table is what we expect it to be (we now only check for no shards at all)
same 1+ randomInt
Same here - more randomization would be nice
maybe randomize the number of shards and their states? (unassigned/initializing/closed)
imho if we don't check on connected we can make this method static which will simplify testing quite a bit.
I mean random number of replicas with random combination of non-active states
Since this query is a pure wrapper, I'm wondering if you should delegate calls to setBoost/getBoost (and then ignore the boost in equals/hashCode)
We should make these accessible in Lucene. Otherwise it might become a nightmare to maintain.
maybe we implement `Iterable<V>`
In the two methods above, I think you should get a local copy of the immutable map so that it cannot get null between the null check and the call to isEmpty/get
I think this will throw a NPE if you create an iterator, then the class switches to CHM, and then you iterate on the iterator since `immutableMap` will be null.
Instead of having this custom finish method, should it implement Releasable? This would also allow for using the try-with syntax
also this class should be final.
I'm not sure this is the right fix. If you pull a reference to the values, add values and then pull an iterator, the iterator will not see the values that have just been added. I think it should rather be: ``` java return new Iterable<V>() { @Override public Iterator<V> iterator() { final ImmutableOpenMap<K, V> immutableMap = UpdateInPlaceMap.this.immutableMap; final ConcurrentMap<K, V> concurrentMap = UpdateInPlaceMap.this.concurrentMap; if (immutableMap != null) { return immutableMap.valuesIt(); } else { return Iterables.unmodifiableIterable(concurrentMap.values()).iterator(); } } } ``` So that the iterator to use is decided at the time when it is requested instead of ahead of time.
And the compiler is right: Before the super constructor is called, this is not yet useable, not even defined according to JLS. And we create the inner class _before_ the super constructor is called (argument is evaluated first)!
this should just catch Throwable I think
yes, make sense
I don't think this is needed you can run with default scope
@s1monw With that change, when running a suite which had a lot of tests, `ElasticsearchRestTests`, when you have a failure in one test, all other remaining tests will restart a full cluster. We should only restart the cluster once until we get another issue.
I think this is not needed
I'm experiencing the same @s1monw . I had missed this change and I would like to better understand the rationale behind it. Why do we have to restart the global cluster if a test fails within the suite? Also, this seems wrong as if there's one failure in the suite we restart the global cluster for all the subsequent tests...this slows down all the subsequent tests quite a lot (e.g. think of REST tests in network mode).
Can you rename it to something else? This is close to `ordinal` which also exists on enums but should not be used for serialization as it would change if we added/removed entries in that enum.
Can you give each of them a numeric id instead? This will allow to rename the enum constants without breaking the bw compat of the stream
The purpose of this thing was to do circuit breaking, I think we can remove it now
You need to override `withCircuitBreaking` so that it returns a MockBigArrays as well
can we move this to our `test` infra? I think we do this in another place in our code (or one of our branches)
Construction now loses the side effect of a `NullPointerException` when this class is misused by giving `null` values for everything except `sourcePath`, which could lead to new, unexpected `NullPointerException`s upon use.
Minor, but this looks like it's closing the `class`/`interface` (but it's not).
yeah, I'm on the fence my self. It's just one (cached) thread we spawn per cluster task (which then executes all of the call backs). Not sure wether you saw this in the profiling as a potential time consumer when it was doing it before. I don't feel too strong about it though.
maybe have a helper method since the logic below does that though...
I wonder if we should spawn this to a background thread as this is still being run on the cluster state processing thread. Just be on the safe side.
then maybe put it into `org.elasticsearch.common.io.Channels`
I prefer to autobox here
remove the static and name it `parse`
you can not override them when you subclass which is a pita
I think you forgot to change it to request.shardId as a single parameter, instead of the index name and shard id both
ie. when showTermDocCountError is true
In order to care about the defaults in a single place, we usually prefer using `Boolean` in the builders, with `null` meaning unset. This way, if we want to change the default to true in the future, we only need to do it in the parser (not in the builder).
I mean that if a term is in the accurate response but not in the accurate one, we should assert that the count in the accurate response is less than the agg-level error margin on the inaccurate one
In these writeTo/readFrom methods, you need to make sure that you can talk to a node that is running an old version by adding checks on in/out.getVersion()
we usually prefer underscore case
This needs to be protected by a `if (in.getVersion().onOrAfter(Version.V_1_3_0)) {`
Maybe we should try to use a vLong? 8 bytes per bucket can be significant if there are lots of buckets
maybe the name should contain `doc_count` to make clear what it applies to
maybe we should have a constant for it
can we change this to Loggers.getLogger(getClass());? it is what it should have been to begin with, which is my fault ;)
can we do: ``` Java if (addFailureIfIndexIsClosed(updateRequest.index(), updateRequest.type(), updateRequest.id(), bulkRequest, responses, i)) { continue; } ```
honestly, BulkRequest should then only allow you to add `Indexable` or whatever we call it and then we can skip all the instance of crap here too :) and trash the `ElasticsearchException` at the bottom
maybe add some java doc? (emphasising that includingDefaultMapping should be false must of the time)
Consider changing to `void applyAuthorization(URLConnection connection);` (and then changing the children to "apply" themselves there; the `EmptyAuthCredentials` would naturally be a no-op). This should help to avoid an N +/- 1 list of `if` statements in the `HttpDownloadHelper` as more types of `AuthCredentials` are naturally supported.
Consider making this an `||` and having it lazily fail on use (or moving the on-use exception to the constructor and fail fast) due to a misconfiguration.
Technically not an "and".
Suggest `Strings.isNullOrEmpty(username) || password == null`, but I'm not certain that a username _cannot_ be empty.
this should be done by just reading username password from the Credentials
IMO this should just hold username / password and the authentication should be done in a sep class.
`instanceof` implies a `!= null` check. Ignoring that, if it is not `null` and it is _not_ a `BasicAuthCredentials` (or `EmptyAuthCredentials`), then consider throwing an exception. ``` java if (authCredentials instanceof BasicAuthCredentials) { // same } else if (authCredentials != null && !(authCredentials instanceof EmptyAuthCredentials)) { throw new ElasticsearchIllegalArgumentException("Unknown authorization credentials: " + authCredentials.getClass().getName()); } ```
Consider adding ``` static final NoAuthCredentials NONE = new NoAuthCredentials(); ``` and making the `NoAuthCredentials` `class` package protected (drop `public`). It may not be used enough to warrant having a `static` field hanging around though, but I liked what you did with the original `BasicAuthCredentials` and `EMPTY` before changing it a bit.
Please use `org.elasticsearch.common.Base64` and not a sun API
there is a license header missing
Extremely minor, but this could drop the `public abstract` part now as `interface` implies it.
This one should extend `ElasticsearchTestCase`, otherwise it will use the junit runner instead of the randomized one.
can we import from our version Guava? I think we should add all of this to forbidden APIs at another pull request
Good point. To ensure that `null` and blanks are captured, then we can do ``` java if ( ! Strings.hasText(name) || BLACKLIST.contains(name.toLowerCase(Locale.ROOT))) { throw new ... } ```
can we put them all in a `private static final Set<String> BLACKLIST` and then just do ``` Java if (Strings.hasLength(name) && BLACKLIST.contains(name.toLowerCase(Locale.ROOT))) { throw new ElasticsearchIllegalArgumentException("Illegal plugin name: " + name); } ```
Suggest making this an ordered array/loop in the interest of maintainability: ``` java boolean forbidden = Strings.isNullOrEmpty(name); String[] forbiddenNames = { "elasticsearch", "elasticsearch.bat", "elasticsearch.in.sh", "elasticsearch-service-x86.exe", "elasticsearch-service-x64.exe", "elasticsearch-service-mgr.exe", "plugin", "plugin.bat", "service.bat" }; for (String forbiddenName : forbiddenNames) { if (forbiddenName.equalsIgnoreCase(name)) { forbidden = true; break; } } if (forbidden) { throw new ElasticsearchIllegalArgumentException("This plugin name is not allowed"); } ```
I think this class as well as the constructor should be make public so a user (or us!) could micro-benchmark script execution themselves.
I personally think those queries should be build using query builders but we can do that in a second step.
can you please use indexRandom to index docs
also use `indexRandom`
Can we add another TODO here? This seems massively inefficient: 1. expressions know if they refer to score, true or false. in lucene, Scorer.score() will only be called, if the expression actually needs it. 2. here it seems, something calls score() always, and passes it to the expression even if its maybe not needed? so it might be called uselessly on the scorer, even if the ranking function doesnt use it. 3. passing score in this hacky way im sure prevents inlining of actual scoring function into the expression and hurts performance.
Oh good point, I missed the call to getAddress
Nevermind, I just noticed this is required for bw compatibility.
It seems to me like this class would be simpler if headers were always not null (and just empty in case there is no header).
I think we should change this to a warning level log (without the return it would be logged as warning in the overloaded shardFailed message)
can we try turning those into constants? I think we should try doing this all the time though.
This effectivly means there is only one field loading concurrently on this service since you are locking on the `loadedDirectFieldData` I am 100% certain about all the implications but I'm 99% sure this is the wrong way to do that. If you want to prevent a single field from loading twice at the same time we have a nice datastructure for this called `KeyedLock` that you can use like this" ``` Java private KeyedLock<String> directLoadingLock = new KeyedLock<>(); //... final String key = fieldNames.indexName(); directLoadingLock.acquire(key); try { // load your stuff } finally { directLoadingLock.release(key) } ``` that way you can just remove all your synchronizaion
also use the constant here
and use the constant here
Else right after a return makes me sad. I'm not sure I have a good reason to think that way, but I do.
I think we should also get rid of `setNextDocId()` since it's part of the scorer by definition? if you wanna make this work in a second PR that's fine too
I think we should use the `ElasticsearchSingleNodeTest.class.getName()` as the cluster name then we know exactly what is going on.
we should set a node name too...
we frequently use randomizing client, I think that it should frequently use the default (null) preference
+1 to that but we don't have do that - lets just stick to what we have
mayb just do `if (++count >=`
I actually wonder if we should have a `MaybeBoolean` class that implements `ToXContent` and can do these kind of merge operations ie similar of haskel maybe
we usually call the getter also `terminateAfter`
I think you can just go with `threshold + 1` on the write side and `threshold - 1` on the read side
maybe make sure it's positive
+1 on removing this
maybe add more docs like `between(2, max)` and use `indexRandom()` so you get holes in the segments ie. del docs. Then you can can randomly set the threshold `between(2, max-1)` etc.
if you use vint you can't write negative values :)
yes, true. Maybe each unicast based test should just stick with its own port range? Not sure how to solve this otherwise. Just want to prevent test failures caused by two nodes taking the same port.
I think we can still run into conflicts between tests here? `RandomizedTest.randomInt(1000)` can be the same between tests and then we have a port conflict.
Should there be sanity checks that you cannot resume a watcher that is already running (so that the watcher doesn't run twice).
Or maybe alternatively use a CopyOnWriteArraySet instead of a list
maybe rename this to assertMaster (as you supply the masterNode that we should match)
nit picking - I would call specificMinimumMasterNodes just minimumMasterNode and update it before the settings (makes the setting code more readable) ``` minimumMasterNode = minimumMasterNode < 0 ? numberOfNodes / 2 + 1 : minimumMasterNode ```
we have a new awaitNoMaster util method
I don't think you need to this, the internal cluster will call the node settings automatically.
the start cluster does this.
Not sure why we get this `if`, I might be missing something but I thought we never update the listed nodes, thus they are always going to be the original discovery nodes with minimum comp version.
typo! missing i ;)
I think I'd prefer to check `listedNodes()` instead of `filteredNodes()`, which wouldn't be empty if the transport client used the right cluster name (`foobar`). Otherwise it feels like we are testing two things at the same time (filtering out the nodes and updating the version)...
can you make it `protected`
use ``` if (!latch.await(10, TimeUnit.SECONDS)) { fail("error message...") } ``` to avoid potentially hanging the test
oh..ok... so make `Operation` implement `Callback`, it'll still clean things up
I think you can drop this interface and move ``` void execute(String action, ActionRequest actionRequest, ActionListener actionListener, ActionFilterChain actionFilterChain); ``` to the Operation (as abstract method)
I think the test framework might complain about this if it's not properly unset. Yet, I we can mute that by extending `private static final String[] IGNORED_INVARIANT_PROPERTIES = { ...` in `AbstractRandomizedTest#186`
You can use `prepareCreate("my-index")` instead of `client().admin().indices().prepareCreate("my-index")`
You could use ``` java index("my-index", "my-type1", "1", "some_field", "some text"); ```
or shorter: `supportsNullValue`? (though I might be completely wrong as my English is what it is...)
I don't think it is ok to use the same configuration parameter for - throwing an error on unmapped fields - throwing an error on deprecated options. I think we should have a new setting.
maybe `collate_match` can be True, False, Null (omitted if Null in XContent)
you might want to pass false to not clear it since you fill it on the next line anyway
I'm not sure this should extend WithOrdinals? (eg. AtomicParentChildFieldData doesn't extend AtomicOrdinalsFieldData)
can we also randomize if we even set this setting at all? i.e. wrap setting it using `randomBoolean`.
should be `logger.debug("...", e, shard.shardId())` :)
and do that in all other classes we do this for serialization in this pull request.
can this be `debug`? it will get very noisy....
I suggest we move the flush outside of the lock, and maintain using a readLock to get the snapshot from the deletion policy. That way we don't have to use write lock here....
I wrote this logic, and I don't like it, maybe turn it upside down, and do "if logger.isTrace -> log.trace the failure, else log.info("....", ExceptionHelper.detailedMessage)
can we use 0 based indexing? :)
To coerce, should be: ``` parser.longValue(true); ```
same here: ``` parser.longValue(true); ```
I think we should move this above the nodeChannels.close() so it will be logged before an eventual consequence.
Same here - I think we should move the log before the close command.
Do you think we would gain something by using directly the long bits to sort? (in the future, we need to switch to the lucene comparators first)
Could you make the reduction create a new aggregation instead of filling the first one? This proved to be error-prone in the past.
similar concern about in-place reduction
> TimeZoneRounding.Builder expects everything thats returned to be an instance of TimeZoneRounding TimeZoneRounding doesn't add anything on top of Rounding so I think it's ok to return/expect Rounding instances here.
Can you add a TODO/followup here to just make SortedDocValues "view" that exposes 'missing' as arbitrary value? I can work on it, just so its always fast...
can we deprecate the method so it doesnt spread? If it spreads somewhere else, then it would need more logic to handle all the cases (e.g. DISI.iterator() == null etc)
mental note to remove this outdated TODO in lucene
maybe just extend the note to say: doesn't check docsWithField since we replace missing values in select() ? Took me a while to figure out why this was safe
while we are at it, can we move this log to debug? its very noisy and it can happen with join retry logic
can we do the assignment in another line, very sneaky :)
should be `setWaitIfOngoing"
this file needs formatting
the utility should be a static class
can we name this maybe just `UpdateTask`
I wonder if we should have a static `EnumSet<State> PAUSE_ELIGABLE = ...` this makes it simpler IMO
just pass `DiscoveryNodes` as an arg
this class should not be part of Guice it should really take the ClusterService whereever it's needed or the actual class it needs and make sure it's a final class with a private Ctor and if applicable make it package private
if you use `NOCOMMIT` instead of `XXX` our builds catch that but the build will fail. I can make sure bill switches off the validation on the feature branches! I will look into that later and will let you know. if you build `-Dvalidate.skip=true` it will be cool
writeString would fail if the default timestamp is null. So I think we would also need to write a boolean to tell whether it is not null? (and an integration test that would exercise serialization)
This breaks the stream backward compatibility. The default timestamp should only be written if the output version is >= 1.4.0
maybe just start a unicast cluster for now
I think that we should do `path == ContentPath.Type.JUST_NAME` instead of `pathType != ContentPath.Type.FULL` In case in the future we want to add a new option...
Is the `if` necessary? It seems to me that the following should work? ``` java for (String pattern : request.selectedFields()) { fieldNames.addAll(indexShard.mapperService().simpleMatchToIndexNames(pattern)); } ```
can we maybe just pass null to `out.writeOptionalStreamable` and leave the impl details to that method
Would be nice to reuse the ReduceContext now that there are several variables
+1 on removing it
s/support multiple command/supports multiple commands
Is this still fast, when we have a packaged application? Does `getResourceAsStream()` need to read all classpath jars, until it finds a corresponding file? We should make sure here that response times for command line tools are crazy fast.
could be a instance variable, as used in all tests
what about creating reusable assertion methods here, along the lines ``` private void assertExecuted(tool, executed, OK) ```
no test annotation (not needed, but for consistency)
resetting the state here makes the test hard to read... can you again maybe create a helper method, that does this ``` assertPrinted(terminal, SILENT) assertNotPrinted(terminal, SILENT) ```
or more simply: ``` java .addMapping("type", "location", "type=geo_point") ```
add a newline here
Missing space after the `for`
Missing space before the bracket
Typo above (`GeometryCollection`) and copy/paste error with the return.
looks like there are two levels of indentation instead of one
this could be a for each loop instead
We don't assume that all actions have the constants, only the ones that we register. This is something that makes me think using mock requests, actions and response would simplify things, as I wrote above. Yet, if this is a good test for consistency we should have it as a separate test and test all the registered actions, not only a few of them.
I would maybe move this to be the actual test method, with `@Test` annotation, and have either an abstract setup method to be implemented by subclasses or even use junit annotations like `@Before` if possible in the subclasses.
I understand the difference between the two...just saying that we are adding an extension point that is currently not used. But if we plan on using it in the future, thats' great. Just reluctant to add complexity when we don't need it.
Ok I see the problem... I still find this hackish (needing to throw an exception to test things), but there's no easy way around it if we want to test different requests and responses. I'd consider using a mock request (one per client type actually) instead and give up on testing those real requests and responses. It would be more unit test friendly cause you'd know the request and the response you need to return (unless it's a nodes info request), you don't need an exception and you can assert on the sendRequest directly. Using real requests it feels wrong to only test a few of them anyway and we know that it's the client that injects the headers (`execute` method), that's what we need to test.
I see your point, fair enough
I double checked and heard that `@Ignore` is needed as well, otherwise IntelliJ tries to run this test as well when running all tests from the IDE.
this should be an abstract class, not sure if we also need the `@Ignore` annotation.
Mocks would only help hopefully trimming the test down and not requiring to throw an exception to properly test things, which is what feels wrong here. They won't help testing it further than this test already does.
sorry, I can't help it... typo s/exected/expected
right, its a mock.. then use `ClusterName.DEFAULT` and you need one less constant.
We need to close this thread pool at the end of the test
I think that if InetAddresses does it already, we might as well rely just on it
can we maybe name this: ``` Java enum IndexScriptExecution { ALL, NONE, SANDBOXED; } ```
maybe add a method to `IndexScriptExecution` like this: ``` Java public boolean enabled(ScriptEngineService service) { // return true for IndexScriptExecution.ALL // return service.sandboxed() for SANDBOXED // return false for others / by default return false; } ```
we should also fix the naming for `DynamicScriptDisabling` to `DynamicScriptExecution`
can we cast to `InternalIndexShard` and call `#store()` on it? I think its cleaner....
Should be unnecessary since shardSafe is called? (as opposed to just `shard`)
Is the copy necessary? (Since `indices` is already immutable)
sorry I didn't see that you created a copy (I missed the "new HashMap" part)
(and also the fact that it will be lost on serialization, etc.)
I think it would be more flexible if the keys were objects? (you could have composite keys, etc.)
otherwise, if you want it for testing, it can be done once in the ctor
can be `protected` and would call it `printStackTrace`
If it can be done as part of another PR I would like it better :)
(but it could wrap a hashmap)
I wonder if we need to do this or if we can just wrap the StreamOutput on writeTo when we write the operation in `TranslogStreams#writeTranslogOperation`
I wonder if we should enable this only for new indices that we know are created with es 1.4
maybe use `IOUtils.close()` handles `null` values
in this case is it worth peaking at the file again and check if the first byte is valid even for version 0? maybe we should do that check first and then move to V1 and fail hard if we see a CorruptIndexExp
can this be in try-with logic.... you are not closing this input stream at all
cool stuff I didn't see that one!
I think we prefer not to use underscores as part of method names, camel case is better
not sure whether we should call them `concreteIndices` here. Seems to me that we usually call them `indices` and they are implicitly concrete in the responses...
Ok sounds good....as long as all the REST tests pass, I'm always afraid of any change to `PathTrie` :) Thanks a lot for the detailed explanation! Maybe it would be better to treat it as a bugfix and get it in separately? I don't have a strong opinion though since it's something that came up with this new api and wouldn't make sense without. Just thinking it would better highlighted as a different change and leave some more history on it this was applied.
sweet! and we should have REST tests for them which you haven't removed, so everything should be fine indeed
I'm concerned that in this case, the per-key list of headers will not be cloned. Maybe this optimization should be removed and just be: ``` java if (customHeaders == null) { customHeaders = Maps.newHashMap(headers.size()); } ```
I think we should not just ignore when something else than a map is provided? Maybe we could do something like: ``` java } else if (propName.equals("fields") { final Map<String, Object> multiFieldsPropNodes; if (propNode instance of List && ((List<?>) propNode.isEmpty()) { multiFieldsPropNodes = Collections.emptyMap(); } else if (propNode instanceof Map) { multiFieldsPropNodes = (Map<String, Object>) propNode; } else { throw new MapperParsingException("Expected map for property [fields] on field [" + multiFieldName + "] or [" + type + "] but got a " + propNode.getClass()); } } ```
I have a similar concern here for when entry.getValue is not a Map
The commons-cli bug only impacts option groups
I see what you mean, I missed the interface is used within the `Transport` impls...right
Well the interface is not used anywhere, it's not that you can plug in your own `TransportServiceAdapter`, it needs to be `TransportService.Adapter`, you see what I mean? That would be possible having `private final TransportServiceAdapther adapter` at line 73.
While I understand why passing `Plugin` here is safe, after thinking about it a bit, I think I prefer replacing the `Plugin plugin` with `String source` to give the flexibility to choose whether this logic should be applied on the Plugin itself (using `onModule` or `processModules`) or on a different `PreProcessModule` (that latter feels more natural to me)
missing t at the end of the method name
missing @Test annotation, I think if the method name doesn't start with test it won't be picked up
Can you add docs? (be it just to mention that the map is cloned)
thinking if those `synchronized` code blocks are needed, if you always check for `null` first, as context is never set to `null` again...
I'm not too happy with the implicit unchecked cast that is happening here. IMO it should either return an Object or require the key to be parameterized (`V getFromContext(Key<V>)`).
@spinscale I think it is needed if it is expected that another thread might be updating the context at the same time (ie. synchronization is protecting the hash map copy rather than the null check)
can't you just use `settings.getAsVersion` here? and on the other end you just use `builder.put(ASSERTING_TRANSPORT_MIN_VERSION_KEY, version)`
The problem here is that it would break multi-version clusters. We still need to read/write vLong depending on in/out.getVersion so that at least positive offsets work.
I think you should not ignore it completely? The contract is that is `simulate` is true then conflicts are reported but changes are not performed.
maybe you can create a `Client` manually instead and make this extend `ElasticsearchTestCase`? at the end of the day you don't really issue the request...
you don't need to rethrow, I'd just make the method throw it as is
Maybe call this "testEmptyBoolSubclausesMatchAll()"? Sorry if I misunderstood what the test is doing, I just think having a github issue number in the name is unhelpful to someone if they see a failure.
It looks like it was replaced by snapshotMetaData bellow and is not needed anymore.
to me it's kind of unclear what this suggester CAN do now vs. what is will be able to do in the future :)
I am concerned about this change. We are leaking communication abstraction into the service layer here. I wonder if it can be avoided by passing necessary structures of CreateIndexRequest as part of CreateIndexClusterStateUpdateRequest or referring to it as TransportMessage or passing necessary pieces of information from TransportMessage as part of CreateIndexClusterStateUpdateRequest.
The copyBytes method is going to copy the entire source regardless of the safety limit that was set above.
The (int) conversion is not needed here.
I see what you mean, that we somehow break bw comp but I would leave only the lowercase variants to make this consistent with other places where we load files e.g. we would never load `elasticsearch.YML` . I think this change is acceptable and in the scope of the PR since we are in fact limiting the files that are getting loaded as logging configuration.
could be shorted using `com.google.common.io.Files.write`
Actually, I think we can do this without a `TemporaryFolder`. We can just write a new file within the `globalTempDir()` that is already available, no need to create a new folder. Also, this should make it possible to merge this new class with the existing `LogginConfigurationTests`.
you can create these files using `temporaryFolder.newFile(filename)`
I think we already do since we don't lowercase the suffix. That's fine with me. I think it's consistent with what we usually do with conf files. Let's try and add some docs for this to make it clear.
we need to support whatever extensions Settings supports and that includes json & java properties formats. I wouldn't worry about restricting it to these three (yml, json, properties) with regards to bwc.
with the current code a `logging.whatever.yaml` file would be loaded. I wonder if this is our intention or a side-effect of the current behaviour. Honestly I would be in favour of simplifying this further and even have something like `if (file.getFileName().toString().equals("logging.yaml") || file.getFileName().toString().equals("logging.yml") )` unless we want to extend this to json and properties files, which I think would be off-topic in this PR.
can we change this to the ES license? :-)
Correct me if I am wrong but these filters have to be executed in a serial fashion one after another, right? So you can make this async if you need to on top of the blocking loop? I would like to see an example where this is used to understand the rational please :)
I don't think this is worth the inconsistency. Please remove that and make it camel cased
given that we also filter responses by creating a new response filter chain and filtered action listener, this inner class is not just a request filter chain... can we maybe merge the two at this point? Seems like in the end we either filters nothing or both (request and response) anyway...
I find these two empty `continueProcessing` methods confusing, if we manage to merge the two filter chains impl as said above, we would get rid of them I think
Can you fix the indentation
similarly, equals uses the hash while hashCode doesn't
Correct [equals](http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#equals%28java.lang.Object%29) implementation supposed to be reflexive. In other words the following test should pass: ``` StoreFileMetaData test = new StoreFileMetaData("test", 0, null, null); assertEquals(test, test); ``` Maybe `equals` is not a good substitution for `isSame` here.
It feels wrong that hashCode is using writtenBy while equals isn't
Changes in this file make the assumption that the only thing that a Engine.GetResult.close release are the underlying searcher, I think it's too fragile
Maybe rename this to setTimeout, and getTimeout below? It is odd having the name overloaded.
This seems very error prone, since it relies on CPU scheduling, network latency, or even whether the test is using mocked networking...
Ok, sounds fine.
maybe we can turn this around and do ``` Java if (electMaster.hasEnoughMasterNodes(possibleMasterNodes)) { // lets tie break between discovered nodes return electMaster.electMaster(possibleMasterNodes); } else { logger.trace("not enough master nodes [{}]", possibleMasterNodes); return null; } ```
ok fine! :)
I'd appreciate if we can do `== false` it's just so much easier to read
I am good with both options.
does it make sense to use the `GeoUtils` constants here? might help people to understand why they are here
fine with me as well. go ahead and push!
Unlikely to matter, but you should put this assignment above the handler registration since it hands out a reference to an incomplete object that _could_ cause trouble.
Ok, I was missing that piece of reasoning. This global ord lookup logic looks good!
Only `_field_names` actually builds the content type and I wonder if it would not be better to remove the `"type"` parameter in `_field_names` parser instead of doing it here for all root mappers. Else it might look like you can define a type for different root mappers: ``` PUT testidx { "mappings": { "doc": { "_analyzer": { "type": "geo_point", "path": "custom_path" } } } } ```
Does this message go back to the end user? If so the fact that a map must be empty is more of an implementation detail than an meaningful error message for the end user. Something like "Mapping definition for field X has unsupported parameters: foo, bar" would be more appropriate.
Arbitrary null values don't really make sense for this mapper so I think it makes sense to remove support for it.
Not sure if this is also supposed to be covered, but I can still do: ``` PUT testidx { "mappings": { "doc": { "properties": { "user": { "type": "string", "norms": { "blah": "blub", "enabled": false } } } } } } ```
fielddata format can still contain arbitrary values: ``` PUT testidx { "mappings": { "doc": { "properties": { "user": { "type": "string", "fielddata": { "format": "fst", "blah": "blub" } } } } } } ``` I am however not sure what is expected here because when I get the mapping the wrong entry will be returned...
This way the fields will not be checked when type is "multi_fields": ``` PUT testidx { "mappings": { "doc": { "properties": { "title": { "type": "multi_field", "fields": { "lala": { "type": "string", "foo": "bar" } } } } } } } ```
Could you extract this "type" string to a constant? (and replace other references to this string to a reference to the constant)
I think it's ok to leave those settings unchecked for now.
It is so greaaaatttt to find such test bugs!
However I think it is important that the default configuration emits an empty json
I think we can remove this exception now.
yea, nice spotting it!. Its not the end of the world, since the node is closing anyhow, but its cleaner.
I don't like this - can we just name is `hasJoinedCluster()`
not sure if it will help but maybe make that an integer such that we can log how often it did join it might help when debugging? just an idea
if these collections are used for iteration I'd use something that has a stable iteration order instead of hashSet which might be different due to object identity and rehashing etc. I butt we should force this on the interface in `electMaster` further down the road
And the related issue here: https://github.com/elasticsearch/elasticsearch/issues/7090
this will not work, right? cause the default is `9300-9400`, which is good, since we want to try another port on the second instance we start on the same machine.
if we do this, then specifying just host without a port will cause to return a list of 100 addresses by default
should we think about the fact that flush might not be allowed since a recovery is happening (that will eventually get cancelled since we are closing the engine)
+1 on the closed indices use case. Good catch. I'm not sure it has effect now. Did some sniffing and we close the shards in too many places upon index close. First place in `IndicesClusterStateService.applyDeletedShards`, then we have another iteration in the beginning of `IndicesClusterStateService.applyCleanedIndices` : ``` for (IndexService indexService : indicesService) { String index = indexService.index().getName(); IndexMetaData indexMetaData = event.state().metaData().index(index); if (indexMetaData != null && indexMetaData.state() == IndexMetaData.State.CLOSE) { for (Integer shardId : indexService.shardIds()) { logger.debug("[{}][{}] removing shard (index is closed)", index, shardId); try { indexService.removeShard(shardId, "removing shard (index is closed)"); } catch (Throwable e) { logger.warn("[{}] failed to remove shard (index is closed)", e, index); } } } } ``` and only then we do `indicesService.removeIndex(index, reason);` which closes the index (but it has no shards any more..)
the deleted flag is only set when an index is deleted. The PR description says we don't want to flush upon relocation. I'm a bit on the fence w.r.t flush on relocation. On one hand it would be good to leave things consistent. On the other hand it may delay things needlessly (this runs on the cluster state thread). In anyway we should change the description of the PR we keep this way..
hey can we rename this V1 to ChecksummedTranslogStream
should we make this stream private here? I mean we might wanna have a dedicated exception for this
really shouldn't we just skip this in the `LegacyTranslogStream`
you can just replace with `IOUtils#closeWhileHandlingExceptions`
Maybe rename this method to addedOrChanged() or something like this to reflect the fact that we don't care about deleted benchmarks here.
Since lifecycle methods are not doing anything and are not invoked either, it might make sense to extend AbstractComponent instead of AbstractLifecycleComponent here.
It seems to me that this belongs to BenchmarkExecutorService. The rest of the methods can be moved into an utility class and then we can get rid of AbstractBenchmarkService.
we need to find a way to keep the headers and context of the original benchmark requests here, when executing the "internal" ones. That might involve recreating the `SearchRequest` via copy constructor passing in the original benchmark request where the headers and context would get copied from.
same as before, we need to make sure the headers and context from the original request get copied to the internal ones. This is a pattern that we need to apply to any internal requests we create and execute, I might have missed some in my review :)
Not needed change :)
I think we should make this a constant. For consistentcy we should use the `es.` prefix rather than `elasticsearch.`
isn't it cleaner to pass `analyzer.getReuseStrrategy()` here? Even though its not really that relevant as its a pure delegate...
You didn't introduce it, but seeing this line again reminds me that this is buggy if Long.compare returns Integer.MIN_VALUE, which is legal :) So it should rather be `Long.compare(o2.getDocCount(), o1.getDocCount())` (without the minus sign)
it can be `null` by default and lazily initialized when headers are actually added
wouldn't it be cleaner to just work with sets here? (instead of converting to arrays)
yeah... fair point... ok... leave as is
Not important, but couldn't this just be an array? String[] possiblePathValues = {"some_path", "anotherPath", null};
(there are a couple of other places in the code that have the same issue)
at some point we should think of a `SuggesterConfig` class ala `IndexWrtierConfig` these ctors are a nightmare
it's usually a good idea to print the actual value as the assert message it can be very helpful if it doesn't reproduce
IMO we can name this calss just `Result` since it's already an inner class in `XLookup` no? so it has the namespace twice?! :)
IMO we should allow the interface to throw IOException that just makes sense here
I'd return an Immutable set... it enforce compilation failure when used inappropriately (if the user tries to mutate the set)
we really need to make these listeners simpler in the case of exceptions :)
can we please use ActionListener here instead and use a struct that holds the arrays? I really don't think we need these listeners
maybe just use `Joiner.java` ie. ``` Java StringBuilder builder = new StringBuilder('['); Joiner.on(",").appendTo(builder, failure); return builder.append(']').toString(); ```
should we also fail if the name is empty? maybe use `String.isEmpty(name)`
> the original query weight is greater than 1 and that the rescore weight is greater than 0 `query_weight >= 1` is not even necessary anymore now that you changed rescoring to multiply the score of all hits with the query weight
Is there a specific reason the first two args need to be reversed? I would tend to keep them the same, but I'm not against it changing if there is some kind of justification.
can we also have a test for an error condition. ie. a field can't be parsed, the doc is empty, the doc is malformed etc.
ok, we can always port it back later on.
I wonder if we should use a hash here instead if some corruption wipes all bytes to 0 or some other value? maybe `MurmurHash3`
Maybe use ConcurrentHashMap? if we can then we don't the synchronized methods.
yes, that make sense
ah ok that make sense
+1 this really cleans up code in several places
Hmm, this assertion can never fail? (NO_NORE_DOCS is Integer.MAX_VALUE). It looks like the other modes have the same issue, I think the intent was to put a "&&" instead of the "||"
I don't see NO_MORE_DOCS changing in the future. I don't dislike having NO_MORE_DOCS=MAX_VALUE, it makes the sequences of integers returned by DocIdSetIterator monotonic from -1 (not started) to MAX_VALUE (exhausted) :)
I think we should return a readable / understandable msg here something like: ``` Java QueryPhaseExecutionException("Result window is too large, from+size must be less than or equal to: [" + Integer.MAX_VALUE + "] but was [" + (((long)from()) + ((long)size()))) + "]"; ```
today we ignore the mentioned exception in the engine, where its actually a real problem. We managed to find the entry in the transaction log, yet failed to read it, this can return potentially the wrong "latest value" for get. The code in the method to retrieve the source should not fail with IOEXception unless there is a real problem here, and this should propagate I think to the client.
alright - lets get this in
oh no it now throws `ElasticsearchException` which I think is weird it should throw IOException
I think this method should throw IOException
+1 on removing this - it's not the job of this thread
This might fail because the currentJoinThread is not null. I wonder if we can keep this simple and have an atomic success boolean this task sets to true + a count down latch for the outer thread to wait on.
AtomicBoolean for success will make this simpler too.
what could possibly be wrong with this :)
s/un a thread/in a thread/
I think it's fine to add such a method to NoCacheFilter
I see. I think it's a bit confusing that this parameter is optional on some methods (with a fallback to the instance parser) and compulsory in other methods. So +1 to fall back to dateMathParser here too
I have just pushed the class ScriptParameterParser to master and 1.x which standardises parameter parsing for all scripts across the codebase. It would be good to use that for parsing the script parameters here too. It accepts four parameters: - lang - for specifying the script language - script - for inline scripts - script_file - for file scripts - script_id - for indexed scripts
Same as before, we might want to update this to 1.3.3 (in all branches)
scratch that - it's caught later on in the code.
We lost some info here - the original error was also including the request and timeout settings (and some more stuff, but I think that's minor)
Feel like this should be more significant than `debug` because it really indicates a form of failure in some scenarios.
It'd be better, to only skip the config files that have the same name, not the whole directory. So for example, if currently es has a config dir that looks like this (for plugins `foo`): ``` /config/foo/bar.yml ``` and the new plugin that installs needs to copy two files there: ``` /config/foo/bar.yml /config/foo/baz.yml ``` we'll skip `bar.yml` but we'll still copy `baz.yml`. This will help us a lot when we guide the user on how to upgrade - instead of telling the user, to copy & modify a bunch of files, we'll only need to tell him to modify files under `config/foo`
Yes, but replace `FileAlreadyExistsException` with `IOException` to maintain the same functionality. Sorry for saying it so confusingly before.
`FileAlreadyExistsException` is an `IOException` and this `IOException` block does the same thing as doing nothing -- `return CONTINUE;`.
Would it be easier to copy the old config to somewhere else, then replace it rather than trying to mix adding new files while keeping old files? I feel like this will be very confusing for users, especially if `file1.yml` had changes coming from both directions.
add some message to the assertions here so it makes more sense than `expected true/false` in case it fails. Can you add an `ElasticsearchAssertions.assertFileExists` (for both `File` and `Path`) and `ElasticsearchAssertions.assertDirectoryExists` as shortcuts, I guess we can reuse that a lot
I'm okay with `foo.backup`. It would also not be hidden from non-Windows users by default.
@dadoonet I don't think it complicates things that much.. it's just traversing the file tree... and yes... sub-folders need to be supported as well. so if you see a folder with the same name/path in both places, recursively merge the two by adding the new files and skipping existing ones. I'd also argue that if in the es plugin config dir there's a file that doesn't exist in the new plugin dir structure, then rename it to "<original_file_name>.<original_extension>.old" (or something like that).
possibly use a `SimpleFileVisitor` here to not override all methods
you can reduce this code by using only the `nio` classes instead of moving forth and back between NIO and `File`, see `java.nio.files.Files` for things like moving `Path`
I still think skipping is better tbh... otherwise the users will be more confused on how to merge the old one with the new one. I don't think the file formats will change that much... but when they do, we'll simply be able to tell them what to change in the files they already configured in order to be compatible with the new version (not sure that will even be needed, we could perhaps support both formats in the code and make it transparent).
thsi is actually concerning... what if the user uses yaml reponse format or cbor then we render still in JSON? I think we need to find a better way to serialise this. I really don't know from the top of my head how to solve this to be honest...
space after `)` -- better format this entire method :)
space after `,`
and the `ExceptionsHelper.` qualifier is unnecessary
can this be a constant
I'd try to keep what gets returned from there from the request inline with what is actually used in the transport action. That's why I suggested to move the logic to the request.
I wonder if this specific default should only be used in the REST layer, or if we should move this logic to the transport action so that it's applied to the java client as well. That way we would have consistency between REST and transport layer...
ok fair enough
and also assert that `startOfThrottle != 0`
no need for this to be public
if we use millis then we really should make sure that time doesn't go backwards!
honestly this seems just wrong. The throttle is never null. It's designed to be used all the time and it's the actual lock that is used inside `class IndexThrottle` that make the difference. I think this should be implemented by using a `TimedInternalLock extends InternalLock` wrapper that is used inside `IndexThrottle` this implementation actually makes no sense to me to be honest.
there is also no test for this at all I think we need one that 1. checks that is works and 2. that it's 0 if no throttle :)
I would use System.currentTimeInMillis, nanoTime has different semantics
would use currenTimeInMillis here as well, also, shouldn't the order be reversed? (currentTime - startOf)
it seems like `MetaData.Custom` should be and abstract class
can we maybe use `persistenOnly == false`
We should deal with rejections if the executor is closed.
make `SendPingHandler implement Closeable` and then use `IOUtils.close(receivedResponses.values())`
we should probably bail here. One nit pick - I would prefer having this rejection logic closer to where it holds. I think there is only one method that can cause this.
with the new close logic, I'm worried about unneeded warn messages in the logs during shutdown. I wonder if we should add a global close flag to the entire class that can be used to suppress this.
maybe make this a `private final AtomicBoolean closed` and in close you can then protect double closing by ``` Java public void close() { if (closed.compareAndSet(false, true)) { //do the things } } ```
You can use subprocess instead: s = subprocess.check_output('git diff --shortstat', shell=True)
For all of these find calls, you can use `in` operator instead: if '(On branch %s' % branchName) not in s:
Is this change intentional? Seems like the metavar should be something like "RELEASE_BRANCH".
on a different note - I wonder if we want to port a slightly modified version of this to 1.x such that folk coming from 1.5 can remove their custom hash before the 2.0 upgrade. I also think we should deprecate the setting in 1.x already
I wonder why we don't implement the `HashFunction` interface for `murmur3` and then simply use this here without all the other code chances? We can deprecate the version with the `type` too and force the new `Murmur3HashFunction` to just use the ID? for new indices we should just barf is somebody sets to use the type.
I'd just use the Id really
do we need so many? (up to 10)
I think 1/2 + 1 == 1 :)
I think we should place it after the other _meta fields - it's less important and shouldn't be first.
can you give an example of the output here? I wonder if we have duplicate information between the shard info and the failures bellow.. also, I'm not sure it should be a top level item here, but rather inline with each op.
same here regarding the ShardReplicaRequest...
Minor - can we move the _shards above CREATED? will look better. now looks like this: ``` { "_index": "index", "_type": "type", "_id": "1", "_version": 1, "created": true, "_shards": { "total": 2, "successful": 1, "failed": 0 } } ```
yeah, I meant the `failures.addAll(Arrays.asList(this.failures)); - this is a constructor, so we know this.failures is empty.
Cancel that. This is an internal class.
Can we fold shard info the constructors of responses? This way it's easy to forget.
Nit picky - we can iterate directly on the getFailures array - no need to check for length
We should try to take the rest status from the exception. See ShardSearchFailure
We suppress and not report all errors which are OK. I don't think we need a special protection here about it.
I think it will be cleaner to have ShardInfo have a constructor that takes all parameters and set that on the finalResponse. This will make sure we will not forget anything in the future.
We should call ignoreReplicaException() imho.
same here - can we move the `_shards` to be the last of the `_` fields? Now it's mixed: ``` { "index": { "_index": "index", "_type": "type", "_id": "3", "_version": 1, "status": 201, "_shards": { "total": 2, "successful": 1, "failed": 0 } } }, ```
I think we want `shardInfo.toXContent(builder, request);` here? like this we loose the request as params
I think we want shardInfo.toXContent(builder, request); here? like this we loose the request as params
I think we want shardInfo.toXContent(builder, request); here? like this we loose the request as params
why not capture the shard info during the failure handling? Then we don't need to search for the iterator..
This shardId seems redundant to me as well.
That makes me think that ShardInfo needs a public default constructor.
@javanna I don't think it is? see org.elasticsearch.action.deletebyquery.DeleteByQueryResponse#iterator
can you rename `k` to something a bit more meaningful? :)
I think we discussed this before, but it didn't change, thus I'm bringing it up again ;) can we add a constructor that accepts `shardInfo` as argument and change the subclasses constructors to accept it there, just to enforce that this info is needed so we don't forget it anywhere. Maybe then we could also remove the setter...
I see, thanks for the explanation!
I'm a bit worried about the differences in the total count (based on shardIt.size()) and the amount of calls we actually make (based on numberOfShardInstace). We may end up in a situation where successful > total and maybe worse successful == total but they mean different shards. I think we should not base the counts in ReplicationState on the ShardIt but rather on the total of actual work + unassigned.
internal class :) I think it's fine
grrr...you are right @bleskes sorry for the confusion. It would be a breaking change for the java API. I don't like it when internal requests get exposed as they are to the outside world :)
I see some places where null is not protected against...
drop the actually? sounds so "uncertain" :)
we don't count shard not allocated / not started/ closed etc. as shard failures - see Search logic. This will end up as a difference between total shards and shard failed. The reason is that there is no way to distinguish this case with the one that our cluster state said they were unassigned.
same here. Not all failures are counted.
same here. Not all failures are counted.
can we mark nodeId as nullable? (or find a way to always have a value)
same here regarding nullable ..
do we want Vints here? (plus assertion about non-negative values in constructors)
Elasticsearch tradition is to make this an EMPTY constant :)
Unused import here (not really a big deal)
Again, need to figure out what to do if ATOMIC_MOVE is not supported
Shouldn't this be using `ElasticsearchTestCase.randomFrom` instead of `com.carrotsearch.ant.tasks.junit4.dependencies.com.carrotsearch.randomizedtesting.generators.RandomPicks.randomFrom`? I don't want it to end up not using the right seed
You already asserted this 2 lines ago, this is a duplicate.
Can we move these up at the top of the class with the other object variable declarations? I think it is more readable than having them 300 lines down into the class.
This "_global" should be a static constant var.
Since "version" and "primary" are used not only here, but below in the `fromXContent`, they may go better in a static `Fields` encapsulation so if they are changed in the future they only have to be changed in one place.
Minor, but missing a " " between `..._PREFIX,shards...`
I think this should be a separate file so things like snapshot & restore can re-use it in the case the state is corrupted, it's easy to miss down here all alone
Typo here, "pathf" instead of "path"
This is not good for backword compatibility. Instead it should do: ``` if (indexSettings.getIndexVersionCreated().before(Version.V_6_0_0)) { String tokenizerName = settings.get("tokenizer", "whitespace"); tokenizerFactory = ...; } else { tokenizerFactory = null; } ```
Can you move these class variable definitions up to the top of the class? It's weird to see them after function definitions
This could be `Strings.hasLength(tokenizerName)`
I don't think we should swallow the exceptions here, instead, this could be a try-with-resources block like: ``` java try (BufferedReader br = new BufferedReader(rulesReader)) { .. read the string .. } ```
let's not log since it is an old index
I think we should close the analyzer if tokenizerFactory is not null. Otherwise, it is never closed. ``` finally { if (tokenizerFactory != null) { analyzer.close(); } } ``` (this looks like a pre-existing bug, it was not introduced by your PR)
Ahh okay, makes sense, good point.
No need for this `else` statement since `lALogger` is already initialized to null
ok good - better than an undefined state!
@bleskes has the concern that this state is not defined in our system (since we protect on it from the "outside"), and the implications of having this clash is not evident. I was leaning towards living with the clash, but since its not defined, this change might be good enough for now.
for safety can you do `"groovy".equals(script.lang())` instead
maybe catch exceptions here and put them into an arraylist, you can then just use ExceptionHelper to rerthrow and surpress....it has a utility for this
Most likely meant `&&` instead of `&`.
Should probably be `final` (hands wrote `null`, but the brain meant `final`).
Oh, sorry, I meant mixed in the sense of one with `@Test` and one without. I am all for separate tests because it gives a lot more granularity. Honestly think the above test could be at least two.
In which case your original solution would have worked too. Sorry
Instead of creating the formerParams and replacing them afterwards could we not create a `nextParams` map which is a copy of the params map and pass it into the node.retrieve method? This would save us having to put things back after the method returns
I think if you do `nextParams.putAll(params)` here you don't need to clear the `nextParams` map or do the `params.putAll(nextParams)` call below as the `nextParams` map will be used if `node.retrieve()` returns something and if it returns `null`, `params` will be untouched for the next attempt at retrieving the node.
Yes sorry you are right, I didn't see that the params variable was a parameter passed in and therefore need to be updated and was just thinking about the params map in the node that would be returned.
I think since you change the finally part you should do this like: ``` Java try { Store.verify(indexOutput); } finally { indexOutput.close(); } ``` just to make sure we are closing the stream asap
I am not sure if we should catch an exception here IMO the exception should bubble up
see above about abstract runnable
Can we remove this line if it is not needed? At a glance it just looks like a broken test that was "simplified".
It seems like the settings be set here are the exact same as in `teardown()`. Perhaps this could be in a `reset()` function called in each? I'm just worried if this must be changed in the future, it could be accidentally changed in one method but not the other.
Why not? http://vimeo.com/105758303
thanks for fixing this
beware that childrenIterator cann be null here, so I think you need to have ``` + if (childrenIterator == null) { + return null; + } ```
thanks for adding this TODO
I think we simplify this code by merging the two use cases. The only difference is the exception that goes into the BulkItemResponse.Failure object. Can we have that as a variable instead of isClosed , i.e., `Exception unavailableException = null` and later check: ``` if (unavailableException != null) { BulkItemResponse.Failure failure = new BulkItemResponse.Failure(request.index(), request.type(), request.id(), unavailableException); BulkItemResponse bulkItemResponse = new BulkItemResponse(idx, "index", failure); responses.set(idx, bulkItemResponse); // make sure the request gets never processed again bulkRequest.requests.set(idx, null); } ```
This can be replaced with ``` java boolean isFalse = isExplicitFalse(value); ``` to simplify and not duplicate the logic
This can be replaced with ``` java boolean isTrue = isExplicitTrue(value); ``` similar to the previous example
Extremely minor grammar thing, these should be: ``` all rebalancing is allowed no rebalancing is allowed primary rebalancing is allowed replica rebalancing is disallowed replica rebalancing is allowed primary rebalancing is disallowed ``` I'd also recommend `forbidden` instead of `disallowed` because it's much less likely to mix up with `allowed` at a quick glance.
This assert message says 2 shards, but the check is for `equalTo(6)`
Same here, with `CLUSTER_ROUTING_REBALANCE_ENABLE`
This seems to be the only "shared" portion for string/value. Perhaps it could be moved out? Or just have a helper method, or even just leave the duplication (it is only 2 lines really, not bad).
I think it might be cleaner to separate this to one elseif handling the string, and have an additional elsif after handling a direct value? This would eliminate the need for the if/else below for value/string, and would clearly separate what happens in each case (and then you don't need the string checking in the other conditionals as well).
I think it's good to explicitly mention that this may throw an IndexCorruptedException
I wonder why this change? initial value is unused and once existing files are set, they are never changed.
little trailing space
++ . I like this better then the listener.
We should remove the Store part. Perhaps make a constructor with a name? these errors are difficult tot trace so we should make it as clear as possible where the error came from (even if the stack trace is lost)
can we invert this logic and do `if (recoveryStatus.tryIncRef()) {` or use `== false`
@bogensberger My thought here was that although the timeout should have no effect given `auto_import_dangled` is set to true, it would be good if there was a test that ensures that this is the case so that the code is protected against regressions. By using a randomised value here we can test with various timeouts and ensure that we don't accidentally change the functionality to rely on the `dangling_timeout` being set to `0`in the future.
This adds the `gateway.type` setting twice here
`FutureUtils.cancel` has a check for a null future, no need to add this check
just being over paranoid I think this class should be final
can this api be package private
can we turn it around and do `if (usage != null)`
I wonder if we should have a `LatchedActionListener` that has this logic where we can just do `new LatchedActionListener(delegate, latch)`? I bet there or other places doing the same thing
Do we also need to protect agains renaming to `IndexWriter.WRITE_LOCK_NAME`
This method can be removed now - it's identical to the base class implementation
can we have a better failure message - "file ["+tmpFileName + "] was renamed but still exists"
Same here - the same as base calss
can we add a one lliner java doc explaining why this is needed (rather then pass through the the primary lock factory)? it's non-trivial to figure it is done to track the location of the lock file, if it's using files..
can we also assert so the failure message will include the offending file? I think it will be easier to trace that the alternative of looking for it in the logs..
Sorry, I completely missed the name of the method before! However, since this is the repo name, which is essentially the base path for the snapshot, why not name it something related to the test, rather than random? This would be easier to figure out what you could delete, rather than having to remember the uuids for repos that were created from past test runs.
Yeah, I think doing this one way or the other would be better, just not both. It is confusing right now.
I just realized this will probably cause problems with the test security manager? It stops anything being written outside where we expect (ie in temp dirs controlled by the test). Right now it lets you write anywhere based on the CWD of the test, but I hope to change that in the future. It would probably be best to create a temp dir? So it won't be any better than what you have here with the UUID as far as knowing which dir to expect, but you can give it a meaningful prefix, and it means it will be cleaned up easily.
do we need this logic? we know what we will send it
also, I think the opened channel needs to be closed at one point
should be cached thread pool, the default constructor does the right thing here
I wonder if it should not also decrement the CountDownLatch if an exception is caught in the SimpleChannelUpstreamHandler ? Something like ``` new SimpleChannelUpstreamHandler() { @Override public void messageReceived(..) { ... latch.countDown(); } @Override public void exceptionCaught(...) { latch.countDown(); } ``` Just to be sure that the client does not hang indefintily.
I was just wondering if there could be bad side-effects such as causing some operations to take longer.
@rjernst can you fix this ^^
nevermind I saw the usage
I think you should just do `instanceof Number` and else call `.toString()`
Maybe the max should be more? Something like 5
ah got it!
I think we can simplify the exception capturing here and just throw it as soon as we catch it (and assertions are enabled).
I think it's only usabe in assertions :) can you give a usage example? . Maybe call it reThrowIfNotNull and allow passing null values into it? might make it more usefull .
since finalizePingCycle will call listener.onPing, I think we shouldn't re-throw the exception but rather let finalizePingCycle do it's thing here: ``` logger.warn("[{}] failed to send pings", t, id); finalizePingCycle(id, listener); ``` at a future change we will do the same with UnicastZenPing
we should log the exception here.
can this be `else if` saves a line or two
I think this test should write multiple legacy version just for kicks....
I double checked and the scheduler can take a value of 0 and the listener removal logic is also OK where postAdded and onTimeout are called in rapid succession. I still think the check for 0 here is clearer. Thanks for adding it.
yeah but then they will be called twice. But OK, let's leave this like it is - keep the change small.
I think a time out of 0 is problematic in the InternalClusterService logic. We should protect for it, either in executeHealth or in the ClusterStateObserver
The first line of the method can be removed now - the endTime variable is not used. ``` long endTime = System.currentTimeMillis() + request.timeout().millis(); ```
can we invert this and skip the `continue`
I think we should at least log this exception it could be toomanyopenfiles etc.
I think this should be final
Can not name this hasFastIterator? I feel the name continues to encourage that such slow filters are ok. If we are planning to add support for broken iterators to DocIdSetIterator as a method (I'm honestly not sure how i feel about this), then i think the method should be something like isBroken() and should be deprecated to discourage adding more broken iterators.
Given that the changes that you made also apply to objects (the `else` branch), I'd also like to have tests that check what happens when replacing a single value with an object, an object with an array, etc.
+1 this also looks useful for objects
since we use this for logging now - I think we should change the node Id to DiscoveryNode - ids are hard to trace.
not 100% sure we wanna return e1 here as failure, maybe the original one would be better given that the fallback call failed? I guess it depends on how you look at it... since we don't check the version of the node we are talking to. Maybe it's good as is, not sure really
FYI if you want (not that it is a problem as it works now) you can leave the randomized number of shards when creating the index and retrieve it through `getNumShards("test").numShards` which you can use for the comparison.
To clarify, I meant the default indices options used if the user doesn't set them.
`createIndex("test")` ? then you can remove the following `assertAcked`
I vaguely remember some differences around the indices options used depending on what we retrieve. Are those still 100% bw compatible when downgrading the request? I guess so since if I remember correctly the get index behaviour is already bw compatible but wanted to double check ;)
nitpicking here...but we could use an array instead of a list and avoid the conversion list to array afterwards
same as above, could be an array
I was on the fence too cause it's internal code but it seems consistent with the other assert that we have on the static block
oh oh I hadn't read your reply when I replied ;)
I think we can change that as the string methods will be deprecated and afterwards removed in master ;) My preference goes to varargs then
I see, I didn't notice that, cool no problem
what happens if we assign to null? wondering why these checks are needed
I think we should leave this method for java api bw comp
I think we should leave this method for java api bw comp
When trying to figure out what exactly goes wrong when the above query is executed\* I noticed that the template string above doesn't look quite right. When changing it to ``` String templateString = "{" + " \"template\" : {" + " \"query\":{\"match_{{#use_it}}{{template}}{{/use_it}}\":{} }," + " \"params\":{" + " \"template\":\"all\"," + " \"use_it\": true" + " }" + "}"; ``` your test is green on my master - am I missing something here or was that just a misunderstanding on how to formulate the template? Caveat: I didn't check re-formulating the other two tests yet. - And finally printing the non-quoted version after staring at the quoted one.
No need for these version checks, a full cluster restart will be required for 2.0 anyway
no need for version checks
I am wondering if it makes sense to implement `getProperty()` for `Aggregations` as well and not just for `Aggregation`. For example in a test I would write something like ``` Aggregations agg = searchResponse.getAggregations(); Object o =agg.get("aggname").getProperty("path"); ``` but if Aggregations also implemented getProperty() I would save another line and it is needed anyway here internally.
I think this should be `Object[] propertiesCounts = (Object[]) histo.getProperty("sum.value");` else the tests don't pass.
we should generate some 0.90 indices here too just to be sure :) they are all Lucene 4.x
unkown -> uknown
"now" -> "not"
It looks like this includes the change from #8383? (nothing that needs to change, just curious)
`listener` should be marked `@Nullable` here
I think it would be clearer to rename `file` to `dir` or `directory` here
Same "now" -> "not" typo here
same typo - copy paste probably
"Directory" is misspelled as "Direcotry" here
we need to change these docs. The lock is not held while the method is called.
Listener can be null here.
typo : now -> not
`index` can be null here, which causes an NPE because the `ShardId` constructor constructs a new `Index` object which in turn interns the name and dereferences the null object.
these should be : ``` .putArray("path.data", tmpPaths()) ```
now that we use a semaphore, we don't need to put it back - we can remove the synchronised as well
left over reference to a countdown latch
much cleaner. thx.
this one is not used - can we remove it until we cat over to the paths API
is this needed? as far as I can tell, the implementation does nothing?: ``` try { if (lock != null) { <-- **lock is null** try { lock.release(); lock = null; } finally { clearLockHeld(path); } } } finally { IOUtils.close(channel); <-- channel is null channel = null; } ```
I think we can better call this in the Store.OnCloseListener.afterIndexClosed(). Feels cleaner to call this once we think all shards have bee removed.
You defined this method as a helper, but are calling `shardLock(id, 0)` manually all throughout the code still, it would be good to change all of those instances to `shardLock(id)`
same here regarding the logging as above if applicable
should we warn on this failure now? because of we have a bug and we can't obtain the lock, we won't delete anything, which is a different semantics than deleting the files and letting the OS handle dangling open file handles? This in theory should not happen because a lock is there
Can this use `nodeEnvironment.shardPaths(shardId)` instead? Then it doesn't need to use `toPaths`
By moving this inside the `for` loop it throws after the first IOException, so it would be better to throw inside the `catch` block (unless moving it inside was not intended)
Can you mark this as `@Nullable` (here and in the implementer) since it's called with `null`
`NodeEnvironment` is already injected in this constructor, as `nodeEnv` (it's in the top line)
I think enforcing this as a List of `ShardLock`s would be better, type safety wise
I think the name of the method is misleading. Maybe call it purgeIndexDirectory? as it doesn't really delete it but rather removes all unlocked shards and if all succeeds removes the index folder as well
Imho this is a confusing description. Maybe "Deletes all shards directories of an index for which a lock can be acquired. If all shards dirs are removed, the top level index directory is removed as well."
minor typo - "indexes shards lock" -> "shard locks"
I think this message is misleading - we don't actually schedule the delete of this index but rather ignore it. Maybe change to "failed to lock a dangling index [{}], probably in the process in being deleted, ignoring." . I also think we should include the exception as it may not be a LockObtainFailedException but something else.
Ignore that. Got my listeners confused.
"now" should be "not"
unkown -> uknown
This exception doesn't make sense, because the actual timeout exception will be a `LockObtainFailedException` that is thrown and not caught. The boolean is only returned if the `FileChannel` couldn't be locked
It would make sense to refactor this into `private Settings newNodeEnvSettings()` since every test requires it, in case it ever has to be changed in the future.
It would be better to change this to: ``` java logger.debug("creating shard_id {}", shardId); ``` To log both the index and shard ID, since `ShardId` has a nice `.toString()` method
Same here for `compareAndSet`
I chased this one up, wondering the same thing and there is no asEagerSingelton for instances. We seem to do a similar thing in many other places.
In our discussion about semaphores I understood a different model we keep a semaphore per index/shard directory (like the on the disk locks but in memory). That would be pruned when the folders are pruned. I see where you were heading. I'm fine with either way.
This can be `assertTrue(called.compareAndSet(false, true))` to avoid the race condition between the assert and the set.
I think this should be an `AbstractRunnable`, so it can either set `isForcedExecution` to true, or handle rejections gracefully, in the event the executor is ever set to a threadpool other than the generic one, or if we limit the queue_size for the generic threadpool (which we have discussed doing)
I would definitely prefer a pure function here that returns a `Set<ShardId>` instead of mutating the method parameter
This many levels of nesting hurts my head! How about refactoring the inner half into a private `findShardIds(@Nullable String index, Path indexPath)` method so it's easier to read? I'm worried about the potential for future typos for anyone else touching this code
yea, there is no need, if you bind an actual instance, it is by definition a singleton (how would it not be, can't really create another instance of it..)
Oh, nevermind on the second point, I see `ShardLock` implements `Closable` already.
It looks like if `index` is null here, we will end up locking all shards for all indices, then hit an NPE, then release all the locks. Would it be better to bail early if `index` is null without trying to acquire locks? It seems a little strange here since a null `Index` is used in some of the other methods to indicate "all indices".
Unneeded import and extra line addition to this file
I think ``` java if (prevParentDoc == -1) { childDocId = childDocs.nextDoc(); } else { if (childDocs.docID() > prevParentDoc) { childDocId = childDocs.docID(); } else { childDocId = childDocs.advance(prevParentDoc); } } ``` could just be replaced with ``` java if (childDocs.docID() > prevParentDoc) { childDocId = childDocs.docID(); } else { childDocId = childDocs.advance(prevParentDoc + 1); } ``` ? (No more check that the previous parent doc is -1, and advance to `prevParentDoc+1` instead of `prevParentDoc`)
Main thing here is that the test fails rather than errors on this line
Could possibly use `assertThat(e, org.hamcrest.Matchers.isA(ElasticsearchParseException));` here instead but I don't have a strong opinion about this either way.
Would it be better to use the Assert.fail(String) method or throw an AssertionError here? That way the test will fail correctly in the test framework
can resolve possibly return null? if so we should check for it
I think we should just stick with what we have for now :/ filter needs a rewrite method
why do we actually need to only wrap this in a `LateParsingFilter` if there is no context? can't we do this all the time and in our tests do the resolve instead? I don't like the special casing based on the context here to be honest
Can we make a NoopGatewayAllocator (alla NoopClusterService) and put it in org.elasticsearch.test.gateway? then we can reuse it bellow
I see. +1 on removing.
I think we can just drop this variable.
Not your code, but surrounding it: ``` java if (restoreInfo == null) { if (request.waitForCompletion()) { // ... } else { listener.onResponse(new RestoreSnapshotResponse(null)); } } else { listener.onResponse(new RestoreSnapshotResponse(restoreInfo)); } ``` Could be simplified ``` java if (restoreInfo == null && request.waitForCompletion()) { // ... } else { listener.onResponse(new RestoreSnapshotResponse(restoreInfo)); } ```
unrelated but I think we should use `DelegatingActionListener` here
can we make this final and initialize it anyway - I also think it'd be good to make this a `private final Map<...` instead of the concrete type
nice - I like those removals
`shard started` is misleading if `shardRouting == null`.
make it final to be a struct ;)
well if a test doesn't call `super.nodeSettings(nodeOrdinal)` that is a bug. We have to enforce it though. IMO we can use a similar way as the test base class does but we don't have to do it here...
alread => already
typo - failIfCancled -> failIfCanceled
same here - In think we should add the shard info either to handler or to the message.
might also want to add a toString implementation on ShardRecoveryHandler or add the shard in question to the assert.
We should catch any exceptions during the cancel and log them so we can continue to cancel any other handlers? Otherwise the first exception will cause us to bail
This is a hard override (via index.mapping.date.round_ceil setting, which default to true) to disallow rounding up. Not sure if this actuall set to false, seems undesired to me.
We should log the the failure here if the close fails
can we maybe un-chain this line it's very hard to read... like ``` Java CreateIndexRequest indexReq = new CreateIndexRequest(request); indexReq.mapping(request.type()); //... ```
I wanted this directory to be consistent with whatever was written through the delegates as well. If there was an existing lock file on the Filesystem then we "loaded" it on startup via `#listAll()`
I would not make this a LockFactory at all. This should just be a wrapper Directory and override makeLock() to wrap the lock returned by the delegate as DistributorLock. Wrapping LockFactories was completely removed in Lucene's test-framework. Check out, how MockDirWrapper handles this - you should just wrap directories and its locks inside makeLock(). Keep in mind, LockFactories should really have no state at all and be singletons. This will in my opinion also make the reflection hacks obsolete, because you dont need to do instanceof checks. MockDir's LockFactory was as crazy as this, but now its super-simple and obsolete. MockDirWrapper just wraps the lock returned by the delegate directory, no type checking needed. Ah now I know the problem: DistributorDirectory should extend FilterDirectory or just Directory, but _not_ BaseDirectory! The makeLock in BaseDirectory is final, because this one uses LockFactory. If you extend the right class, you don't need a LockFactory.
OK, did not know this :-)
One way to implement this: Ignore the LockFactory of the inner directories and require the LockFactory to be set on DistributorDirectory itsself (by extending BaseDirectory and taking a external LockFactory on ctor). In that case the Distributor would know the exact type of lock, because it is responsible for locking. The underlying directories would just need no locking at all (NoLockFactory). But this would be a change out of the scope of this issue. But I like the current impl better. Locking should be the responsibility of the directory that actually holds the lock. This implementation is now implemented the same way like FileSwitchDirectory. FileSwitchDirectory just has the additional "feature" that it delegates the makeLock() call by its file extension, too (and no longer places it in primary dir), see MIGRATE.txt in Lucene. Please also keep in mind, that the lock file itsself is an implementation detail, so maybe we get another lockFactory in the future that does not create any files at all (e.g., by locking the directory itsself or writing some information to its metadata). So to me it looks wrong to see the lock file as required to be listed in listAll(). Lucene itsself does not depend on that (because lock files are unknown to lucene), Lucene just uses the makeLock() method, nothing more.
Is it a problem if we just track a non-existing file? To me this looks like this is already broken for NativeFSLockFactory, because this one may reuse already created lock files (the existence of lock file does not mean its locked). So we can just record here "there may be a lock file to track".
this method can be private now
This could be deleteIfExists
Rather than filtering ourself, we could alternatively pass prefix as glob to newDirectoryStream, and it would follow filesystem rules.
we could pass a glob with regex:xxx to newDirectoryStream if we want
can this one use content() too just for consistency with all the other tests in this file? not a big deal, im sure there is more work to do in tests eitehr way.
metas-tate -> meta-state
I also think we should re-throw the exception here and not silently fail
I think this should move inside the `try` block, yes we'll double-log the exception (here and in `deleteMetaState`), but it will be tied to a message about trying to delete the dangling index so the execution path will be clearer in the logging
It would also allow you to change the ``` java if (delete) { channel.deleteOnClose(); } channel.close(); ``` to ``` java channel.deleteOnClose(delete); channel.close(); ```
I just realized, for this log message and all of the ones below it here, we only log `index` and totally omit `reason` because there is only one `{}` in the log message...
Hmm... I'm not actually sure how this would play with Snapshots, so it may be a no-go.
I wonder if we should extend `ChannelReference` to implement `Closeable` and have the `.close()` method decrement the ref and throw an error if it's not now at 0 (which it should be since it's being closed in this case). That way we error immediately if the translog is closed but something incremented the channel ref. What do you think of this? Dunno if you think it might make ChannelReference too complex.
Should use `{}` logging style instead of string concatenation here
`super.tearDown()` should be called in a `finally` block
Can we please avoid Paths.get here? We should always minimize its usage since it makes things hard to test, etc. I would do something like: ``` translogName.resolveSibling(translogName.getFileName + ".recovering"); ```
I only mentioned it because if we really have to keep this, then StandardOpenOption.DELETE_ON_CLOSE could be an implementation. But this one has race conditions too, this delete-on-close stuff is why Lucene's lockfactories were buggy for years. Lets defer it to a new issue, ideally we just nuke it completely.
Are we sure this is the correct logic? this logic means, if the file "definitely does not exist". But i think we want !Files.exists()? Unless the file "definitely exists"
Because `deleteOnClose()` is an immediate verb, I would be fine with renaming it `setDeleteOnClose()` if that sounds any better
I think the `"translog-"` string should be a public global static var named `TRANSLOG_FILENAME_PREFIX`, and should be used as the glob pattern for the directory stream. Then all the hardcoded "translog-" strings in here can use the PREFIX var (I did this the hard way in my version of the branch where I typo'd the prefix and it took forever to debug, so having a static prefix to use would be nice). This PREFIX can also be used in `LocalIndexShardGateway.recover` since it's hardcoded to `"translog-" + id` there too.
Extra space between "=" and "Files" here (not a big deal)
I think it would be better to pass a boolean in to this method, since it's ambiguous from the name of the method whether it sets a var (could be named `setDeleteOnClose()` if it were setting something) or actually does the deleting.
Another for `Files.notExists(...)`
This can be `Files.notExist(...)`
1+ portCounter.incrementAndGet() % 9 ? (now we have a collision for 10 & 11 )
why not round robin on this? I think the randomness still allows us to have collisions and will keep us wondering. +1 on the insight that suite and test scope don't co-exists! Also, this makes us one step closer to using it randomly in our global cluster scope.
this is used for the unicast discovery - can we this back to the class? (or reuse the old variable :) )
The max TCP port is 65535 , min 30K gives us ~30K or 30 JVMs.
Randomness is awesome when it's needed, but also adds complexity. Also, we need to let the OS free the old ports before reusing them. I think having some gaps is good? I'm hoping a cycle of 9 is good enough, but we'll see...
for other reviewers wondering where this change comes from, the first char is already checked earlier in this method
Why not add this directly to the CancelableThreads class since there are no other uses of it? That way we can keep it as a final class and don't need to create an anonymous class here.
I personally like the fact that those settings were extracted from the code. The name (default) also indicate that these are defaults can be overridden. Don't feel strongly about it though - up to you whether to keep as is.
I think it is important to keep different classes on the client-side so that we can have more type safety and potentially add some methods to only eg. avg in the future
can we name this `subtractShardsMovingAway` ? it's clear what it means
subtractShardsMovingAwayRen -> subtractShardsMovingAway
May be `if (!FileSystemUtils.isAccessibleDirectory(dicDir, logger))`
@dadoonet "iff" is a synonym for "if and only if"
I think this can use the FileSystemUtils method you added to get all paths as an array for a directory, instead of creating the stream here.
This can be only `Files.isDirectory(hunspellDir)` since `Files.isDirectory` returns false if the file doesn't exist.
We could also use `FileSystemUtils.isAccessibleDirectory(Path, ESLogger)` to make sure this dir is readable.
I would add a flush(), since we expect people to see those bytes and we want to be independent of the filesystem impl (what if it uses buffering, thats its choice)
maybe not appropriate here, but we can do this with one underlying read of metadata via Files.readAttributes (you then have isRegularFile() and size() available from BasicFileAttributes)
I'm trying to understand this isHidden check. Is it actually unrelated, and instead has something to do with the processing of '.' that follows? We shouldnt have code with assumptions that hidden equates to .'s in filenames, thats broken.
Not related to this PR but I think that we should check if what we are trying to remove is not part of the BLACKLIST. Someone could potentially provide a plugin which contains his own `bin/elasticsearch` script, which looks scary to me.
I dislike this idiom here, because if e.g. encoding is wrong it will silently replace with U+FFFD. Is this done anywhere else in the codebase? If so, maybe useful to have a method, that would e.g. set onMalformedInput/onUnmappableCharacter and so on.
instead of this, can we use Files.newBufferedReader whenever we do this? Its not only more concise, it throws exceptions for malformed input or unmappable characters by default.
another case for pickier decoding via a helper or similar
Not related to this change but we could write this: ``` java log("Installed plugins in " + environment.pluginsFile().toAbsolutePath() + ":"); if (plugins == null || plugins.length == 0) { log(" - No plugin detected"); } else { ``` It could help people to have a better understanding on plugins location. (IIRC I saw this request on the mailing list).
relativize can be tricky if paths have different roots. is siteFile really guaranteed to be absolute too? In lucene i coded this "minimal path" with the following idiom: ``` root = root.toAbsolutePath().normalize(); path = root.relativize(path.toAbsolutePath().normalize()); ```
more cases for newBufferedReader in this file I think.
Can we remove `throws IOException` and write: ``` java try { if (!Files.exists(initialSettings.v2().pluginsFile())) { Files.createDirectories(initialSettings.v2().pluginsFile()); } } catch (IOException e) { displayHelp("Unable to create plugins dir: " + initialSettings.v2().pluginsFile()); System.exit(EXIT_CODE_ERROR); } ```
May be a try with resources here? ``` java try (InputStream is = Files.newInputStream(pluginPropFile)) { pluginProps.load(is); description = pluginProps.getProperty("description", PluginInfo.DESCRIPTION_NOT_AVAILABLE); version = pluginProps.getProperty("version", PluginInfo.VERSION_NOT_AVAILABLE); } catch (Exception e) { // Can not load properties for this site plugin. Ignoring. logger.debug("can not load {} file.", e, esPluginPropertiesFile); } ```
No need for the newline here
I would implement this one as Files#readAllBytes(Path) if we are gonna keep it.
@dakrone Ha thanks! :D
I guess the first thing to figure out, is if we can avoid relativize() call at all. Basically its a complicated method, the two paths have to both share the same root (so e.g. one cannot be absolute and the other relative) or you get IllegalArgumentException, if they are equal it returns empty, symbolic link behavior, etc. I just want to know why we this isAbsolute -> relativize check and if we can avoid it for those reasons. The only time i used it in the lucene tests was for presentation purposes in the verbose-wrapper, to create the smallest possible filename to keep infostream small.
Should this one just be unconditional Files.createDirectories? It doesn't throw exception if it already exists.
Note, alternatively we could call Files#readAllBytes(Path)
I think we should remove this exists check? createDirectories will ignore, if the path already exists _and is a directory_ but will do the right thing if its something else like a regular file
Unrelated to this change, but are we sure we need to follow links with this walkFileTree? This is a little complicated and scary, and the method has to track files it has seen and perform loop detection to try to make it safer. If we don't actually need to follow symlinks, then we dont have to worry as much.
add space after `=`
This try-catch throws an exception while the one modified below calls `listener.onFailure(ex)`. Should they match? I suspect this should call `listener.onFailure(ex)`.
This isn't needed, since `Files.createDirectories` will throw a `FileAlreadyExistsException` if a file already exists at that location and is not a directory.
Although this is some kind of hidden feature, there might be people relying on us following links here... I'd open a separate issue if we want to remove it to give people the chance to speak up if they need it.
Is it correct? What if there is a `reverse_nested` aggregation in-between? (ie. nested > reverse_nested > top_hits)
Is the fetch servie smart enough to do a single request if two docs are on the same shard? If yes, I think it's worth it.
I like the exception better for the reason that you mentioned!
you could extract this in a private method to share some code with the scritp execution below.
I don't think this is going to happen - lucene opted out of Serializable for a long time now I don't think we should add it back. I'd rather drop our dependency on it to be honest!
can you make this final? you should not call non-final method from ctors
can we make this `private final` :)
maybe ``` Java IOUtils.closeWhileHandlingException(openIndexOutputs.values()); openIndexOutputs.clear(); ```
it would be awesome to have some doc-strings on these settings
doing this via reflection would be nice too :)
a space crept in.
little typo - shareds
nevermind I should read the description
Instead of using an expected exception here, can you use a try/catch in the test and verify the exception messaging? Not exact messaging, just something verifying this is indeed working (since the error before your change was already throwing a SearchPhaseExecutionException).
I would reword this to "script_score returned NaN"
.jva -> .java
settings seems unneeded.
I think this catch not needed. It will be caught higher up.
@uboness I mean that it is called by the transport client going through the transport service directly (as was the nodes info call before), it is not exposed through clients, so java api users can't call it explicitly (the `Client` doesn't expose such api).
the transport client exposes this api via the `execute` method
I don't like it much when constructors have side-effects. Can we maybe move the API from ``` java new PidFile(path, true); ``` to something like ``` PidFile pidFile = PidFile.create(path, true); ``` to make it clear that there is something happening (since there is a verb)
Also, `.length()` should be compared before `hash()` in my opinion so it can short circuit without comparing the entire `BytesRef` if it can be avoided.
This multi-line `||` is super ugly to read without syntax highlighting (on github, for instance). How about: ``` java boolean same = local.isSame(remote); boolean hashAndLengthEqual = (local.checksum() == null && remote.checksum() == null && local.hash().equals(remote.hash()) && local.length() == remote.length()); boolean consistent = same || hashAndLengthEqual; ```
I think this can just be ``` Java Files.move(sourcePath, targetPath, StandardCopyOption.ATOMIC_MOVE); ```
can we just call this `getEngine` the `safe` part is unnecessary
can we use `== false` instead of `!` it's so much easier to read and burned my fingers too often
maybe use `coordinates.children.isEmpty()` instead
maybe use `coordinates.children.isEmpty()`instead
maybe use `coordinates.children.isEmpty()` instead
Can you replace the blanket `put` with a spinning `replace` since we need access to the old breaker? Currently there's a race condition if two threads register a breaker at the same time since we do a get-then-put, so you should be able to do: ``` java CircuitBreaker oldBreaker; do { CircuitBreaker oldBreaker = breakers.get(breakerSettings.getName()); breaker = new ChildMemoryCircuitBreaker(...); } while (breakers.replace(breakerSettings.getName(), oldBreaker, breaker) == false) ```
I think it's okay to make these lowercase now that they aren't enums
I wonder if the entire function could be simplified to the form: ``` current = min while (current <= max && iter.hasNext()) { // get next bucket // loop current up to nextBucket // set current to nextBucket rounded } while (current <= max) { // handles both empty case and trailing case } ```
That assumes `list` can't contain null..if that is not the case ignore
It's unfortunate java doesn't have something like this :/
Regex.simpleMatch already has a null protection, no need to put it here...
can we make this mehtod just do a simple matching - returns true if any of the values matches any of hostIP and publishIP? then we don't need the Boolean and the null return. We then also push the crazy OpType dependent decision to the calling method where it's clearer why we do it.
I don't think we should delete the index at the beginning of the request. Instead, we should throw an error if the special index name exists. That way, if it is somehow an index that was created by the user, we would let them know, and not silently delete their data.
Now that we have immutable mappings in master, I think that is a worthwhile goal. I believe it would be different from this PR though, not a quick change.
I think we should collapse the two above methods, they are always called in sequence.
I would really just check for newlines.
I think checking for newline is better than relying on pretty printing having space between key/object...
Shouldn't this be equal to the `jsonBuilder().string()` above, without adding `.prettyPrint()`? And a nitpick: please add a space after the comma..
why a single data node? If you need that, you can use `numDataNodes=1` instead
can this be `== false`
ditto on wording
we can remove the 1 + , now that 0 is not reserved for the global scope. This should be `portCounter.incrementAndGet() % 10;`
it -> is (existing typo)
can we add the [] around values? we should make this consistent with other logs `merge segment [%s] done: took [%s], [%.1f MB], [%d] docs`
The cast is useless now? (There are a couple of other places that have this unnecessary cast)
gotcha, thanks for explaining.
wondering if this should call `this(stats.queryCount, etc.)`, since the addition is not required given that we are creating a new object
I know the result is the same, but we are not adding to something existing, we are creating a new object. The addition is not needed and I think it would make the code cleaner if we called the other constructor.
All metadata mappers have this kind of behaviour which is a pita to maintain... I'm wondering if we could come up with a better solution.
Can use the '{}' syntax here for `DATA_BLOB_PREFIX`
Or we can let poorly formatted values pass through and throw exceptions at the end if values are missing, similar to how we do for queries
Holy cow, this thing reads like House of Leaves, can we change it to something like: ``` java if (ParseFields.SNAPSHOTS.match(currentFieldName) == false) { throw new WhateverException(...); } while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token != XContentParser.Token.FIELD_NAME) { throw new WhateverException(...); } if (parser.nextToken != XContentParser.Token.START_OBJECT) { throw new WhateverException(...); } while ((token = parser.nextToken() != XContentToken.END_OBJECT) { ... etc ... } } ```
I suggested renaming `files` to `newFiles` above so that this fails if someone changes the name of the local var (right now it would just be silently bad)
we need a version check here if this goes in 1.x
we need a version check here if this goes in 1.x
canceled -> cancelled
;) I was struggling with the same thing!
canceled -> cancelled
Are we sure the ref should be decremented before canceling the threads? It seems like it should be the other way around
Someday we're really going to have to standardize on American "canceled" or British/Australian "cancelled"... :)
Cancelable -> Cancellable
+1, it feels more natural to have the `decRef()` always be the last thing, even if it requires another nested try/finally
This should probably be renamed to weightSum as well.
Extra space :)
Oh, woops, good :)
Maybe rename to something like noopAfterRuleTask? To make it clear by default we do nothing here, it's just a placeholder in case a subclass wants to set its own AfterTestRule.Task.
Does the ordering of this new test rule matter? Should it really be "innermost"? (I don't understand the implications of this TestRule ordering...)
Hmm, this looks a bit hacky to me... What about doing something like: ``` java public long nextRoundingValue(long currentWithPostZone) { long currentWithoutPostZone = postTz.convertLocalToUTC(currentWithPostZone, true); long nextWithoutPostZone = durationField.add(currentWithoutPostZone, 1); long nextWithPostZone = postTz.convertUTCToLocal(nextWithoutPostZone); return nextWithPostZone; } ```
Maybe just throw Exception? (the error variable generic type should then get changed as well)
I don't think this needs the string/boolean. They can be regular enum values. And the constants can be inside the enum, so that they can be used as if they were additional values.
But you can replace wherever the boolean is used with equality to the enum value: `orientation.getValue()` -> `orientation == Orientation.RIGHT`
I think we can remove this `if` completely, cause we call the same method given the same input at the beginning of the method, this condition will never be true here.
Actually, I did some digging, this if was introduced with #6475, and I think its purpose was to check that state of indices after aliases resolution. This is a bug that we should address on a separate issue, that should be about index state checks when resolving aliases, since it seems we don't check that at all at this time.
I see what you mean, cause `failClosed` is used elsewhere as well... oh man this method would reuse a rewrite, I always wondered if we should just remove all these optimizations around the single index case and this `possiblyAliased` check as well. I would consider copy pasting the check anyways because you added an `if` in the method that has to do with `forbidClosedIndices`, it should make things more readable.
yeah it might be ok to do this since we are only parsing it on a search thread. I wonder if we can add an assertion for the threadpool that it is not a nework thread ever? but opening a diff issue is good
hey @martijnvg I double checked with @clintongormley and we both think it's better to add the actual index that was closed, not the alias. Knowing that an alias is closed has little sense, better to report back which concrete index was closed.
I think we should mention the index, cause that is the useful bit (e.g. which index is closed), also because we never really hide the fact that users are using aliases (e.g. when indexing into an alias, the response will contain the concrete index as `_index`, not the alias).
yes, that's what I was thinking, I'd maybe check state != CLOSE rather than checking for OPEN, but that's a super minor concern that doesn't change the result at this time
if the argument name is `failNoIndices` you should provide `! indicesOptions.allowNoIndices()` as argument
wondering if the other way around would be better: create a new array instead of set and add open indices to it from within the loop.
I see your point on changing the name, that makes sense cause allowNoIndices != indicesOptions.allowNoIndices
Are we sure we wanna rename `allowNoIndices` to `failNoIndices`? Wondering if t becomes more error-prone than before :)
if the argument name is `failNoIndices` you should provide `! indicesOptions. ignoreUnavailable()` as argument
name needs to be replaced here...that is why I don't like these optimizations :) they make the code error-prone ;)
I think you should remove the `!` here, throw exception if `failNoIndices` is `true`
Closing the store doesn't necessarily mean the shard is being deleted, I tested this and this codepath can happen when the index is closed, so I think this should be "failed to close store on shard closing"
or "failed to close store on shard removal"
Out of curiosity, why create a class for this instead of an anonymous class in `IndexService` capturing the local `shardId`? There are no other instantiations of this class other than the single one
Same here, I think we should go back to the `ImmutableSet`, the types should keep people from thinking they can change things with these collections
IMO `descentsFrom` would be a better name
can we explain why it may be useful for discovery plugins? This missing bit is exactly why it got removed in the first place :) I also wonder if we can find a different way to achieve the same result, avoiding to expose a public method that is used by plugins only. That said I don't know this part of es that much...
Out of curiosity, could we do all the types in parallel instead of blocking and waiting for each type to complete before moving to the next type? (It's probably out of scope for this PR, but I'm wondering for a future enhancement)
This logging statement has a `[{}]` but no argument to fill it
nitpick: it would be nice for the second clause to be aligned vertically with the clause above
just for ref, im not sure this is really valid.
do we need the Iterator/do-while? should this just be: ``` for (Accountable child : segment.ramTree.getChildResources()) { toXContent(builder, child); } ``` this would be better than the current assert + do-while code shape. I don't see what the current assert guards against either.
FYI I opened https://issues.apache.org/jira/browse/LUCENE-6148 so that we don't have to use Iterators.size
can we use a regular foreach here on dataNodes? I think its cleaner
can we make this depend on the version so that with future versions that do support it we won't have to go back and enable it? We do this already for other features.
for master you don't need to specify the gateway.type we only have what used to be local!
you can simplify this a bit here: ``` Java NodeStats unluckyNode = randomFrom(Iterables.toArray(nodestats.getNodes())); ```
How do you feel about always emitting a `profiles` map, that map just be empty? I think it's a cleaner implementation than a key that sometimes can be missing depending on the profile map, it makes client diagnostics easier also (ie, you know where to look, even if there isn't anything there yet)
You should be able to use the "default" constant you defined here so the test will still pass if it's changed in the future.
And here as well.
does this make this issue a breaking one? I know, always the same dicussion but I never remember the outcome, it potentially breaks plugins that implement their own transport...
Ok, then for the profile settings you only read the publish_port and publish_host instead of the other settings. I see the difference now. Before it seemd like you wanted to read exactly the same settings, then I wondered why that part couldn't be shared.
but it wasn't replaced by any other similar line somewhere else? I think this line is quite important, as gets printed out at startup in the console by default like the following: ``` [2015-01-06 10:18:01,288][INFO ][transport ] [Gertrude Yorkes] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/192.168.0.27:9300]} ``` I don't think we should remove it.
you can put these into `assertAcked(client().admin().indices().prepareUpdateSettings("test").....` to make sure this doesn't happen again
Extra space here between "public" and "class"
Can you name this something more descriptive? maybe `shardIdToRouting` instead of just `map`
I think this could use the `rebalance` function in `CatAllocationTestBase`? It looks like it's performing the same function
Probably a good call. I try to always keep them totally 1.0 for my sanity.
may add something like "thread ["+ ..+"] executed a blocking operation but is a network thread" . I think it will make it easier to debug.
can we use this `((availableProcessors * 3) / 2) + 1` even though it's equivalent
nit: space between `<` and `0`
DEFAUTL -> DEFAULT
Can this just go into `createNewEngine`? It's the only caller of this method.
Can we name this `writerConfig`? It's weird to shadow the class `config` since it looks like the same thing (though it isn't)
I think this can be removed (here and from the interface) and then `this.shardId` can be used in the only caller, the `.get()` method
Can the collapse the multiplying by 50 into the setting itself? It feels arbitrary and I have no idea where the 50 came from
Does this need to set `change = true` also? It's missing from this `if` block
As well as the default buffer size
This string could be a `static final String` also I think
Can probably just call this `INDEX_CONCURRENCY` instead of `INDEX_INDEX_CONCURRENCY`
DEFAUTL -> DEFAULT again
the version null check is a bit scary, when can the version be null? I would rather add an assert, so we catch it if it happens
Opened #10522 , please reference this one too in your commit message ;)
I created #10521 for this, to give this issue some more visibility. Fix is still fine in this PR, just add Closes #10521 to your commit message.
I don't know, I believe METADATA_READ is probably enough but can't really judge, let's hear from @imotov ? ;)
that looks good, thanks
I get that, I was just wondering why those default templates bother here
+1 on keeping it `METADATA_WRITE`
I don't have strong feelings about it. I just find it confusing.
IMO it is write because the cluster is changing (an action is happening in the background), and metadata because it is not changing data (it is changing the underlying storage, but from the user perspective the data is not changing).
I don't think optimize should be `WRITE`. It is an administrative type action, you aren't actually passing data, just telling the system to do something.
I am always getting confused by this one and refresh. Shouldn't this be `WRITE` and not `METADATA_WRITE`? We don't really change any metadata here.
Given that we changed the serialization code for this class, would it be possible to write a small unit test that verifies it? We already have similar `*RequestTests#testSerialization` tests that randomize the version and make sure that everything is fine.
This is tough one. During snapshot operation, we create a record about running snapshot in metadata. So, technically we write into metadata. It's not a persistent record though.... so it all depends on the definition of the metadata. Maybe it was a mistake to put snapshot into metadata though. Now I am thinking it should have been custom cluster state level element. So, we might want to move snapshot into cluster state custom and use READ here.
Verification is called in two cases - when we create repo and when we explicitly call verification. We already check for `METADATA_WRITE` in the put repository operation, so I think we can use `METADATA_READ` here.
same as above, this seems the same method as before
just a reminder: version might need to be updated here too depending on which branch we backport the PR to. You might have thought about it already, but these are the things that I usually forget about when I push :)
maybe I would split this in two ifs, the outer one about version, then the inner one around levels.
the version here might need to be adjusted depending on the version we get this PR in e.g. if it ends up being 1.6 should be `before(Version.V_1_6_0)`
not related to this change, but I get confused by these calls with empty index. Will they ever return a block? Seems like we should dig here on another PR.
this `id` gets sent over the wire in `ClusterBlock#readFrom` and `ClusterBlock#writeTo`. your change makes it backwards compatible only for reads, cause a 1.6 node that gets 1.2 detects and converts it. But what happens if a newer node sends 3 or 4 to an older node? We need to add some logic based on version of nodes. Also, I'd try and make this method package private, not sure why it's public it shouldn't IMO.
Let's stick with `METADATA_WRITE`.
I would be more precise on the version, cause it's not clear if it is from 1.5.1 or 1.6.0.
fantastic thanks a lot
`ClusterBlockLevel.values()` ? don't think we need the constant
Indeed we might restore persistent settings and templates. We need both.
.... because it remove a record created in create snapshot. So, if it's METADATA_WRITE there it should be METADATA_WRITE here.
can you explain why? :)
This might be `WRITE` actually - we are creating an index or modifying an index after all.
version reminder here too, and s/splitted/split
ok, maybe we should think about improving this later on ;)
I think we can omit catching this and failing when caught, that's default behaviour, what matters if the `finally` I guess
sounds good, this one will require the version as argument though
you migth want to use //nocommit so maven notifies you when you forget about it ;)
nevermind, I see you moved it to a more specific test class, that's fine.
I think you can just do the `== null` part of this and the next `if` condition, since if the other were equal to null, we would have already returned in the first `if`? ``` if (x == null && y == null) return if (x == null) ... else if (y == null) ... else // x and y are not null ```
I don't think the test name needs the issue number. The rest of the name explains what the test is checking.
This, plus the final return, can be simplified to just return the value of an `==` comparison between the two values.
logic subscribing to what is described in the docs (ignoring null assigned to any other properties) would be more like this... making defaultTimestamp null if so, else the string provided. ``` else if (fieldName.equals("default")) { defaultTimestamp = (fieldNode == null ? null : fieldNode.toString()); } ```
Isn't the same NPE possible above in all the other conditions? Should we just continue the loop before these checks? ``` if (fieldNode == null) continue; ```
same here - we can drop the termsSet.clear(); later on.
If we don't cache the fields, we should remove the fields.clear() at the end.
ok, fair enough
one more thing (sorry!). For the language clients, I think it would be good to also have a small REST test that uses search_type count, just to verify that all of the clients (and our REST layer) still support it.
now that it doesn't return a value, maybe rename in to loadIntoContext , or add something to the java docs to indicate where the output is put.
+1 thanks Adrien, I was about to complain that we don't test search_type count anymore :)
Again, it would be cleaner to init `i` to `offset + 1` so that you don't have to add `offset` in every iteration.
Is the `component != 0` check needed? Seems like the only way to get past the `||` is for that condition to be true.
It would be good to have direct unit tests for these utility functions, especially checking the boundary cases with the passed in offset and length.
It would be nice to have this take the args in the same order as computePolyTop (array, offset, length)
I think it would be cleaner to set translated outside of the if statement. ``` boolean translated = incorrectOrientation && rng > DATELINE && rng != 360.0; if (translated || shellCorrected && component != 0) { ... ```
Not a critical part of the test but presumably this should be named "product2" for clarity
Could childDocIdBuffers store a reusable iterator? Would avoid the object creation overhead every time a parent is re-collected.
Oh I see, it's the ZTable stuff. Sorry for the noise :)
interesting, what is the reason for this funny upper bound? :)
will need to add `std_upper`, `std_lower`, `confidence_upper`, `confidence_lower` here otherwise they won't be able to be used in sorting or `getProperty()`
usually we leave these to be set in the constructor so that the Parser class takes care of setting default values. Also these variables should probably be final
oh I see, this class is just a namespace for buckets now
Not a big deal, I'm fine without it
Dude! Linebreaks! Strive for 80 columns! (or at least 100 or 120) :D
Oh nevermind, I see why not now :)
It doesn't look like you are using any of the regular `getThenSet` features of the AtomicReference, can this just be a mutable variable? (Not really a big deal either way)
Yeah, I was saying we can remove it now, since I was the one that originally added it
Yeah, I see now. We'll clean it up later.
I think this is wrong? I this is an indexing request on a replica and we're here, that means that there is already a doc with this id. In this case we want to just ignore the request. +1 on what Simon said regarding not changing this code.
same here. ElasticsearchAssertions.assertThrows wil help
we can use ElasticsearchAssertions.assertThrows. Slightly cleaner code.
this will cause the cache to be cleared on each settings refresh, even unrelated ones (once the path one is set), I think that the place where we compare the current patch vs. the new one and if there is a diff, we should clear the cache.
should remove the "force:[{}]" in trace logger. @s1monw
Why do we have a separate engineSafe() method? Can't we just move that to engine() and remove engineSafe()? It makes me feel like engine() is unsafe :)
I think we need to add a check that other settings do not spring back to their defaults, which I believe was the original issue (update one setting and have all the rest go back to default).
missed that. All good then. Sorry for the noise.
checking here for null is not benefetial I think, it can't be null...., same applies to other null checks while iterating
in order to make this benefetial, you need to change the previous line not to use the Safe variant, since it throws an exception if the index is not there
This is the way the check should work, but I'm afraid its not totally complete. Imagine a 4.x commit point with some 3.x segments. I actually think lucene 5 does the wrong thing here right now and will throw some IllegalArgumentException or SPI error. I will look into it. In the future, this would be a cool test index for us to have cc @rjernst
I think it's a problem to call fail engine here as we're holding the write lock. failEngine has a code block that acquires the read lock: ``` } finally { closedOrFailed = true; try (InternalLock _ = readLock.acquire()) { // we take the readlock here to ensure nobody replaces this IW concurrently. if (indexWriter != null) { indexWriter.rollback(); } } catch (Throwable t) { logger.warn("Rolling back indexwriter on engine failure failed", t); // to be on the safe side we just rollback the IW } } ``` Maybe we should reset indexWriter to null and then skip the read lock in failEngine if that's the case. Not sure.
occurred with 2 Rs :)
I would prefer this instead: ``` java if (currentIndexMetaData != null && currentIndexMetaData.isSameUUID(indexMetaData.uuid()) && currentIndexMetaData.version() == indexMetaData.version()) { metaDataBuilder.put(currentIndexMetaData, false); else { metaDataBuilder.put(indexMetaData, false); } ``` Less code blocks at the price if a longer condition, but this is just my personal style.
I would log the exception here, not just the message, it will provide much more details, and its bad if it happens I would imagine. Also, I would try and explain it better in the logging, for example, tried to do lightweight check, but failed, resorting to full checksum
yea, in that case, let's not add the addition message, and solve it on the logging side m
I Am concerned that we will miss the actual message because its wrapped, my vote is the detailed message one
no need to reason an "ex.getMessage", we already log the exception...
This is the only place that commits w/o pinching the xlog right? Why do we do this? Is it so the snapshot won't fail when a recovery is in process? I think somehow we should make our xlog reference counted in the future, so that a recovery can "incref" it to prevent deletion before the point-in-time it reserved, and then flush doesn't need this boolean param anymore: it would just decref its reference. Later ...
"will lazily allocated" -> "lazily allocates"
Wonderful these can now be final!
right I see that
I see, so we wither use the PercolatorQueryRegistry straight-away, real-time, or we go to lucene and do additional things that make the whole thing near real-time. Clear, thanks!
To be coherent with the `preVisitDirectory()` method I think we should use the `FileSystemUtils.move()` method instead of Files.move() that doesn't support moving non-empty directories between different filesystems.
+1 on adding the shardId. I believe the exception needs to be first in the parameter list.
Totally missed the extra `builder.put(settings);` line added in one of the commits. All good. Sorry for the noise.
save -> safe :tongue:
what I mean is that we don't need have a method that builds new settings, we can just call `metaData.getSettings()` where we need them.
we need to add that we return false if no folder was found for this shard.
Left over Note
extra space between 'for' and 'index'. Also, we typically log the index name first under []: `[{}] failed to ack index store deleted' . If I read things correctly, this also logged when something went wrong in innerNodeIndexDeleted so maybe change to "fail to ack index & store deletion"
do we want to unify this with nodeIndexDeleted? I think it's better to have this called once the store is deleted in IndicesService#deleteShardStore .
Do we need this? the settings are already immutable
Also here I wonder if we should be safe and synchronize this. Do we really need the metaData? we can get the setting from the injector (which we check anyway). Also I think a better name is hasShardUsedContent (we return false if there is nothing to delete.
same here - I think it's better to log the info message if the deletion was successful.
indexService is no longer needed.
I wonder if we should do this with no timeout at all. Potentially log warning after 30s and retry.
same here: no need for the [].
index's toString gives you '[index_name]' no need for '[{}]'
I think this should be a trace log with "[{}] trying to delete store (reason: [{}})" (note the index name as well) and log the info after successful deletion. We already log a debug message (or higher) in all callers of this method and it will not result in an info message saying "deleting" when we actually failed. ``` logger.trace("{} trying to deleting store (reason [{}]}", index, reason); nodeEnv.deleteIndexDirectorySafe(index, 0, indexSettings); logger.info("{} store deleted (reason [{}]}", index, reason); } catch (LockObtainFailedException ex) { logger.debug("{} failed to delete index store - at least one shards is still locked", ex, index); ```
I wonder if we should use the cluster state for this check. I'm worried about people passing in a dated cluster state here. Maybe a cleaner model is to make this method synchronised (to avoid async collision with the create code) and check the existence of the index instance in the #indices map member of this class.
++ on making this explicit
hey guys I did some work a while ago on the delete index api acknowledgement in #4218 which is the only api left that doesn't use the "standard" acknowledgement code. That said we decided at that time not to migrate it to keep two different actions: one for deletion from cluster state, one for deletion from store. Not sure I follow these PR's changes when it comes to how they affect acknowledgements, but If we want to make the two a single action, can't we just use the standard acknowledgement code and drop the custom actions? I think it would simplify the code quite a bit.
I wonder if we can remove more code here. We already have the indexUUID of the shard in question (it's part of the cluster state) and we check it on the remote nodes. if they acked they have the shard we can skip the cluster state checks and update on the state update task but rather just instruct IndicesService to verify this indexUUID is still current (or no other one exists) and delete the shard content if we can get the shard lock. This will things much simpler imho.
indicesDeleted doesn't check for indexUUIDs. We have a separate method for it in this class `cleanMismatchedIndexUUIDs` - in this spirit of bringing all deletion code together - I think it's good to make indicesDelete aware of UUID switches (mark old as deleted), move the `applyDeletedIndices` to be executed where `cleanMismatchedIndexUUIDs` is called now and then we can remove `cleanMismatchedIndexUUIDs` make all go through here.
"and instance to" -> "an instance of"
This will break all plugins and will force all authors to update their plugins. Not sure if there are any other option but if not we should mark it as a breaking change. Can we add it only to `AbstractPlugin`? I mean add it as a new function but not replacing the existing signature.
I believe the above two lines need to be moved outside of the `if` statement they are in (`if (request.scriptedUpsert() && (request.script() != null))`), as currently the parent and routing will only be set if it is scripted upset, and not if it is a vanilla one.
typo, all flies -> files
cool. lets look at it on another issue.
Don't we need to throw the exceptions list here, like we do before: ``` ExceptionsHelper.rethrowAndSuppress(exceptions); ```
Can you explain this case to me? I am just curious, because all commits we need to read (4.0+) have a CRC32 checksum as the last 8 bytes. Its just that since lucene 4.8 we also added the rest of the footer (FOOTER_MAGIC) to be consistent with all other lucene files in structure. So if we want, we can always at least just check that the last 8 bytes are the CRC32 checksum for any segments_N (just don't look for a footer or call checkFooter).
minor semantic difference: over [here](https://github.com/s1monw/elasticsearch/blob/fix_recovery_finalization/src/main/java/org/elasticsearch/indices/recovery/ShardRecoveryHandler.java#L304) we throw the unwrapped corruption exception, not the remote version. I think we should do the same here and throw corruptIndexException
The reason we delete the temp files is that when a new recovery starts, it will create a new folder and copy files over again and we could (were) filling up disks that way. This is not an issue here. I think we should be consistent in our policies. We chose to leave complete corrupted shards around in other places, so I think we should do the same here? I don't feel strongly about this, though. This was the gist of my question.
+1 lets get rid of it! If we don't use it there is no need for the complexity!
Right I see... I think the `Version.indexCreated` should just return null in that case, but that is outside of the scope of this change.
Since all 1.x versions will have the created version in the index settings, the empty settings check can be removed.
I meant `node` from the for loop.
can we extract this logic and the logic above (where we prune/ dedup incoming cluster states), into a static method and unit test it? would make me sleep better :) This is super important.
can we rename this to shouldIgnoreNewClusterState? it's not only about being dated.
can we call this log: ``` received a cluster state from a different master then the current one, ignoring (received {}, current {}) ``` also note that disco nodes already have [] in their toString.
can we do ``` if (currentState.nodes().masterNodeId() == null) { // we welcome our new overlords return false; } ``` One less nesting to put on the mental stack
isn't enough to have another waitForEvents to make sure that the previous event was successfully published? I really think we shouldn't have this time based solution. 10s makes me shudder :)
We can call it suspend if you want. To me LongGC is easier to remember as it is what we talk about all them time.
can we check for null before we start and then validate it's still null on the end? an alternative is just to synchronize this block, which is simpler (but remember to do the same in stopDisrupting)
we need to say something about the fact that we only skip cluster states if we have a higher cluster state version from the same master
can we call this selectNextStateToProcess? it's not always the most recent
Is the error message correct? also, I don't it's needed containsInAnyOrder also test for size.
yep. missed it. sorry for the noise
strictly speaking, this doesn't need to be volatile. We update it under a lock which guarantees the visibility of the changes.
+1 on warn. Can also do this check outside of the cluster state update task? it's a shame to go into an update task. We will still need this check in the cluster state as we may have two masters publishing while we are in the join process.
Can we add a LongGCDisruption variant that allows using the startDisruption and stopDisrupting to control the GC? These extra params feel clunky (and yeah, I probably did it before too :))
can we add some out of order insertions here? just to be a little more evil
nice one. Good to add.
same here regarding the versions
can we add some randomization here around the version - check that new has a higher version then old and vice versa
here we would validate that each node made two switches - one to remove the old master and one to accept the new.
thanks for adding this!
I wonder if we want a trace message here...
I think we can remove this one because the lock is aquired higher up (and we should check there).
I think we need to throw an exception here, just as we do above with the currentFlushing check.
is this still a bind exception all the time? If not we should update the error message
Should we call remove before put? (Was just trying to think about what would happen if source == dest)
I think its awkward we wrap these files in InputStreamIndexInput here, and have the method take InputStream, when it could just take DataInput? The one lone other usage of it, in a separate file (BlobStoreIndexShardRepository), could just wrap its InputStream with a o.a.l.util.InputStreamDataInput. and then the hashFile() would just be a simple readBytes() call into the byte array. I know this isn't new in the patch, but this path adds more indexinput-wrapping since its not calling hashFile(String) anymore.
Note: I was wrongly assuming that DocIdSet.EMPTY returns a null iterator, which is not the case.
this one sounds familiar :)
minor formatting issue
I could be wrong but it looks like the argument to withAllDeprecated indicates the new setting which has replaced the deprecated options and is only used in the error message to point the user to the new option. so for `pre_zone` we would use `new ParseField("pre_zone").withAllDeprecated("time_zone")`. It might be worth checking with @s1monw that this is indeed how it should work
Or actually just "ignore for old indexes" would probably be sufficient, since the version is clear from the condition.
good catch with the sync here!
@jpountz could you have a look at this one? It made me nervous (not sure the stronger typing is safe).
Do we need the `.score(..)` here? For the user all they need to know is the score function that failed. The method is probably not relevant to the user in the message and will be in the stack trace for a dev investigating anyway
Ah ok, I missing that method below, sorry.
we can do try-with but we need to have 2 try blocks. since the write lock needs to be released last. but in a try / with blokc it's released before the finally block is executed
Should this include the segments file? (e.g. call infos.files() instead) The old listAll() would have included it too.
can we use the index name first in the log, like `[{}] locking all shards, num_shards [{}], index, numShards`
I had a look and the settings code has been dramatically improved with 5.0 already, hence this fix is not required any longer. Nothing to do then, but thanks again for pointing this out.
Hmm, I don't know anything about ElasticSearch, don't even use it  I just stumbled over this pull request as a showcase of error-prone. I don't think I have the time to set up my environment to do any proper PR or to test a change like this, sorry.
no worries, thanks for catching this @ePaul !
Can you remove the leading underscore? Variables should start with a lowercase letter according to http://www.oracle.com/technetwork/java/codeconventions-135099.html
Is it possible to name this something else? I see `writePre20Settings` and I think pre-0.20.0 version
I don't think this is the best place to apply minShouldMatch. The difference between this and say match query is that it supports multiple fields (i realize the default is _all and this patch will work with that). So in this case, this query will return a boolean query across say title, body, author fields. the current application of minShouldMatch will be unintuitive there.
Yes! I like it much better!
can we make this a regular "readInt" call, and use `-1` to denote that we couldn't compute it? otherwise, 0 can be very misleading
can we check that nodeOrdinal is < unicastHostPorts and throw an exception? I think in theory we don't need to enforce this but can search for ports on the fly, but let's leave that to a day we need it.
I think we can increase the 99 range to 1000 and also change calcBasePort to ignore the scope component as it is not relevant any more. I want if we should jut remember the last based port and continue from it (round robin-ing) to make in-JVM collisions less likely
I think it's enough to say `could not find enough open ports in range [30000 - 31000]. required [8].`
can we use >= instead of == ? it's strictly speaking OK if we synchronize this method, but it always scares me to use equality when using shared variables - we might miss it.
I think you can just do this? ``` if (info.files().contains(markerFileName) == false) { return true; } ```
Really you just need to look @ the first int of the segments_N ... if that's == CodecUtil.CODEC_MAGIC, the index was already upgraded to 4.0+. Then you don't need to step through each info.
Retruns -> Returns where -> were
+1 to have `fromXContent` and `parse` be static
Okay, I think I understand why it is this way. What I am concerned about is the different ways that a `TermQuery` is constructed here, there's: ``` java new TermQuery(actualField, actualValue) (new TermQuery()).fromXContent(context) (new TermQuery()).parse(context) // <-- weird that this is not static ``` What I think would be better is maybe static methods that generate new versions for all except the plain construction version: ``` java new TermQuery(actualField, actualValue) TermQuery.fromXContent(context) // <-- static, returns new TermQuery TermQuery.parse(context) // <-- static, returns new TermQuery ``` I dunno, maybe it's a gut feeling :), but the current implementation feels very "loose" and _too_ flexible in what the "correct" way to create a new TermQuery, making the methods static instead of mutating the current object feels more functional (in both senses of the word!) to me. I _personally_ would rather have `TermQuery()` constructor be private, but I guess that's an entirely different discussion about builders versus non-builders...
ok sorry I am not too familiar with these methods :)
why is this log uppercase? :)
and i guess where we instantiate singleton, we just let Kernel32Library be null if it cant load? I dont know if its cleaner actually, it means callers woudl have to check for that. i just think its wierd to have all methods have to check isLoaded and do nothing. Currently there are only a few, but if the class increases...
Can we remove this boolean? There is only one constructor for this class, so if it succeeds, then we loaded. otherwise we have no object to call addConsoleCtrlHandler() on, so we can remove the check for it there, too.
Can't recovery -> Can't recover
the only case that we don't cover here is the classes that by mistake don't extend `ElasticsearchTestCase` (plain junit tests) whose name doesn't end with `Test` or `Tests` either..... pretty sure this never happened before. not sure we want to do anything about this. Maybe we could just separately check that there are no plain junit tests in the codebase.
can we add something to indicate where this comes from? something like unexpected error while processing cluster state version [{}]
can we use a similar message to the log bellow: "cluster state from a different master then the current one, rejecting (received {}, current {})
I think this test can be moved somewhere else and use the single node base class? this doesn't really have a disruption in it and will be slow...
Ah sorry missed that
I opened https://github.com/elasticsearch/elasticsearch/issues/9995 for this :)
Thanks a lot for this! Definitely better to generate that in a temp dir on the fly instead of having those files as part of the git repo!
Use constant when possible
Use constant when possible.
I think while we're changing this, it would be nice to change this from "removing" to "removing from dangling index set", otherwise it is kind of confusing about whether data is actually being deleted (which it's not)
Missing the `indexName` argument to the debug log here
this method should just return the `Set<String>` or better `Set< IndexMetaData>` and `processDanglingIndices` should assign it. That way this method is `idempotent` and easier to test. We also don't need to load the `IndexMetaData` again
We check that the indexMetaData is not null, but it looks like we do nothing if it is null, do we need to warn or throw an exception when the state is null? Right now it looks like it will be silently ignored if the state is null
typo here "ot" -> "to"
typo: dictionnary -> dictionary
typo: dictionnaries -> dictionaries
This might make the code in MovAvgModel easier as each weighting type will have its own class which knows how to calculate the moving average and we can just call a single consistent method on that class
could we not specify it with the following? ``` "movavg": { "bucketsPath": "the_sum", "weighting" : { "single_exp" : { "alpha" : 0.5 } } } ```
Does it make sense to have the Enum and method name the same? I have no preference as to whether we call it `Weighting` or `WeightingType`
Here's Python: ``` >>> ', '.join('(byte) 0x%x' % x for x in cbor.dumps({'foo': 5})) '(byte) 0xa1, (byte) 0x63, (byte) 0x66, (byte) 0x6f, (byte) 0x6f, (byte) 0x5' ```
please use just close here
can you use `IOUtils.close()` here please it will close all even if there are exceptions
nah this should just pass a string.
can this just take a string? no need to pass the service here
maybe we should just build a map for this? I don't like the linear nature
can we name this just `current` I think this 1 is not necessary!
this must be final!!
imho, we should just always persist the newRouting. The oldShardStateMetadata just complicates things and as far as I can tell doesn't add much value - we call this function very rarely.
I think this is dangerous at the moment - versions can go down if a master goes down halfway during the publication of a cluster state - the new master may have not yet received the new version. I have plans to make this better, for what it's worth.
same here, I think it will be easier to read with "shard state persisted despite of persist=false"
neat picky - formatting. (add {} )
can we add a failure message saying "inactive shard state shouldn't be persisted".
we can pass along a reason to persistence to be used as writeReason
OK. I wonder if it's needed as they can be assigned everywhere. But this is definitely a different change (if at all)
It's currently remote controlled by the creation of oldShardStateMetadata in updateRoutingEntry. If we want to keep the optional persistence based on the previous state, we can do it directly on currentRouting have it all in one place (either in updateRoutingEntry, or replacing persistMetadata. Something like this: ``` if (persistState) { final String writeReason; if (newRouting.active() == false) { logger.trace("skipping state persistence as it's not active"); writeReason = null; } else if (currentRouting == null) { writeReason = "freshly started, version [" + newRouting.version() + "]"; } else if (currentRouting.version() < newRouting.version()) { writeReason = "version changed from [" + currentRouting.version() + "] to [" + newRouting.version() + "]"; } else if (currentRouting.equals(newRouting) == false) { writeReason = "routing changed from " + currentRouting + "] to " + newRouting; } else { logger.trace("skip writing shard state. previous version:[{}] current version: [{}]",currentRouting.version(), newRouting.version()); writeReason = null; } if (writeReason != null) { final ShardStateMetaData shardStateMetaData = new ShardStateMetaData(newRouting.version(), newRouting.primary(), getIndexUUID()); try { ShardStateMetaData.write(logger, writeReason, shardId, shardStateMetaData, currentRouting != null, nodeEnv.shardPaths(shardId)); } catch (IOException e) { // this is how we used to handle it.... :( logger.warn("failed to write shard state for shard " + shardId, e); // we failed to write the shard state, we will try and write // it next time... } } } ```
yeah, it might trip every once in a while (if we do a good job testing), but I'm OK with leaving it as is for now..
+1 on adding UUID
I wonder if we should void this check if indexUUID or that.indexUUID is IndexMetaData#INDEX_UUID_NA_VALUE . We can still run into this value when reading existing state files.
how do you feel about renaming this "force merge" instead of "optimize"? (may be out of scope, just curious)
I think this should be: ``` java Throwable newT = new OptimizeFailedEngineException(shardId, t); maybeFailEngine("optimize", newT); throw newT; ``` So that the OptimizeFailedEngineException is captured as part of the maybeFailEngine
Did you confirm we sometimes hit this and not just ACE? (The "indexed" CountDownLatch should make it likely...)
I think this `close()` should be in a `finally` block in case the write fails
this is very confusing.. please use INLINE though
we are talking about toString method?! No way we add a distinction in there... if we pass this to the user then fix it in the rest layer
ok fair enough. I think for this delicate issue we should force the right setting IMO
ok let's drop it in master and keep it here for BWC
not really important
I really like this class since it's so self-contained and has all the tests etc. no weird guice bloat etc. NICE!
I think we should rather pass the logger to it or don't log here at all. we only use it for this: ``` Java logger.warn("ignoring [disable_dynamic] setting value as conflicting scripting settings have been specified"); ``` IMO we should rather collect the conflicting settings and provide it as a list and then log in the `ScriptService` with all the settings that are conflicting...
Honestly I prefer inline, but all the user-facing stuff has been called dynamic, so I'd probably stick with that.
Perhaps this function would be easier if it were structured like `detailedMessage(t)`? Then this function can just return when it finds an ES exc, and exhausting the loop can have a single return statement at the end. I think the null case should be extracted out since it is the same whether we do simple or detailed exceptions? The caller locations in convert will look more inline then, like: ``` if (t == null) { buidler.field("error", "Unknown"); } else if (channel.emitStackTrace()) { builder.field("error", detailedMessage(t)); } else { builder.field("error", simpleMessage(t)); } ```
I actually meant an error (http 400)? Messages can be ignored. This is just an invalid configuration.
maybe move the conditional logic back out to the caller? the null check isn't needed, and the function name implies it always adds this trace, when in fact it depends on the request.
You could move this back to the while condition? ``` while (next != null && counter++ < 10) ```
I realize the example already here used this form, but I think it is clearer to use `== false` for inverted conditions.
Yes I think so.
Not very expert on this aspect but I wonder if and how rewriting affects other queries explain output? Also, tests would help here just to validate what we print out.
I think it is not clear here exactly when IndexMissingException is expected to be thrown or not. I would rather move the if on top and have different asserts path based on that. FOr the expected exception one you can then do: ``` try { //do something fail("shouldn't get here"); } catch (IndexMissingException e) { //assert on exception } ```
we throw `IndexMissingException` all the time when `allowNoIndices` is set to true. I don't think we should act differently here.
I don't think this is needed, also not meaningful as it would print out a "random" number, it's not something that users can understand
if you get merge conflicts on these MetaData calls that should have been static, don't worry, I think I recently merged a PR from a contributor that fixed just that.
Can you annotate this and the previous `indexSettings` with `@IndexSettings`? (I still wish we had separate classes, but it makes it easier to identify in the event this gets renamed to something non-specific).
while I get that the preference for a for loop... but the inconsistency of how xcontent is parsed is annoying.. if we do it everywhere using a `while (...)` then we should stick to that... if we want to change to a for loop, then lets do it across the board
good point, I found this naming here tricky, includeFields is better, it just described what it does.
while I understand that we already have `aliasFilter`... I really think it's a bad name... the alias functionality leaks into the search context (which has nothing to do with aliases). I'd rather call it something like "includeFields"... or something generic like that (the fact that these are coming from an alias is just a use case IMO)
this is a confusing message... what if it's a boolean?.... also, it's annoying to get parsing error without any constext... "expected where?"... Have the method accept a `String fieldName` and change the message to: ``` throw new ElasticsearchParseException("expected an array of strings for field [" + fieldName + "] but found a [" + parser.currentToken() + "] token instead"); ```
obscure error message... let's start by `"could not read alias fields filtering from xcontent. expected an object but found [" + parser.currentToken() + "] instead"`
can we just throw ElasticsearchException since it has the HTTP code baked in and it's also a RT exception
just initialize it and make it final - it just compliicates the code
it's annoying that we write this code over and over again!
can it also just be the empty set all the time. I really don't like null invariants
can you assign the key and the value here before we use it? it's way easier to read
please make sure you remove the conditional on master
this will result in wrong counts even though it's volatile. We should use an atomic long or sync this block
can we assert that the first 2 bytes are actually `E` and `S`
hmm i see what you mean. If the formal term for it its `differencing` then maybe the current name is ok. I haven't really thought about what we'll call the `arithmetic` agg since, as you say, the plan is that it will support more than just subtracting series. Its basically a 'parallel time series scripting agg' which is a bit long for a name ;)
I wonder if this is a good name for this reducer? I haven't got any good ideas for alternatives but to me a differencing reducer would subtract one series from another rather than subtracting offsets from a single series. It took me a while to be clear on what this reducer was actually doing. I will defer to someones better judgement though as I appreciate that this might be a standard statistical term
I think the nodes can stay as an array. The constructor just converts to an array again.
Should it pass 'false' as 'docValues' instead of null too? (for consistency)
I'm not sure this logic is correct... fieldType is mutable right? We should compute a final boolean in the ctor if we need this instead.
minor nitpick: there seems to be an indentation issue with the closing bracket
maybe make the PublishClusterStateAction#serializeFullClusterState public ? we won't need to worry about this then.
can we call this last seen clusterState? it doesn't need to be volatile as it is changed under a lock.
I think we should bake the use in the name like `wasReadFromDiff` and put a docstring on it
can we swap member and constant we know the constant is not null :)
extra space before `readDiffFrom`
scrap the method, but the block (saw the listener call now).
I think this code will be simpler if we have two methods - sendFullClusterState and sendClusterStateDiff , each dealing with it's own serialization (and have the cache map passed to them). We can have sendClusterStateDiff fall back to sendFullClusterState if needed , which will mean no resend method..
Indeed redundant but clear (I had to double check because I wasn't aware of the convention, as most people reading this will be). I suggested the `to` & `from` to make it shorter so less of an overheard. as said, just a suggestion.
can we call this MockNode? (and it's very call btw)
can we maybe name this with something that indicates its the version of the after ClusterState? `afterVersion` sounds bad. If we use `to` and `from` in the ctor we can have toVersion, fromUuid, toUuid etc :)
can we call it diff? it's just a single one
maybe call this readDiffAndApply? this doesn't really read a diff and return it.
can we just use `getUuid()` no need to have two
can we also add a line saying this variant allows for custom value deserialization based on the map using a KeyedReader? I got confused by why this is useful (until I saw how it is used).
can we add a check for whether we sent a diff? I want to avoid a potential infinite loop.
I think we can check the beforePart == null out of the if(!..equals) and it will make it cleaner.
when is this needed? I wonder if this marks that something is wrong and we should throw and exception.
can we be slightly more verbose on why this is use full? (different deserialization logic based on key in parent map)
Can we call these `from` and `to` and the non Diff fields `toVersion` `fromUuid` and `toUuid` ? sorry to be a pain but I think it will be clearer.
Nit picky: I wonder if we should make this ctor get all the parameters and construct the message locally. Will be easier to read the code and use, imho.
can we use ReleasableLock and try with resources? (or a plain old synchronize :) )
static is not needed.
can we have unittests for this stuff I think it's pretty much only tested in integration tests. We should abstract stuff away to make it unittestable!
we miss a log line here.
Please break BWC and forget about 1.6 - we have to get 2.0 out there and we have to fix this without BWC in mind!
yeah I remember... ;)
can we name this `CompleteDiff` don't use simple please :)
this can be removed now, no? it will be cause a duplicate with the full cluster state log..
hmm no idea really need to think about that one? should this be a //nocommit
instead of having an annotation can we just have a method that we can override? I would like to not make this something people should use.
it would help me a lot if we could assign state() and previousState() to a variable instead of chaining all the methods
oh cool the read is in the ctor! nice!
should we catch exceptions here to make sure we cancel everything we need
I wonder what effect this change has on rivers... not sure...maybe next step would be to remove rivers code from master too? :)
I was wrong. To remove a _river we should run: ``` DELETE _river/rivername/_meta ```
can we log with a "-->" prefix? i.e., "--> done ... " . Makes it easier to find & grep.
I think there is a race condition here - it may take some time for the dangling logic to kick in (it's async via network calls). I would assertBusy until the index is visible in the cluster state, then go into ensure green.
Let me re-iterated what my concerns are regarding my current approach - It overrides default path handling of the InternalTestCluster without needing to. - It overrides path logic in NodeEnvironment w.r.t where to put the data. NodeEnvironment expose the API to get it. - It starts another node where we can make it simpler and use the async standard node start logic of InternalTestCluster (minor) My point here was not w.r.t randomness but rather making the code simpler and straight forward.
> This is not correct? Each node created by the cluster uses a random data dir. All the cluster nodes put their data next to each other, under the current working dir. After you run the tests, check for example: `target/J0/data/` . As I said, I really don't think you should remap data paths here. Just use what the test cluster decided for you. Not starting an extra node is a bonus.
Well, obviously this is not about the line of codes. It better integrates with the internal cluster + the NodeEnvironment choice's of data paths. No hard coded reference to it or the node ordinal. It creates node data where it is supposed to be, as opposed to a random place somewhere etc (I can come up with more if need be :) ) . If in the future we want to introduce any randomness (which imho is a good thing here because you want to see that bwc works under different node environments, but that's another discussion), we practically get it for free.
btw - the test uncovered some issue with the dangling indices import. You might run into a node not connected issues - working on an independent fix.
Yeah, it seems it is. We treat non existing indices as red. Thx for educating me.
The message is a little weird. I don't think "next release" should be mentioned.
I think these reducers should be (de)serialized in readFrom/writeTo? Maybe it's worth having a percolation with reducers test to make sure everything works fine.
Side note: we need to rename Reducer.FUNCTION to have a more explicit name.
just saw it in the factory validation, nevermind :)
Ignore the last bit about gap policies...I was confused about how this bit of code worked. Started playing with it and cleared up my confusion :)
Any way we can unify this with `BucketHelpers.resolveBucketValue()`? Or perhaps move this into the helper class and rename both of them to be more specific (`resolveHistoBucketValue()` and `resolveMultiBucketValue()` or something?) Also, I foresee this needing to handle gap policies too...unfortunately. :( For example, an agg like Autocorrelation needs to ingest a histogram (which might need gap policy) but emits a set of sibling buckets that represent correlation lags.
You need to use .equals on a Double
not in this PR, but in this file... can you change `ParseField("bucketsPath")` to `ParseField("buckets_path")` please don't use camelCasing
I think it should be a LinkedHashSet since we rely on doc ID order for tie-breaking.
does it make sense to put the "fail a shard" logic under a method? to make sure we don't forget to put it in failedShards etc.
I think "allocated" should be "allocate" here.
I think this message is wrong? applyMappings only touches existing indices.
I think this should be trace
This can be just `new Tuple<>(`
I think this can be optimized further. Here we are updating status of shards that are participating in the restore process. There are only two possible outcome of this operation for a snapshot its ether done when all shards are done or it is not done. It doesnt matter if we are applying a single shard or multiple shards  there is still only one outcome per snapshot. If a snapshot is done we need to check if all shards in this snapshot has started and if they are not  create a listener. In other words instead of having an array with one element per shard it might make sense to have a map with one element per snapshot.
Same here. It might be better to make this `TRACE`.
This will be logged for every single shard. So, I think it might be better to make it `TRACE`.
can we use Pattern.qoute for the prefix? just being paranoid..
can we call this `getChannel`
We should have access to the services on the coordinating node, and they should be usable there, so I think this will come out in further refactoring.
Note that you can use Objects.hashCode(function) directly which will make sure to return 0 if the value is null.
Should we make this message a bit more user friendly? I'm not sure may users will know what a tangential point is
can we just `return new BytesRef()` in this case? I don't know if the text characters are null if we can really rely on text offset and length? Maybe a better check, not relying on null check is: ``` if (parser.getTextLength() == 0) { return new BytesRef(); } ```
s/No/no we tend to have all lowercases logs/exceptions. Not that we are always consistent with this, but that is the preference.
same as above for non exception case
I am good
I think its worth it? most of the time the response handle is SAME, so its not a big problem, but it allows to not overflow the same thread pool if it happens
good test, would be better to move it to a brand new class though, as this existing test class starts a single node cluster but you don't need to send any requests to it. This is a pure unit test, you can call it `StringFieldMapperXContentTests` and make it extend `ElasticsearchTestCase`.
sorry, scratch that, you need parser, which needs the indexService, which needs an index on the cluster. It's all good, leave the test where it is. ;)
The BlobNameFilter interface is no longer needed and should be removed as well.
you can just use a glob here ie: ``` blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix; try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*") { //.... } ```
we miss an import here, I believe
also the TreeMap<String,Object>() can be simplified to: ``` TreeMap<String, Object> orderedFielddataSettings = new TreeMap<>(); ```
can we add bogus non relevant settings? Would be good to know they don't mess up with things.
Is there any way to do this check earlier? maybe in the preCollection method? Only wondering as we will have already done the main collect phase by this point so it would be nice to fail faster if we can
ok, fair enough then
Another option for these is to reverse-lookup their ordinals against the dv instance, and you just exclude those ordinals. This should make this option much faster as it wouldnt need to traverse the Terms at all and do contains(BytesRef)
Why do we need both? Is it because there are so many things going on in this file? I don't understand why we wouldnt just need the CompiledAutomaton for the terms.intersect operation, why do we need a ByteRunAutomaton too? Having both seems silly anyway, but if we must do it, try to assign the ByteRunAutomaton from the CompiledAutomaton. The majority of the time it will be non-null: ``` /** * Matcher for quickly determining if a byte[] is accepted. * only valid for {@link AUTOMATON_TYPE#NORMAL}. */ public final ByteRunAutomaton runAutomaton; ```
what is this includeValues/excludeValues? Should it just build the same matcher too (maybe require SortedSet).
Can you add a TODO here? Something like: ``` // TODO: specialize based on compiled.type: for ALL and prefixes (sinkState >= 0 ) we can avoid i/o and just set bits. ``` I can look into it later.
how will you be able to detect duplicates? hmm... maybe I'm missing something here... the way I saw it, each plugin will effectively define a static its contexts (probably static constants of `SearchContext.Plugin`) and then pass it to executable/compile on demand
Oh right... Good point. Yea, I think it makes sense that a ctx should only be registered once
would be nice to force the ctor to accept to arguments, one for the plugin name and another one for the context within the plugin... this way we can semi-enforce consistency of context keys and avoid conflicts between plugins... a la: ``` java public Plugin(String pluginName, String pluginContextKey) { this.key = pluginName + "_" + pluginContextKey; } ```
another newline removal
I'd prefer to have translogId.v2() set to null, as opposed to be indentical to next. To me it would be clearer, but if you feel differently I'm OK with leaving as is.
Typo, "tanslog" -> "translog"
if we use isEmptyCollection of hamcrest, we'll get the recoveredType content in the error message
we can use assertThat(translog.currentId(), isGreaterThan(currentTranslogId) and get nice messages automatically.
Another option, if we make the recoveryState available on the performer (as suggested for the translog op counting), we can put it in start here and add an onStart method to the performer which changes the state to TRANSLOG.
I think this is tricky for gateway recovery because it will report all the recovered operations at once and not as it goes. I The `TranslogRecoveryPerformer` can easily have access to the RecvoeryState (it's on the IndexShard). I think it will be better if we increment it directly there.
I know this was copied over from another place, but I wonder if we should give preference to the recovering file. If I read this correctly , if we have both recovering and non-recovering, it is now random which one we choose.
This is very close tot the snapshot API and I think they should be merged. I'm ok with leaving this as is for now, as the change I'm working on touches all of this anyway. It's a shame to do double work here.
The recovery state reporting is not clean now, as we start the engine and replay translog under `TRANSLOG`. I wonder if it's worth while capturing the START phase at all (i.e. how it takes to open the lucene index). Maybe we should just rename that phase to CHECK_INDEX and only apply it if it's needed ? (i.e., `checkIndexOnStartup == true`)
I think this can be set to final and not intialized (`final Set<String> typesToUpdate;`) . It's only set once and it has to be set now.
a transformer and performer. Quite a guy :)
can we move the translog related code out of createSearchManager and move it here (or have a dedicated method for just that). I think it's good to have it all together.
If we want to go with pure getter, which is fine with me, we should probably change our setters too then for consistency? I think we have `public MatchAllQueryBuilder boost(float boost)` here. Not talking about the fluent thing, just the name of the method, either get and set or no get nor set prefix .
new ArrayList<String>() => new ArrayList<>()
now I see why `QueryParseContext` here too. we should probably split the QueryParseContext in two data structures, one needed for parsing and one needed for toQuery (e.g. mapping etc.)
I think getters don't hurt and should have already been there. At the moment stuff can't be set but not retrieved, which seems silly. If we want to keep bw comp then let's leave the setter alone and follow the same convention with getters too
I have yet to get to that alternative, I am lagging behind :)
I would probably take out the string comparison, once we know that the queries are equal, I think that's enough, we shouldn't test other methods on them, it's out of the scope of this test.
I see now... yea it's odd here cause this query has a single float field, looks better on more complex queries (especially cause you don't really see that among many fields)...
can we add [{}] to the filename
[{}] for path.
Why not wipe the entire source directories? I think it's good not to leave empty folders behind? we also leave lock files behind...
can we add the index name, shard id and the paths looked at as the exception? it's especially interesting because needsUpgrading checks for the state folder. Also, do we care about nodes that have the state folders but no data in them? should we fail the node in that case? if so, may change the message to say "no shard state found in any of [FOLDERS], please check and remove them if possible".
one little line, so much work :)
I wonder if we should log a warn level log if we have multiple shard states at this stage. It means something went wrong.
can we name this selectNewPathForShard? , to contrast it from `loadShardPath` (findShardPath sounds to me like go and find an existing shard path).
missing { } :)
this seems to be called when `upgrader.needsUpgrading(shardId)` indicates true, so this is not needed. Do we want an assert `needsUgrading(shardId)` instead? if we want the log (which should be debug imho) it should be moved to the calling function. It's not visible now.
nevermind. Misread the test. It actually writes a shard copy to all paths.
`Arrays.toString(paths)` already adds [] , no need to add them
yeah, it's OK. I was just wondering...
should the start/finish flush messages be inside the if block? seems like we would only want to see them when a flush actually happens
You don't trust Files.move :)
maybe it is a matter of style, but i think its easier to handle the exceptional case like a guard up front: check stats.docCount == -1 and set to -1, otherwise sum. this is not really important to me.
and maybe drop any improved description here too
This needs to handle the -1 case.
What if this.docCount was already -1? then it should stay -1 right? (in the else case here)
catched -> caught
Weird markdown seemed to silently remove some of my text...I was trying to say `FieldStats<java.lang.Long>` (which is what I think you meant by your last statement).
I don't think it is a good idea to call randomInBetween from here? We should save the result to a variable outside of the loop.
You are right, I messed up :) I was pretty sure we had dedicated apis for search templates, but I confused the REST layer with the Java api. Leave it as-is, sorry for the confusion!
I meant `client().admin().indices().preparePutTemplate()` or `client().admin().indices().putTemplate()`.
you are entirely disabling the cache if you don't assign the result here to compiled.
the problem is that when you fix it the test fails ;)
can the methods like getFileStore() be package private? I don't see anything outside of this file using them.
I don't understand the need to use Occur.FILTER here (or in other parsers) versus Occur.MUST. To me these are just parsers for queries, and thats the correct logic, the fact it is wrapped as a Filter means these will all become non-scoring clauses.
Its true but its kind of confusing to understand i think, its like an unnecessary optimization where I think clarity is more important. but really its not a big deal to me, I hope ultimately these go away anyway.
Do we really need singletons here? I'm worried about boosts getting changed somewhere. It seems completely unnecessary when we have a method to create a new one.
To me this is still a mess. can we ignore all these options and return a BooleanQuery using FILTER clause? BooleanQuery is smarter than this one, for example it looks for approximations when considering bulk scoring and stuff.
why have the condition at all? Just always overwrite? Same for disterrpct above
We should be returning `null` in this case. If nothing matches the bounding box, nothing can match the polygon.
And i just did not have the time to yet yesterday remove the stupid asserts from SpanScorer. Please, lets not drag this stuff in again. If oyu want to push fine, but you will see a second push from me removing all this crap.
Why do we need the copy-paste at all? This whole thing seems like a code duplication of PayloadTermQuery.
I think this is fine for now, but note this stuff got excessive in all the spans and still would not find bugs. In order to make things debuggable i had to solve it another way: https://issues.apache.org/jira/browse/LUCENE-6411 So I am not sure about all the asserts, to me I just get lost in the code.
That is just what PayloadTermQuery does, too. So again how is this different from PayloadTermScorer? Even the variable/method names are similar.
no need for the annotation if the test method name starts with "test" :)
I had not though about option 2 but I really like the fact that it does not require to add a new API. But if this argument does not resonate to you, feel free to push the change as-in, I don't have too strong feelings about it.
I think we need an extra check here to see if this was a local or remote execution. If local, we'll have double logging and double shard failed messages.
+1 to having a TransportPrimaryShardOperationAction that will chase the primary (if I get what you mean correctly)
I think we can just move this all out of the try block and then we don't need to assign this to any value since int he finally block we simply decrement the counter. this would simplify this a lot I think
this should be out of the try finally. and then we can avoid the counterInc boolean.
I think we can simplify this a bit and maybe have a class that does that ie this: ``` Java static class IndexShardReference { private final AtomicReference<IndexShard> ref = new AtomicReference(); void setReference(IndexShard shard) { indexShard.incrementOperationCounter(); assert ref.get() == null; ref.set(indexShard); } void removeReference() { IndexShard shard = ref.get(); try { shard. decrementOperationCounter(); } finally { ref.set(null); } } } ``` this seems cleaner? or do I miss something
``` java final RefCounter previous = ref.getAndSet(indexShardOperationCounter); assert previous == null; ```
Another `_` java 9 will be mad at
hooray for try-with-resources!
This seems a little strange, so multiple threads can read the same operation twice and then advance into the middle (or over) a subsequent operation? I think the `AtomicLong` gives the impression of thread safety but we should clearly mark that this class is not thread-safe.
"This class is intended for a single thread use." - can we change this to a more aggressive "This class is NOT thread-safe"? I want to make sure there is no confusion in the future for any other devs using this class
"new" -> "now"
You should be able to collapse this to `IOUtils.close(this.current, uncomittedTranslogs)`
Typo "uncomitted" -> "uncommitted"
Not sure if this already includes the shard context, but if it doesn't can you change it to: ``` logger.debug("{} found local translog with id [{}]", this.shardId, id); ```
This method can be public, static, and take a String instead of a Path, which would allow us to unit test it. It will require moving the logging up one level higher though, but still nice to be able to unit test.
Same here, maybe add that this has a cost, so we need to protect logging statements that use `estimatedNumberOfOperations` so it doesn't cause additional locking when verbose logging is disabled
Another here, warning for cost for logging :)
You can use: ``` java exampleAllocator = ExceptionsHelper.useOrSuppress(exampleAllocator, new RuntimeException(file.getKey() + " is still open", allocator)); ``` And then you don't need the `if` statement.
You could also (if you don't want to use the `useOrSuppress` above, just collect the exceptions into a list and use `ExceptionsHelper.maybeThrowRuntimeAndSuppress(listOfExceptions);` without the `if` statement
I think this should be named `newView`, otherwise it's ambiguous whether it returns a new or a current view
Hurray no more weird `while (true)` loop
I think java 9 is going to choke on `_` here, if I recall correctly
I think we should throw an exception when `id < 0`, which should never happen? (unless Bad Stuff)
Needs to be protected by `logger.isTraceEnabled()` now that `translogId()` locks the readlock, either that, or grab it into a temporary variable before the logging and return statements.
Why not leave this at 0? You can keep the assert and then you don't have to worry about someone serializing an empty `TranslogStats` and running into problems with `out.writeVInt(estimatedNumberOfOperations)`
This might be more readable and succinct with `&&` chaining, what do you think? ``` java return timestamp == create.timestamp && ttl == create.ttl && version == create.version && id.equals(create.id) && type.equals(create.type) && ... etc ... ```
Typo here, "Trasnlog" -> "Translog"
can we have a shorter name here maybe `forceReadFromFileChannel`
can we separate the flush and make this a `SnapshotIndexCommit snapshotIndex()`
we should also catch `NoSuchFileException`
hmm I see it's to opt out...
oooh awesome! :)
`s/shadow replicas/shadow shards/`
This should change as per https://github.com/elastic/elasticsearch/pull/10621 (might happen with a rebase/master-merge)
Should probably annotate this as `@Nullable` since it can return null, just to make it explicit
I don't think we need this any more on master, since we can break backwards compatibility. In that case we can throw the UOE
Also, please annotate with `@IndexSettings Settings indexSettings`, there's nothing worse than `indexSettings` being renamed to `settings` at a later time and then not knowing which Settings it actually is.
I'm kind of concerned about the Guice interaction since this is an injected constructor if this call throws an exception. I know Guice freaks out if an exception is thrown in a constructor (for instance, if it couldn't create the translog file due to disk full or something)
Since you have access to the `IndexShard` you can use `indexShard.routingEntry().primary() == false && IndexMetaData.isIndexUsingShadowReplicas(indexSettings)` for this.
`flushFirst` is never used? This method is unconditionally flushing right now.
Can you add the `translogId` to the log message here? It makes tracking stuff down on shared filesystems much easier.
Should be annotated with `@Nullable`
Nevermind, I see it later on in the `commitIndexWriter` method :)
Whoops, you already did, ignore this!
Can you update the logging in `SharedFSRecoverySourceHandler`? The `phaseN` logging messages are now incorrect with the phase changes.
Typo, finalzlie -> finalize
Should protect this with `logger.isTraceEnabled()` to avoid the extra locking if logging is disabled.
Typo, "Trasnlog" -> "Translog"
hmm maybe name it `markCommitted(long translogId) throws IOException` I think it sholud be IOException here
I think this should be a concurrentSet to be honest we remove stuff from this under locks and I don't like the runtime otherwise.
these ElasticsaerchExceptions are bogus remove them
can we do `if (closed == false)` or use an AtomicBoolean and use `if (closed.compareAndSet(false, true)) {`
`IOUtils` can handle null
this is just wrong IMO. The data safety is here the lucene index and the translog. We HAVE TO make sure its synced to disk before we call rollback.
not sure if we should make such a big deal out of it
just name it `read`
you can omit `ElasticsearchException` here it's unchecked
+1 to clone()
honestly we only call this in one place... I think we should just remove it and expose `readSize` and `read` blow
I added an alternative pattern above how to pass on the ref..
just name it `readSize`
I'd return a dedicated return type not a tuple.. tuples are ugly on these interfaces introduce a new class!
I spoke to @rmuir and this is enabled 99% of the time in master (file leak detection) we can just remove this layer.
I'd do ``` Java if (channelReference.tryIncRef()) { try { FsChannelImmutableReader reader = new FsChannelImmutableReader(id, channelReference, length, totalOperations); channelReference.incRef(); // reader private reference return reader; } finally { channelReference.decRef(); } } else { throw new ElasticsearchIllegalStateException("can't increment translog [" + id + "] channel ref count"); } ```
we can remove it - mockFS takes care of all of this
please - I can help if you want
confuses the shit out of me everytime :)
I usually do this a little different like this: ``` Java if (channelReference.tryIncRef()) { try { FsChannelImmutableReader reader = new ... channelRefernece.incRef(); return reader; } finally { channelReference.decRef(); } } throw new TranslogException... ``` just an idea...
++ to this pattern
we don't need this anymore mockFs takes care of all this. /cc @rmuir
another option here is to remove the `success` and just call `translogs.clear()` once you are done and simply close the lists elements all the time
lets also rename snapshot() these names are not good
wtf is this yield doing here? ;)
can you just use `Iterables#concat(uncommittedTranslogs, Collections.singletonList(current))`
fine no worries
waitForMappingUpdatePostRecovery defaults to 30s, give that the timeout now results in a failed shard (good!) I think we should be more lenient. Especially given the fact that local gateway recovery runs on full cluster restart where the master might be overloaded by things to do. How about something very conservative like 15m (which is what we use for the same update mapping - see RecoverySourceHandler#updateMappingOnMaster)
maybe add also what the method does, although quite self-explaining
can we remove an "elasticsearch_" prefix if it exists, I think it will be cleaner? down the road, we can also remove the specific ElasticsearchIllegalArgumentException and such, it was added historically to get the correct status code, but now we also identify IlleglaArgumentException and return the correct status code, so the need for those became irrelevant.
I wonder what difference there is here between empty params and null params... I thought there wasn't any and we should stay away from the null invariant to prevent any NPEs and such, as we do as much as we can. Sorry for the ping pong @jdconrad I know I made you reduce the number of constructors, I just thought that 2 is better than 4/5 :) Anyways if you guys think that null has some special meaning we can keep it...
@uboness that's what I see doing with new classes in other PRs, I am just adapting to that tendency here. I don't know if any formal decision has been made. No strong opinion on my end, but I tend to consider this user facing through java api at the end of the day, although we dont' get there yet with this PR but we should at this point.
no need for `else` here
I believe there's a difference between `null` params and an empty params. specially when it comes to parsing.. `null` basically says "used whatever default is defined in the server"... with empty, it's not possible to conclude that. +1 on keeping it null
I think we have to add a null check for `Script` too
@javanna is that something we decided? new APIs have real getters/setters? I thin it'll create such inconsistency across the codebase. Right now it's kinda clear - user facing API use real getters/setters, internal don't.... but maybe a decision was made around it and I didn't notice
this import doesn't seem to be needed
Also the iterator.
I still wonder if for this simple case we should just to return boost != other.boost , doesn't make sense to call `Objects.equals` to me.
let's not make this hold this PR, but let's keep track of this potential issue and address the need for generifying in a separate issue
this is fine for now, but I'd rather have more runs on CI than spreading these repeats all over the place. We are most likely going to forget about them, already too many things we have to watch out for in this branch.
I wish there was a less impl dependent way of checking that named queries/filters get parsed (without instanceof etc.). Looks good though I don't see many other ways to do this. Btw this is code that every single query should support I believe (apart from match_all?), maybe we should try and make it more generic if possible (not only test wise).
can you use //norelease instead of TODO? then we are sure we don't release if this method is there cause the build script stops the build in that case
sorry for changing my mind, what do you think about having our own exception here e.g. `QueryValidationException` ? when we'll hook into the search request validation we can just wrap it in the ActionRequestValidationException or something along those lines. I wouldn't query builders to depend on transport action packages ideally.
indeed I suggest to move away from `Preconditions`, for now using ActionRequestValidationException sounds good, maybe a specific query exception would be better but we can figure it out later on I guess
I think we should stick to the current validate api for now, although it does feel weird that we don't actually throw exception. What happens is that the listener gets notified of the exception in `TransportAction#execute`. We can't yet hook into the validate mechanism as the `SearchRequest` only contains a `source` bytes array in json format, but at the end of the refactoring we should have different elements that hold the different parts of a search request, among which the query which can be validated as part of `SearchRequest#validate`. This is to me the only way to make sure that validation happens whenever needed.
Or maybe you meant to add the simulate check here? Otherwise this members would be modified when simulating (simulation is like the "validation" pass in mappings merging)
its not clear to me we aren't double-wrapping here if leafreaders were shared from the previous.
ok, i see it. Its just a little non-obvious due to the way searchers are bubbled up. maybe we can add an assert in the future.
I think it would sound better if you changed it to "--> try restoring while changing the number of replicas to a negative number - should fail"
I think I would move all settings validations including custom path settings validation above into a `validateIndexSettings` method and then we can call this method from the RestoreService. We will have an unnecessary validation for the number of shards there, but it's a small price to pay for not missing any other settings validations that will be added in the future.
not sure what this test tests to be honest but we only pass the param on so no need to really test it. the randomization up there is fine.
OK, great then!
can we debug log the default? also leaning to have info the "non default" setting, thats what we try to do most times in other components to try and keep the startup logs clean and informative.
can we make `"security.manager.enabled"` a constant please
can we assigne `indexService.mapperService()` to a var? just to remove the chaining :)
can we remove this? I don't think it's needed.
can we use `== false`
actually I just got a bit confused because both classes are in the same file...
just an idea: we could skip the creation of the `ReplicationPhase` entirely if shadow replicas are being used.
I find it confusing the we have the same field names for this in both ReplicationPhase and PrimaryPhase.
This doesn't look correct to me: `of course` would be analyzer as a single token `course` at position 0 while it should be at position 1. I think the right fix is to put back `tokens.add` where it was before and instead to initialize `position` at -1.
same here - no optional
we don't need the optional string here as we found out :)
we can't take this one out of the try catch, it protects agains corruption. I think we should rename `buildMetadata` to `loadMetadata` and set both commutUserData and metadata in it.
hmm at some point we need to apply backpressure... not sure if we need to do it in this change though
I really wonder if we should have `Strings.toString(ToXcontent x)` I think we have this code in a lot of places
we have `ToXContent.EMPTY_PARAMS`
also here add the op type in the message
cool can you update the title of the PR then? :)
I think we should always return 1 here. REST tests should run with default number of replicas (1) even against a single node, because that's what clients tests do too. We can never expect green unless we manually set number of replicas to 0 in a test, and we should align to clients builds here I think to prevent failures that don't repro for us. Thoughts? BTW running against a single node is very helpful because it allows us to realize when we make the wrong assumption in a REST test, which in the past was only going to fail for clients and never for us.
+1 for tests, I am slightly worried that we will forget about this location parameter in some places in the future, was wondering if there's way to enforce it, so that we print out the location whenever possible.
ok lets go with null then
Hmm can't do `ok and ...` because Python will short-circuit and won't print the failures... Maybe just: ``` if check_env_var(...): ok = False ``` for each check? Or leave it as a list :)
Can we just add max(HeapInit, HeapMax) feels like a quick win here.
`com.sun.glass.ui.Size` appears to be unused.
If they are different then mlockall will not really work on unix either. That is because it may map additional stuff later!
do we want to use `Xmx` here? `getHeapInit` is `Xms`, I mean, typically, this will be the same, but by default, they are different in our config
@gmarz @kimchy but it is okay for them to be different simply because we allow it :) The worse case is that we kill starting with MinHeap (growing to max immediately) instead of otherwise the worse case being mlockall'ing too little.
I would simplify this entire method to: ``` if (o == null || getClass() != o.getClass()) { return false; } return fieldData.equals(((FieldDataValueSource)o).fieldData); ```
Thanks for regenerating, I was only referring to the null checks that were in the return statement before!
I see. Not sure if its worth it, if doRewrite makes things complicated in its own way, given that createWeight is final and it will catch the issue.
Can this be final? If a subclass overrides this but does not call super, its caching will break.
Should it be singular? Just asking because with `units`, I would be tempted to put `seconds` while I should actually put `second`.
I see. Maybe we should consider adding a wrapper similar to `TimeValue` to avoid string manipulation. But this is no new issue, let's defer...
Even though this one should work, exact double comparisons tend to scare me a bit: should we use `null` instead of `-1` for non-existing x-axis units? (and store it in a Double)
We have some other APIs using the american spelling "normali**z**e", should we be consistent here? (eg. IndexSearcher.createNormalizedWeight or `normalize_lat` on geo points)
I'm thinking something like: ``` json "highlight": { "cat_*": <other stuff>, "*": <stuff> } ``` As is now the last matching regex wins but it does so by highlighting the field twice. That's what I meant.
Why not add ``` java if (highlightFields.contains(fieldName)) { continue; } ``` around line 94? That'll prevent two regexes that find the same field from highlighting it twice.
the dollar sign option :)
I'm good with that, and a strict YAML parser doesn't barf with this syntax, which is a bonus :)
we can use `// norelease` here to ensure we never release with this in it
give it the tests name that might make it easier to figure out where it leaked if it leaked
should we subclass `IllegalIndexShardStateException` instead of `IndexShardException` then we don't need to change `retryPrimaryException`
yeah nevermind I was confused about some internal classes
ok fair enough....
can we call this `index.shared_filesystem.recover_on_any_node` (and rename the constant) and move it to IndexMetaData? I am concerned that people will think its a flag that supports non shared file system cases
I mean to close `node` and safe the conditional... but it's Releasable so you can use `Releasables.close()`
I think the message should be "security manager is disabled", because the assume would only print the message and ignore test if the security manager is disabled.
this if and `toAbsolutePath` are not needed
minor: unused import now that WINDOWS is a static import below
was puzzled about the naming of `release` here for a second for adding a filter context. Also, the below code, just means, this context can never be reused by not being added to the queue, right? (Just want to make sure I get this right)
static import of Matchers or just static import `Matchers.is()` (but thats my personal preference over `equalTo`...)
I think this is scary (its this way in another file in this change too). we loop through requests and each one assigns location variable, and only at the end do we processAfter(location). So is there an implicit guarantee that location's file will always be the same (some concurrency guarantee) and then from there that location's file pointer is only increasing? If so, it would be great to at least have asserts or hard checks, e.g. assert location == null || (location.file = newLocation.file && location.pointer < newLocatoin.pointer) or whatever this is. Or perhaps there is another way to make it less sneaky.
not sure either, I just thought we introduced `parserName()` to have our temporary `toQuery()` method working. ``` //norelease to be removed once all query builders override toQuery providing their own specific implementation. public Query toQuery(QueryParseContext parseContext) throws QueryParsingException, IOException { return parseContext.indexQueryParserService().queryParser(parserName()).parse(parseContext); } ```
I didn't mean that the static parser name will go away, I only mean the parserName method in the builders... sorry for the confusion
yea.. that'd be great to have
nice method name :D
save -> safe
`upgrade API` -> `the upgrade API`
Maybe add minimumLuceneVersion() to index metadata as a placeholder for #11095? Then the impl can just be createdVersion().luceneVersion in this PR? It would just make it more clear here that the lucene version is the driving factor.
I think this name should be IndexMetaDataUpgradeService? Otherwise it sounds like there is a "metadata index".
Ok, didn't know about those...I guess keep for consistency...
this method seems to be wrong here the class is concerned with fetching data in an async fashion, I think what we do with it or what we call as a result should just be somewhere else. I wonder if we can just have an ActionListener that we pass to execute this onSuccess and onFailure since those seem to be the two mode we have.
can we just pass the action to the `AsyncShardFetch` class instead of subclassing this seems so close, maybe the two actions can share a common interface
can we implement `Closeable` and use an AtomicBoolean to signal it's closed I like the `if (closed.compareAndSet(false, true))` pattern
can we factor this out of this class and remove the useless `@Inject` from the `AsyncShardFetch.Started` & `AsyncShardFetch.Store`, make the `GatewayAllocator` instantiate these classes directly. We can then have a simple `OnShardFetched` callback that we can pass as a ctor argument that does the reroute for us. Similarly, the `GatewayAllocator` should also register the cluster change listener and if you ask me I'd drop all this generation thing and replace the entire cache if we are getting a new master in the `GatewayAllocator`.
can we assign `routingNodes.ignoredUnassigned()` to a variable please? it's used in some places and reads just easier
this is super ugly I think `AsyncShardFetch.Started` and `AsyncShardFetch.Store` should be an impl detail of `GatewayAllocator` no need to bind this or anything
What do you think of the name `finish(...)`? `doneFetching` is a little boolean sounding to me
I think `Store` and `Started` should not be internal classes, it makes the `AsyncShardFetch` file smaller and easier to digest if they are moved to their own files.
i think `== true` can be skipped
I think "median" is a better function name here? Limiting to 3 letters doesn't seem to buy us anything.
This method expects an ordinal to be returned so it's not really applicable...
Can we keep the order of arguments for this and super ctor consistent? So all the args that will go to the super class should come first, in the same order to the super ctor? It is confusing to see args moved around (here the order is consistent but in others it seems not)
This code will be never executed. I think you meant to put it after the if statement where we check if we have any repositories registered.
Having my refactoring goggles on I was hoping it was possible to avoid introducing new Parsers/Builders, even if they are dummy ones. I see why this makes makes sense here though. Just wondering if if would be possible to wrap a simple existing Parser (like e.g. MatchAll) and have it produce the DummyQuery instead of having a completely new Builder/Parser pair we need to take care of in the refactoring later. Not very important, though.
I am starting to see that the default boost doesn't get printed out but other default fields do. Makes sense to me but maybe we want to be consistent? I think we should have this a separate discussion, make some decision and do the same everywhere (I have the feeling we are not yet settled yet on one way or another)
I don't mean to block this PR specifically, these are just things that are popping up the more I review similar PRs
I guess we just prefer primitive types over objects :)
yea I think it makes sense, indeed we shoudn't complicate this too much, leaf queries get their own tests anyways.
one too many whitespace between List<C> and the method name
makes sense, lets leave list for now
shall we move this method to a new QueryTestUtils class or something? I see it being used in other tests in the future and I am not sure it belongs to the test base class
all these random values need to be saved out of the loop...you know what happens otherwise? :)
We can move this declaration down inside the try? It's not used outside...
Can we remove the `Translog.TranslogGeneration commit` arg entirely and just use `translog.getGeneration()`? Seems like all callers do that...
can we call this `getQueryParserService`
Can we avoid DEFAULT here? This code assumes that every Path in `roots` uses that filesystem. Technically, they could be different. I would prefer if we avoid get()-like methods for cases like this, and just resolved simply it, like this: ``` for (Path root : roots) { Path normalizedPath = root.resolve(path).normalize(); if (normalizedPath.startsWith(root)) { ... ```
Why do we need a map to store two options? can we not just store the options in two local variables here. If/when this expands into more parse options and its too big to pass around as separate variables we could create a class to hold these options.
I'd prefer this to be `@Nullable` as well... relates to xcontent serialization
I'd say yes... if you want to be able to parse script as a string, you want to be able to serialize it as as string. I believe serialization should be symmetric - you write what you read. For this reason, I believe the script type should be nullable. if you read a script like a string, the read state should be preserved for the writing.
> Run TransformOnIndexMapperIntegrationTest.getTransformed() with seed -Dtests.seed=CCF6041A004DDD9D to see why maybe you can explain why here? without knowing much.. it smells like a bug in transform
> Making type @Nullable will solve this as we will know the difference between an inline script entered as an object and one entered as a simple string that's exactly my point... I believe the `Script` construct should reflect exactly what the user inputs.. hence the `@Nullable` type approach
sounds great thanks
no both Request and RequestBuilders are java api. one can choose which one to use. you can always do client.search(SearchRequest) without going through the builder.
if we do make it nullable @colings86 we have to make sure some value gets provided as part of the `canExecuteScript` call, where the type is required to decide whether the script can be executed or not.
we can use Writeable here instead of Streamable so fields can become final and default constructor can go away
yeah, I think in 1.6 we should just keep on using the old exception and switch to the new one in 2.0
this new exception is going to trigger errors too if we try to serialize it to an older node
Maybe let's just call Objects.equal(script, other.script) for simplicity? I know you did not introduce it though...
this getClass() test is not necessary, super.equals already takes care of it
I think it's fine this way but it's important to not forget to rename in master afterwards.
same as above, this breaks bw comp for the java api
Error if a mix of old style and new style params used alongside each other
Error if old style and new style params alongside each other
wondering whether `CompiledScript` should hold the `Script` and `ExecutableScript` should hold the `CompiledScript`
Error if old-style params passed alongside new-style script
Throw error if old-style params passed alongside new-style script definition
can we just use compare on Long? It gets confusing with < and == on `Long`
I see, ok, it can get interleaved logging, so would be nice to have it in a single one, but not urgent
In my dreams, merging would either throw an exception or return a new independent mapping so that we wouldn't need this validation phase :)
I don't think you have to add here `TermsLookupQueryBuilder.NAME` to the array
I'd rather merge TermsQueryBuilder and TermsLookupBuilder, this is going to be a problem anyway when registering builders for serialization if we want to keep registering the parser only and deduce the builder from it . I don't see any value in having two builders for the same query, it becomes confusing for java api users too. Also, the easy way of creating queries should be through querybuilders, we can keep the existing `QueryBuilders#termsLookupQuery` but it will return a TermsQueryBuilder instead. We can also add more methods to `QueryBuilders` to create a terms lookup query that holds all the needed params.
I have a small preference over public abstract rather than abstract public
in all the builders, you can remove the class name, just `return NAME` is enough.
There is actually a [standard](http://checkstyle.sourceforge.net/config_modifier.html) for this if you particularly enjoy standards.
yea thanks for the link, correct order is public abstract, which I think we've been using in the rest of the codebase.
would be great if this logic could be unit tested.
can we do this `for (IndexShard shard : this) {...` not a big deal though
I also think `clusterInfoService` is only started on the master
I think this causes a memory leak since IndexService is created per index and if it's closed you need to remove this. you should add this listener in `IndicesService`
This seems error prone if the structure could change in the future? I dont know a better way though..
I'm wondering that maybe we should directly use TermListBackedStringFilter in that case: I wouldn't expect automatons to speed things up a lot and having TermListBackedStringFilter only in this catch block makes it hard to test.
Even it we remove that logic, it would be good though to keep the explanation that we don't use an automaton for the term list case in order not to blow up the maximum number of states.
ah I see it has problems with `null` got it
I think in this case we should add `null` to the lagWindow and not calculate the diff value for the bucket that is `lag` buckets from this one too. otherwise we will get out of step when we encounter missing data. This would match the behaviour in the derivative agg.
You can use end instead of `out.position()` here.
Can you at least give the file a descriptive name? We have multiple tests that all have the same file name, but in a different package. "fields1" seems like it might be reused later, and having a non-unique name makes it hard to quickly open a file from an IDE.
cat we add a trace log here with the exception? Thinking it might be helpful to be able to know why...
this is unneeded? we check before we call...
missing Test annotation, just for consistency with other test methods
actuallt are we sure it's good to just ignore it if field is null? I would rather leave as it was (it throws NPE) and throw IAE ourselves, not much difference though
I know Simon preferes if (Strings.isEmpty(field) == false) , shall we change it then? ;)
given that we have almost proper setters here that return void, do we really want to make them return this? no strong opinion, just thinking out loud
we don't need `== true`, I think we use this explicit version only for negations, instead of the `!`
I think we may need to move Settings to the Builder. It makes more sense that the parser knows about the builder, cause it creates it, rather than the builder knowing about the parser.
Where have you seen this convention? I use uppercase almost always, and that is mostly what I have seen...
I think these Booleans can stay boolean primitive types? we now have the public constants for defaults, I think thats good enough
this can go back to boolean if we move back Settings to boolean too
one more line break? :)
this can go back to boolean if we move back Settings to boolean too
you can remove the validate call for now, we will fix all queries soon, I promise
I wonder if these checks for the lucene query are necessary given that we already have createExpectedQuery and the comparison between that query and the result of toQuery
this can go back to boolean if we move back Settings to boolean too
oh boy I am sorry! I thought it was moved down in the same class, your initial change made sense, next time fight back :)
sorry I re-read it, missing is appropriate, maybe we could rename the constant to MISSING_FLAGS then
you can rebase and get the changes upstream now ;)
super minor nitpick: lowercase the message? s/Supplied/supplied seems to be a convention in our codebase :)
super minor nitpick: lowercase the message? s/Supplied/supplied seems to be a convention in our codebase :)
I think it's not needed
I believe this message needs to be changed now, since the first value is the "used" value now
can we add some more info about what expected found and the field name? maybe "expected a simple value for field '..' but found a [TYPE]"..
You can remove the http enabled in settings in this class now? Thank you for this! I think it should make these tests a bit faster!
sounds good, we were even thinking about merging those, we will not move them to separate packages for sure.
I was wondering if we should use some convention on naming, like `INSTANCE` or `PROTOTYPE`, I have no strong preference though
I understand this, but this sound confusing to me. You would have some member in each builder that is only set for the prototypes, need special constructors for the injection. I understand your proposed solution with the static method access much better.
Minor problem when I run this: test complains that it has to call super.tearDown().
I don't like adding a context to StreamInput since the context makes little sense for the stream itself but is rather useful to the parser of this stream? I like the first idea better.
well then you have to have a dedicated parser interface - I wonder if this is a general thing we should have on stream input though
another option is have this ``` Java public interface StreamInput { public <T extends NamedType> T readType(); } ``` and ``` Java public interface StreamOutput { public void write(NamedType type); } ``` and this can resolve all the stuff internallly and if it doesn't know how to resolve it barfs? ``` Java public interface NamedType extends Writeable { public String getName(); } ```
yeah I'd leave everything to the stream just like java seriazliation works
My preference would go to adding a serialization context to readFrom.
Can we check that the hits information is correct in the response too? i.e. the hits are empty, the totalHits is correct and the maxScore is zero
debug or info? I would be more for debug
can you remove this TODO? I'm not sure we are going to implement this after all, nobody needs it for now :)
maybe overkill, but is it worth keeping a list of highlighters ordered by precedence instead? it could even allow plugins to provide their own precedence, but that's maybe too much? I am not sure, just asking
That's right - you need to call them out explicitly.
This is what I meant, yeah. I'd have made it a `private static final ImmutableList<String>` instead of `Immutable<Highlighter>` but it doesn't make much difference.
This probably isn't required - other highlighters can just be called out in the request if you want them.
It does seem a bit cleaner to use a list. For what its worth my highlighter would just `return true` from canHighlight because its defaults let it pick its hit detection strategy based on what is in the mapping.
thinking out loud, maybe I am getting confused, but in order for a field to get highlighted, doesn't it need to be stored too or we need to have the _source at least? but metadata fields, which match `*` are not part of the `_source` hence they need to be stored or excluded from highlighting by definition. I have the feeling we should do something more to address that...
+1 that is what I would do too
you can drop `public` here
can we add an assert to make sure that highlighterType != null here? it really should since we know that plain highlighter always returns true, but the assert would make it more explicit that it is expected
There should be a "resolve" button in github like there is in googledocs so we can ignore long resolved recommendations.
Makes sense to me. The random null pointer exception you'd get later on if this went wrong would be unpleasant to users. Probably best to use an explicit check rather than an `assert`.
Yeah - at least needs the ALL_UPPER_CASE naming. Probably should be moved to the top of the class too because that's where I usually look for static stuff.
the == false is done on purpose to make these comparisons more explicit
as said above, the double negation here feels weird to me, but again maybe it's just me
oh boy... would be great to have them then
wondering if extracting this block to a new method would make it a bit more readable, I have no strong opinion though
You have an extra `,` before the URL
I think we have at least one similar test that does the same, we can maybe share the dummy client to minimize the code repetition. It's not that we cannot use mockito, we could, we would need to have a bigger discussion around it, some people are in favour of that and some others are totally against it. I personally don't think mockito would solve all our problems, we should strive to make elasticsearch more unit testable, easier to say that to do though :)
acuquire -> acquire
can we make this getAgeInMillis and do the conversions here? we make use of millis everywhere so it will make for an easier to work with api. The nanos resolution is not guaranteed anyway.
Nit picky - Can we capture the initial `long oldestCreationDateInNanos = System.nanoTime();` and use this as the "now"? Just worried that the queue can be empty (after the initial check) and we will still get a non 0 value ..
fix please for max time in queue
Yeah, sorry, it can be ditched, I read it incorrectly
Same here, nevermind again :)
Oh nevermind :)
Yeah, that is what I meant. I guess we should check and protect for having no current version nodes, no? On Fri, May 29, 2015 at 5:44 PM, Britta Weber notifications@github.com wrote: > > @@ -77,7 +77,8 @@ public void testReusePeerRecovery() throws Exception { > > indexRandom(true, builders); > > ensureGreen(); > > if (randomBoolean()) { // just make sure it doesn't break anything - we seal before we actually bump replicas > > - backwardsCluster().internalCluster().client().admin().indices().prepareSealIndices("test").get(); > > - logger.info("--> trying to sync flush"); > > - assertEquals(SyncedFlushUtil.attemptSyncedFlush(internalCluster(), "test").failedShards(), 0); > > yes, should be backwardsCluster()... we always have current version nodes in the test unless annotated otherwise. is that what you mean? > > --- > > Reply to this email directly or view it on GitHub: > > https://github.com/elastic/elasticsearch/pull/11417/files#r31336414
don't we need backwardsCluster().internalCluster() here? also, are we sure we have at least one local (client) node? o.w. there is not SyncedFlushtService instance..
oh boy I didn't realize we could get rid of createEmptyQueryBuilder, this is great!
shall we flip this outer if? `if (fieldNamesMapper != null && fieldNamesMapper.enabled()) {` looks a bit nicer to me
I always wonder if we should use the PROTOTYPE constant here instead, cause that is what we need I guess. If so we should change all other tests accordingly
I think it gets weirder to retrieve a prototype builder from an actual builder. Going through the parser is more logical to be honest. We can change when we need it later on.
Instead of having this public ctor should we have one that has this signature: ``` Java public CompressedXContent(ToXContent xcontent, XContentType type, Params params) { // do the serialization here with checked output stream etc } ``` that way we can hide the _CRC32_ impl detail entirely
This should probably be `synchronized` too since you're protecting access to `delayedAllocations`.
I wonder if we can somehow come up with something that better normalizes across failed cloud instances? If a machine is pulled, but its replacement can come up within the allotted time, then it would be ideal to not trigger the recovery because we're waiting on the dead machine (based on its InetAddress).
It may be worthwhile to add `equals` and `hashCode` to the `TransportAddress` interface so that any future implementation does not skip it, excepting the singleton variants like the Dummy one (mostly worried that `BoundTransportAddress` may one day implement the interface but not implement them).
I think we should extend our cloud integration plugins to add some kind of a stable identifier as a node attribute (if not already doing so) and auto-configure this setting to it. /cc @dadoinet @tlrx
Yeah, I guess I'm thinking of people deploying without static IPs (certainly not recommended). These most likely won't suffer the same problems that this solution helps though, so it may be a moot point altogether.
"joined the cluster back" -> "rejoined the cluster"
Looks like a merge issue? (the '+'s)
Ah yeah I suppose that might be ok, in this case it's user-defined input so that's pretty awkward but it beats breaking.
Yes, pass the value as a json string.
I just realized we aren't even talking about setting names, but the valid values for the `format` setting. This argument to use ParseValue does not make sense. We don't support camelCase in eg the `index` option. We should not do it here, it will just add more work for users if we allow them to _start_ using a new value that will just go away in the future (and will require them to change the value to what they would have found in the first place if they had tried using camelCase and seen an error).
> I would also be fine with removing all the camelCase options for all formats in this PR to make it consistent. This is the kind of statement that stalls progress. Requiring huge changes just to make a small improvement should not be necessary.
We already do not support camelCase for all settings, and I don't think there is any consistency even within the same query/field type/whatever.
I am only talking about the date formats here, not across the whole codebase (i can see the above statement might have been a bit ambiguous on that). All the multi-word date format values above support both a camelCase and an underscored version. That should be consistent, whether that means supporting both for now or only supporting the underscored version I don't have a strong opinion but its hardly a huge change to update the date format values to be consistent and its not a huge overhead to maintain an extra 2 camelCase options given that any change to that policy would require a change to all the other date formats too
But I think this is confusing. If I have a format specified as `yearMonthDay` that works then I would expect to be able to change it to `epochSecond` and it would work. Supporting some values in camelCase for date formats and not other values is very confusing to a user. I'm all for removing camelCase but we should be consistent with it, especially when its different values for the same setting (in the case different values of `format` for date fields).
we should convert this to use ParseField since that will deal with deprecated names etc. in the future
In 2.0 ParseField no longer supports camelCase IIRC. The advantage with using ParseField is that it standardises the checks that we do to make it consistent across the codebase (or will once all the Parsers have been changed to use it). It also makes deprecating an old option name in favour of a new one, much easier as we don't end up with these messy if statements with multiple ||'s. And also means its much easier to correlate the builder classes with the Parser since they can both reference a ParseField constant and the builder will alway use the preferred name for the option.
I would also be fine with removing all the camelCase options for all formats in this PR to make it consistent.
I think this is confusing if we support camelCase in some of the options in this parser and not others (even if they are new). We should either support camelCase for all options or for none to be consistent.
I don't think it will be resolved (rejecting camelCase) as its a huge change across 100s of files (because most parser use this style and not ParseField) and I don't see us doing that change quickly
I don't think it matters. We should not force making huge changes to the entire codebase in order to not add things which will just be deprecated and/or confusing to the user.
extra newline here please remove
Had missed that `predictions[i]`. Think in that case it's better as is.
can we check part of the message so we know we got the right exception? ditto for the other cases like this in this file
is index name and type something we really need to randomize in this test? i think we shoudl just test date backcompat here and leave problems with index and type names to other tests...
Do we really need this randomization? seems like the wrong place to test the version created setting is working properly.
thank you for renaming this.
I think this results in double logging, we will log this here too, no? https://github.com/elastic/elasticsearch/pull/11545/files#diff-9a7b643d56430763f42de913657ca798R125
can we report the right version we found ? note that we would probably need to change the the logic in the gateway allocator to check for both -1 version and exception (now -1 means both).
can we add a shortcut for `all`? we lost it...
shard_s_Stores -> shardStores
Another simplification - if we push the code at https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java#L81 into ClusterIndexShardHealth's constructor, we can use it here and just make it a simple lookup in the enum set..
can we just call it `getName()`
I think for now lets just go with `getName` and remove the `Class<C>` here we can resolve collisions later
can we add a little note about avoiding race conditions between read and writes on a shared FS ? I think this will people we not see why we opted to EMPTY_COMMIT_ID
leave -> leaf
his -> This
We talked about this and it's going to be tricky. Nevermind...
+1 to remove this and always notify!
Yay, you can remove this TODO!
Sigh :) I'll add a `public getMergeInfo` to `OneMerge` to Lucene ...
Good point, I was thinking about fielddata_fields but we can't get them anyway if a doc is only in the translog...
Woops, sorry, you're right! CMSProvider.this.maxMergeCount is the true value ...
can we call it testNoInnerQueries? I would like to avoid confusion with our nested query (nested type) here
clarify the error message specifying what needs to be non null? the inner query...also remove empty, doesnt make sense here
nevermind I see it was already there, then it should be ok
ok I do not like the way we use null here but I see the complication of changing this now. Let's leave it as is then and move on ;)
I see ok that sounds good to me then
space between method params: ``` randomIntBetween(-20, 20); ```
the `numPredications < 1` check feels like an assertion... should probably be the first statement in the method
no need for `else` here: ``` java if (numPredictions < 1) { throw new IllegalArgumentException("numPredictions may not be less than 1."); } if (numPredictions == 1) { predictions[0] = next(values); return predictions; } ```
in fact, to make all the models work in the same way, we could make `MovAvgModel` declare the following abstract method: ``` java public abstract <T extends Number> double[] next(Collection<T> values, int numForecasts); ``` Then the `predict` and `next` methods can just be declared in `MovAvgModel` and be the following: ``` java public final <T extends Number> double[] predict(Collection<T> values, int numPredictions) { return next(values, numPredictions); } public final <T extends Number> double next(Collection<T> values) { return next(values, 1)[0]; } ```
That's java 1.8 construct
You use dateTimeFormatter.format() for equals but dateTimeFormatter for hashCode
Since this isn't a blocker we'll cleanup these defaults in a separate 2.x issue. Opened #11682 to track.
super.equals already ensures this, since it does: `if (getClass() != obj.getClass()) return false;`
please ignore - misread the constructor - seems like a null is a valid value.
These new values need to be wrapped around a version check
IMO I think this should be an if statement that throws an `IllegalArguementException` instead of an assert since a user can easily pass in null at runtime here. That way they will get an exception when its set rather than a weird exception happening when we try to use the variable. Same goes for `charFilters` and `attributes` below
I guess it's because the async shard fetch logic - the shards are not assigned when the call returns.
can we remove this try catch? let the original exception just bubble up...
this is the default..
nit pick - can we start with node1 and relocate to node2 ? :)
waitForyellow can go away...
can we add async variants to do this? startMasterNodesAsync, startDataNodesAsync..
just FYI - you can do setSettings("index.number_of_replicas", 0)
this looks good. i forgot how crazy URL was here with null versus empty strings for different pieces...
I think this needs to call PathUtils.get (not Paths.get) so that it does not use the JVM default filesystem, in the case one is set differently in tests.
@jdconrad sure. The place where the expression script will be used is here[1]. Is this enough for what you need? If not, please let me know. [1] https://github.com/elastic/elasticsearch/blob/a216062d88205c0d109ac00a432b45bc90a14fe8/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/seriesarithmetic/SeriesArithmeticPipelineAggregator.java#L109-123
It might be a good idea (possibly in a different PR) to have a method on `ScriptEngineService` called something like `getSupportedScriptContexts()` which each implementation can use to define what script APIs they support. I imagine there are/will be other language that don't support some script APIs and this would not only allow them to use this too but would also remove language specific code form the ScriptService, which should probably remain language agnostic.
I think it would be cleaner to move the assert into the catch, and add a `fail("expected script exception")` after the call to `run()`.
Ok fair enough, Hadn't considered the settings aspect of this.
Okay, doesn't matter if the builder only outputs verbose version as long as we support the other one still.
I think `query` can be null here in 1.x and before, we should double check before calling a method on it.
ok no big deal we can keep it if it makes sense for consistency. Or if we added those list methods to bool query builder maybe we should go back and remove them? less code is always better, especially if it can be avoided by looping. Allowing a List makes me wonder why we don't support Collection and varargs too, which would mean too many add methods :)
yes my reasoning is that a compile error makes you think about validation rather then forgetting because there's a default empty impl that does no validation. I tend to prefer an empty validate in all queries that don't need to validate, although that's verbose. Plus that is what we do with ActionRequest as well.
thanks, let's say I prefer to be verbose now so we don't forget. Once we are done we can remove if that makes sense :)
please return a Map instead no google guava stuff in public interfaces
This is redundant. getDelayAllocationExpirationIn also calls getAllocationDelayTimeoutSetting()==0 and returns 0 in that case.
Got confused by this and had to go to the code :) - I think this will be clearer "returns the time in millisecond until this unassigned shard can be reassigned."
Minor: can we call this findSmallestDelayedAllocationSettings? Next confused me implying it somehow depends on now.
can we fold this into ClusterHealthResponse? that way we can test this as well as part of the unit testing.
++. Thx On Wed, Jun 17, 2015 at 9:09 PM, Shay Banon notifications@github.com wrote: > > - return 0; > > - } > > - TimeValue delayTimeout = indexSettings.getAsTime(DELAYED_NODE_LEFT_TIMEOUT, settings.getAsTime(DELAYED_NODE_LEFT_TIMEOUT, DEFAULT_DELAYED_NODE_LEFT_TIMEOUT)); > > - return Math.max(0l, delayTimeout.millis()); > > - } > > + > > - /** > > - \* The delay in millis when delaying assigning the shard need to expire in. > > - */ > > - public long getDelayAllocationExpirationIn(Settings settings, Settings indexSettings) { > > - long delayTimeout = getAllocationDelayTimeoutSetting(settings, indexSettings); > > - if (delayTimeout == 0) { > > - return 0; > > - } > > - long delta = Math.max(0l, System.currentTimeMillis() - timestamp); > > - if (delta == 0) { > > I think I got what you mean, I will improve it to check on negative value and return 0, to accomdate cases where its fast enough where currentTime is the same as timestamp > > --- > > Reply to this email directly or view it on GitHub: > > https://github.com/elastic/elasticsearch/pull/11712/files#r32663140
can we change this to await busy? then we don't need sleep ...
this can very verbose (600 shards on a node). I'm doubting whether we should have this as debug and have an info level log saying "allocation of [X] unassigned shards delayed".
I think I see what you are doing here (protect against system clock going way back in time), but I wonder if we should be so strict. 0 is a dangerous value - I'm worried that a very fast machine will render this check ineffective, so I would go with `delta<0`. I also wonder if a short ntp correction will release pending shards. Maybe use `delta<-delayTimeout`? .
as far as I can tell we are scheduleing on a fixed delay every 10 seconds so I wonder if we should just use this and don't schedule another task? I might be missing something
all sounds good to me. innerQuery is fine.
I think I already explained that even that PR has access to mappedFieldNames, that is something static, and all of the random queries can depend on it, I don't see the problem
but why? :)
it's fine, when I did the refactoring SpanQueryBuilder became an abstract class without any problem, but now it needs to be a marker interface again cause java doesn't support multiple inheritance ;) We just need a cast, sorry for the noise I had missed this change to be honest but now I get it, thanks a lot for digging!
what naming convention do we want to follow here? I saw around some innerQueryBuider(), here innerQuery(), maybe we even want to drop the inner prefix... let's decide for one of the options and go for it everywhere.
as I mentioned a million times now, when we will say let's not use it and remove it everywhere I will stop whining about it. I don't want to see three methods with annotations and one without it, it hurts my eyes. Either with or without it, not in the middle, at least in the same test class.
we should `@Test` to forbidden API
can we just us a `Map` here instead of the guava one
can we use `== false`
can this runnable be an `AbstractRunnable`
can you assign `event.state().nodes().masterNodeId()`
can we keep this simple and just assign a new map here and make it final removing all the weird checks if it's null
can this case have a default case that throws `ISE`
I really think the root cause of the problem here is that we accept this on the server maybe we can require all settings to start with `index.` on the server side? (different issue if you want)
+1 to randomize on 1 node cluster as well - wasn't aware of this to be honest...
note how the Throwable may be null. Also, it would be great if we can capture the reason. So I would suggest we make an IOException here with a message that has the reason in it and capture the failure as a cause.
at some point we should really add these ourside of the classes itself IMO. but that is totally unrelated
I wonder if we should shortcut if modified == false . I'm thinking about a join storm upon master failure which will lead to a series of "received a join request for an existing node " where we don't really need to do anything.
oh man that class is a nighmare. really I just realized how fucked up this is grrr.
I also wonder if we can trash this method then altogether a make `GatewayAllocator implement ClusterStateListener` and add it to the `ClusterService` once constructed or maybe inside the ClusterService...
not related to this change, but I think it's good to do FutureUtils.cancel(registeredNextDelayFuture) here. Might save on some cryptic error messages during shutdown..
Just an idea how about making rerouting an AtomicReference<String> where a non null value signals the currently next reroute task name? this way we can log here logger.trace("already has pending reroute due to [{}]. ignoring [{}]")/
or just: ``` java if (Objects.equals(similarity(), other.similarity() == false) { conflicts.add("mapper [" + names().fullName() + "] has different similarity"); } ```
I think because the readAllowed() is now part of the acquireSearcher() method.
++ to this simplification
sneaky, I missed that :)
have you run this through maven? I think `String.format()` without a locale is part of the forbidden API
maybe just write `elasticsearch-plugin-name Elasticsearch 2.0 Core plugin` (fixes the awkward spacing problem) Also the phrasing assumes that elasticsearch commercial plugins are not official somewhat? Not sure if its just me being non-native speaking.
why no `private static final ImmutableSet<String> OFFICIAL_PLUGINS = ImmutableSet.<String>builder()`? any generics voodoo I miss? :-)
not really scope of this PR, but otherwise I will do it.. if we use `String.format()` for those URLs, they will be more readable
shouldnt this solve the problem and not the above ``` String.format(Locale.ROOT, "https://download.elastic.co/org.elasticsearch.plugins/%s/%s-%s.zip", repo, version)); ```
to -> so
Oops nevermind, I misread.
here too, toQuery might return null, not sure what happens
Maybe we could call the remaining equals() implementation in the query builders slightly different? When I read this code and don't know about the abstract superclass, this might look a bit odd since it's doesn't really overwrite the canonic equals() but it sort of looks like it does.
lower case F please :) - "found shard on ..."
gotcha the subtle difference is that here the parser passes in null....alright.
I think we should have these checks in validate. and if we don't want to rely on validate in doXContent have if conditionals there and ignore what's null just to prevent NPEs, but I wouldn't want to do validation and throw IAE in doXContent, where we just print stuff out.
maybe we should go through the add method here to make sure that null values get rejected properly
I see one thing that looks weird in this class: ``` @Override protected void doXContent(XContentBuilder builder, Params params) throws IOException { if (positiveQuery == null) { throw new IllegalArgumentException("boosting query requires positive query to be set"); } if (negativeQuery == null) { throw new IllegalArgumentException("boosting query requires negative query to be set"); } .... ```
missing whitespace between `null` and value :)
good catch!!!! not sure why our tests didn't uncover this, will dig.
I think you can also replace a switch in toQuery
you can remove a switch in this class (fromXContent method) and use Operator.fromString
I'd say it's fine with IAE
Here we should be able to get correct flags? Maybe not always, but at least in the context of aggs, I think the search context is available
can we change this log to indicated why we ended up here? (now it says only translog ID)
I think it's good have 0 as an option here too? I.e., index and search a freshly opened engine ..
yes and that should be enough. and that is also why we keep Object in the builder. but we can simply pass the Object that we have over to the fuzzyQuery method, with no conversion
I think it's a simple enough change to make as part of this PR.
gotcha, I didn't realize that we were creating a new query not going through the mapper, which accepts a Term. This is correct as-is, sorry.
ok lets keep it but please let's not create a list for this, we can just do if INT.equals(field) || DOUBLE.equals(field)
then I guess the equals method is wrong too cause auto != AUTO . I think I would try to uppercase (or lowercase) when parsing and make this case insensitive that way.
then let's use it now otherwise this change is half baked
Did this change in the builder or does the case check stay? Can't find it, maybe missed it.
sorry, my bad.
this would make sense especially given that their setters accept primitive types
wonder if we should make these Integer and Boolean just int and boolean primite types.
nitpick: can you change these constants to static final rather than final static (just a common convention on order of modifiers)
boost will be gone once you rebase, same for queryName
CONSTANT_SCORE_REWRITE is the rewrite for all multi-term queries but FuzzyQuery: see the FuzzyQuery constructor which calls `setRewriteMethod(new MultiTermQuery.TopTermsBlendedFreqScoringRewrite(maxExpansions));`
From the discussion before I think that removing the special case for FuzzyQuery inside filter context is okay here since CONSTANT_SCORE_REWRITE ist the default MultiTermQuery rewrite method anyway. @jpountz could you have a look at this change since I think it's related to the filter/query merging.
If we already break the builder API by changing the value-getter, it would be great to somehow also restrict the constructor to not accept every value type anymore. Maybe deprecate this constructor and instroduce new ones for String and Number? Not sure if also Date type is handled correctly when parsed to String here. In general I'm unsure here to how far this PR should change the status quo. I think only selected Object types worked here in the past, it should be stated somehere (docs?) what actually works. In our own code we really only use the constructor with String argument, so at least internally we could easily change that, but not sure how many client code this would break.
Since `value` internally is a String now, we can change read/write here as well.
If this stays an Object we have a problem with builder equality when we do the roundtrip doXContent->fromXContent that we currently do in the tests. I had similar problems in TermQueryBuilder already. Imagine an Integer (126) that is rendered to JSON (or other XContent) via `writeValue(Object value)` which inspects the class of the Object and then chooses a type-specific method to write it. On the parser side, this is always read back as a String ("126") with `parser.text()`. So if we check those builders for equality we will fail with `126 != "126"`. For the lucene query produced in the end this difference won't matter (we convert all values to String there), but I think it would be great to keep that property that when we have builder A, generate XContent for that and parse it to Builder B, they should be the same. One option would be to push up the String-conversion we later do in `toQuery()` anyway to the constructor. I'm not sure if the java API currently is to lenient here by allowing any `Object` as argument, since parsing only works for object types that correctly parse back as String. Another option might be to allow parsing to numeric values using `objectText()` like in CommonTermsQueryParser. Not sure about which way is better to be honest.
I think the filter flag needs to be checked earlier, already in the parser. `isFilter` is a statefull flag in the QueryParseContext that gets set/unset when parsing one of the (former) filters. ParseContext.isFilter at this point doesn't reflect the state it was in during traversing the query tree. Check QueryParseContext.parseInnerFilterToQueryBuilder() to see where state changes can occur.
I will take care of this.
Oh, I'm sorry I mixed VInt with optional, my fault. Must be the heat. Please ignore.
implements BoostableQueryBuilder should be gone once you rebase
this could be static
what changed here? I can't spot the difference compared to before
this could be replaced with Objects.requireNotNull
I think we have constants for this now: https://github.com/elastic/elasticsearch/blob/8238f497d85d785a61ebc017b8ac96fc5fcd28ae/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java#L32
I think I would revert this change, no need to call the other constructor here, so the precondition can happen first, which makes more sense. Same in other constructors
I see why this happens, will fix s we can remove this hack
I see but I wonder if we should remove this method and simply accept Object and add a Fuzziness constructor that accepts Object instead
it makes sense to make them public then I think
this should not be public, private is fine
Are we sure we can use lessThanOrEqualTo here? we can also just assert that time() > 0 imho.
I see what your are saying but I dont think we can rely on this. The nanoTime() is not guaranteed to actually have nano precision (just resolution). it is only guaranteed to never go back > On 29 Jun 2015, at 10:39, Masaru Hasegawa notifications@github.com wrote: > > In core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java: > > > @@ -154,7 +154,7 @@ public void run() { > > if (randomBoolean()) { > > timer.stop(); > > assertThat(timer.stopTime(), greaterThanOrEqualTo(timer.startTime())); > > - assertThat(timer.time(), equalTo(timer.stopTime() - timer.startTime())); > > - assertThat(timer.time(), lessThanOrEqualTo(timer.stopTime() - timer.startTime())); > > I think lessThanOrEqualTo is correct. (because it's rounded down to nearest decimal value) > > If we use nano seconds, when start time is 1ns and stop time is 1000000ns (1ms), time() would be 99999ns but it's 0ms because of TimeValue.nsecToMSec. > But if we use milliseconds, above becomes 1ms - 0ms = 1ms. In this case, time() < stop() - start(). > When start time is 1ns (0ms) and stop time is 1999999ns (1ms), it's 1999998ns but time() will be 1ms and stop() - start() = 1ms. > > That's said, I like time() > 0 since it makes it simpler. > >  > Reply to this email directly or view it on GitHub.
open reader doesn't need to check for <0 and throw an exception any more.
Same could be said of newCost and newModel
Does it actually matter if the sizes are different here? we have chosen to make them the same above but it doesn't break the algorithm if they are different so I don't think this assert is necessary.
Maybe use the same convention as we use in the other builders here. Have these variables as Double objects and default to null here. Then check for null before including them in the XContent output and leave defining the defaults to the parser? This applies to all the models
This could be final? Also, applies to parameters in the other models
count the expected errors too like we do in other tests? also we never do (invalid, invalid). I think randomizing things may improve this test and coverage too, like we do in other tests.
I just looked at the original parser code and the docs again and saw that it explicitly allows leaving out the `filter` section: ``` // we allow for null filter, so it makes compositions on the client side to be simpler ``` The JSON side of this query seems to be currently seems super lenient (basically both filter/query can be null), all null-values map to some defaults. If we restrict this on the JAVA-API, we should maybe be explicit about it in this case, saying which nested queries to use to mimic the JSON behaviour when leaving the clauses there. I'm on the fence about adding null-checks in constructors after reading the original parser code, maybe we should also allow it in the Java-API but internally always set the defaults to avoid null-checks all over the place. Sorry for this back & forth.
alright that's what I thought too, sounds good
Also think this is right here. In JSON you get null here for ``` { "query": { "filtered": { "query": { }, "filter": { "range": { "created": { "gte": "now - 1d / d" }} } } } } ``` Which means the user specified something as query but its the empty set. In this case its not clear what to filter, but also not save to return any of Match_All/No_Docs query, since that would mean something different depending on any (potential) enclosing query, e.g. when this is used in "must" or in "must_not".
+1 for doing this in the old 2-arg constructor and forwarding to it here using the match_all default.
+1 on introducing a constant somewhere and reuse it in the parser as well.
I think we have to guard against null values coming from the java api in all the constructors
remove the set boost
remove the setBoost
you can remove randomization of boost and queryName
but the parser will never set null with out refactoring right? So we could have defaults in the builder and convert set null to set default value like we did in other place? Just looking for a way to stay away from null values
I am wondering whether we should just use null here, which would become EmptyQueryBuilder anyway in the builder
Boost will be set in BaseQueryTestCase. No harm here, but doppelt gemoppelt (I want to try to gradually introduce this expression into the english vocabulary like Kindergarten and Schadenfreude) ;-)
I was having the same question in another query I was looking at. I think we made sure in constructors that the two builders are never null, but I was also wondering if if doesn't hurt having extra checks, in case e.g. some later change makes it possible to sneak in null query builders somehow. On the other hand, test should cover this. Sorry, undecided here, that's just what I had in mind in similar situation.
:) good catch
right I think tests are made for that. I wouldn't want to have checks for something that we handle earlier and that should never happen here, that is my personal opinion though :)
that that :)
this can be null only if the query element wasn't there at all I think, if it was there but empty it would become an EmptyQueryBuilder during `parseContext#parseInnerQueryBuilder`. I will leave it to @cbuescher though to decide whether what we do here is correct :) I am not sure
maybe we could pass in null to the builder since the default is already handled there, that way the odd generateDefaultQuery could become private too :)
a constant could cause problems though if the constant gets misused (e.g. changing its boost etc.), method is ok sorry
I think filter and query can never be null here? not sure whether we should validate this here.
right that makes sense when we test hashcode, sorry I missed that
shall we use MatchAllQueryBuilder as a default value in general, outside of this constructor? If not set, it will be match all? When set, we check that it's not set to null in setter.
I just realize that there might be a bug in the existing code already. We only seem to add the query to the named queries if it's a BooleanQuery, the other cases return early. Not sure, but we might want to change that.
don't worry too much about this method, we wil get rid of it soon. I am preparing the field for this change :)
you can use the util method printBoostAndQueryName here now (once you rebased)
this will need to become protected doToQuery as soon as you rebase
once you rebase you need to implement doHashCode instead, and can take out boost and queryName here, they are already taken care of in the base class,
once you rebase you can take out boost and queryName here, they are already taken care of in the base class
rebase and these getters wont be needed anymore ;)
you can now use the default coming from AbstractQueryBuilder instead
no worries as soon as you rebase you won't need to take care of boost and _name, it's done automatically.
not sure about this name change to be honest. This query is the one that we use for testing, we start with it, we convert it to string, we reparse it and we compare the result with newQuery, which is actually the expected query.
This needs to be another method (`parseInnerFilterToQueryBuilder`) which replaces `parseInnerFilter` and also takes care of switching the interal `isFilter` flag.
btw. I am currently thinking about how we can at least make the inner QueryBuilders we use always non-null, but will need to do this in separate PR and ask Adrien about it.
I think we need additional null checks here. Before the refactoring we only needed to check lucene queries in parse() method for null. Now also their `toQuery()` can potentially return `null` (e.g. if this FilteredQueryBuilder would be nested in itself). Same goes for the filter.
Can't find `validate()` here. I think we went for always having empty implementation in builder class, even if there's nothing to validate.
it wouldn't be empty here at this point, it needs to validate the inner query and filter, see #11889
take out boost and queryname once you rebased
take out boost and queryname once you rebased
this is a leftover I think
I see how this gives context to the error message, wondering if there is an easy way to be more specific in case there is more than query with the same name in the complete query (like a `match` in both must and should clause of BooleanQueryBuilder). I guess that's not possible right now, but would be great if inner queries knew about their parents, then we could output the full path here.
you can replace with //norelease so we don't forget but at least you can get this in while we fix this problem in master.
we shouldn't make this change here in the branch, but rather in master. Let's get that one in first in master and then this one in our branch.
didn't we say that we are going to use constants from QueryParsers? maybe I am missing something though
good! as for when we merge the branch...well we will do it when it's ready, most likely not before 2.0 but we don't know yet. One other thing about backporting fixes is that the branch is already big enough with the changes that we are making. If we can isolate non related fixes we simplify things a lot and clarify what happened when for the future.
this way you always a single flag. I think I would do something similar to what we do here: https://github.com/elastic/elasticsearch/blob/feature/query-refactoring/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java#L65
I think you can answer for yourself at least for this one: would you be comfortable with 2.0 released with still this bug in it? Mayeb this fix should even be backported to 1.x, it's a bug and should be treated as such
oh boy I didn't spot this :) we should definitely backport this to master
This was a bug previously? Seems so, just trying to understand.
randomInt cannot be within the loop, otherwise it changes at every iteration...
ok didn't know that. yet another bug fixed in master then it seems
same as I mentioned in the prefix query PR, wa may want to backport this fix for the java api to master.
this should stay as-is I think
this should stay as-is I think
I think the regexp should be an object here. We should be consistent with what we did in term query and similar. The value is an object and can either be a number, string or BytesRef. We use internally bytesref for string when parsing through the parser, but java api users can set string and we take care of the conversion.
by passing in "", null I think? you can skip randomizing with such a simple query if you want though, just add the case where you have both errors at the same time.
well if it was a string all the way I wouldn't change it maybe, but given that the parser parses an object, I think this was a bug in the java api previously. We should rather have an Object here than rcompareed to moving to Strings everywhere
before the refactoring the parser holds most of the code. builders are used only from the java api and what they do is they allow you to set stuff which gets then serialized in json format anyway for the parser to parse it on each data node. if the parser previously supported objects, but the builder only strings, that is a bug in the java api to me. What rules is the parser, and we should do what the parser supported in our new builder, which will be also used for communication between the nodes when the refactoring is done, meaning that if we keep string the bug will not only be in the java api but in es core in general.
> this is only used for internal serialization and is not exposed in the api. so I think we are good? You are right, metadata is only exposed through cluster state that uses custom serialization which omits this.
of course how could I forget about this, I pushed it 30 mins ago :)
ah ok I think I get it, we wouldn't get here before cause what is now the empty_query was null and you could never call toQuery against it. Now we have the empty query that returns null and we have to move some null checks to the different toQuery methods.
here too, not sure why we have to return null now, also we can use query.clauses() to count the added filters
yea I meant leave as-is and open new issue :)
ok lets get this PR in and address this problem separately, I think we overlooked this when migrating the bool query. This check should be moved to the toQuery method. We should start looking into dividing the context into a query parse context and a to query context I think
other question, sorry! but I still find this confusing... previously we had null here for two situations: 1) empty filter/query 2) queries that after getting parsed would become a null query (their parse method returns null) the first case falls now under the EmptyQueryBuilder case, which we handle here. But there can still be queries whose toQuery returns null, which need to be ignored, but that we know only in the toQuery, but we still set the minimumShouldMatch only based on case 1). I think setting the minimumShouldMatch like should happen later in the toQuery, which kinda makes sense, it needs to be done only on the resulting lucene query, not against the query builder. So we can potentially add empty query here to the clauses as long as we move this check for null should clauses only in the toQuery method, if that makes sense
ah I get the difference now. then maybe say "if either inner query buider returns null" null-query makes me think of the empty-query, maybe it's me
if it doesn't have clauses we don't set the boost and ignore the queryName. I think it would be a bit more readable if we had two if branches, and the set boost and named query handling after the if which kicks in in both cases.
Besides jokes I see your point on NoOp naming, let's leave empty then, it doesn't convince me 100% but I cannot come up with a better name
IgnoreMeQueryBuilder ? :D
cool thanks for the explanation!
is empty the same as {} ? I never know if start object and end object should be here or handled in the caller method.
if we do what I suggested above the return value shouldn't be needed.
I see, bummer, I guess we can't do much about it ;)
I think saying that it should not be allowed in the query DSL is a bit misleading, cause it is allowed and we parse it properly. I know what you mean though and why you wrote that, I need yet to come up with a better explanation for this...
right I missed that good catch
I am still on the fence. I don't like null values, but I think there is a little difference between the empty query and something that was not set. That is why I don't think the default should be empty (although that might seem very convenient), which is something that requires users to put something (although incomplete) in the json. But with null values null checks are required, and relying on validate seems to be ok but not 100% correct because the java api exposes a lot of public methods that potentially throw NPE if they had no null checks, when executed against a non validated query. This last problem is not huge but it bugs me. In the end I think your last proposal around making mandatory queries required and have null checks in constructor is our best bet. We really want to remove null checks and make sure that those queries are always non null and that is the only way. But it's a breaking change for the java api, which reminds me that we might have already broken bw comp with similar changes without realising it :)
it is good to check for null values, but the default is null, which means that if remove null checks (e.g. in doXContent) we might have NPEs coming from java api at least. I am on the fence on what should be the right behaviour, here too we validate that the field is present in the json, but we don't validate that the field was set through java api. We should unify our behaviour through the different queries and do the same in both parser and builder, as much as possible.
this could lead to NPE if from the java api no set call is performed
good thinking, I think it's the right place
"just created files" <- what do you mean exactly? if one waits 2h they will be able to read it? if not I would just go with "the permissions on the store don't allow reading"
> there is no way to distinguish between file doesn't exist and file exists but you don't have permissions to read it on this level Argh, that's bad... Thanks for improving it Igor!
I think we can skip the unwrapCorruption and always use the IOException which captures the reason as well.
can we use failShard(reason, true, e) here? will be clear what it does. Also change the java docs please.
out of memory (source: [" + source + "])
engine -> underlying store.
marks the shard store
can we just make it an overload of failShard with a boolean, like in the engine? . I don't know what soft means :) Also if we do that, let's change failShard(string reason, throwable e) to just call failShard(reason, true, e) ...
Maybe it's worth being explicit that this needs to map variable names to paths, given that it is one of the most important params of this agg
Maybe we can make it slightly more efficient by storing eg. a "totalNumKeys" parameter on the aggregator which is equal to filters.length when showOtherBucket is false and filters.length+1 otherwise. Otherwise if you do not need the other bucket (default), you would waste 1/filters.length of the bucket ord space
Beware that bucketOrd has the following definition: ``` java final long bucketOrd(long owningBucketOrdinal, int filterOrd) { return owningBucketOrdinal * filters.length + filterOrd; } ``` So we need to somehow multiply by `filters.length + 1` instead of `filters.length` when we compute the other bucket otherwise there will be several "logical" buckets reusing the same "physical" bucket
why is serailization likely to fail? it might, it might not...
I think this can be better put in the findmaster() logic. If the "elected" master is not of a good version we should log a warning and return null (no master found)
It turns out it's hard(ish) to remove ElectMasterService from guice.. not doing.
I wonder how long this would work ;)
Here you call super method while in toQuery/doToQuery, doEquals, doWrite etc... superclass calls abstract method. Any reason why this is different here? Might be more consistent to follow one pattern and have the superclass always call concrete implementation? Just an idea really.
Maybe move this up before the if? Shouldn't make a difference as long as addNamedQuery doesn't copy, but just to show that chosenQuery is "complete" when adding it to the named queries.
This might be redundant now.
We could avoid the anonymous class by just passing `metaData` to `PriorityComparator` and doing the work there. It seems like we used the anonymous class to avoid a constructor, but I recognize this is largely style.
I think we should try to do it in an atomic way? ``` java if (queryFetchResults.compareAndSet(shardIndex, null, result) == false) { throw new Exception(); } ```
we can maybe use similar technique as we do in `QueryParsingException` and also report the location
I think we should use `writeAtomic` everywhere just to reduce the complexity.
We should change this to smile
Can this be "metadata"? We don't hyphenate anywhere else in the code.
Same here, maybe "index-metadata"
Oh I see it below, makes sense, `wrapper` just sounds like a noun instead of a flag
snpashot -> snapshot
I like that as well.
different change - but I think we should have the 0.90.0 as Version.minimumDataCompatibilityVersion ..
nit: naming issue...
same thing - I think this can go away too
can we get rid of this exception? especially now that we have the fancy formatting.
yes! thank you.
wondering if each index in the list should be added separately... such that the header will hold a list of indices and not a list of a single comma-delimited string of a indices
this is great
If we add a ResourceNotAvailableException, I think it will be a better base class here. Shard is there, but not ready to make the operation.
Java docs says ResourceNotFoundException
we use this super(...); addIndex(); pattern often. I wonder if it makes sense to add an ElasticsearchException ctor which takes an index as an argument and does this for you. Same goes for shardIds..
we miss catching the replacement of IndexMissingHere.. either IndexNotFoundException or ResourceNotFoundException (making ShardNotFoundException redundant)
As this is only in the test I think we are save assuming that randomIntBetween can only return values between 0 and 3, so leaving the largest value as default should be ok. Current implementation is fine as well of course.
I'd go for either check in the constructor or here.
this needs to stay because the method can be called from any other class, it's a public static method....thus validate might not be called at all before calling this method.
Does this work? Above PROTOTYPE is initialized with null which will result in an exception being thrown here.
again, this is the reasoning: if we check for existence of a field in the parser, it means that the only way it can be null in the builder is when it comes in through java api. In that case we might want to fail fast and throw error in the constructor/setter already rather than in validate. If non validated values might come in through the parser as well then validate is the way to go. In this case it makes to do as Christoph suggested. In term query builder I think it still makes sense what we do (again, you can test it to see the differences), same for common terms query.
I think I mentioned before that I am looking into removing the requirement for the createExpectedQuery method and avoid duplication of code that is indeed annoying. That said, till then, either we test or don't test. I prefer the former. I don't think no tests is an option, which is what the method does now.
as far as I can see in the term query the parser doesn't check if the value is there or not, hence we check it in validate for both java api and rest layer. You are free to test it and verify that what I'm saying is correct. These things are subtle and we are changing them a bit as we go and we learn new queries, so it is always possible that we used a slightly different approach in queries that we refactored a month ago....
I would rather test this by calling the newFilter static method. In fact the check that we left there instead of relying on validate is there ony to protect other code paths from the outside (newFilter is public)
which other queries do you mean? You mean check against this specific field in similar queries or just in general. I think the question is "when can this happen?". If stuff can happen in both java api and rest layer, validate is the way to go. If we already perform some kind of validation that makes sense in the parser, then having null here can happen only from the java api and we should maybe try and fail straight-away.
thanks for writing that test, that helps a lot
wonder if we can move this check to validate, I don't like the fact that it happens only when you call toQuery
before you would randomly return unmapped field names, now you never do. randomIntBetween(0, 4) was ok, case 4 was ok too, you can just throw UnsupportedOperationException from the default or something.
while you are at it, s/nalyzer/analyzer ? :)
Not sure, but maybe we can add null-check for innerQuery here like in other nested query builders (e.g. BoostingQueryBuilder)? That would avoid potential NPEs or null checks in doXContent later.
Same here, I think List is already initialized in L51.
I think IdsQueryBuilder() without specifying any id returns `Queries.newMatchNoDocsQuery()` lucene query. EmptyQueryBuilder returns `null` when `toQuery()` is called, so this won't work.
I think this public constant is a bit scary, as it's an object and can be modified (think queryName or boost). I think in this case I would not add a constant for the default value, create a new query all the time.
I think I would make a breaking change here. Let's drop support for the string value in the builder and add it to the breaking changes. The parser still supports `none` and `all` but the builder only accepts a query. Then the method below needs to pretty much be moved to the parser.
yea I think so too
we should add ClusterService and IndexNameExpressionResolver to IndexQueryParseService so they get injected. Then this method could pretty much be moved to INdexQueryParseService like this: ``` public boolean matchesIndices(String... indices) { final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(clusterService.state(), IndicesOptions.lenientExpandOpen(), indices); for (String index : concreteIndices) { if (Regex.simpleMatch(index, this.index.name())) { return true; } } return false; } ``` QueryShardContext would need to expose the same methd and delegate the IndexQueryParseService
we could test here the case where we have multiple errors as we do in other tests.
you don't need to create a new array. the argument is a vararg
I think we have to remove this given that we just decided to leave the match_none query for internal use only.
I thought this query would be useful to users as well but looks like we prefer to keep it internal only. In that case we can remove this parser class, it will be similar to EmptyQueryBuilder. A query that can be streamed but has no corresponding parser. We register it via `namedWriteableRegistry.registerPrototype(QueryBuilder.class, MatchNoneQueryBuilder.PROTOTYPE);` in `IndicesQueriesRegistry`, like we do with `EmptyQueryBuilder`.
given that we prefer to keep the match_none internal and we are removing its parser, we can drop this test too.
once #12937 is in we can do the following here: ``` String[] indices; if (randomBoolean()) { indices = new String[]{getIndex().getName()}; } else { indices = generateRandomStringArray(5, 10, false, false); } IndicesQueryBuilder query = new IndicesQueryBuilder(RandomQueryBuilder.createQuery(random()), indices); ```
once #12937 is in we can do the following here: ``` QueryBuilder<?> finalQuery; if (queryBuilder.indices().length == 1 && getIndex().getName().equals(queryBuilder.indices()[0])) { finalQuery = queryBuilder.innerQuery(); } else { finalQuery = queryBuilder.noMatchQuery(); } Query finalLuceneQuery = finalQuery.toQuery(context); if (finalLuceneQuery != null) { finalLuceneQuery.setBoost(queryBuilder.boost()); } assertEquals(query, finalLuceneQuery); ```
Is this because of https://github.com/elastic/elasticsearch/issues/12145? Just curious.
I say goodbye you say hello ;-)
can we open an issue for this so we don't forget and link it here? I know that we will remember to get back to these queries but I also want to remember to re-enable this test ;) also the norelease look weird within the bugUrl, I would take it out
I don't think we need this here. very similar to discussion here: https://github.com/elastic/elasticsearch/pull/12037#discussion_r36962184 . it is needed because you use lucene equals to compare queries and because the boost from the main query overrides the inner boosts.
To be clear, the difference between indices query and wrapper query is that the latter doesn't support boost nor query name, while the former does. Saying that the query doesn't support boost and query name is just a way to work around an issue and skip some test checks that should work instead.
mmm this is not 100% correct. although it makes little sense, you can still set boost and queryname on the result, which would end up modifying the prototype and all subsequent match_none queries read.
well we don't need to extend this anywhere, plus these changes don't have anything to do with this PR at this point, would prefer to leave them out.
ah ok I see
I think we should backport this fix to master, similar to what we did with fuzzy and regexp query
This can't happen, and if for some reason after the switch fielname/value are unassigned the returned query will throw exceptions at some point (validation, doXContent, toQuery), so given this is a test method I'd remove it.
oh wait we already take care of that in the base class, forgot about that, I think it's fine then
I don't think removing getClass is a good move, that means that a span term query and a term query with same field and same value would be equal
Got it, sorry didn't fully realize how value is initialized in any case before. So scratch this remark.
this is same as what we do in term query. We randomly choose a value depending on the type. We might choose the mapped field for that value type, or just pick an unmapped field for it.
I would leave it as-is, it needs to extend BaseQueryTestCase
It might be possible, but I would try to avoid it in this case. I would go for either using both BaseTerm classes or none.
do we want to open an issue around this bug that we are solving here around the java api? We could probably backport the fix upstream too, should be easy.
Should this be `rewrite` field instead of `fieldName` (mentioned twice)
if PrefixQueryBuilder won't extend BaseTermQueryBuilder anymore I think we can't extend BaseTermQueryTestCase either here
you don't need do handle queryName yourself anymore, nor boost
I think the whole method could just be: ``` return randomBoolean ? MetaData.ALL : randomFrom(currentTypes); ```
if we add a null check to the String constructor we can remote this check here given that the parser already looks for the existence of the field too.
I would add a null check here for the type given that it's only used from the java api, we can fail fast rather than doing it in validate
this way we only test against non existing types, I think it would be worth to test against types that we have. Have a lok at what we do in IdsQueryBuilderTest#doCreateTestQueryBuilder for instance
let's see if we get objections on that PR or not
I am afraid for consistency reasons we should for now go for path(..) and drop the set prefix on these new setters. We will fix them altogether at a later stage.
makes sense to me as well now, sorry for the noise.
this is NPE when termsLookup is null. I am tempted to just expose a termsLookup() getter rather than all the specific getters. Remember that the specific setters are there only for bw comp, but the getters were not there before, so we don't necessarily have to add them.
it doesn't make sense to allow that, so I would just make the change here and consider it an improvement. We are more fixing a bug than breaking things I think.
no, the existing TermsLookup shall remain, but we have to add serialization etc. to it.
I am not sure what we have yet to decide here. What Christoph is suggesting makes sense, let's update the PR accordingly
the idea would be to validate that this doesn't happen to prevent mistakes, and always serialize everything we have.
I would probably throw an exception instead of accepting null here.
I double checked, injection is good as-is, thanks.
you are perfectly right Christoph, let's merge the two and keep the existing class.
ok keep it then. I am not sure though what needs to be optional, if the client here, or the service in the parser service. I thought the former, not the latter.
not if we test less. Right now we try to test more but most of the test is not meaningful, cause we call the same methods in both sides.
I think this could be final, as I can't find any setter for it
You can still override only one method when you extend a class. Anyways, keep the service, it's fine
> I say we refactor TermsLookup into TermsLookupBuilder when we refactor TermsLookupQueryBuilder. not sure what this means. when is the queryParseContext used in TermsLookup? I wouldn't be surprised if it's never used....
Ok, what is your proposal then? The current test is not ok I think.
fine with me
what's the difference between TermsLookup and TermsLookupBuilder? I am thinking if we can just make TermsLookup Writeable etc. and avoid adding a new class that holds the same info.
does it really make sense to accept null here as a value? I think I would remove the if
as I said above, not sure I see the added value of this class, I think I would drop it for now
I think I prefer the Client directly here. This service makes you think that there is no client needed, but it just hides that client away :) I think I prefer to have the client here but keep it contained and add the fetch method directly to IndexQueryParsreService, at least for now
Don't want to digress, but this would be a perfect usecase for Mockito. Just saying, as long as this works. Might be something to look into later.
I think it would be great to have different values for same fieldname and type. Working on the SpanQuery builders, I introduced BaseQueryTestCase#randomValueForField(String fieldName) which at least works for String, Boolean, Int and Double fieldname, otherwise returns String. Might be useful to use this here as well, potentially with minor modifications.
No, you are right, I didn't realize the need for api users before going through the whole changes.
It might get written out correctly via toXContent, but I doubt it works when only going through the java api.
Sorry for the noise, realized all constructors are delegated to the `TermsQueryBuilder(String fieldName, Iterable values)` constructor, so all good.
Thanks for pointing this out, missed that all other constructors delegate here, my mistake.
Sorry, I wasn't refering to the fieldname but to an additional validation that that not both variants specifying the terms (of termsLookup / values) are used at the same time.
As mentioned above, I'd opt for setting the fully constructed lookUp oject here in case it is valid.
Maybe we can add a check that only either termsLookup or values are used. Otherwise we won't be even able to serialize this object correctly since the serialization is either/or.
ah I see what you mean I thought the set to null could happen only with the java api. I don't know either way I find this weird. Maybe Christoph can come up with some better idea.
can we have the handleTermsLookup method directly here so we don't have to spread the TermsLookupFetchService dependency around. No need or a public getter then
Trying to catch up on your discussion here, to me it seems like the TermsLookupQueryBuilder just tries to add serialization and toXContent (which kind of makes it a builder I guess) to the existing TermsLookup class. The Later just seems to be used in the TermsQuery anyway, so merging (deleting one) the two should be no problem. Please correct me if I am missing something.
Two concerns here: If we convert Strings to BytesRef here, shouldn't we also do so in the (..., String... values) constructor? Also, since this seems a convenience method, I'd prefer converting the content of the Iterable to be converted to some array and then use the (..., Object...) constructor, as there are already too many ways of constructing this query builder.
Do we need this anywhere else or only in this class? In the later case I think I'd vote for removing in favour of simple null-checks in the appropriate places.
good catch! that means we are not properly testing this case either given that we didn't catch it.
ok let me know if I have to look into that or you can manage.
It seems the general problem with the existing setters is that they can lead to partially constructed lookup objects. I'm leaning towards disallowing setting null in all the lookup setters here alltogether. The user currently using the java api can avoid the setters by simply not calling them. In the parser we can avoid doing this for the optional index / routing by first constructing the TemsLookup (Builder whatever) and then setting this via `termsLookup(...)` setter.
I think we shouldn't have this special case for Iterable, as I mentioned above I think it would be good if that constructor delegated to the Objects... constructor and do the potential String->BytesRef conversion there.
no clone, please. I still think it would be slightly better if we mutate the object in the caller. I guess we need some second opinion...
it doesn't break anything given that this class was only used internally, it was not part of the java api. Only this change makes it part of the java api.
either way please create a static array containing these fields on top close to where we create `mappedFieldnames` so this selection is not buried in this method and we see that we might have to change it if/when we add new fields.
we don't need a context in this test, these two lines can go away
I think this declaration/initialization can be moved to inside the if
can you add a small introduction about what this is? e.g. Parser for terms query and terms lookup
we discussed this and we decided to leave it for ease of use, just keeping only the most common (required) arguments, index, type, id and path
looking deeper, I see that we set a non null TermsLookup object only when we have it in query, which causes a validation error when values are set too. We should keep it that way then, this is as good as it gets.
I meant that other way around, not in the else, set termsLookup only if values == null
I wouldn't. if you pre-generate values would stay the same throughout that same run.
how about introducing a writeable object that holds the lookup info? then to determine if it's a lookup or not would be checking if that object is null or not. we could have a new setter for that new object, and also keep the existing setters that create a new instance if null (for bw comp).
I think that we can do without this flag, it could be a util method that returns a boolean depending on whether values are set or lookup details are.
I think you should use QueryShardContext#isFilter but that is something that @cbuescher is working on, he should be able to give you some more details on that
I am not sure how this can work, is the flag ever set? anyways I think we should remove this flag and change this logic as stated above
we shouldn't need this here in parse phase
do we decide what to do based on fields that we haven't read yet? I think this conditional will always be false. Which also means that we are not testing this, which we should.
I think we will need to serialize this boolean flag so we can fix the problem we right now have in the readFrom
I think we should leave these two above for bw comp
I think this is ok, the isFilter() flag should be set when parent queries that are already using toQuery() produce some inner query in a filter context.
ok, but we don't need to call this constructor in that case though and pass in `null`. That way we just open the door for null values. I would try and be more strict here.
I think we may want to use an optional injection and inject through setter instead, similar to what we do in `ScriptService#setClient`
maybe we can avoid exposing the Client and instead expose only one new method with the needed functionality in INdexQueryParseService and simply proxy it in the context? That would prevent us from spreading around the big client dependency
we should move to have an inner terms lookup builder. BTW You don't necessarily need to have a flag in your class, you just have to serialize what the method returns and read based on that on the other side. If you read correctly the method will return the right result. The flag would anyway depend on other instance members as far as I can see.
Can we get back to this once we need this complexity and keep it as simple as possible for now please? Can we hardcode the OBJECT_FIELD_NAME exclusion and be done with it? queries also have access to individual field names if they need that.
not really it was without type before...
I thought it would, plus I don't get why we do the same thing in different ways depending on the query. Maybe it's me though.
I understand the that the way the we test the created lucene query at the moment is not affected, however since we already have the test for `expected list of terms` hat checks the terms at least for string fields, I think it would be really good to have this tests. Changing this shouldn't be too hard, and also the random query we create here would feel more "natural". Please correct me if this change is hard to do, otherwise it would be really great to have random values here.
this cast causes a warning. We can instead change the return type and argument of `convertToStringListIfBytesRefList` to `List<Object>` and `Iterable<Object>`
my bad from previous review, as I said above, change to `List<Object>` ad `Iterable<Object>`
feels like it would be more correct to set termsLookup only here in the `else`, it should always be null otherwise.
I think I would remove this constructor, seems like it's only used in tests and we can replace it with empty constructors + setters
how much work would be to "decode" the values and expand the test? I am wondering if it's worth doing or not.
not sure this suppress warnings is needed
I think we can remove getClass() from here and make this class final to prevent any problem around that
Really like the unification of the `getRandomValueForFieldName()` helper method above. Here I think it would really improve readability if the method wouldn't take `exclude` arguments but rather all the fieldnames to include in the random choice. More like `randomFrom()` really. I had trouble understanding the tests below, if they read `getRandomFieldNameAndValue(OBJECT_FIELD_NAME)` I first thought I get an Object value back. So even if this makes the list of arguments in the tests longe, I think this should be the other way round.
Same here as above, since both methods are related and read similar.
maybe we could have a `default` here which could make this switch a bit more readable rather than assigning value before the switch in any case.
Wonder if we need this flexibility. Do we ever provide multiple fields to exclude? Also do we ever exclude any field other than OBJECT_FIELD_NAME ? Also do we ever call this method without excluding OBJECT_FIELD_NAME? I think we can simplify this quite a bit.
it's a minor thing but why would you assign a variable multiple times when it's not needed? default is a better fit here, it improves readability as well.
you don't need to expose the client to everybody. You can contain which objects depend on the client simply having a handleTermsLookup method or something along those lines in IndexQueryParseService. That one will use the client but that way the client will not be exposed through public getter.
I don't see this change implemented here.
this curly bracket should be on the previous line
all of these methods should create an instance of termsLookup if null I think , and we shouldn't initialize termsLookupBuilder at line 70.
I thought we said we would move this method to IndexQueryParseService so we can avoid exposing the Client.
sorry I think I am mistaken here, looking deeper, I think we might need to remove execution from the builder in master instead given that we do nothing with it. Will do that.
note that because we moved the initialization of values, we might end up passing in null even in case we are not doing a terms lookup, in case the query is invalid. One more reason to add this bit to the validate method.
the way we check the resulting query here reminds of the previous createExpectedQuery, as we still leverage lucene's equals and because of that we run into issues. Also, it makes little sense to test the result by calling the same method that we call in prod code (handleTermsQuery). I would keep these checks more lightweight then.
we are using a mocked service, we can retrieve the generated values through the mocked service I guess. Even pre-generate random values when the service is created. I think that would be already better than returning index, type etc.
Can this be package private? I think its not needed except by bootstrap and bootstrap for testing.
We can use the SuppressForbidden annotation on top of the class to fix this.
maybe in a followup we can think about removing these -1s... see what platforms fail, and better fine-grain the stuff (e.g. add assumption for WINDOWS, IBM jdk, whatever it might be). Then we know when and where stats are available.
Also I don't know how easy it is, but if we pass the aggregation context in the init() method, we might want to remove this parameter from the create() method
can we have a helper method that gets the method and makes it accessible? and then have `getFreePhysicalMemorySize = getMethod("getFreePhysicalMemorySize ")`. This will simplify the code, and also fix the bug here where if we fail one of the following code fails, we will assign the same method to a different method handler
just my personal preference, I don't like instanceof checks that's it. you are free to leave them if you prefer them ;)
missing generics type same as above
missing generics type same as above
missing generics type same as above
public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> implements MultiTermQueryBuilder<FuzzyQueryBuilder> {
we set the rewrite method twice it seems? probably a bug in the original parser
we can use constants here from QueryParsers
here too we do the same twice
ok lets leave it, it doesn't seem like a problem for now
look at BoostingQueryBuilder for instance. I explained in other PR when it makes sense to have checks in setter/constructors rather than validate, this is something that we are adapting as we go. I will say it again: check in setters/constructors are for things that can happen only through java api. depending on the parser, we might check for existence of elements in the parsers, meaning that null values will never come through from the parser to the builder. In that case null checks are only for the java api code and can be moved earlier on. If the check is valid for both rest layer (parser) and java api, then we should have the check in the validate method.
given that the parser will never pass in null here, I think we could add a null check here as we did in other queries, so that if somebod using the java api sets null will get back error immediately. then we could remove the check from validate
You can't grow an Arrays.asList instance, but you can still use `set(int index, T value)`
can we use `Arrays.asList` here - it's also immutable by default
s/payload is/payloads are
doesn't matter that much, I thought -1 didn't make sense :)
s/payload is/payloads are
oh damned it's BWC I guess...
uh good! :)
nit pick - we can skip the `== true` I think :)
can we please only have one value for this. there is no need to have more than one IMO
unrelated but can we add an assertion at the bottom of this method to ensure `waitForCounter <= waitFor`
yes, we can't do too much about this, so it is better be defensive here.
is it really possible that we receive join requests if `ZenDiscover#doStart()` hasn't been completed yet? this feels odd to me
Super-minor, but missing a space between `if` and `(` here.
same thing in the second arg check above
I would say that we should have `-v` as verbose as it stands now and just support `--version` for the version parameter and not have a short form for it (if thats possible). If not, I think this is a good solution. `-v` is fairly universal as a flag for verbose.
For the record, I was a bit confused at that method, which I first read as if it was Comparable.compareTo while it's not
not sure what we should do in master, seems like things are broken either way. This is fine in our branch cause that's how we move forward for now, till we have found a better solution.
yea I know we already do that, and I don't like it very much... I would try not to have the utility class calling test classes if possible.
cool then we can take those out so it is clearer what we have and don't have to do :)
one too many ;
would it make sense to move it to RandomQueryBuilder then? I don't like that class to get acces to test classes...it should rather be a utility that the test classes call...one way only.
that is a good idea, but maybe add the switch later on then? it really makes no sense right now :)
Simon would say replace with `== false` ;)
I would modify the error message here, one cannot use the terms query instead against multiple fields, you should rather combine multiple term queries in a bool query I think
+1 on this @martijnvg
same as in the regex PR, I wonder if we should have parser.text() or parser.textOrNull()
where is this method called? Looking into why we have so many different concreteIndices methods here :)
maybe it's me but this change seems fishy... we do much more than before now here, also not using indices options coming from the request... but first of all are we sure we want to support writing into index names containing date math expressions? in my mind it was more about reading, search api etc.
this //TODO will have to stay right? maybe we want to open an issue instead to keep track of this? also s/data/date
Can you call it something like "defaultTimeout" instead? I was a bit confused this number might come from a user while it's only a default that we fetch from the settings.
oh nevermind :)
OK fair enough.
can we just fold `SearchSettings` into `SearchService` we already have enough classes and abstractions I don't think we need another one.
I think it would be clearer to have the negation with each check directly (then parenthesis don't matter)? ``` if (load.indexUUID.equals(indexUUID) == false && IndexMetaData.INDEX_UUID_NA_VALUE.equals(load.indexUUID) == false) ```
I'd just annotate it with `@Nullable`
Would it be beneficial here to return an empty string instead of null? If not, maybe just annotate this with `@Nullable`
-- reinitialize shard
I wonder if we should make this a hard exception, potentially in the AllocationId constructor. When we start using this ID, a null value will create havoc in other places and will be hard to debug..
mind fuck :)
You can change the equals template in intellij to use Objects.equals
optimization nit, but maybe we can just have one list, and reorder to push active one to the start, we do something similar in primaryFirst. This will mean we don't have to create 3 lists, just one
can me extract this into a method, it is used in 3 places
no need to shuffle here, since it is always a single element list
I wouldn't rely on the order the replicas exist in for this behavior, note it works in the primary case cause there can be only one :)
nit: shard routing already has [] in it's toString
Maybe include the exception in `UnassignedInfo` instead of a detailed message? So that the entire exception is included once the cluster state is serialized, like we do on other places to now and it is useful too.
I think this should be a ParseException instead of an IllegalArgumentException. This would be in line with what we do elsewhere in the codebase (such as in the mapper parsers where any left over parameters after parsing is done are throw in a MapperParsingException).
left over? it is not used
why did you add it? I mean, it is very internal...., it will mean cluster state API will become so much more noisy
this change requires going over all the places we use `equals` in ShardRouting....
should the break be outside of the if/else? we found match
we only need an array here - we don't do anything with the version and we copy it into one anyway: https://github.com/elastic/elasticsearch/pull/12335/files#diff-ad5388a03f5e080b452190f4eb47f33aR244 Might as well use an arraylist from the beginning.
Miss the sorting based on the version. Sorry for the noise..
can we add an assertion going out that going out of the primary shard allocator we don't have any primary shards in the unassigned list, unless we expect them to be there? (primaryAllocatedPostApi is false or restoreSource != null)
OK. > On 20 Jul 2015, at 14:01, Shay Banon notifications@github.com wrote: > > In core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java: > > > ## > > - AsyncShardFetch<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> fetch = asyncFetchStarted.get(shard.shardId()); > > - if (fetch == null) { > > - fetch = new InternalAsyncFetch<>(logger, "shard_started", shard.shardId(), startedAction); > > - asyncFetchStarted.put(shard.shardId(), fetch); > > - } > > - AsyncShardFetch.FetchResult<TransportNodesListGatewayStartedShards.NodeGatewayStartedShards> shardState = fetch.fetchData(nodes, metaData, allocation.getIgnoreNodes(shard.shardId())); > > - if (shardState.hasData() == false) { > > - logger.trace("{}: ignoring allocation, still fetching shard started state", shard); > > - unassignedIterator.remove(); > > - routingNodes.ignoredUnassigned().add(shard); > > - continue; > > - } > > - shardState.processAllocation(allocation); > > - changed |= primaryShardAllocator.allocateUnassigned(allocation); > > - changed |= replicaShardAllocator.allocateUnassigned(allocation); > > I will do the assert when I remove the primaryAllocated flag in a different change > >  > Reply to this email directly or view it on GitHub.
great to see this behaviour move to UnassignedShards!
can we add a note here why this is optional? the validate request suggests otherwise...
Wow, that's a big difference! Do you know whether it is lossy compression or not? If not then indeed compression seems to make a lot of sense. :-)
Elasticsearch guarantees that two nodes that have the same major version can talk to each other, so I'm wondering if HDRHistogram has a similar warranty so that upgrading HDRHistogram in a minor Elasticsearch release would not break Elasticsearch's wire protocol.
it can't happen in practice, but I would like better something that would do if (tdigest) {} else if (hdr) {} else { throw new AssertionError(); }
Would EngineSearcherFactory be a better place to do the wrapping? Otherwise I'm concerned we might not call incRef() and decRef() on the same index readers.
maybe add the set of wrappers to the message so taht it would be easier to debug
I'm a bit concerned that this will be the case almost all the time except when we wrap. Can we avoid this if statement and just recreate an IndexSearcher all the time? (the else branch)
I think if we get in that other PR I just reviewd we can reuse here the new method that you introduced there? :)
I know what you mean about creating the json, plain strings would work I guess... we have this same problem in other queries (e.g. term query)...something we have to keep in mind for later.
we should start thinking about testing the parsing phase for things that we never output from doXContent.... :)
This is much cleaner and nicer to read!
good catch ... I hope a test failed somewhere :)
we don't want it to be retried... that was the cause of the failure
can inline this method? it's just a one liner and the name is kind of funky - node 1 and node 3 are baked into it.
can we use org.elasticsearch.indices.recovery.RecoverySource.Actions#START_RECOVERY ? it's a better indication that the recovery is started.
I'm not sure this does what you want? it blocks the thread handling the request. Maybe use latches here to signal events and control the disruption from the test? I think it will also be clearer to read.
This is kind of confusing, I think instead of trying to use this method as both a comparing method and an explaining method, maybe create an additional method called `differenceBetweenMaps` or something that returns the explanation and leave this one as a `boolean` method? Otherwise the name doesn't really match its type, and it's confusing in the comparison to be using a string as the expected null value instead of the build-in junit explanation parameter on all the `assert*` methods.
also can it be final
hm. so this hangs now every now and then for a minute. I think it is when the coordinating node is node_1. Then the cluster state observer waits for the next cluster state which does not come and the index request is only executed when the observer times out. We can send the request via node_2 but I think we actually need a way to handle this better.
"current shard has to be started inorder to relocated " -> "current shard has to be started in order to be relocated "
+1 I like plugin examples!
I'd write this `ImmutableMap.of("term_vectors_fetch", new TermVectorsFetchParseElement());`. Not that it matters either way.
Nice to have an example of how to implement fetch phases as a plugin!
Ah nevermind, I see where we check it above :)
nit: space after if
sorry I missed the ==false
I don't think it should be static
maybe add `directory` or `dir` at the end of the string
i think this is the right workaround for now...
good that we partially remove this logic!
Can we use URLClassLoader.newInstance() instead? It doesn't require a createClassLoader permissions check i think, because the returned subclass is controlled and does proper security checks.
can we say "sync id match" instead of "full sync". The latter implies also all bytes matched. I think it will be easier to understand.
we miss an L here REAL_L_OCATED
fine, it's a small detail anyway
we can omit this set no? maybe do ``` if (randomBoolean()) { queryBuilder.clause(null); } ```
I missed it, indeed it should be moved to setupSuiteScopeCluster
Can you inject some clock interface rather than use System.nanotime and sleep? Or crank the reroute interval down to 0? Something like guava's Ticker.
Does it even have to be a map? It feels like a set would do just fine here.
Does it matter whether it went from above the high watermark to normal or above the low to normal? Could you get away with just storing a single Map? I figure that'd be simpler.
Then we should leave it, the situation in general improves here already.
Should we increase visibility in this case? Makes only sense if we can then really test more, though, otherwise okay to leave it I guess.
Just curious about what this does.
Was this test not useful anymore? Maybe just test the builder here.
not 100% sure bad request is still good here, have yet to find a better status code though....
I wasn't sure about that. If it is temporary we can leave it, let's just make sure we don't forget if we need to take any action in the future
Can you propagate the boost to this query? If this query is enclosed in a BooleanQuery, it could have an impact on the normalization factor.
We're doing one extra execution here... I think we should have `while (retry < maxRetries)` and `if (shouldRetry(e) && retry < (maxRetries - 1))`.
I also wonder if we should log `TRACE`/`DEBUG` issues for this.
`retry < this.numberOfRetries` is implied here due to the outer check.
So negative delays count delays? I figured they should count as not-delayed.
One too many `*`s here I think.
Count _as_ delays I mean.
no, the catch there should be removed as well....that is what my IDE tells me :)
I know, still the class will stay but these methods should go. I would mark every single method //norelease
again, I misread my IDE warning, just replace with a single catch
I would add an `assert this.context != null` here just to make sure
I would add an `assert this.context != null` here just to make sure
can you add a //norelease here too? context should really go away after all queries are refactored
nevermind, I guess it depends on how you look at it. at the end of the day this parse method does parse + toQuery, having QueryShardContext is fine given that it will still happen on the shard. Also given that the QueryParseContext is much more lightweight, it makes more sense if done this way. Plus the parse method will go away, so leave it as-is.
I had a look at all these parse methods. We might need to clean them up, we have too many variants of it, most of them are used in tests. But in general they are used by any component that needs to parse queries: percolator, highlighting (supports a separate highlight_query that gets parsed as part of highlighting), query rescorer, translog (old delete_by_query)...... I think those parse methods should be converted to fromXContent and return a QueryBuilder instead.
I think we should fix this now given that this query is already refactored, this seems like a bug
lots of things here will need to be moved to the data node too once we refactor this query I think
Also, will this stay around or go away at some point? If it's supposed to be gone we should mark as //norelease
I think this catch can go away
we can remove this catch
something odd happened here :)
I think conceptually this should be QueryParseContext instead, if it needs to do more (toQuery) then we need to figure out how to create the QueryShardContext too out of it, but the other way around seems confusing to me. Sorry I see we are going back and forth on this.
you can remove the QueryParsingException catch, it's unreachable
this constructor is unused, both in our branch and in master. We can remove it I think
no I would do it later, definitely
yea thats fine, this whole class is marked //norelease , that makes sense
maybe we should change this and move the check to the data node already? not sure //norelease is good here, or maybe I am missing something
not sure why we go through the creation of QueryCreationContext to then retrieve the QueryParseContext, when we have the QueryParseContext in the first place
I think this should be moved to QueryCreationContext and exposed here through it temporarily
I have the feeling this should not be handled in parse phase but only in toQuery....
I think this should be moved to QueryCreationContext and exposed here through it temporarily
I think this should be moved to QueryCreationContext and exposed here through it temporarily
seems weird to have a query creation exception with text "failed to parse". This is still a method that parse all the way from xcontent to lucene query, not 100% sure what we should be doing here. Maybe the change is ok but we need to add some TODO or //norelease that says we have to get back to this method and split it in two phases at some point.
I think the exception should still be QueryParsingException here
rename to `getQueryCreationContext()`
can we call this timestamp? I think the "lastXXX" part depends on the context of calling it. If we do, would be good to rename the other ones in this change.
can we configure the delayed allocation to not be the default (`1m`) but something high enough to trigger what we are trying to fix, like `200ms`? This will speed up the test.
we probably want to rest here as well: `registeredNextDelaySetting = Long.MAX_VALUE;`
Would it be useful to have 0 have the special "I've not even tried yet" meaning? I think setting it to now is kind-of funky because you aren't actually doing an allocation when you build the object.
I implemented the early cat APIs that way and @kimchy wanted to nest instead. If the cluster's busy then you only have one dangling request.
Not sure if we want this optimization, but we could always run these three requests in parallel, putting the responses in three `AtomicReference`s and using three `CountDownLatch`s to wait until all three complete.
Now that I'm seeing these, I wonder if the default names should be `attr` and `value`. We could add aliases if we need longer. Since it's such a small API it's probably fine with the short versions.
I think I would still like it better as it avoids reverse-engineering a toString() impl.
Also an explanation for what a null parameter means in this context would be cool.
This means we are counting target relocation shards twice. Once with the source shards, and once here. We can remove the assignedShard.relocating part yet.
Just a note here. We decided that by convention we will use the same naming as maven. `groupId` has now changed to `org.elasticsearch.distribution.[packaging]` so I think we should also reflect that change here and use `org/elasticsearch/distribution/[packaging]` where `packaging` is: - rpm - deb - zip - tar
maybe put this in an `if else` clause? For me this makes it clearer what is pre 2.0 and what is 2.0 behaviour.
It's not a big deal, but I liked that it allows to - make StreamInput not depend on NamedWriteableRegistry - stack registries: you could stack registers on top on each other, while the current approach requires that you provide a registry that knows about all namedwriteables at once. I don't have use-cases in mind, but thought this could be useful.
We need to cast indeed, but I want to give the compiler opportunities to find errors, which is never possible when one starts definiting methods whose generic parameter is only used in the return value. By the way I'm thinking that we could make casts more safe by making category a class instead of a string, and this class would be the base class of the object that the namedwriteables can deserialize
I think it should be: `<C> C readNamedWriteable(@SuppressWarnings("unused") Class<? extends C> categoryClass) throws IOException {`
`s/Class<C>/Class<? extends C>/`
`s/Class<C>/Class<? extends C>/`
`s/Class<C>/Class<? extends C>/`
I get it now, yea I feel more comfortable making these changes in the query-refactoring branch cause we have more extensive tests, otherwise we should introduce tests in master too. It doesn't fix any bug anyways...
right, missed that :)
ok let's not change anything. To clarify what I meant: I know we need to support both formats: filters as top level, filters under the filters element. But don't we end up supporting filters under any element as well? Anyways, I am ok with keeping it like that.
why catching throwable here? seems a bit too much
I _think_ (Collections.&lt;String, DiskUsage&gt;emptyMap(), Collection.&lt;AllocationId, Long&gt;emptyMap()) is more "java" here even (and maybe especially) if it is a horrible mouth full.
Space after the equals. Its a silly small change but it helps my eyes.
Oh thats cool then. Icky but cool.
I think you can shorten the mapping here: `addMapping(type, "name", "type=string, analyzer=stop"` that hurts my eyes less :)
By the way: lucene expressions "know this"
For the agg refactoring I am doing the following: https://github.com/colings86/elasticsearch/blob/enhancement/minAggRefactor/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java#L306 basically the super class defines a template method which is implemented by the sub-class to create the instance and then the super-class can call the methods it needs on that object to set what it needs to.
Why not create one temp dir, and then a subdir off of that? There is nothing that guarantees tempdirs are all created side by side.
I mean if path.data is set eg. in the config/elasticsearch.yml to be another location
one step further: I think we could deprecate/norelease these two parseFilter methods and make sure that our refactored queries don't use them, cause they have been moved to toFilter in the corresponding builder.
Great test! Maybe one more that tests total compiles 2 or 3 or something - just more than one. So we can see how the number doesn't increment in weird ways.
just use `Collections.unmodifiableList()`
Now the logging here is really "fun". Maybe pull the pluginName to the top and use it in the []s. I'm trying to think through what this will look like for the temporary directory versions. Maybe it makes sense to make the whole temporary directory thing a more first class citizen - like a field or a subclass of this rather than just passing in randomness.
We should also check the name matches that in jvm plugins. As a follow up, we should at least remove description from jvm Plugin interface, and possibly also name (possibly a little harder, just requires passing around PluginInfo instead of Plugin I think).
Also, can you add an element to maven enforcer plugin for plugins/pom.xml so it fails build cleanly and early if this property is not set? We should also insert a check in pluginservice, if it differs from the directory name, someone manually meddled
Actually I just checked and removing name and description from the Plugin interface should be easy. The only thing to think about is what to give for those properties when plugins are loaded from the classpath (plugin.types). I think here the name should just be the classname, and description something like "plugin loaded from classpath"? I don't know what other info we really have.
This file is new in 2.0, we can change it
Not a big deal. I was just thinking of community plugin developers but it's not hard for them to fix. We should may be add this in BWC doc.
I think the whole point of this PR is to add this new parameter and make it required, so I don't think we should fall back? Otherwise the leniency that we are trying to fix is still here.
Hrm, can we just call this "name"? None of the other settings are prefixed with plugin.
I just saw @s1monw replacing `Lists.newArrayList` with `new ArrayList<>()` a few days ago. Maybe that way is out preferred way now? +1 on removing the warning though.
This looks not quite right. I'm not 100% sure, but I'd do: ``` + logger.warn("unable to start GCE discovery service", e); ``` and let the logger do its job with the exception. I'm tired though - airplanes. So don't take what I type at face value.
please use Arrays.asList while we re here
can this be synchronized please
No, there are only two ways for sets/maps, because of how multibinder works. I think it is more confusing to have some things bound using a binder, and others with registration/settings. Classes that ES controls should be registered, and set. If there was a way to just not allow multiple multibinders to work I would say we should do that, but I don't think such a thing exists.
+1 on registration directly via the `RestActionModule` I don't think multiple multibindings is supported, but tbh I wouldn't do that if it is/was. By exposing the extension points as registration methods on the Module you communicate what can be extended and what not, what accepts multiple implementation of X and what just replaces a singleton of X, etc...
Having limitless extensibility is not a good thing... as a plugin developer, I want to know what I can and cannot do... what I can extend. Otherwise I can easily do something I really shouldn't, break something along the way, without even knowing I broke it. Having well defined extension points and effectively limiting the extensibility of es in general: - helps us make sure plugins cannot break things (as they'll be restricted to what we allow them to do) - helps the users know what they can do and rest assured that they're not doing something they're not supposed to So overall, personally I'm less concerned about not capturing all the extension points at first run.. I'm more concerned about first capturing control over it. Then we can start opening up extension points as we see fit in a controlled manner.
Also, we should really move this discussion to another issue. I think this PR is fine as is, this was just a suggestion for a follow up/thought.
oh sorry, I was not reading the diff correctly, it looked to me like setting sourceToLog was part of the constructor. Nevermind :)
maybe this setting should have a more descriptive name too
maybe also rename the setting? (in addition to the constant)
ah! you do manual assertions instead. ok.
you don't have to assert on anything if an exception is expected
From first glance to me its not clear why all these assertions are the same. When is this not the case and might it be easier to just test those cases? Not sure because I don't know how the resolution works though.
can we get away with passing `null` instead of `TrustSelfSignedStrategy`? I'd prefer to have the certificates we trust only stored in the truststore
I like these as they wrap guice ceremony.
This whole method is now only useful for backwards compatibility. I don't really think this is needed any more.
Ah yes, thanks!
Maybe call this `HighlightersExtensionPoint`? Also the type could be moved here? ``` public class HighlightersExtensionPoint extends ExtensionPoint<Highlighter> { ```
I'm wondering if the parent really helps define equality here? Additionally, by adding this we will do more checks than necessary given that we compare both sub-aggs and parent aggs
One more thing, maybe you want to have a testEqualsAndHashcode like we recently introduced in the query-refactoring branch, just to make sure that equals and hashcode are properly implemented.
I wonder if we should start already sharing some common code between our BaseQueryTestCase and this class....wouldn't want to complicate things though. Also our base test class in not in master of course so that woul already complicate things...
That's certainly a funky corner case, right? Its a broken plugin that does that.
Why not `copyBinDirectory(sourcePluginBinDirectory, destPluginBinDirectory, terminal);` instead? As far as I can see you pass `pluginHandle` just for logging reason here. You could pass the plugin name instead: ``` java copyBinDirectory(sourcePluginBinDirectory, destPluginBinDirectory, pluginHandle.name, terminal); ```
Also, we were testing in the past that `sourceConfigDirectory` is actually a dir and not only an existing file. Did you change that on purpose? Or should it be `if (Files.isDirectory(sourceConfigDirectory)) {`
Ah! I see now. You don't do mean to fail. You aren't deleting the destPluginBinDirectory, you are deleting the extractLocation to clean up. Got it! +1
this part of the change looks a bit scary to me. I'm wondering how hard it would be to do this 'state' management and wrapping of collectors outside of IndexSearcher so that we don't have to check what the query is.
close is supposed to clear as well so this shouldn't be necessary to call clearReleasables
I think assertThat(throwables.get(0).getMessage(), containsString(...)) might be a bit better, especially if we don't specify an error message ourselves. Same for assertThat(throwables.get(0), instanceOf(...)).
seems like this method doesn't belong here anymore, as it's shared between different classes. Seems it should be moved to some validation/exception util class. Actually, I am wondering if we can reuse the existing `ActionRequestValidationException` or something along those lines.
maybe this method could be package private? Also, I would prefer a different name, like `getIndexSettingsValidationErrors()` for instance.
good move, folding this into the base class..
doesn't this add another field name that wasn't there before? Is this method used? do we need to implement toXContent? I'm looking a the implementation of RecoveryResponse
can we call this RecoveryNodeResponse? (it's no longer about a single shard)
use ArrayList? one less usage for non standard code...
cool this is not a response any more...
Now that we use a simpler shard iterator, this is no longer up to date.
this will annoy the forbidden API after rebase + squash. Heads up
add action name pls.
kk. was referring to both the maps and the lists later on > On 28 Aug 2015, at 20:40, Jason Tedor notifications@github.com wrote: > > In core/src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java: > > > - for (int i = 0; i < shardsResponses.length(); i++) { > > - Object shardResponse = shardsResponses.get(i); > > - if (shardResponse == null) { > > - // simply ignore non active shards > > - } else if (shardResponse instanceof BroadcastShardOperationFailedException) { > > - failedShards++; > > - if (shardFailures == null) { > > - shardFailures = new ArrayList<>(); > > - @Override > > - protected RecoveryResponse newResponse(RecoveryRequest request, int totalShards, int successfulShards, int failedShards, List<RecoveryState> responses, List<ShardOperationFailedException> shardFailures) { > > - Map<String, List<RecoveryState>> shardResponses = Maps.newHashMap(); > > @bleskes Are you referring to Maps? That hasn't been forbidden yet (but it will be soon). > >  > Reply to this email directly or view it on GitHub.
I guess something went wrong.. no biggy.
relocating targets are something we create on the fly - most of the time they are not needed and do not really exists in the routing table (but do in routing nodes!). I think we can just call this allShards..
wrap the plugin names in `[` and `]` for consistency
just for kicks can we have `reason = "works around https://bugs.openjdk.java.net/browse/JDK-8034057"` it's just more obvious which bug is meant
can we use `== false` instead of `!`
Oh nevermind I see it will happen below.
@HarishAtGitHub sometimes less is more. Too long an exception just means that the user doesn't read it all.
Actually the main confusion in the original issue was that the error that was thrown did not say which field in the request caused the error. However with this fix as it stands, if the user sends the wrong type of object for a field (say a String instead of an object) the error will not tell them what is wrong. I think we should add the token to the message, and possibly also the token we expected in this case too.
Those token names are important, as they tell you what type of JSON token was expected. They may not make sense to you yet, but they are invaluable for figuring out what to look for, eg I was expecting `VALUE_STRING` but got `START_OBJECT` (ie i was expecting `"something"` and got `{`)
maybe make this variable final? just better indicate it will never change
I thought about this and it is wrong. We need to specify the existing official repo here. So, if we release 2.0.0 we retain the beta release in there. Just remove this line and the `download.elasticsearch.co` is used to sync and all works as expected.
253, can go, this one... ``` print (' export S3_BUCKET_SYNC_FROM="%s"' % (s3_bucket_sync_to)) ```
I think we have a problem here. The version that is supposed to be supplied as a parameter should only consist of major.minor version like `2.0` (so that all 2.x version go into the same repository) - this one is `2.0.0-beta1` though.
last `%version` should be `%major_minor_version`
last `%version` should be `%major_minor_version`
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
I c... ok
shouldn't we use here `!REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT` instead of `false`
+1 for these checks. Another way to accomplish the second check is to use address.isUnresolved(), but this is also fine.
What uses this? And why is forbidden APIs not angry about passing in String,int here... i feel like i banned that method. I dont like it as its wired to 127.0.0.1 in non-test code.
I don't like Files.exists in general but personally I think this is the right decision for a good surgical fix!
FWIW I've used this in the past for production ES clusters to have a set of common settings (elasticsearch.yml) and node-specific settings (elasticsearch.json) to merge two files with settings. That said, I still think it's safer/better to remove this feature and fail if more than one config file is found. It reduces the complexity for reasoning where a setting came from.
> still think it's safer/better to remove this feature and fail if more than one config file is found. It reduces the complexity for reasoning where a setting came from. +1
ok, then i am fine with it! Maybe we should followup and consider moving it to a base class and we do this stuff in a consistent way everywhere. I bet we will find test bugs.
Here would be something of an alternative maybe for the future, when java 8 is minimal (it would be less annoying due to effectively final and lambda): ``` public void test() { assertError(IndexOutOfBoundsException.class, () -> { int foo[] = new int[5]; System.out.println(foo[6]); }); } // test helper for expected exception // TODO: can we replace Runnable with TestMethod or similar interface that throws Throwable // so checked exceptions arent annoying? static void assertError(Class<? extends Throwable> expectedClazz, Runnable foo) { assertError(expectedClazz, null, foo); } // test helper for expected exception, with expected message static void assertError(Class<? extends Throwable> expectedClazz, String expectedMessage, Runnable foo) { try { foo.run(); fail("didnt hit expected exception, expected: " + expectedClazz.getSimpleName()); } catch (Throwable t) { if (!expectedClazz.isAssignableFrom(t.getClass())) { throw new IllegalStateException("got the wrong exception, expected: " + expectedClazz.getSimpleName() + ", got: " + t, t); } if (t.getMessage() == null && expectedMessage != null) { throw new IllegalStateException("exception had null message, but expected: " + expectedMessage, t); } if (expectedMessage != null && !t.getMessage().contains(expectedMessage)) { throw new IllegalStateException("expection message did not contain expected text: " + expectedMessage, t); } } } ```
It is a matter of taste i suppose, but i still hate all junit support for exceptions like this. The rule sucks because it has side effects, if we write another test that uses it, it can easily have some bogus leftover state from a previous test method? Personally i still do it tests like this: ``` try { something(); fail("should have hit expected exception"); } catch (SomeException expected) { assertTrue(expected.getMessage().contains("expected text")); } ``` This sucks too, in that its easy to forget the fail() part and have the whole test do nothing.
And one downside even with being initialized before each test method, is some tests do multiple asserts like this inside a single method. So we'd have to clean up those tests to be separate test methods, but thats still fine.
Further down in the function it looks for the file and appropriately errors out if they were to be missing, I don't think this will be an issue.
Are there other files that might not be in there and we're ok with that? Should we log a warning or something? I suspect its just fine for the directory not to exist but if some file inside the directory doesn't exist when the directory does thats probably bad. Like, in production. In tests is fine to just eat the exception.
nit: missing ending paren
I can't come up with any more seriousness :)
Users that have indexes created before 2.0 should not be precluded from using the new setting name. This looks like it _only_ allows using the old name with indexes before 2.0.
I _think_ we have a deprecation logger. We should probably log something when we see `position_offset_gap`.
We should tell the use to use `position_increment_gap` in this error message.
Do we really need to ignore the setting in post 2.0 indexes? Why not just support both for a while? You already check above that both aren't specified.
I don't think that matters. This is a new setting we are adding, as far as the API is concerned. I don't think it is a problem to validate the user didn't try to configure the same thing in two different ways.
I would throw an exception if both are specified.
I see you have backcompat below for when this setting is passed in through a string field. However, i think we also need to have it for custom analyzers here.
We shouldn't need to log if we through an exception. I don't like the hard cutover but I can live with it. If I discount the hard cutover my only thing is the exception message.
I think we need to support backcompat here. You can look at version.created in indexSettings, and then only accept the old setting name if before 2.0 final. It would be nicer if we forced the upgrade once, when loading the index, but I don't think that is currently possible without a bunch more work.
I think the confusion came from the case when someone using `position_offset_gap` (in a pre 2.0 index) also specifies `position_increment_gap`. This should be an error, not just making one "win".
I'm a fan of `assertThat(e.getCause(), instanceOf(IllegalArgumentException.class));` because the error reporting is better.
It gives clients an upgrade window where they can safely continue to use their old mappings for a while. Its normal not to push a new version of your application in the same release window as you push Elasticsearch. In fact I'd wag my finger accusatorially at anyone that did. So you want the old code to work with the new Elasticsearch. And the only way to rig that is to support both names for a while. Either we add support for the new name in 1.7.X and re-release or we support the old name for a couple of releases.
the @Test annotation is unnecessary
you should pass ctx.reader().maxDoc() instead of 0 for things to work properly
This constructor doesn't seem to be necessary.
can we give that a better name that describes what the function does? something like `checkCommittedAndSendCommitIfSo()`.
same as above, function name says nothing about what it does.
It looks like with the latest changes serializedStates can no longer be null, so we should probably remove `Nullable` here.
`if (serializedStates != null) {` is no longer needed
I forgot about that test. I think it's good for now.
can we mark this as nullable (and doc when it's null)? also, can we move it next to the setter, and make the naming consistent with the rest of this class? (i.e., shardId)
also, why not use indexShards() now that we see this as a write op - then we don't need to change the visibility of the shards methond on Operation Routing
Why have this check? I know I keep this variable around, as well as one for java 8, and set JAVA_HOME based on it when I want to switch.
Nothing to do here for this PR, but how can we keep this up to date? We already have this list in like 3 other places it seems...
nit: an -> and
Can we just use a simple try/catch here? I don't see why we need to use hamcrest complicatedness...
or junit for that matter. try/catch is much more readable (and the way most other tests do this)
I don't think we need to protected ourselves here against violation of transport service semantics (just one callback).
this test is about timeouts, right? feels unnatural to throw a normal exception here. You can also rename to the test to say "handle exceptions" and check all exception are handled correctly (timeout or thrown) - imho a deidcate non-timed out error is better (i.e., it tests it returns immediately and not after a short timeout).
partially replying to the original message - the idea is to test how this action behaves under different error conditions - a connection error (which is thrown here), a general exception / shard not available (which is typically given as a response , not thrown here (although it's equivalent at the moment).
why 1000? this should never hang right? we can just use get()? if it hangs it's an issue.
I meant assign it out of the if and use it in all places :)
do we want to check listener.isDone? (because it is supposed to return immediately). get() waits.
also, can we remove the boolean return value from doStart and remove the timeout handling from the public void onTimeout(TimeValue timeout) method of the callback given to the observer in line 245? just call doStart.
`isCustomDatePath` -> `isCustomDataPath`
fair enough, I dont think toXContent should change state (Collection.sort changes the lists it gets). The class become thread unsafe (while it used to be thread safe). You can also sort in ThreadPool#stats, if you prefer? > On 31 Aug 2015, at 13:14, Tanguy Leroux notifications@github.com wrote: > > In core/src/main/java/org/elasticsearch/threadpool/ThreadPoolStats.java: > > > @@ -188,6 +206,7 @@ public void writeTo(StreamOutput out) throws IOException { > > @Override > > public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException { > > builder.startObject(Fields.THREAD_POOL); > > - Collections.sort(stats); > > I added it here because I just want the XContent output to be sorted. I think we don't really care otherwise and thus don't need to sort everytime we instantiate a ThreadPoolStats object. But I can move it if you really prefer. > >  > Reply to this email directly or view it on GitHub.
I would use the same type name as the StatsPipelineAggregator here so it's easy to see they relate. That's what we do on other aggregations. The contexts in which these are used and deserialised are different so the names would never clash
I would use the same type name as the StatsPipelineAggregator here so it's easy to see they relate. That's what we do on other aggregations. The contexts in which these are used and deserialised are different so the names would never clash
I would just `throw e;` here
I would just `throw e;` here
Could we either do the percentiles of percentiles here or have another test that calculates the max bucket of a terms agg which is calculating percentiles over a histogram? This way it would check that the agg can both reference an inner sibling pipeline agg and be referenced by an out sibling pipeline agg.
Should be `SingleValueMap`
you can remove the entire `catch (IOException e)` block here it's already handled correctly above
can we tunnel through the cluster state used in the action? that way we don't need to worry about consistency between the two (concurrent index deletion etc.)
I don't have a better idea, but this makes me slightly nervous since nothing in the API prevents you from doing ``` java Analyzer a1 = new NamedAnalyzer("foo", someAnalyzer); Analyzer a2 = new NamedAnalyzer("foo", otherAnalyzer); ```
This was a bit hard for me to read due to the order in which comparators are checked. Could it be rewritten in a more idiomatic way, ie. ``` java if (o1.isPrimary() != o2.isPrimary()) { return o1.isPrimary() ? -1 : 1; } final int secondaryCmp = secondaryComparator.compare(o1, o2); if (secondaryCmp != 0) { return secondaryCmp; } final int indexCmp = o1.index().compareTo(o2.index())); if (indexCmp != 0) { return indexCmp; } final int idCmp = o1.getId() - o2.getId(); if (idCmp != 0) { return idCmp; } ``` It would be helpful at least for me to see more quickly in which order comparisons are performed.
hmm, is there a typo somewhere in the end (s/shared/shards maybe?), I don't understand it. But otherwise, I would have been happy with just "Apply secondaryComparator last as it is more expensive than other comparators"
`o.e.i.Streams.copy` will close both `in` and `out` after the copy is complete, but `ByteStreams.toByteArray` did not do this. This could potentially be problematic in the case of `in` here; we will need to think through this carefully.
can't this just be `if (IndexMetaData.isOnSharedFilesystem(indexSettings) == false || closed)`
nit: you might be able to save a few toString lines by extending ToXContentToBytes here. no big deal though
I do like this change, it simplifies things a lot. What I am wondering though is if it was simply wrapping a search source, or also hiding that inner_hits don't support everything that can be put within a search source. For instance, the `query` element doesn't seem to be supported, as well as other parse elements I'm afraid. This becomes a problem in the java api only I think, cause you can potentially set things that aren't supported. Maybe it is ok for now....
nit: no need to go through the getters for these three fields
I don't understand why there is either a setter (with null check & exception) here and then direct assignment to fields. Using either one of these would be fine I think. Same for the other options in this constructor.
I think we can simplify here and print everything out, default values included, that's what we went for in all of the other queries too.
Can be removed here, queryName and boost are all handled already in AbstractQueryBuilder#toQuery().
forbiddenapis doesn't like this
In some previous discussion for other builders we tried to set default values for non primitive types that have a default, setting to null effectively means use the default, same as specifying `field: null` in the json query. Maybe we can keep this here (and in the following setters that have default values)
I see! I think your change makes it clearer actually, looks good
should we remove this validation on query and type from parser if we add it to validate on the builder. The idea is that we call validate right after parsing (on the coord node), that applies to java api so can be seen as a centralized validation for both rest layer and java api.
should we check that the query is not null too? seems like the parser does it.
sorry! totally missed that.
cool ok that's fine.
sure things changes now that we know for sure the target branch, that said making everything final would be better to do once we merged back to master to prevent merge conflicts here. Same with renaming XYZQueryBuilder to XYZQuery, and moving to proper getters and setters.
yes lets do it later otherwise we have to remove setters and break things.
I think the type and queryBuilder instance members could be made final
makes sense, probably also the `QueryShardContext.setTypes(types);` should be postponed.
good idea, I think this should solve my concern above about setting things that are not supported through the java api.
I always wondered the same, I think we don't given that everything works without... that said we do have a lot of empty constructors with the `@Inject` annotation. Up to you... ;)
Okay, we might have to revisit some other queries we refactored then.
scratch that I see how you use it now ;)
Full default value handling story here: https://github.com/elastic/dev/blob/master/design/queries/general-guidelines.md#default-handling
same should be done for minChildren and maxChildren too. Wondering now if it makes sense to move to `int` rather than `Integer` for all three fields
can we order the clients? :) I'm OCDing ..
this actually waits until the recovery is completed and the shard start message was already sent to the master. Not sure if this is intended.
`S3SignerType should not be available for Frankfurt region`
`S3SignerType should not be available for China region`
`} catch(IllegalArgumentException e) {`
`} catch (IllegalArgumentException e) {`
`public static void configureSigner(String signer, ClientConfiguration configuration) {`
I think we should only log.warn here. I can imagine that at some point AWS may support this in China and I would not block users for this. May be we should not control that at all and let the user specify whatever he wants. I mean that if this plugin is used with other S3 compatible platform, we can't do those checks.
Should we actually control this? I know, I wrote that code as well at the begining but the more I think about it, the more I think it's not that useful to block... May be only log.warn with "[{}] signer might not be supported"...
`} catch(IllegalArgumentException e) {`
Why do we want to support both `_version` and `version`? Can't we just pick one? Btw, I think Luca recently fixed camel case conversion of strings that start with an underscore.
this is not needed now, right? it's part of the other test.
we can delete ForceMergeFailedEngineException now, right? It's not used.
I think we should just rethrow `t` and remove that exception. We can't handle this just like refresh since a failing refresh means dataloss while this one might just be a configuration issue or so. We should just pass t to `maybeFailEngine` and rethrow IMO
can we put it in their own catch clause similar in how it's done in refresh: ``` } catch (AlreadyClosedException e) { ensureOpen(); maybeFailEngine("refresh", e); } catch (EngineClosedException e) { throw e; } catch (Throwable t) { failEngine("refresh failed", t); throw new RefreshFailedEngineException(shardId, t); } ```
this deserves a sep issue I guess but good catch
should these be private? not sure who is using it...
I'd probably keep the clauses in the order they were in before. Not a big deal though.
Won't the `indexName` be wrong below, since it is just `_parent`? Maybe we should just have a special case here for `_parent` for now? Something like: ``` String indexName = fieldMapper.fieldType.names().indexName(); FieldDataType fieldDataType = fieldMapper.fieldType().fieldDataType(); if (fieldMapper instanceOf ParentFieldMapper) { ParentFieldMapper parentMapper = (ParentFieldMapper)fieldMapper; indexName = parentMapper.getJoinFieldType().names().indexName(); fieldDataType = parentMapper.getJoinFieldType().fieldDataType(); } ```
You can use joinFieldType.name() right? Instead of `joinField`
I don't think we need a ref here? This field type is always associated directly with this mapper.
Why do we need this? The public ctor is used for new types, when intializing all the metadata fields. But in that case, there is nothing shared so no need to initialize the join fieldtype (it should not be used unless/until _parent is set on the new type, in which case it will be parsed and the protected ctor will be used).
same here - throw IAE
I think this should throw IAE if you pass null - that's 100% of the time a bug
weird, I thought this was very similar to query_string where we do have AUTO as default, but eventually it isn't, sorry.
as soon as I get #13284 we should have a randomMinimumShouldMatch method...
remove the iterations please
hopefully having a default for fuzziness makes it non optional and simplifies things slightly here too
I think for this one you really need null, I have the same in the query_string query too.
Nice, I like it to have some more controlled, non randomized tests here for basic behaviour.
Can we make MultiMatchQuery.Type a Writable similar to Operator etc...? Relying on serializing an ordinal feels safer to me than using the names.
Some of these checks can potentially be removed after changing internal field to primitve types / using defaults.
fyi, I removed the ZeroTermsQuery class from MatchQueryBuilder entirely in #13402, moved everything plus serialization to #13402, so we might remove this setter and only have one.
Wonder if it makes sense to rename the inner `Type` here to something more specific to not get it confused with the three `MatchQuery.Type`s that it's using internally.
fyi, I added serialization to the enum I moved to MatchQuery (#13402) maybe we can reuse this.
Same here, we can change to primitive `int` and use default values for this and some more of the fuzzy options. This will also simplify doXContent, can also be used in parser then.
can you leave this class for now, add the same TODO as in the parser? We can't have a parser without a builder in the query-refactoring branch.
just because it seems like it's used only internally, that's it. No strong opinions though ;)
shouldn't we check that the returned mapper is not null and is od nested type? we have this in NestedInnerQueryParseSupport: ``` if (nestedObjectMapper == null) { throw new QueryParsingException(parseContext, "[nested] failed to find nested object under path [" + path + "]"); } if (!nestedObjectMapper.nested().isNested()) { throw new QueryParsingException(parseContext, "[nested] nested object under path [" + path + "] is not of nested type"); } ```
maybe it's not a problem, but shouldn't we call reset where we are missing it also then? just wondering if this hides some problem in our test infra
if it gets too low level the instanceOf that you have is enough here, we don't have to go that deep I think
IMHO we can also postpone such clean-up until after the branch is merged to master, just need to remember that we left a few things behind.
these can all be final
this needs documetnation and I wonder if we can rename it to just `TermVectors`
this should be `isExists`
this should be `isArtificial`
false is the default already
is it worth doing the conversion from and to geohash every time here? Could it be better to not do the conversion and store two doubles per bucket instead of one long? I guess its a trade-off between execution time and memory
ok, fair enough. ++ for setting up compatibility with GeoPointv2
InputStream or OutputStream? This is confusing because the method is `InputStream readBlob(String blobName)`
Yep, this looks great
nasty... we have to backport this to 2.x and 2.0, and master if not filter is still there.
I think this should be strict too. The problem is that the not query has some alternate version that uses deprecated syntax. I think that should be moved to a separate test like you did with recent PRs.
it's a shame java need this...
can we pass empty string and empty bytes ref to this so it's a valid ctor? we should fail if these args are null
++ for ordinal and tests then
why is this? what's wrong with `1.f`
can we wrap this long line
please fail if required stuff is null
this looks awesome!
I think for simplicity we can use the default float value here directly? just skips a condition down the road I guess.
Or do like we did in other Parsers: Have constant in the builder that holds the default value and re-use that in the parser. Removes the "magic number" in the parser and skips the condition.
but it feels like we are back with our own ordinal then, more LOC too with two switches compared to the previous `Operator` impl :) can't we just handle bw comp once we actually change the enum? of course the main issue is remembering about this, and tests are a good way to remember...
can we use a switch statement here maybe to read and write? like ``` JAVA switch(id) { case 0: return TERM; case 1: return RECURSIVE; } ``` and on writing we can do: ``` JAVA switch(this) { case TERM: out.writeVint(0); break; case RECURSIVE: out.writeVint(1); break; } ```
can you push a commit to 2.x that marks score_type as deprecated? ideally using ParseField
let's make sure we deprecate it as well in 2.x
Feels a bit weird that one method is returning a list and the other one an array
Why do we need to convert to an array? In general I think we should try to use either lists or arrays everywhere instead of doing back-and-forth conversions? (not for the cost of the conversions, more for consistency) Maybe this should be a different PR though, it looks like an existing issue to me. It was just weird when looking at the diff
can this use a non-lenient boolean parser? it shoudl be hard, like really hard, to turn this off. not the other way around.
Not sure if this would work, but could you keep this override but make it relative to the number of nodes on latest version, essentially removing the need for hte new "num lucene latest version" method? That is a very awkward parameter for a general integ test that is really specific to backcompat tests...
We should do this
We should set random boolean here too. We should be testing both the default (unset) and using the method will all allowed values (i.e. true and false)
We don't need to keep the constructors/methods for bwc. For the Java API we are able to break backwards compatibility here and we should if it makes the use of the API less trappy (as it does here).
these don't need to be static
can you use `== false` instead of `!`
can you move this up to fail before we assign
can you move this up to fail before we assign
I think the add and set methods for those lists here are too complex. I think we should only have setters and the setters should make an unmodifiable list out of the values as well as copy them.
`this` is unnecessary
can we check the value we are reading here? an assertion would be ok
nit: this seems unused
Also, given that type and index are required if id is specified we should remove the setters for type, id and index and remove the zero-arg constructor for Item so the user is forced to correct initialise the object with either the index, type and id, or the index, type and doc
This will need to change to read a boolean to tell it whether it needs to read a doc or Id so it can use the right constructor and doesn't trip up passing nulls into it
We should break anything we need to to make the Java API clean, easy to use, and less trappy. Given this is only going into 3.0 we should take this opportunity.
I don't think we should allow null here I think this should throw an exception if null is passed in
Good idea to add this safety net.
@colings86 suggested to use a different method name over in #11969 which I think makes sense.
`coerce` and `ignoreMalformed` seems to be related maybe we can enforce that both are updated together? or at least add a TODO
Given that only GeoDataGenerator uses this, can we move the method to that class. It seems weird that GeoDataGenerator references ESTestCase
why? `""` is not a sensible default. I think `null` is better here
These messages seem wrong. There are no top or left parameters
We should throw an IOException with a nice message if the read int is greater than `values().length`. See ShapeRelation for example of what I mean. We also need unit tests to make sure the ordinals don't change. See ShapeRelationTests for an example of that.
Can we remove the lat() and lon() methods? We don't want people setting lat but not lon so it don't makes sense (IMO) to allow them to be set separately
nit: remove `this` as its not needed and will be consistent with the below lines
Maybe "Note: this also counts relocation targets as that will be the new location of the shard. Relocation sources should not be counted as the shard is moving away"
Similar to [`CopyOnWriteHashMap#copyAndPutAll`](https://github.com/elastic/elasticsearch/pull/13533/files#r39337198), what do you think of the following? ``` Stream<Closeable> closeables = pluginsService.indexServices().stream().map((p) -> indexInjector.getInstance(p)); IOUtils.close(closeables::iterator); ``` This saves an allocation of an `ArrayList`, and possibly multiple backing arrays as the elements are added to the `ArrayList`.
What do you think of this? ``` public <K1 extends K, V1 extends V> CopyOnWriteHashMap<K, V> copyAndPutAll(Iterable<Entry<K1, V1>> entries) { CopyOnWriteHashMap<K, V> result = this; for (Entry<K1, V1> entry : entries) { result = result.copyAndPut(entry.getKey(), entry.getValue()); } return result; } public <K1 extends K, V1 extends V> CopyOnWriteHashMap<K, V> copyAndPutAll(Stream<Entry<K1, V1>> entries) { return copyAndPutAll(entries::iterator); } ``` It's not necessary but the call site on line [519](https://github.com/elastic/elasticsearch/pull/13533/files#diff-80e3b9bf08264f2b719b2f8ce044ae80R519) can then change to not call `Collection#stream` but the rest of the call sites can remain as is.
I might take the opportunity to roll these into one statement like: ``` java if (precision < 4 || 18 < precision) { throw new IllegalArgumentException("precision must be between 4 and 18 inclusive"); } ``` I figure it gets more information back to the user on failure. Not a big deal though.
This doesn't look right.
Well, it shouldnt be. The suppressed exceptions are like 'children' of `cause`, they don't need to be shuffled around. The current code is basically attaching grandchildren as children, which will be confusing.
We should nuke all this logic after the `super` call, because now we init the exception with `ex` as root cause, so it will still keep all of its suppressed exceptions.
This is not related to your changes (can be followup) but is an obvious bug (just discards `args` completely).
move these to TRACE? they don'e really signal any issue but people may thing so...
same trace thingy
trace? this runs very frequently..
I missed this method is only called under the condition that something changed: ``` if (!changes.isEmpty()) { calcAndSetShardBuffers(activeShards, "[" + changes + "]"); } ``` ++ to leaving on debug.
Minor typo of `local` instead of `locale` in the exception message.
Like, just for checking that assertions are enabled.
Ah! I was blind to the line with the white background.
the initialization of the context with two null arguments makes me wonder why we don't need those arguments there.
The first condition seems to be redundant, but it might be ok to leave it in for clarity.
if `bits % 64 == 0` this will spit out one more than it used to I think.
Nice, much better. This removes a ton of confusion at startup, it just was not possible to easily fix without fixing this holder class the way you did.
I think there is something wrong with GCE http client code and we will need this around it, we grant the permission as a workaround. It would be really good if we could get this fixed in their code though. It makes it difficult for apps to protect credentials etc to their services!
@rmuir are we OK with this? Is there a better way (not sure at all, just double checking).
same here- I think this should be a hard exception.
(this refers to the access controller block)
I also dont' think we should swallow the exceptions here? Someone asked for a gce address and we failed to get it...
@dadoonet I think the log message can still say "Failed to fetch metadata from google" ? more than just client creation can go wrong here..
if just read metadata it will also be easier to implement a fetch all interfeces, sort interface name, fetch interface ip sequence
why is this public? it's only used internally, right
I think we want to blow up, right? this means that something that was explicitly configured doesn't work... null doesn't seem like the right return value (i.e., it means: please continue and try something else)
shouldn't we throw an exception here? we got an explicit request for a GCE address and we failed to resolve it.
when do not want to do it (and throw an exception as said above)
can we sometime check _gce_ ? also check illegal values and make sure it blows up correctly.
use the real ctor :) - applied to all the ctors below!
I'd not do this, just pass syntactically valid values to the Prototype ctor
ok I understand better your intention now. I think it is still weird from a user perspective to have to pass in `null`, not loving the nullable arguments. That said it is not a huge deal, can leave as-is.
can we check that intValue is non-negative? (we use it in vint)
this must be `2000051` rather than `2000003`
Ok, then it's fine.
Nit: you could use `Collections.emptyList()` instead of `new ArrayList<>()`
Actually this line is not the actual problem, but this one is: ``` java final BreakIterator breakers[] = new BreakIterator[UScript.CODE_LIMIT]; ``` together with ``` java breakers[code] = parseRules(resourcePath, env); ``` If `code` is greater than `UScript.CODE_LIMIT` then this will result in an `ArrayIndexOutOfBoundsException`. But I do not know whether it is possible that this condition occurs in practice.
Is it necessary to call a public non-final method from a constructor? This can cause issues when somebody subclasses `IcuTokenizerFactory` as initialization order gets important. I'd opt for making `#getIcuConfig()` private which avoids the issue. Alternatives: Declare `#getIcuConfig()` or the class as `final`.
I think you should port this to the new settings infrastructure.
I did not check this in detail but if `UCharacter.getPropertyValueEnum()` returns values > `UScript.CODE_LIMIT`, then it would break your code that populates the `breakers` array below. In that case I would add an explicit check and throw an exception.
This has to use the new settings API.
This whole loop reads fairly low-level. If config files can be considered small, we could just read them much more concisely with the Stream API (untested): ``` java String rules = Files.readAllLines(path) .stream() .filter((v) -> v.startsWith("#") == false) .collect(Collectors.joining("\n")); ``` All the low-level stuff is gone. But this relies on Java 8 features and will only work on master.
It is based around the class `Setting` and validates lots of stuff (among other goodness). Here's a short summary for your PR (untested): 1 Define a (list) setting as a constant in `IcuTokenizerFactory`: ``` java public static final Setting<List<String>> SETTING_RULE_FILES = listSetting("rule_files", emptyList(), Function.identity(), Property.IndexScope); ``` 2 You need to register the setting in `AnalysisICUPlugin`: ``` java public void onModule(SettingsModule settingsModule) { settingsModule.registerSetting(IcuTokenizerFactory.SETTING_RULE_FILES); } ``` 3 Retrieve values: ``` java List<String> ruleFiles = SETTING_RULE_FILES.get(settings); ```
We can rely on auto-closeables here (i.e. we could use just the try-with-resource statement). Not necessary if you use my suggestion with the Stream API.
we need a consolidation of all the score mode / type we have at some point, not here though
then use `Collections.singletonList()`
Do we not also need a NAME constant here which is the name of this function in the NamedWriteableRegistry? Same with the other Functions
Yeah. I can see there are lots of integration tests for each function but it would be good to build out unit tests for them too. As I said, it doesn't need to be done in this PR but it's something we should do IMO
++ thanks for changing this :)
Probably not in this PR, but it would be good that now we have the functions refactored, to do in depth unit tests to ensure they work as expected
Forbidden API: ``` Forbidden method invocation: java.lang.String#format(java.lang.String,java.lang.Object[]) [Uses default locale] in org.elasticsearch.cloud.aws.blobstore.S3BlobStore (S3BlobStore.java:211) ```
Forbidden API: ``` Forbidden method invocation: java.lang.String#toUpperCase() [Uses default locale] in org.elasticsearch.cloud.aws.blobstore.S3BlobStore (S3BlobStore.java:204) ```
missing end of line char
Since the method [setMaxElapsedTimeMillis](https://developers.google.com/api-client-library/java/google-http-java-client/reference/1.20.0/com/google/api/client/util/ExponentialBackOff.Builder#setMaxElapsedTimeMillis%28int%29) only accept a maxWait > 0 this `if` condition must be changed.. I'd write: ``` java if (maxWait > 0) { retryHttpInitializerWrapper = new RetryHttpInitializerWrapper(credential, maxWait); } else { retryHttpInitializerWrapper = new RetryHttpInitializerWrapper(credential); } ```
I think we should change this so we output a `validation_method` field which can have the values `ignore_malformed`, `coerce` and `strict`. Then the parser should parse this as well as parsing the deprecated `coerce` and `ignore_malformed` boolean fields
nit: I started to like the `(expression == false)` notation I saw in several places.
I don't think we should initialise to empty string. Why not add an assert here to check for null? This should never be null here as that would mean we got a value before a field name.
`ignoreMalformed()` would also be true for `GeoValidationMethod.COERCE`, I understand this doesn't change much in terms of the parser logic but maybe it would be more consistent to just use the getters here and rely on the logic they hide.
I think we should remove this method and the ignoreMalformed() method and instead have a validationMethod() method here instead
I saw this problem being dealt with in other place by setting currentFieldName to empty String. Worst that can happen then is that it is treated as fieldName in the query, which we should validate later and throw IAE then.
ok, I don't ahve a strong opinion on this and don't mean to be blocking your PR, as I said it LGTM anyway
yea the idea was to move to `String[]` where we don't need to call `add` anymore... not sure it is possible though.
does it make sense to check for null beforehand and throw exception? we don't want anybody to call this method and explicitly pass in `null` I guess.
I would remove the word current here as its the strategy the user is trying to set that is the issue not the one already set
I like be a little more in this case.
The fqdn was used before though too. It is not introduced here. It makes it clear which exception it is (e.g. org.elasticsearch.script.ScriptException vs javax.script.ScriptException), and it makes it easier to remove exceptions from here in master (vs having an import statement too), which is really needed.
cool, we just had to do it twice while queries were going to be moved over, now we do it in a single place. Thanks for clarifying.
might be something that was previously supported in and query and or query which are now gone. not is the only survivor. That said I am leaning towards leaving that logic in the context anyway? It feels weird to be looking queries up from the parser here and throwing generic exceptions like "no query registered". And maybe open an issue for discussion about deprecating this syntax. maybe we should have a single way to specify things here.
+1 to use the lucene exception
extra space here in the log message
Also, since "recover" and "restore" are very similar and easy to confuse, I think it'd be nice if this were named "`recoverState`"
Since `shard_store` isn't a reserved keyword maybe this can be "starting recovery from store ..."
and -> an
I think this would be nicer as `recoverStateFromStore`
I mean to call out that it doesn't squash some indices together, just to squash the segments all the shards in some index.
This could use some rewording. I know it probably came from a copy but it could use mentioning that its per shard. Also, the half thing is new to me.
Since the account settings are supplied by user, I would feels better if we used URI to build this string. This way we will have at least some basic validation of the things that go into this URL.
If the `store` is the only thing we need, we could set `Flag.store` flag for `indices` here.
Conversion should be very cheap, so I would prefer to always convert to avoid a branch.
This need not be fully qualified right? (Remove the org.apache.lucene.index. prefix)
Can you rewrite this as an array of ctor references and iterate the list to build the map? It might be a bit easier to read.
Can we move this and the `if (shard != null) {` outside of the `try`? I.e. shrink wrap the `try` around only the code we expect those scary exceptions below to be thrown from (the `shard.updateBufferSize` call).
Not -> Note
Thank you for cutting over to a better clock :)
Woops, never mind: maybe put params around `status.activeIndexing == false` ;)
nthe -> the
Pre-existing issue: I think 30s default is too large, because an index that was inactive and suddenly becomes active with a 512 KB indexing buffer for up to 30s of heavy indexing is suddenly writing many, many segments. It would be better if the first indexing op to arrive to an inactive shard could force IMC to wake up and re-evaluate. I'll open a separate issue about this ...
Not really related to this change but we can remove now the check for cloud.enabled
Could you replace null above with `TRANSLOG_BUFFER_SIZE_SETTING`? (This is a separate issue, but I never backported to 1.7.x...)
Duh sorry you're right ... don't change it!
Got it. Thanks.
I think it'd be marginally nicer to return the number freed rather than a boolean here.
This can just be `throw throwable` since we check if it's not null immediately above? Or else we can remove the check and make it just an `else`
I think we have this code now in many places, we should factor it out into `ESTestCase` or a test helper or something.
We can avoid arraylist resizing by creating it with `diff.different.size() + diff.missing.size()` and then doing `addAll` for the diff parts
(Not part of this, just asking hypothetically)
thanks for improving this, this part is easier to read now IMO.
Personally in code like this I would just throw AssertionError. it make code uglier in many cases to do this, but there are times where its better to be paranoid...
> should we just do the naive thing and handle the last 8 bytes case via a naive loop of writeByte() for each byte, so that the footer logic is only in one place? +1
Or use [`Math.toIntExact`](https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html#toIntExact-long-) on `writtenBytes - checksumPosition` and do away with the extra `if` and `else` (less lines of code)? ``` int index = Math.toIntExact(writtenBytes - checksumPosition); footerChecksum[index] = b; ```
What do you think of simplifying the last few lines with the following? ``` int checksumIndex = Math.toIntExact(writtenBytes - checksumPosition); System.arraycopy(b, offset, footerChecksum, checksumIndex, length); ```
> write past EOF :dancers: +1!
I find it really confusing to have `footerChecksum` at the top-level that is a byte array and `footerChecksum` here that is a string, with only `this.` to indicate which one is being used.
this is a different issue, but while we change this line, I think we should also set the `previousTypes` variable :)
By the way should we throw an error if the loader generates a null value? I'm unsure if we need it and it makes the cache harder to manage (or just bad if we keep on regenerating null values)
I always think of `Iterator` as sitting between two entries rather than on the last entry it returned. But I see your way of thinking and am fine with keeping `current`.
I suggest that we take advantage of this change to remove support for time-based expiration, which we don't need
I was just trying to see how we could make some variables final so that the compiler ca help us. In addition this one variable looked to me like you would almost always want o use a custom weigher.
I think the unlock calls should always be in a finally block
for some caches it would be nice to make sure to not compute twice the same value
For things like fielddata I think it's an important requirement
Any interest in implementing `Accountable`? Accounting for caches is a thing we like.
ok let's avoid the concurrent put/computeIfAbsent issue for now, we can try to improve in the future if we observe slow concurrent access
I was getting confused by invalidateAll - on second inspection you hold both locks when clearing the maps.
I don't think it changed the readability much - it made the checks simpler but then it left me wondering why two variables were needed. I was doing the "why does this have to be here, let me think hard about it" think.
That'd be a "no" then.
Could you explain why this is needed instead of checking `expireAfterAccess <= 0`? I think it'd make the class more readable.
Or not. It looks like you are allowed to modify the segment's innards while you have this lock.
yeah, I was thinking we could validate the input with a regexp first, but maybe it's easier/safer to reimplement the parsing logic
Can we fix callers to concatenate streams instead of iterators? (which is already supported in the JDK)
I wonder if it now makes more sense to have this setting on a shard level (since shard owns the inactivity). We can fall back to the old, node level settings (or just remove it if this is 3.0 change only and deprecate on 2.x)
I know it's not part of this change, but it bugs me (for a while now) that the indexing memory controller owns the buffer size for active indices but inactivity is controlled by the engine/shard. I wonder if we should move these settings to the memory controller.
can we do this once we check the write is indeed allowed? Also I think it will be clearer if we have this in a dedicated method (`markLastWrite` or something like that)
I wonder if UnsupportedOperationException is a better choice here- we don't have an indexwriter...
I don't think this doc is correct - the return type is void
can we make this log include the decision about whether to refresh (and also the current IW memory consumption, while at it)
meh - missed the addition of the other one (just saw the removal of the log in the if), but yeah - one is nicer..
Can you move the getAndSet to its own if statement? It has a side effect and its mixed with stuff that doesn't and when I first read the code I didn't notice it.
I'm used to wrapping debug logging stuff in `if (logger.isDebugEnabled())` tests to prevent the message construction in the (very common) case that debug is disabled. Here it probably doesn't matter because message construction is dwarfed by the refresh call.
fare enough. It would be a settings per index, but I get your point.
I think we should return active.get() == false and also - set it if we became idle...
The name of the method confuses me a bit because it's not an "update" method but rather a "remove specific blocks" + "add" method. This seems to be specific to some index blocks in a given context (restoring an index) and this method is located in `ClusterBlocks.Builder` class... I think the removals should be explicitly located in the `RestoreService` instead.
Yes, but you can change this setting during restore and it would be nice to test changing settings during restore anyway.
I am not sure RestoreService would be the right place for it since addBlock would need to be moved to the same place and it's currently used all over the place. I don't have an issue with renaming it to `addIndexMedataBlocks` but since IndexMetadata is the only parameter, repeating IndexMetadata in the name might be redundant.
So +1 on renaming and pushing like this
``` java System.getProperty("es.logger.level", "INFO"); ```
So what does this _look_ like? I imagine it doesn't look any different unless you set the logging level to something lower-than-default.
That'd be cool! I was hoping something like ``` java Files.write(loggingConf, Arrays.asList( "logger.test: INFO, console", "appender.console.type: console"), StandardCharsets.UTF_8); ``` would be possible. Either way is cool with me.
Thanks for writing this. Wonderful.
Ooops - that method is new in Java 8 and you'll be backporting this to 2.0 - so you'd need the `StandardCharsets.UTF_8` anyway. I still think its marginally easier to read my way because you don't need `getBytes` and `StandardOpenOptions.APPEND`.
There's a bit of a problem here. A builder as it is presented here, is a stateful construct, meaning, you use a builder to build a "configured" instance of a processor. But the passed in registry should not be stateful... effectively, the builders in the passed in registry should act as "factories". either separate the two notions (builder & factory) or have the registry hold "prototype" builders and then change the `void fromMap(Map)` to `Processor build(Map)`. Personally I like the separation of the constructs - builds clear API with clear roles and responsibility for each class. But core moved more towards the "prototype" direction, so perhaps a prototype here will be more aligned with core.
it's just yet another unmanaged thread in the system..
well.. as I said, **IMO** the separation is cleaner. But if you keep the prototype way, you'll eventually need to keep the current `build()` method and introduce a new `Processor build(Map)` method **next** to it... it's a bit messy tbh... the same applies to the `Pipeline.Builder`. Another option is leave things as they are (so leave the current `void fromMap(Map)` method) and then introduce a factory to a builder... however you look at it, we need a factory somewhere... a construct that is registered by "type" that can create the appropriate builder/processor for that type... something along the lines of: ``` Processor.Builder builder = registry.get("type").create(); ``` Another option is to mandate all builders to have an empty ctor and use reflection to create those. So instead of injecting a set of builders, you'd work with a set of builder classes and then the code will just create new instances using reflection on default ctor.
why do you need `CopyOnWriteHashMap` here? you don't really edit the map, you only replace it.
this whole updater should be based on scheduled executor, where every second you'll run "update" task and when you're done you'll schedule another update task for a second later
I would keep a `Map<String, Classs<? extends Processor.Builder.Factory>>` as a variable on the module... add a `public void registerProcessor(String, Classs<? extends Processor.Builder.Factory>)` method to the module and change this multibinder to a map binder that will just map all the registered processors.
then call here `register(SimpleProcessor.TYPE, SimpleProcessor.Builder.Factory.class);`
wondering whether it'd be better to just call the index `".ingest"`... it'll give us flexibility if we'll need to store other things later on aside from pipelines
Yeah - I'm sure that is what happened. Ok - cool with me!
Just remove it? :)
`Weight weight = searcher.createNormalizedWeight(filtersFunctionScoreQuery, true)` would be better I think as it will include weight normalization
it will also give a chance to the AssertingIndexSearcher to perform additional checks
I think the parallelization should be on per doc level (not per bulk)... this approach will apply to all scenarios (regardless of the number of concurrent bulk requests you have). naturally, doing so means we'll need to block the bulk until all its docs were processes, but that's doable.
> I think in that case the `ingest` thread pool should be the default, in case no thread pool has been configured on a pipeline. yes.. that was my intention with the "default TP"
can we set the timeout to 0 here? otherwise tests half the time takes 1s
can we set the timeout here to 0? in general we always try to make unit tests finish as quick as possible. this one waits for 1s per run.
check listener.isDone() as we don't expect a retry here I think
can we add that to ClusterStateCreationUtils? It might be useful for others as well
check listener is not done
indeed, did not see that.
Oh I see. I think this doesn't really need to be there - the threaded action listener is already setup by the client if needed. See `AbstractClient#execute(action.Action<Request,Response,RequestBuilder>, Request, action.ActionListener<Response>)` : ``` public final <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void execute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) { headers.applyTo(request); listener = threadedWrapper.wrap(listener); doExecute(action, request, listener); } ``` We can remove this in a follow up change (when we plan to clean up TrasnportMasterNodeAction), but simplify the test complexity and remove unneeded threading, let's wrap the listener in the test ourselves with a SAME execution: ``` .execute(request, new ThreadedActionListener<>(logger, threadPool, ThreadPool.Names.SAME,listener)) ```
true. nevermind then
can we check here if the listener is done? (just checking if I got it right this time :))
yeah that is true. nevermind then
If we are deprecating now, we either need to upgrade the mappings in master, or throw an error? The current code would allow still specifying the setting? I think we should error on new indexes (at minimum 3.0, although if we should error for 2.2+ or something, I dunno). But the deprecation warning should be for all indexes that still "support" this setting.
this needs to be synchronized.
Why not have the standard to string? A multiline return value is difficult to work with in a debugger...
we can use in.readVInt() here, no? it's always non-negative... (same goes for other counters and also note that you'd have to change the writeTo message of course)
can we make this return a DiscoveryStats interface which extends both toXContent and Streamable? ZenDiscovery can return a ZenDiscoveryStats object and other discoveries can decide what they want to expose. This also means we need a method `readStatsFrom(InputStream)` to read a DiscoveryStats object from a stream (only the discovery implementation knows how to deserialize it).
OK. I see - valid point. If you're up to it we can implement a whole registry based on NamedWriteableRegistry and NamedWriteable. Alternatively I'm good with hard coding DiscoveryStats to include whatever ZenDiscovery needs (pending cluster state queue stats only now). We can open it up later if we need to.
Re the cat api - I'm fine if this is not included in them btw - this is a super expert stats.
nit cat we explicitly call the other constructor with null? i.e., `this(null)`
Missing `assertAcked()` or call to .get()
I'd rename this method to `writeRandomBlob()` since this is generating random data and writes it to the container
As far as I can (brief check only) they are always null, but it wasn't part of the API to change properties that are not part of the json being parsed. Not a big deal..
You could make it the same with an `else if` instead of `else`: ``` } else if (theAnalyzer != null) { builder.searchAnalyzer(theAnalyzer); } ```
Strictly speaking this is different than the previous code - if none of index analyzer , search analyzer and theAnalyzer is set, we reset the indexAnalyzer and searchAnalyzer on the builder to null. We didn't use to do it...
The `version` field from this change and the `customs` field from a previous change are not incorporated into `IndexMetaData#hashCode`.
Are we really okay with all of this repeated parsing and boxing and unboxing in the calls to `numberOfShards` and `numberOfReplicas`? Am I missing an obvious reason why we are not parsing these settings (the other being `number_of_replicas`) exactly once and storing the parsed values in fields? Also, I think it's best if in general we not invoke instance methods in constructors (it's not in this case, but it can be bad).
Is the `list` completely unnecessary if we just keep track of an index in the `while` loop and repeatedly call `builder#primaryTerm` as in the `for` loop? That will remove an allocation (and then the JVM does not even need to do escape analysis).
can we call this initializePrimaryTerms ? I find allocate confusing here
s/can't used/can't be used/;s/their/they/;s/subtile/subtle/
should we assert that reader.getCoreCacheKey() == engineSearcher.getDirectoryReader()? Forcing the core cache key handling to be delegated to the inner reader could be trappy otherwise
can this also be `IndexingOperationListener... listeners` please
Can this be a `IndexingOperationListener... listeners` that way we don't introduce a hard dependency on IMC
if we remove the removeWritingBytes / addWritingBytes and just use a `AtomicLong` on IndexShard we can move `indexingMemoryController.bytesWritten` into `indexingService.postDelete` / `indexingService.postIndex` and therefore can remove the dependency between IndexShard and IndexMemoryController which I think we should not have, What do you think @mikemccand
can we please use abstract runnable and override onReject() to ensure we don't barf if that threadpool shuts down.
those are hard to debug I can tell u :dancers:
these can go away if we cut over to a `IndexingRequestListener...`
can we have a simple unittest that equals and hashcode don't violate it's contract? ie if `p.equals(q) then p.hashCode() == q.hashCode()`
lets just use IOException that's much better for this purpose and it's checked
this should just throw IOEXception no need for a shadowing ConfigException
the `grok` field can be final too
these unit tests are great! We are going to need more of them :)
(we just load all files in the `ES_HOME/config/ingest/grok` directory)
I think you still can add the named patterns, since the files are still on the class path? ``` java grok.loadFromStream(getClass().getResourceAsStream("/config/ingest/patterns/aws")); grok.loadFromStream(getClass().getResourceAsStream("/config/ingest/patterns/bacula")); // next file ```
s/HashMap<String, Object> fields/Map<String, Object> fields
@talevy Can you extract this IOException change from the PR and commit this to the branch? I can then benefit from it in the geoip PR too.
+1 lets make this consistent with how now things work in IndexModule.
Maybe change the fields type to `Map` instead? The fact that it is a hash map is an implementation detail.
IllegalArgumentException is unchecked no need to put on the interface you can just throw it
s/HashMap<String, Object> expected/Map<String, Object> expected
really I think we should not register class instances but rather instance of a the factory or rather use a simple interface we define ourself? like look at how IndexModule works now
why do we have this API bloat with all the Builders? Can't we simply use a constructor? All the factories etc are not really needed IMO
I don't think we should make the patterns dir configurable? Outside the ES_HOME directory ES has insufficient permissions to read files. I think the patterns dir should always be `$ES_HOME/config/ingest/grok/patterns`.
What if we just load the grok expression only from the config dir. (`ES_HOME/config/ingest/grok`) We just make sure that when we create the distribution we also package the the ingest config into the zip file. Instead of loading stuff from the class path we can get it via the `Environment` class. (check the geoip PR how it is injected there) This has as a nice side effect that users can also define their own named grok expressions without us adding extra code.
there seems to be similar issues below
(similar naming consistency issues below)
there are more similar problems below
can we make the list immutable for safety? using Collections.unmodifiableList
This will fail the build, `ThreadLocalRandom` is forbidden. You can use `Randomness#get`.
``` java assertThat(provider.fetchCount, is(1)); ```
``` java assertThat(provider.fetchCount, is(3)); ```
Can you remove this annotation? It's not needed anymore and this will fail when you'll rebase on master.
Can you remove this annotation? It's not needed anymore and this will fail when you'll rebase on master.
``` java assertThat(provider.fetchCount, is(2)); ```
Should this be ``` java + final boolean enable = settings.getAsBoolean(SETTING_CLUSTER_INDICES_CLOSE_ENABLE, true); ``` ? Settings can't really be cleared right now but if they could be this would stick the value to whatever it was set at before clearing. Maybe it doesn't matter but I'm certainly more used to seeing the default value here.
I'd recommend using the same syntax Lucene does: ``` bq.clauses().iterator().next().getQuery() ``` Just to follow their conventions
I think we'll run into an NPE in the failAndRemoveShard code: ``` private void failAndRemoveShard(ShardRouting shardRouting, IndexService indexService, boolean sendShardFailure, String message, @Nullable Throwable failure) { if (indexService.hasShard(shardRouting.getId())) { ```
we used to protect agains null here -> I think we should still do this? I'm thinking about failures that happen during index deletion..
`engine failure` -> shard failure
nit: since we're moving away from the word fail in the name, can we rename `ShardEngineFailListener` to `ShardEngineEventListener`
++. "Concurrency level" is always 255 now. Kinda.
thanks for digging in. i only dug into the aws one and have not looked at the root case of the gce problems. If it involves weakhashmaps, maybe its something easy we can fix for them as well to avoid pain.
i mean, maybe we can fix their code to not need setAccessible at all? I can't imagine why it would need that.
buildInListeners -> buil_t_InListeners (and in other places..)
copy paste :)
This should be `eventListener.indexShardStateChanged(indexShard, null, indexShard.state(), "shard created");
can we move this back into the try? I'm worried that exceptions wouldn't release the shard lock .
why did you take this out of the try catch block? I'm fine with keeping this as is, but then the catch IOException clause is probably redundant: ``` catch (IOException e) { ElasticsearchException ex = new ElasticsearchException("failed to create shard", e); ex.setShard(shardId); throw ex; } ```
These cause compilation errors for me...
same holds for other logs bellow...
I know that this is how it used to be but we add an explanation that this is called before the index is added to the cluster state? created is misleading.
Since we now use a logger with an index prefix , this will result in double index name logging `[index][index][0]`
ok @jpountz then we will also have to set it to the final lucene query, cause at the moment we I think don't do anything with what we parse or allow to set... this whole parse business is misleading unless we do something with it. Having setters is a different story as they are in the base class, which we did to avoid handling boost and _name separately on each query.
and actually a boost on a match_no_docs query can matter when used as a sub query as it will contribute to the outer query weight
IMO consistency is important... if we state that all queries accept a boost and a name, we should be consistent here as well. This is important specially from the client/user perspective where often they have defaults for commonly supported features...
here too, as well as the following error messages, they all need to be adjusted
cannot specified here as well
regardless of where boost is, isn't it ok if we replace only when there's only one occurrence of it in the string query? Otherwise we skip the test? I think it's a best effort that should be ok 99% of the cases... unless I am missing something
Ah! Well if its a huge pain then its probably not worth it. Its just that if you do something drastic one of the simplest metrics to make sure you didn't break anything is to count the number of tests. And if some tests skip sometimes and not other times you have to manually validate those tests. Its not worth optimizing for this case though if its difficult.
Or even something like `containsString("unknown field [bogusField]")`!
nit: if you use assertThat(e.getMessage(), containsString("bogusField")), when this fails the error will be much more digestible than just "expected true found false"
oh right....that is what I was missing, inner queries may come with their boost as well, would work only for leaf queries :)
right I had missed that previous check, sounds good then
Can you regenerate the query in this case? Skipping tests based on something random can make refactoring like the `@Test` work that I just did difficult. Even though this is super rare.
Somewhat simpler: ("timed out while retrying [{}] after failure (timeout [{}])", action, failure) . I'm doubting between DEBUG and WARN for this log...
nit: can we move this next to the other predicate? I think it's good to have them together.
we can throw exceptions on the background thread and the test should fail with uncaught exceptions on a thread.
can we increase the timeout on the request? if one runs a debugger the test may fail to retry because a timeout happens, making it confusing.
ok. fair enough.. > On 04 Nov 2015, at 13:22, Yannick Welsch notifications@github.com wrote: > > In core/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java: > > > @@ -48,6 +50,19 @@ > > - A base class for operations that needs to be performed on the master node. > > */ > > public abstract class TransportMasterNodeAction<Request extends MasterNodeRequest, Response extends ActionResponse> extends HandledTransportAction<Request, Response> { > > - private static final ClusterStateObserver.ChangePredicate masterNodeChangedPredicate = new ClusterStateObserver.ChangePredicate() { > > yes, but then it cannot be static anymore (as inner class does not allow static member) > >  > Reply to this email directly or view it on GitHub.
boo! :) just ignore :)
I think we can simplify this and make sure we have 1 shard, no replicas.
I think we can clean up the http/transport/gatway settings here
argh, I forgot UnicastZenPing doesn't support local addresses by default. Sorry for the noise.. (we should fix this at one point)
fair enough, leave it.
oops. Missed it. You're right.
if you want to know i happened, let's use logging..
I know this is how it used to be, but can we make the if be more like the `masterNodeChangePredicate` name and check the the master node is not null and have changed? (we now test for a cluster state change)
lets' introduce a dedicated exception for this. We can upgrade discovery.zen.NotMasterException to be in the cluster package and use that.
can we make these final fields on the parent class? they don't have any state.
using ActionListenerResponseHandler will simplify this lightly.
nit: reading all this makes me think if we could get parseContext.parseFieldMatcher() once with a shorter local name at the beginning and then shorten the lines here a bit. Just for readability. I know at this points it's probably some tedious search/replace action, just throwing this in as a thought.
Can you switch this around and use the preferred name as first constructor argument? This way it looks like there's something special with this field, which I guess its not.
Can you have a quick look again if this should be MATCH_PHRASE_PREFIX_FIELD instead? In which case it would also be good to catch this in tests if it is wrong.
Good to read, but this more or less repeats what ParseFieldMatcher.match() already does. I think it just adds another stop on the road to the actual match-implementation, which is not even in ParseFieldMatcher but in the ParseField itself. I was thinking about simple shortening like `matcher.match(currentFieldName, PARSE_FIELD)`, but I think its fine the way it is right now in the PR, not worth going trough all the files again IMHO.
Maybe update, looks good to me now.
To me it looks like type/types is for the array version, type/_type for the single key-value version. So I'm not sure if "types" should count as deprecated name.
While using ParseField also for values is useful, I'm wondering if it might be confusing on the long run. I would at least rename BOOLEAN_FIELD and the following to something else, since technically they are not fields. No strong opinion though.
This was called "path" before.
I'm not sure if this might be a case where both names are allowed, at least the doc state both version, not mentioning any deprecation.
Took me a while to understand why this is here: so I think NAME_FIELD so far was used only in the short version of prefix query, which is: `{ "prefix" : { "user" : "ki" }}`, so by deprecating "_name" it would throw exception in STRICT mode. So by using AbstractQueryBuilder.NAME_FIELD in the allowed position and this one in the deprecated short version position this should be okay. Can you rename it to something like DEPRECATED_NAME_FIELD or something though? Maybe somebody else should look at this also.
Maybe rename NAME_FIELD and BOOST_FIELD here as well, since they are only here to support the deprecated short version.
After going throught the whole PR I'd probably not suggest this anymore, unless you have some nifty awk / ide search&replace magic at hand. Sorry for the noise.
we can remove this stuff: https://github.com/elastic/elasticsearch/pull/12966 it's deprecated since 2.0, no reason to keep the deprecated version of it.
maybe verify that adding a fields under an existing field works? (for example under `fizz`) and also verify that getting an object field returns the entire map? (for example fetching `fizz` should return a map with 2 entries (the existing one and the newly added) )
maybe use `Booleans#parseBoolean(...)` here? (even though it is outside the ingest package, we can add a todo here mentioning that we may need to fork it later on when moving ingest to a dedicated library)
wish a boolean could just be `true` of `false` :) but we have this logic somewhere else in `Booleans#parseBoolean(...)`
instead of if statements maybe use switch statement? ``` java switch (toType) { case: "integer": // do stuff break; case: "float": // etc. } ```
can't we just call this feature `trim`? `trim` personally makes more sense to me.
what can be done here is that the regex can be compiled in the constructor: ``` java Pattern pattern = Pattern.compile("my_regex"); ``` Then in the `doGsub` method, you can do this: ``` java Matcher matcher = pattern.matcher("my_string"); matcher.replaceAll("replacement"); ```
maybe move all these if checks to execute method? (I personally find that easier to read, when we get in do\* method we know we're good to go) Like this: ``` java if (update != null) { doUpdate(data); } // next if ```
maybe wrap all these maps in `Collections#unmodifiableMap(...)`? To enforce that no changes can be made.
if we can assert that, it would work for me too
the test makes sense indeed. +1 to ban Query.setBoost in src/main but not src/test like @nik9000 suggests. I think this should work if you only touch dev-tools/src/main/resources/forbidden/core-signatures.txt instead of all-signatures.txt.
no need to remember all individual boosts, you can just multiply: ``` java float boost = 1f; while(query instanceof BoostQuery) { BoostQuery boostQuery = (BoostQuery) query; boost *= boostQuery.getBoost(); query = boostQuery.getQuery(); } ```
that works for me, let's just make sure that we have a unit test that sets both the slop and the boost on a phrase query
let's have an assert and drop the branch
I'm ok either way, just proposed this one because it looks simpler to me
This method takes a phrase query and is supposed to create an equivalent phrase query that just has a different value of the slop, so we need to transfer the boost? Maybe the method should look like this now: ``` java private Query applySlop(Query q, int slop) { float boost = 1f; Query underlyingQuery = q; while (underlyingQuery instanceof BoostQuery) { BoostQuery bq = (BoostQuery) underlyingQuery; boost *= bq.getBoost(); underlyingQuery = bq.getQuery(); } if (underlyingQuery instanceof PhraseQuery) { PhraseQuery pq = (PhraseQuery) underlyingQuery; PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.setSlop(slop); final Term[] terms = pq.getTerms(); final int[] positions = pq.getPositions(); for (int i = 0; i < terms.length; ++i) { builder.add(terms[i], positions[i]); } pq = builder.build(); pq.setBoost(boost); return pq; } else if (underlyingQuery instanceof MultiPhraseQuery) { ((MultiPhraseQuery) underlyingQuery).setSlop(slop); return q; } else { return q; } } ```
There's just `HighlighterSearchIT` IIRC.
I think you might be able to pull this out into a helper method.
Ah! The star imports come back. Its fun watching these things wash in and out like the tide.
Errr. Oh man. I should know this one but I don't!
Does this change come from the boost change or something else? It feels related but not the same.
I usually prefer the way without the else but I don't object to either one.
Yeah! Good point. Maybe applySlop is called after applyBoost? That isn't a good reason, but it might be the reason.
OK - I think we are double counting the boost here. Its hard to tell though.
Nah - you are right. Don't fight the IDE - without a strict policy its just silly. And we'd need a nice IDE configuration to make them never happen.
Ah! Got it.
thanks a lot for taking care of this, I couldn't figure out why this tests was so slow :)
maybe we should had a LoggingShardFailureListener instead of the the default noop. That would mean we can only log trace here and allow the listener to decide what to log, potentially adding information like what it is going to do due to this failure.
I'm wondering if the ReplicaPhase shouldn't implement the ShardStateActionListener interface it self, which will make things simpler and save on the param.
Just a formatting issue here. Add a space before the curly bracket: ``` java if (cannedACL == null || cannedACL.equals("")) { ```
space before curly bracket
Same formatting issue as below
Throw the exception here
I'd throw the exception in `initCannedACL()` method instead of checking for a `null` here.
Can you use Junit's assert so that it also works if assertions are disabled? (I think we randomize this)
or just: "return ok;"? :)
minor note: we consider shards inactive until the first indexing operation has happen, so I think this part is OK regardless of the change.
cool. good to know. thanks
Is there anyway of outputting the name(s) of the fields we couldn't load doc values for here? Probably useful for debugging
Oh I see: I think `.resize` is OK here since `BigLongArray` is paged ...
Builin -> Builtin (forgot a 't')
I meant empty as in the settings are empty.
I like dummy because it implies fake and the index is fake - not just empty.
tokeinzer -> tokenizer
This is great! No more @Inject!
Would be nice to have a better message with this assertion, perhaps "thread pool type must be one of [cached, direct, fixed, scaling]"
Can you expand this to the full form, it's too easy to accidentally lose it without the `{}`
:( Or I guess, reverse winking frowny face );
Nevermind, we already have docs for it in the actual docs, so we can leave it out here.
Eh, I'm +0 on adding it, so it's up to you :)
can this be a norelease
just make it a norelease
That's just a minor thing but I think the recommended order in the Java styleguide is `static final`.
That's just a minor thing but I think the recommended order in the Java styleguide is `private static final`.
That's just a minor thing but I think the recommended order in the Java styleguide is `private static final`.
In case you don't need the parameter, you can remove the null check as the constructor already checks that as a precondition.
I'm still unhappy about this: if for some reason you need to add another wrapper, it could hide the FilteringGeneratorDelegate and JsonXContentGenerator wouldn't know anymore that the content is filtered. I'd rather like something that does not depend on "instanceof".
Do yo need the parameter `filters`? The only call of `evaluate`just uses the field `filters`.
watch out with back porting..
Heads up when you merge with master, I just merged another test where the original test query is modified assuming it is json, so that will need the same treatment as you do here I thinkg: https://github.com/elastic/elasticsearch/pull/14255/files#diff-9dc314365d49d84bff0645c2f9dfd7adR356 (and Overwrites in HasChild/HasParentQueryBuilderTests)
This does not compile; `FakeRestRequest` does not have a constructor with two arguments. This is a recent change and I think you just missed it when you rebased.
This file needs a license header.
Thanks for getting this case too!
In these cases its acceptable to use randomize testing's `rarely()` or its like to cover either branch randomly.
I think part of the bug is also that we don't have an `else` statement here where we throw an exception. Which means that if you provide a parameter with eg. a boolean value (for which we don't check above), it will be silently ignored. Can you add such an `else` statement (you can look eg. at MissingParser for an example).
I tend to find the '==false' way easier to read
Should we just use `parser.isValue()` here instead? I think we do this in other places that we are coercing the value into a number
talking about not using clone here, but simply doing `new QueryShardContext(...all the needed deps)`
Set to true to ignore unavailable snapshots.
describe what the parameter is for
clean-up imports, these new imports seem to be unused.
Same as in GetSnapshotRequest, replace this by "Set to true to ignore unavailable snapshots"
Might be better to use the default of the request (in this case this coincides, but explicit is better in case of refactoring): getSnapshotsRequest.ignoreUnavailable(request.paramAsBoolean("ignore_unavailable", getSnapshotsRequest.ignoreUnavailable());
please don't use String concatenation in logger calls, but positional parameters, for example `logger.warn("failed to get snapshot [{}]", ex, snapshotId);`
Snapshot could not be read
I think this check does not add much (I would skip it)
I wonder if we should this api to the forbidden api pointing people at the utilities... (like we did in the write variants).
instead of suppressing the entire method can you wrap the forbidden calls in static methods in here and only suppress forbidden there? also applies to the other places :)
can we call these indices "short_delay" and "long_delay" ? I think it will be easier to read.
Don't get me wrong - I like the loop above - I just don't think is sufficient to prove to ourselves that we recreated the problem.
This looks like the exact same implementation...can `BaseGeoPointFieldType` just be renamed and used in both classes? There isn't a reason the field type needs to be a static inner class of the mapper, they are just there for convenience since they are so small and tightly coupled with the mapper parsing code.
Where are these mapper helpers used? I've only found them used by tests. Maybe we don't need them? I removed a number of them before for metadata fields.
nit: no need for extra parens
w00t thanks !!
nit: assigne shell.size() to a local var? seems safer to me :)
I was referring to the fact that the setting netty uses are the result of a merge of multiple things: ``` // merge fallback settings with default settings with profile settings so we have complete settings with default values Settings mergedSettings = settingsBuilder() .put(fallbackSettings) .put(defaultSettings) .put(profileSettings) .build(); ``` If we can make that static and available we can now for sure they use the same thing, even if people change stuff.
guys take this offline if you want - let move here.
I think we should separate the two and push this as is. Your code refactoring has more changes than this functional change and on the security end I think we should be careful. let get this in and cleanup the stuff afterwards
I guess it is a design decision for future integrations to have a Map for the config instead of a proper java object? If we had the latter (e.g. a Configuration object) the readStringProperty that is now in ConfigurationUtils could be part of that object. Seems a bit cleaner...not a big deal though
make the error a bit more understandable for users? Like "processor x doesn't support some of the provided configuration parameters" and list them like you do already...
I thought I saw some `List<String>` already but I can't find it anywhere, seems like we will add that when we need it then.
I think it makes sense to do it here too, especially the need for remove especially should be contained in the utils method I think.
ops turns out I had seen it because I was working on it for the date processor, i will add the needed method for string arrays.
wondering if we should add the first 10 shard ids to the explanation
OK. Just double checking :)
can we call it overrideStatus and add java docs? Also if you feel strongly about it, you can only do this in the 2.x back port and do something else on master.
I think it will simpler to have a status variable and set it in the constructor to whatever clusterHealth said. Then this becomes a simple getter and setClusterHealthStatusOverride becomes a simple setter (setStatus)
it would be great if we can get more info about why this reroute happened. Maybe add a String reason parameter to the reroute method? This is called in many places for different reasons. (node left/join/shard store fetched etc..)
watch out with the back port! needs to be wire compatible with 2.0 ..
same here - would be good to have a brief summary of the shard that failed and why.
`dangling indices allocated`
I think we can just tunnel through the reason
this should actually be "elected as master"
I think we can make this method even friendlier with the following signature: `logClusterHealthStateChange(ClusterStateHealth previousStateHealth, Result result, String reason)` .
we typically just use string concat. I think we can just do "shards started [{}]"..
make that `deleted indices [{}]` (the list can be long pushing the deleted to the end of the line )
with inflating `indexShardLimit` by 1 in `canRemain`, this message might be confusing.
Here it still says `on a per index basis` -> should be corrected.
oh nevermind, I just found the method that called it with null :)
What if the user specifies 0 here? Previously that meant unbounded.
I think the following would read better to me: ``` java if (nodeShard.index().equals(shardRouting.index())) { nodeCount++; } ```
Same here, I would call this "build_hash"
nit: this is normally called the "short hash" so reversing it sounds very odd. :)
oh I see ok - I think we should remove that at least in master since I think it's still there but shouldn't
sure, maybe a separate discussion but I guess we will need to log something sooner or later. anyways we can discuss this later on. let's get this in first
maybe log it debug or trace? not sure how many times this can happen...
You don't even really need to say "a TimeValue" here because its in the return type. Its only important when its a long because you don't know if its millis or nanos.
I think two separate PRs would make sense - one for Azure and another for S3.
I feel like this can be simplified further, but this is a good start.
This should probably be private.
can we please move this into the engine and don't add another service to our already overdesigned system. I try to hard to fold them into other classes and the 10 SLoC are not worth it really.
Another `-1l` I'd consider replacing with the field [proposed previously](https://github.com/elastic/elasticsearch/pull/14651/files#r45225330).
This is an example where I think that using the field [proposed previously](https://github.com/elastic/elasticsearch/pull/14651/files#r45225330) might clarify things in the sense that it will be easier to search for all usages of that field rather than `-1` or `-1L`.
The name change did not go through, but there are now methods `writeZLong` and `readZLong` in master that support negative longs.
Thanks for bringing that to my attention. I think that at a minimum the method should renamed. I've opened #14780 to do this and add a method for encoding any long.
A lot of the instantiations (of the derived classes) in the tests and a few other places that ultimately pass through this constructor use an explicit `-1`.
Nit: "are a" -> "a".
Nits: "based class" -> "base class" and "involves are a" -> "involves a"
I am very convinced it should go into the engine ;)
[Field](https://github.com/elastic/elasticsearch/pull/14651/files#r45225330)? These are tricky to find right now. :)
Here's another place to maybe use a [field](https://github.com/elastic/elasticsearch/pull/14651/files#r45225330).
I'd consider replacing the usage of `-1l` in this line and the prior with the field [proposed previously](https://github.com/elastic/elasticsearch/pull/14651/files#r45225330).
do we also want to have the publish address just to be sure? that's what nodes typically use to communicate with each other.
I think the extraction here is redundant no - just pass the transport service...
can we add java docs? this starts to be confusing...
I saw it. Just wondering how `path.isEmpty` works as I would expect `path.isEmpty()`.
Should it be `isEmpty()` here? Wondering how this is compiling but I might be wrong.
yea I missed that, I think it makes sense to do it there too.
now that we have `GsubExpression` (hurray!). we should probably define an equals, where the `pattern` field is compared using the `.pattern()` command since that seems to be the proper way to check equivalence between `Pattern` objects. then these assertions would be simplified as well.
I'm a little concerned about all of these "choose a random version" because it will severely reduce the test coverage for the new code? If we are concerned about backcompat, lets have dedicated backcompat tests for pre 2.2 behavior being maintained...
Ok, I'm fine with that..
Well, I think this needs to be fixed here. There is no index created version in field data settings, this is an artificial thing that it sounds like you have added to workaround some other issue.
We should be writing out the settings in the "new format". There is no longer index_analyzer. So in the case of search_analyzer being set alone, when we serialize, we should write both analyzer and search_analyzer.
You can simplify this to: ``` boolean writeSearchAnalyzer = // logic if (writeSearchAnalyzer || analyzer logic) { // write analyzer } if (writeSearchAnalyzer) { // write search_analyzer } ``` This will also keep the same order (analyzer followed by search_analyzer) that we had before.
it doesn't reset the ignored list..
there is an `hasUnassigned` method already, so yeah, I'm +1 on being explicit here...
imho having `hasUnassigned` and `hasUnassignedShards` methods is very confusing.
+1 to not swallow the original exception
Just discussed it with Robert and indeed this fsync is not necessary.
maybe it would be useful to log the exception message with this warning, just to know what could be the reason for the failure
oops right, createTempFile not only creates a file object but also on the filesystem
typo: "always called" twice
we typically follow the package name as setting prefix. So `action.support.replication` here.
this is dangerous. I'm not sure we can rely on this. Also testing the exact amount tests generic Transport functionality. I don't think we should do it here. Just keep it simple.
Just curious, if not, I think maybe it should be `failure_timeout` instead of `failure.timeout`, but it's a very minor change either way.
I think no timeout is a good default for now.
we swallow the exception from the master here. We should do something with (log it?). Also - I'm only OK with committing this now as is (i.e., potentially ignoring shard failures due to timeouts) if we plan to follow up with some proper retry mechanism in the next step. If not, I rather not have a timeout for now.
This changes the empty options.. no good..
in generation, this should be a variable that's resolved and cached. We shouldn't do settings lookups with each op.
nit: iff does not need an "otherwise". Either make iff to if or remove Otherwise.
Remove full path from ShardId (it's imported) `* @see #resolveCustomLocation(IndexSettings, ShardId)`
Oupss, I missed that. Thanks
It would be nice to explicitly test on which platform getSystemCpuLoad returns -1, if any, like it is for the load average
This change breaks backward compatibility between 2.1 & 2.2 (pull request is labelled v2.2.0)
this seems to ignore the incoming options param
Oh nevermind, it's private, just visible here.
Seems strange to have no only a `builder()` method, but also be able to do `new TransportRequestOptions.Builder()` to me
I don't think that we should move this code into the InetAddresses.java source file. That code is from the Guava code base, and is licensed to the Guava authors (see the license header). By moving this code which is not from Guava here we will create a confusing situation with respect to the licensing of the code. Let's take this code to IpFieldMapper.java.
I don't think that this is right since it will match the string "256.512.256.512/33" but that is not a valid IPv4 address/mask. While there are regular expressions that can correctly capture an IPv4 address, I'd prefer that we just write a simple state machine that parses an IPv4 address because it will be vastly simpler.
I'm tempted to also do it in a follow-up pr as this code is really just moved around.
Also captured in #14862.
can you use `null` instead of POSITIVE/NEGATIVE_INFINITY? this is the way the rangeQuery method expects to be notified about open bounds
This will silently ignore octets that are captured by the regular expression, but are in fact not valid octets.
+1 > On 18 Nov 2015, at 16:34, Daniel Mitterdorfer notifications@github.com wrote: > > In test-framework/src/main/java/org/elasticsearch/test/ExternalNode.java: > > > @@ -233,7 +229,11 @@ synchronized boolean running() { > > > > ``` > > @Override > > public void close() { > > ``` > > - stop(); > > - try { > > - stop(); > > - } catch (InterruptedException e) { > > `#close()' could either be invoked directly or from within a try-with-resources block and I did not come across either. > > Unfortunately, bubbling up is not an option as we implement Closeable from the JDK. Hence, I sensed restoring the flag is the most sensible option in this case. > >  > Reply to this email directly or view it on GitHub.
+1 to throwing the exception.
I think the test should fail. We don't know what else have happened due to this interrupt nor whether the node is ready.
Looking what this does, an interruption is ignored and will only used to cause an extra TTL check or shutdown. I think doing nothing is the right move here.
Imho, the interruption is dealt with here. We don't need to bother the code higher up with it.
same here - since we have on onFailure handler, calling is the equivalent of re-throwing the exception, imo.
OK. I'm good with fixing this inconsistency by throwing an InterruptedException (and catching it up just like we do with LockObtainFailedException
I like the type safety!
I think @talevy is talking about the `value` and not `path`. I think it is ok to add a null key.
Update the delay _left_ ..
I don't think we need the unassignedDelayedShards anymore, right? now that findSmallestDelayedAllocationSetting only takes delayed shards into account.
+1 to capture `System.nanoTime()` at the beginning of the method
can we capture System.nanoTime() at the beginning of this method so all shards use the same? it's not broken now, but will make it easier to reason about.
can we replace the Math.max with an assertion? it should never happen and we shouldn't protect for it.
getLastComputedDelayInNanos -> getLastComputedLeftDelayNanos
Oh, got confused , which is the point :) getDelayCalculationTimestampInNanos -> getUnassignedTimeInNanos
missed that. Nanos is strictly speaking better, but not a biggy.
Yes, that would be clearer
Let's rename the setting `registeredNextDelayMillis` to make the unit explicit
Let's call this `timestampMillis`
Maybe call this `delayTimestampNanos`
I think this should have `delay` somewhere in the name, it's kind of confusing to have two `getTimestamp*` methods
Again, putting the unit in the name would help here, unless someone reads the docs they can't tell whether it's millis or nanos
Again missing units :(
I also think we could now make this static and unit test _just_ this method, which would be really helpful
oh hahahah, I can't read, that's an L
I think we might want to add this to the docs for delayed allocation (just so users are aware)
getDelayCalculationTimestampInNanos -> getLastComputedLeftDelayNanos
a nice side effect :)
why did you decide not to do the approximation we talked about? i.e., `System.nanoTime() - (Math.min(0, System.currentTimeMillis() - this.timestap))* 1000000L;`
lastComputedDelayNanos -> lastComputedLeftDelayNanos
timestampMillis -> unassignedTimeMillis delayCalculationTimestampNanos -> unassignedTimeNanos
Can we support milliseconds through an explicit format parameter? We just got rid of this fallback mechanism in date fields for 2.0. It can be trappy when a user has no way to disable this builtin format.
do we really need so many tests? this is just about parsing? It can probably just have unit testing for this..
+1 to _not_ build in another hard coded fall back (we don't know what a number means - might be seconds). I do wonder what goes wrong here. The default date field formatter should fall back to mills: `strict_date_optional_time||epoch_millis` .
this feels weird. We have a parameter and not use it. Can we add java docs and throw and operation not supported if someone used it? (this goes for all of those)
should we have a followup issue for this? I don't think this is the only place we have this problem...
I'm not sure why, but the usual convention for freeing resources when the request is done is to have a method called `finishHim`. Mortal combat reference? Anyway, the nice thing about this convention is that it gives us a place to look for resources to be freed. But above I mention reusing a thread pool of some sort anyway.
Ok - I see where it is called. These checks are a bit too distant for my taste.
s/Assumption: // Its part of the contract for the bulk api.
Should we be spawning a new thread here? Other places ES is pretty careful to use some pool.
Is it right to just eat the exception thrown from the listener? At least log a warning or something.
> Backos off Typo
But the Iterators it returns aren't.
`synthesizeResponse`? I just want something to make it obvious that it the result of squashing together lots of responses.
Also, `.get()` is much more common than `.execute().actionGet()`.
Maybe pull this guy into BackoffPolicy? I figure it'd be useful for anyone uses BulkProcessor in tests. You'd want to make the DELAY configurable, but that is pretty simple.
We typically do this light weight coordination on the same thread. I.e., Names.SAME . This does nothingother than spawn another bulk request. This will cause a new thread to be spawned as we don't do anything else with the bulk pool on the client. To be honest, I don't think the transport client should have so many thread pools. I'll open a different issue for that.
Just wrap and rethrow and let junit report the exception.
> I have just moved code around, so this implementation is not new. Fair enough. I'd still log a warning just to help debug any mistakes in the listener. If all goes well and the listener catches any exceptions then we will never call it.
One day `getMessage` will be added to the forbidden apis. May as well not make it worse.
Throwable's toString eats stack traces. I'd just `throw new RuntimeException("unexpected failure", e);`
I guess `BULK` is the right thing here. Usually BULK is used on the receiving side but these are the same in spirit as its just coordinating things.
`success` just made me think "the whole operation is successful" not "successfully started operation".
These are so common in elasticsearch its not worth mentioning.
yeah, I meant that we do the right thing and back off by default. Can you please change it? sorry for being late..
> I'd rather play safe here and use client-side locking on all list accesses in order to avoid that another change introduces a subtle visibility issue I always get a bit jumpy about just locking things because I don't have a great understanding of when Java is allowed to reorder instructions around locks vs volatile. Its worth looking into more.
Fine by me. I tend not to use `get` unless I'm implementing a method to get a property just because getters are so common in java. If I ever think of the method as building a thing out of state then I don't use `get`. Its kind of silly because the whole point of getters is that you can build the thing. `getAccumulatedResponse` is fine.
> This method is private and only ever called from a single thread so there is no need to recheck. I'm just weary of having the failure handling case so far from the success case. I figure its harder for someone to break it if its closer together.
So I'm thinking of using this in the reindex API which'd make it used inside of Elasticsearch. Taking a ThreadPool is fairly normal for the internals of Elasticsearch. I suppose if you wanted to keep API backwards compatibility then you could make it optional and the whole thing to fail if one isn't provided but a backoff needs it.
Nit: drop the `,` here.
sorry I meant `org.elasticsearch.common.xcontent.ObjectParser` all the time my fault
+1 on using static strings - I didn't realize that the XContentStrings things are not usable when parsing.
if this is done only in tests then the test should advance instead this makes 0 sense to me
this feels weird. I think this partially comes from the fact that the allocation id field name is serialized by this object. Instead I would change the toXContent of this one to start with startObject. Then in [ShardRouting](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java#L719) we can do: ``` if (allocationId() != null) { builder.field("allocation_id"); allocationId.toXContent(builder, params); } ```
can we use the same static Fields we use in other places? ``` static final class Fields { static final XContentBuilderString _INDEX = new XContentBuilderString("_index"); static final XContentBuilderString _TYPE = new XContentBuilderString("_type"); static final XContentBuilderString _ID = new XContentBuilderString("_id"); static final XContentBuilderString _VERSION = new XContentBuilderString("_version"); static final XContentBuilderString FOUND = new XContentBuilderString("found"); static final XContentBuilderString FIELDS = new XContentBuilderString("fields"); } ```
For the record, I'm asking because I expect the setter to be called once while these create methods can be called _many_ times
Oh, I missed that. Thanks for clarifying.
I see. I just tend to encode these constraints in the type system, that's why I've asked.
Ok, than that's fine for me. So overall LGTM.
Why did you choose to make `#randomRepoPath()` static? As you pass in the test case instance to `SnapshotSharedTest`anyway? I assume you decided that because the implementation only relies on class state, not instance state.
Would it make sense to change the signature of `ExternalNode` too, so it takes a `Version` parameter instead of a `String`? This would ensure at compile time that we pass really just a version instead of an arbitrary string.
This is wonderful! Now `observer#observedState()` is invoked in exactly one place in the source file.
Nit: Can we rename `shardReference` to `ignored`? I don't know about Eclipse, but this will at least prevent IntelliJ from issuing unused variable warnings.
The logging brackets are off here: `[{} to [{}]]`.
Why is this `volatile`? It doesn't look necessary to me.
Nit: The default constructor `AtomicBoolean#<init>()` can be used.
If we're doing a reroute - I don't think we should retry on retryPrimaryException. That one only holds for the primary action.
> it can be but not removing to have a easier time backporting this to 2.x ACK.
can we add to this: "we have to do this after succesfully indexing into the primary in order to honour recovery semantics. we have to make sure that every operation indexed into the primary after recovery start will also be replicated to the recovery target. If we use an old cluster state, we may miss a relocation that has started since then."
let's just skip the iterator and use IndexShardRoutingTable.shards() (put it in a variable and use it in do run as well)
get shards here is an overkill. Let's add a operationRouting().resolveShard() method that returns a shardId.
I think we can reduce the scope of this change by exposing a resolveShardId method that resolves index,type,id and routing to a shardId. And then we don't need to touch these. Also, I know that type is not used now, but why not pass it? is there a place we don't have it? I hope we can back port this change to 2.x, so having type here will reduce the change.
what are testing here? sounds like primaryPhaseExecutesRequest
OK. just saw the shardId() method. Good. the rest still holds ;)
can we rename internalShardId to resolvedShardId ? also add some java docs on how it should be used. I think that , as a separate change we can move the index field to another class DocReplicationRequest that is a subclass of this and a parent of IndexRequest and friends. It will be cleaner. No need to do it now.
we can now use the fancy exception message formatting.
to clarify , the retryPrimaryException is only relevant for the handler of the primary action, not the reroute one. If it comes back from a reroute it means the target node already retried it and we can just fail.
can we have an explicit boolean for this? feels hacky...
I wonder if we should resolve things again here. I think we should make it as simple as possible here. Maybe have a Primary request (or use internal request) which has the shardid already built into it. All we need to do in the action is lookup the index shard, see that we have it and do our thing.
good these variants go away...
exception is in the wrong location :( (was already the case)
nice that this is removed.
can we change the method name to resolveRequestAndCheckBlocks? I had a worry that the request is not resolved by the time we check the index routing table based on request.shardId.
this doesn't mean the index is not active, but rather that it doesn't exist or is closed. I don't think we need to retry in that case. [Old cold would throw `IndexNotFoundException` in this case](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java#L203).
++ on the assert
Nit: I think it will be safer to have this boolean as a parameter and determine the action here. I'm weary of arbitrary string input.
IMO this is over doing this. I think passing just the meta data is over restrictive. ClusterState feels more natural. I would prefer this stays as ClusterState
I'm sorry for flip-flopping on this, but thinking about this again (with the new model we introduced), I think it's simpler to call this just shardId , which must be set either at construction time (for things like refresh/bulk/flush actions) or when the request is resolved. Feels unnatural to call this resolvedShardId when it can be part of the constructor.
I think this should also concatenate the underlying `FlushRequest` `request#toString`.
I wonder if this should just be the implementation provided in `TransportReplicationAction`? It appears there are only two classes that currently provide a non-trivial implementation of this method.
Same [thing](https://github.com/elastic/elasticsearch/pull/14852/files#r47091246) as in `ShardFlushRequest`; I think the underlying request should be concatenated here.
I prefer when the `readFrom`/`writeTo` are symmetrical. Specifically, since we can't use `readOptionalStreamable` in `readFrom` due to `ShardId` only having a private constructor, I think that we should have a symmetrical `if` here in the `writeTo` rather than using `writeOptionalStreamable`. The symmetry makes it easier to check that the `readTo/writeFrom` are in sync.
There should be an `else` clause to set `shardId` to `null` if the stream indicates there is no `ShardId` in the stream.
it's used in 2.x and asked Areek to keep it to limit the scope of the change as I hope we can back port it.
> it's used in 2.x and asked Areek to keep it to limit the scope of the change as I hope we can back port it. ACK.
> whereas passing in the metadata hints at using it to validate the request before performing any operations I'm not sure this hints what you suggest. To me cluster state is simple in terms of thinking - you get everything, do what ever you want. No a biggy though. If you feel strongly about this, let's just keep it.
I think this is the right thing but can we log a debug level log here? this is extremely exceptional for people to index into and delete their index concurrently. We should have some visibility into it. As it is strictly speaking OK from an API perspective (people can do that), I wouldn't go with WARN/INFO, but with DEBUG
maybe also test a nested conditional setup? (So have conditional and then another conditional in the matched or unmatched list)
happens in other tests too.
I think we can simplify this and support other number types too. All concrete number classes implement Comparable (unfortunately Number class doesn't). So instead of converting to a primitive number we can convert to Comparable class and compare on that: This would look something like this: ``` java if((operand instanceof Comparable && other instanceof Comparable) == false) { throw new IllegalArgumentException("Number isn't comparable"); } Comparable left = ((Comparable) other); Comparable right = ((Comparable) operand); @SuppressWarnings("unchecked") int cmp = left.compareTo(right); switch (operator) { case LESS_THAN: return cmp < 0; case LESS_THAN_OR_EQUALS: return cmp <= 0; case GREATER_THAN: return cmp > 0; case GREATER_THAN_OR_EQUALS: return cmp >= 0; } ```
your call on whether to change this, but we also have `Strings.EMPTY_ARRAY`
the outer parentheses are useless. On the other hand useless parentheses would be better spent to make precedence explicit when bitwise operators are involved. so I would do `long mask = 1L << (32 - networkMask);`
what about throwing an IllegalFormatException instead? I'm a bit concerned about catching IAE as this is a very generic exception.
just please don't add one. There are too many classes already.
I'm thinking it could be more user-friendly to suggest a correction, like `"did you mean " + ipToString(accumulator & (blockSize - 1)) + "/" + networkMask` in the error message
ok, fair enough
ok, yeah I can't see a difference either
two license headers? :)
I see, ok!
can we not use underscores as part of method names? (unless we decided that we do and I missed it :) )
My idea was to make the BulkRequestSource hold what it has to hold (the failed items), be able to retrieve them and act accordingly from processBulkIndexRequest, rather than have logic to deal with failures within the BulkRequestSource itself. Exposing different getters might help as well, that's another option. I tend to think that extracting the "processing" part would make things cleaner but I may be wrong.
++ thanks a lot
this is good as we already have a unit test for the filter. Wondering if that current BulkProcessingState object needs its own unit tests outside its use within a filter.
`delayTimeoutNanos` can be smaller than `(nanoTimeNow - unassignedTimeNanos)`. For example if `delayTimeoutNanos` is initially 1 minute, and later changed to 10 sec (after 30 seconds).
use simpler constructor.
use simpler constructor.
this needs to be called SearchTimeoutIT in core
I'd have added an integer to `TypeParser` and sorted them by the integer resolving same integers alphabetically or something. And set the FieldNamesFieldMapper to MAXINT. But I think what you did here is ultimately simpler.
Is it good enough to put this at the end here? Like, if a pugin wanted to add a root mapper.
Maybe leave a message about why iteration order matters here because this is where the reader of this class first sees that it might.
should be NotMasterException? (see method that was removed from class ClusterStateUpdateTask)
same here - can we add a note about the batching? i.e., not batched
In this case you can use the `setSource` version that takes a field name and save a few characters. No big deal either way though.
nit: can you break this into multiple lines so its easier to track w/ `writeTo`
can you provide something that is actually a geo-point so that this does not start failing if we improve validation? (eg. "3,2")
can we reduce visibility? eg. pkg-private
it is not lenient if you throw exception when you don't find something :) I somehow thought that strings would be ok here, to have more flexibility and less dependencies on core. But maybe it is me being paranoid. We can probably start with your approach and see if it's ok.
I wonder if we should have a getMetadata(String name) method rather than all the distinct getters for each one of them. and maybe throw exception if the metadata is not there when we look for it.
maybe omit lowercase from the method names here? (since these tests also run for uppercase and trim)
should we maybe make ConfigurationUtils more flexible to enable its use here? I understand that the error message may want to be customized in this case.
I think the fact that a list with the key name of `fields` is hard-coded here, maybe we should give a more specific name to this? should we call this `AbstractStringsFieldProcessor`? or we can force the factory to implement a `getStringsFieldName` or something of the sort to make the `fields` name to be required to be defined by each implementing processor factory. I understand that there are no other users of this whose field name is not `fields`, so I am also OK with leaving it as is.
Just a note when back porting this piece of code needs to be changed. I think we should use java8 where possible and in this case back porting is trivial and the isolated. Only in cases where the back port change wouldn't isolated and difficult we should hold back on java8 usage.
If `shardsClosedTimeout` is a `TimeValue` then its `toString` should output a sane suffix. Forcing this into seconds doesn't seem required.
`}` and `else if` should be on the same line
e != t :) I think this will be simpler to read if it has a top level with TransportActions.isShardNotAvailableException(t) and then the right handling bellow it.
it is to me, buy hey, taste :) up to you.
wondering if we should enforce immutability on this level... feels more natural to do it in the build()
paranoid! :) (re double immutability)
I am not sure if the iteration order is important somewhere down the road but if we serialize over the network and build another set from it we can end up with different iteration order. If it's not important that's fine just wanna raise it.
lets pass the original set here and make it unmodifiable in the ctor
I think that's OK, at least for now. We only care about member of operations.
"active_allocations" ? +1 on making it a constant.
also we need those _PER_ shard.. so the allocation ids for shard 0, alloc ids for shard 1 etc.
I think this is easier to understand as it makes a 1-1 copy of the current active shard allocations in the routing table: ``` for (IndexShardRoutingTable shardRoutings : indexRoutingTable) { Set<AllocationId> activeShards = shardRoutings.activeShards().stream() .map(shardRouting -> shardRouting.allocationId()) .filter(allocationId -> allocationId != null) .collect(Collectors.toSet()); if (activeShards.isEmpty() == false && activeShards.equals(indexMetaData.getActiveShards(shardRoutings.shardId().id())) == false) { // only update active allocation ids if there is an active shard if (indexMetaDataBuilder == null) { indexMetaDataBuilder = IndexMetaData.builder(indexMetaData); } indexMetaDataBuilder.setActiveAllocations(shardRoutings.shardId().id(), activeShards); } } ```
do we need to expose this? I rather not and iterate using a shardId int. Keep the API surface small.
these do not seem to be used. I don't thing we need them? I rather make sure there is always a set for each shard.
It would be great, if we move to a DiffableStringSet, to make this version return a Set<String> and bellow the hood return an immutable set. This will prevents people from messing with the internal state from the out side.
I don't think this needs to be an AtomicBoolean? Its no longer a member variable so it should already be thread safe.
I think all of these static methods can now be instance methods.
nit: there's already a `.translate()` method, so I would rename this to `translated` or `isTranslated`
++ you're right. Without reverting back to a member variable or unnecessarily implementing something I don't believe there is any thing else.
Ah right, these methods don't touch member variables so they are better left as static. Aside from readability I think in that case no vtable is needed so its better performance? Keep as is!
don't you want to reset first and then set the parseFieldMatcher? :)
I think see the point of not setting the source if it was not modified, but when it comes to metadata, should we just set them back all the time? anyway we end up with a single flag for all of them...
done as part of https://github.com/elastic/elasticsearch/pull/15020 too
I will take care of the simulate changes too.
good point, we will once we have the processor that updates them, which is being worked on.
you mean removing the ensure open? +1 on that. Sadly I see the same out of lock pattern in the Translog class as well: ``` ensureOpen(); ImmutableTranslogReader toClose = null; try (ReleasableLock lock = writeLock.acquire()) { ```
I believe we don't need this now.
this made me worry we don't log these failures anymore.. In this specific case I think we are best to just let the exception bubble up, but it does raise a more general issue - if people put exceptions in the builder, it's their responsiblity to report it. we should probably add something to the internal cluster service to auto log it.
wonder if this is the best way to get a client, can't come up with better ones though at the moment....
Writeable does that too but allows to have final fields and drop empty constructors
ok thanks for the explanation. @s1monw any magic that we can do to fix this? :)
would it make sense to add this nice iterator to core? the ingest plugin could reuse it in this case.
Maybe it always should have been....
Just so you don't have to build the object with so many nulls and throw it away again. Its no big deal.
ok, it didn't have any meaning...
we can also use readOptionalStreamable and &writeOptionalStreamable
we need to do this in the validate method. The list of requests is exposed and people can change them after adding.
maybe be nice and add that you reuse the bulk request. So "refresh is not supported on an item request, set the refresh flag on the BulkRequest instead".
can you remove this one
I think we should remove this if, call deepCopy recursively in any case, so that the main else works in this case too. Again being paranoid, I know...
yes and make it recursive I guess, that sounds good.
Nit: " . " -> ". "
Nit: " . " -> ". "
`Collections#shuffle(list, random())`! :)
The existing `ESTestCase#randomSubsetOf(int, Object...)` should just delegate to this new method here.
Also good. :)
Maybe just clear a range of bits with `FixedBitSet#(int, int)` instead of clearing bit by bit? The implementation looks to be more efficient and would just require care around the offset wrapping.
Typo: "recover" -> "recovery"
Typo: "se" -> "so"
`(seq# requested [{}], local checkpoint [{}]` is missing a closing `)`.
Typo: "temporary" -> "temporarily"
Warning! Possible bikeshedding ahead! I think it should be `index.seqno.*` so it matches the package name? And I think drop the `index_`? Similarly for the other [setting](https://github.com/elastic/elasticsearch/pull/15111/files#diff-7efaad9c1a20fcc9fde96322ebc02e0fR42).
Why not just make `SeqNoStats` implement `Streamable` and just use the existing `readOptionalStreamable` and `writeOptionalStreamable`? That's consistent with most (all?) the existing stats objects.
The `<=` will need to be escaped.
Just one more data point wrt. to our doEquals vs. @ Override equals discussion earlier today: Using the latter and explicitly calling super might have made reading this piece of code easier for me (even with a few days between last going through a refactoring and coming back now I needed to think twice why not all parameters that are being read/written below are being checked here)...
Pretty verbose - but nice to see so detailed mutations.
I honestly don't get it... the way I look at it, we moved from having the `env` "polluting" the `Factory` interface to having it now "pollute" the `Factory.Provider`... why don't we just either wire the concrete factories into a map binder, or, if we don't want to wire them, see if we can pass in the Env. directly to the Module ctor (just like it can accept `Settings`)
why do we need to merge this again since we are still holding on to the lock? I don't necessarily understand why this is helping us as well but that might just be because I don't know this code very well.
Maybe we could avoid compressing just to pass to the mapper service? Seems like the api for merge could take non compressed and we can use helpers to wrap a decompressor when we already have compressed? Just a thought for a spinoff issue (Ive had it on my mind for a while).
Notice the difference in the first parameter to MergeResult. This is the "simulate" argument. The first time we don't change anything in the merge, only check for any problems. Ideally we could move this simulation to something like we have here with check compatibility. I had a branch for a this long ago, but it was a complex change.
Nevermind, i see the serverBootstrap.shutdown() now. (it was hidden in the PR view)
I think this will be clearer if we say - "// precreate incoming indices and popluate them with the relevant types"
For the record, my other pull request #15123 is changing this logic to add all types anyway when creating such temporary indices since this is needed for cross-type validation that we need eg. for parent/child (today we are missing some validation checks when the master node does not have the mappers locally).
What do you mean "inner fields"? Passing a string "foo.bar" is not the same as creating an object field foo with a property bar.
adapt error message as mentioned above
is this a pattern or just a separator? wondering about naming
adapt error message etc.
oh right sorry I had missed it's a single value for these processors. sounds good.
I like this simplification!
> toArray() returns an Object[] depends on which toArray method you use. Just move to the one that accept an array as argument. Or iterator is fine too.
In all other methods in this class we don't call containsKey. I would stay aligned with those here.
seems like we use field or fieldName throughout the different processors. Can we settle on the same name for all processors? I think field is enough.
when we have `field: null` containsKey will return true, but the method will return null as remove returns null, which was the actual value. I think that null shouldn't be returned and we should throw an exception instead, like we do in other methods. Just call remove and check the returned value, without containsKey.
+1 on just `field`
Care will need to be taken here when backporting to 2.x lest backwards compatibility be broken. :)
add the exception? :)
Is this new or code that was moved? I was looking for where this came from but couldn't find anything.
I was wondering wherer the bucketsPaths passed get used here, but they might only be necessary in other impls of PipelineAggregatorFactory I guess. I also found the order in which the super class reads something from the stream, then calls this method to create the actual implementation and then reads some more a bit hard to follow and a bit tricky. Maybe there are ways of simplifying this, but unfortunaltely I don't have a good suggestion at this point. Maybe the whole readFrom/doReadFrom separation in the abstract class and the implementations could be merged, which would mean some duplication in code but would be easier to understand and maintain? Just a suggestion though.
OK sorry, I had missed the dependency resolution issue. Then I suggest to keep your current solution for potentially revisit when all aggs are refactored and we have a better picture.
nit: Collections.emptyList() to avoid unchecked assignment warnings.
> nit: Collections.emptyList() to avoid unchecked assignment warnings. See #15187.
Yes, that was actually what I meant (add ParseField to this class and leave HighlightParseElement as is)
We should add an else block here too so we throw an error if we get a token of a type we are not expecting
Can we not make the AbstractHighlighterBuilder define a fromXContent() method so it has the code for parsing the common attributes and then defer to a doFromXContent() to read the implementation specific attributes in HighlighterBuilder and Field? I am worried that if we duplicate this code in each implementation we will end up with parsing inconsistencies over time
We should add an else block here too so we throw an error if we get a token of a type we are not expecting
Change we change the parsing to use ParseField? we have moved over the queries and are moving the aggs over to ParseField whilst refactorign so it would be good to include this in the rest of the search request refactoring too
Thanks, it is probably also worth changing the other exceptions thrown in this method to ParsingException as it is a more appropriate exception to throw here compared with e.g. IllegalArguementException
In most other parsers (e.g. GeoBoundsParser) we do this by adding the following `else` block to the relevant places in the parser: ``` java } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) { throw new SearchParseException(context, "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].", parser.getTokenLocation()); } ```
Sorry you are right, we should be using ParsingException. That snippet was the pre-refactored version. The difference is that ParsingException does not need the SearchContext (not available on the coordinating node) and actually points to the location in the request for the error (the XContentLocation). Please use ParsingException in this PR since this is going to be parsed on the coordinating node
Bleh. Not worth it then. Sorry for the bother.
But that is not equivalent? Arrays.toString is a static method, and different than result.buildConflicts().toString()
LoggerMessageFormat handles the toString on the array. It'd be `throw new IllegalArgumentException(LoggerMessageFormat.format("Merge failed with failures {}", result.buildConflicts());`. I you don't think it improves readability then I'm happy for you to merge as is.
I don't know how often this is called, depending on this maybe it makes sense to store the formatter somewhere for later reuse unless `format` changes? Is only called a few times maybe not worth the trouble.
Curious about where the stream name gets read on the receiving side. Maybe read/write could be changed to be more symetric, but I haven't completely checked the deserialization code.
I took another look at MovAvgModelStreams, and although I'm not completely sure it look a lot like what NamedWritable is doing, so I was wondering if Stream could be replaced by it.
Can all the classes here be in the same package too? I don't really see why these handful of classes need a package hierarchy.
we should make this entire class package private and try to contain visibility here as well.
can this be moved to securityutils please? we don't want to expose a lot of this as public
Lets please not do that. The thing is it would break: the codebase of the utility class itself would screw everything up. To me, the right answer is not to make these acc blocks easier: its to have less of them, by fixing the actual problems so the blocks are not needed :)
please log the exception here as well we really wanna see what was going wrong
all I want here is a mechanism that always works. I think if we rely on a backgroud task our system is broken and we have to fix it.
I don't think it's important for now
why is shit guice exposed at all? Can't we reduce the number of @Inject here please it's such a pain to unwire all of this. Can we simply remove the @Inject and all the wiring and to in `PipelineStoreBootstrapper` ``` Java ReloadPipelinesAction action = new ReloadPipelinesAction(settings, pipelineStore, clusterService, transportService); ``` we can go even further and also don't wire `PipelineStore` and just call new in the bootstrapper as well.
did you plan to add here the list of nodes or something? looks like there is a missing argument.
lots of unused imports can be removed here.
if the api is really internal, I think we can simplify this. Do we need to use a client here? Can we instead use the transport service directly? In that case we wouldn't need the RefreshAction, and the RefreshRequestBuilder. Otherwise the api ends up being exposed anyways, no matter if we say it's internal, but it doesn't have a corresponding REST handler, which makes things inconsistent.
ok...but client depends on the transport service anyway no? I think I don't get it
this method is entirely unused and you don't need that `pipelineStoreProvider` at all
what is this provider doing here? We can't do this in our production code this will take hours for decouple again We either pass a client or not but not yet another indirection.
can we use `== false`
sorry but we have to find other solutions for this than using these injection providers. We can't sustain this kind of software engineering here.
the generic thread pull should never reject, unless it's shut down. that's it's semantics. I would also vote for a trace log in the raiseNodeDisconnected, but that's another change :) It's just confusing imo if you shut down a cluster and see these messages.
I think this can be trace. maybe the message can be "ignoring node failure (reason [{}]). local node is shutting down"
Can we just explicitly test these? Let's have a test that the field is removed, no error, in the old ones, and another test that tan exception is thrown for newer indices. Randomizing the version is fine, but let's keep it to randomize within the versions we expect to have a particular behavior, so that we keep full coverage of what we are testing on every test run.
you are getting better and better at naming.... I like myPreciousMap a lot :D
Why do we need a good github search when we have @clintongormley :)
I think we should throw an exception if `mapper.nested() != Nested.NO`, as this would not make sense (and I'm pretty sure some users have dynamic templates to make all their objects nested by default).
512 \* 1024 :tongue:
I would just access the recoverySettings directly - test changes will have effect faster.
thank you :)
I think the message needs to be changed here..
new ByteSizeValue("512kb").bytes() :trollface:
Probably don't need to compute `headers.size() - 1` for every header, could move it out of the loop
can we deprecate the INDEX_RECOVERY_INITIAL_SHARDS setting? also add a breaking change note + remove docs for it? I'm fine with doing this in another PR, but we need to update the meta issue
we can one more legal state here - old index with allocation ids.
nice and clean :) another variant you may want to consider is creating an index with way more shards than nodes and see that we get to yellow (now we will wait for a quorum of copies)
do we need the version check here? it's folded into allocatedPostIndexCreate() ? I think it will be simpler to read if we remove this from the if and add an assert on this, explaining why we expect it like that. Something like: ``` if (lastActiveAllocationIds.isEmpty()) { assert indexSettings.getIndexVersionCreated().before(Version.V_3_0_0) : "trying to allocated a primary with an empty allocation id set, but index is new"; ```
I'm fine with logging "allocation id [null] of shard" . Will save on the if :)
new is not possible with an older version...
argh. Hidden by github ui. all good.
can we add java docs? I'm mostly after an explanation of matchAnyAllocationId
highestVersion = Math.max(highestVersion, nodeShardState.version()); is faster and cleaner
I liked allocatedAs better :) (note that the name of the java JSON field is ALLOCATED :))
I think we can work around it on the node side of things - it should expect to find an existing copy.
I think this protection can go away now? It's a protection against an index staying red when opening it (because now primary can be found). We now do a better job with the active allocation ids and we will just assign new empty primaries.
you are right. Brain circuit breaked - interpreted the || as && . All good.
but this can also be done in another pr.
I'm not a big fan of ActionListener<Void>? Maybe we can do this differently and replace it with two functions? Runnable for the onResponse() part and for onFailure use a Consumer.
Right. I mean the timeout waiting for the cluster to form on the test execution side. I think by pulling out all the waiting we do for the port, and that "started" message you're returning faster to the client (the actual backwards compatibility test process). Which is fine, but that means it'll have to wait longer for the node to join the cluster.
If its early I think this is safe - but now maybe the read timeout on test side will be too low.
10 seconds is pretty fast for some of these CI machines to get the whole ES process up. Does this happen pretty early in the process? Either way, this change might be shortening the timeouts that the CI nodes need to get ES up in time.
I'd prefer `param.substring("the 13 character string".length());` or something like that.
here is a better suggestion: ``` static Random random; static { if (System.getProperty("tests.seed") != null) { random = new Random() { // ensure no bits are consumed by e.g. a test class's clinit, since that will never reproduce. // randomized runner has a similar check against its random() @Override protected int next(int bits) { throw new IllegalStateException("consuming random bits at illegal time, this will not reproduce!"); } } } else { random = new Random(); } } /** this is set by ESTestCase.beforeClass, to a new Random instance based on runner seed, so each test class is "reset", since tests are run in groups on the same jvm. in ESTestCase.afterClass, set set it back to an exception-throwing one again, to fail tests instead of making them non-reproducible */ static void setRandom(Random r) { random = r; } ``` I don't know how many classes you have under this category, but each one would need to be handled in this fashion if you want to repro a single test case that failed in jenkins.
I wonder if we want to ban `new Random(int)` to make it harder for folks to do `new Random(System.currentTimeMillis)`. If so then `Randomness` is probably the right way to go. Otherwise I like `GOOD_FAST_HASH_SEED`.
I'm clearly not getting my point across. Please understand that multiple tests are run in the same jvm during jenkins!!!!!!!!!!!!
> @bleskes When parsing the system property "tests.seed" we look for a hexadecimal number (because this is the format that randomizedtesting provides seeds in). When parsing a setting (e.g., DiscoveryService#SETTING_DISCOVERY_SEED", we look for a decimal number. Argh. You are right. Sorry for the noise.
Just to make it better, setting it here in clinit is not so ideal, because its not perfectly reproducible when multiple tests are run in the same jvm. you will still have perceived reproducibility issues vs jenkins if you go about it this way: because the random will be initialized _once_ and then we will run 8 test classes against it. then, when you later try to reproduce the one that failed, the sequence will be different (even though the initial value is the same), because the other tests are not also invoked. but if you do it this way, it at least allows "whole build" reproducibility, which is an improvement. That means e.g. if you nuke your local execution hints file and run 'gradle test -Dtests.seed=xxxxxx -Dtests.jvm=yyyyyy', it will match what jenkins did. But nobody does that.
and setting it here is too late anyway, for example StringHelper in lucene is already initialized. changing tests.seed is simply a non-option. If you want things to reproduce in your IDE, you need to pass tests.seed to tests: its just that simple.
no i do not. but this IDE cannot compromise the actual build, which is 'gradle check'. changing tests.seed in this way can compromise the build, because then the values for other things looking for this (such as lucene) depends on class initialization order.
if we do that, lets do the 'looking' and set a static final boolean so it does not impact real code.
ReflectiveOperationException can be used instead of both of these
Why not just use System.getProperty("tests.seed") ? is this an intellij auto-complete thing? I see this anti-pattern quite often and i wonder why its done.
can we just change this to System.getProperty("tests.seed") != null? Then that method can be removed.
i'm trying not to ocmplain about the intellij wildcard imports, but in such a big class with _many_ imports they are especially evil. we should turn this feature off, it is horrible.
I think its too much trouble: you could also do this: ``` private static final Random RANDOM = new Random(StringHelper.GOOD_FAST_HASH_SEED); ``` that one is already setup to respect `tests.seed`, otherwise its set to `System.currentTimeMillis`.
Hopefully the answer is no, we do this here only so the test is reproducible? but then if that's the case I wonder, how do we test that the setting is not really needed, cause it shouldn't be? :)
but, it seems we look for an hexadecimal number? (which is OK because normal long are also parseable as hex, but you know, pedantic)
don't change tests.seed, i dont care what intellij does, this is wrong to do.
> but then if that's the case I wonder, how do we test that the setting is not really needed, cause it shouldn't be? :) @javanna imo this should be a pre-release smoke test.
I don't understand why the new method would ever need to be used in tests. random() can safely be used and is always more clear. We should only use the Randomness method for non-test code, and keep it contained. Collections.shuffle(List, Random) is always ok.
don't set tests.seed, it is already set.
seems like this is not needed anymore given that we don't go through InternalSettingsPreparer anymore? sysprops will always be ignored I think
This should be `aliasAction.aliases == null || aliasAction.aliases.length == 0`
Without this we get: ``` { "error": { "root_cause": [ { "type": "null_pointer_exception", "reason": null } ], "type": "null_pointer_exception", "reason": null }, "status": 500 } ``` ``` [2016-02-09 15:56:02,492][INFO ][rest.suppressed ] /_aliases Params: {} java.lang.NullPointerException at org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest.validate(IndicesAliasesRequest.java:289) at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:62) at org.elasticsearch.client.node. ```
The bug is here.
Oh poor java lacking Self types.
I think it was better to at least test a few different query types since then it removes the possibility that the query is accidentally hardcoded somewhere
Yes please do make build() public. SearchParseElement will be deleted at the end of the refactoring so there should be no new implementations as part of the refactoring
Typo: "Dynamics" -> "Dynamic"
I'm not a fan of growing this class so much. I'm happy to leave the tests in the class they were in.
can we remove this class from the commit? AFAICS only formatting changes.
similar concerns as IndexRequest
similar concerns as DeleteRequest
you should change the writeTo method to take care of nodes on older versions like in DeleteRequest
similarly to the read, you need to protect the write with a version check. Additionally since you changed the parent setter to not set the routing, I think you would need to do the following: ``` java if (out.getVersion().onOrAfter(Version.V_2_2_0)) { out.writeOptionalString(routing()); out.writeOptionalString(parent()); } else { String routing = routing(); if (routing == null) { routing = parent(); } out.writeOptionalString(routing); } ```
otherwise you could index into an alias that target a specific shard and yet index in other shards by specifying a routing key, which I guess could be seen as a bug
you would need to protect this read with a version check so that it does not fail when reading data coming from an old node - this would look like this: ``` java if (in.getVersion().onOrAfter(Version.V_2_2_0)) { parent = in.readOptionalString(); } else { parent = null; } ```
can we just have this variant not the one that takes a String.
can we just do it even for `size == 0`
same suggestion as for master operation use java8 default impl in _TransportRequestHandler_: ``` Java default void messageReceived(final Request request, final TransportChannel channel, Task task) { messageReceived(request, channel); // override if you need the task } ```
any chance we can make the `parentTaskNode` and the `parentTaskId` lazy and just expose them as package private setters. That way we don't need to pass them to all nodeRequests but only set them on the node requests that are tasks requests. This is really polluting the interface IMO.
same here these strings are only used in one place just use them directly and trash the Fields class
can we use getters here like `getNode` `isCanceled`
I don't think we are perf critical here of any sort, can you just use the constants where they are used? I think they are only used in one place and these Field classes are really unnecessary
name is `getPerNodeTasks`
can we just do it even for `size == 0`
I like this patter a lot :)
Something like: ``` Task task = taskManager.register("transport", actionName, request); execute(task, request, new ActionListener<Response>() { ... } private void execute(Task task, Request request, ActionListener<Response> listener) { ```
You won't even need the guard if you are just merging to 3.0.0, right? 3.0.0 doesn't have to be wire compatible with 2.x
I think this file needs formatting `if(` -> `if (`
do we need this Reader interface? can't this just be `Funciton<StreamInput, T> reader`
nit: can we rename this to `getTasks`
extra space makes everything not line up!
I suspect you can build the arraylist here with the expected size being the count.
I know this is a pattern we use everywhere but I don't like the name `success` to mean "I've forked this request and now it's the listener's problem" but I don't know a better name.
Can you make a `DelegatingTransportChannel` which just blindly delegates to another and then overwrite that with the one that unregisters the task? It'd make it more obvious what this is doing.
Can you add something here about why you can't just send the task itself back over the API? "Tasks are allowed to have state so they can't be returned over the API so this represents a snapshot of that state." or something like that.
`+ query` should do it.
ok fair enough I didn't try it :)
can we make this ret val unmodifiable please
lets skip the guard - not perf critical and gets called more often then
can this see `unregister task for id: [{}]`
I am surprised that we don't have a default impl for this :)
should this listener be volatile? I also wonder if we should fail / throw an exceptin if the listener is non-null when we try to set it? -- there is a nice class in lucene called `org.apache.lucene.util.SetOnce<Listener>` which does that for you
ok can we rename the getter then to `getFailedNodeExceptions()`
java 8 FTW
same here - just pass a new instance
getters please? :)
Or I can just merge as-is, don't worry about it :)
I think we should explicitly set this to `false` so if Jackson's defaults ever change, we won't be bitten by it.
typo: cluste.name -> cluster.name
We can pass a name in the constructor if need be? On 11 dec. 2015 9:53 AM +0100, Martijn van Groningennotifications@github.com, wrote: > Incore/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java(https://github.com/elastic/elasticsearch/pull/15363#discussion_r47332645): > > > @@ -304,7 +304,27 @@ public void onFailure(Throwable t) {>observer.waitForNextChange(new ClusterStateObserver.Listener() {>@Override>public void onNewClusterState(ClusterState state) {>- threadPool.executor(executor).execute(AsyncReplicaAction.this);>+ transportService.sendRequest(clusterService.localNode(), transportReplicaAction, request, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {>+>+ @Override>+ public void handleResponse(TransportResponse.Empty response) {>+ try {>+ channel.sendResponse(response);>+ } catch (IOException e) {>+ logger.debug("failed to send retry on replica response, action [{}], request [{}]", e, actionName, request); > > got it. btw about the generic helper, we always have a custom log message or want to deal a failure differently in many scenarios, so I think a generic helper isn't going to help much. > >  > Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/15363/files#r47332645).
we should call onFailure here. I wonder if we should have a utility pass through TransportResponseHandler that takes a channel and passes the response to it.
also, using a non-inner class means we can free the memory (i.e. cluster state copy) rather then have them accumulate with every retry
I meant the listener we pass to the transport
the constantScoreQuery wrapper is not necessary
Can we not ignore but throw an error back to the user? We shouldn't silently ignore something the user has passed in, it may be they are confused about the api and don't realize that part of their request is being ignored.
Ditto here, guard with version check
@jimferenczi How do you know the the user didn't make a mistake in his code and set routing when he did not mean to, but meant to only set parent? Being lenient is bad, but being silently lenient is even worse.
we should check the exception is what we expect (assert part or all of the message)
minor nit: "int he" -> "in the"
foo all switch statements in those tests, I think it would be less error-prone if we fail() in the default case? (not as much for now as for when we'll modify these tests)
Nit: The indentation is off on this line.
can we make those two constructors call the 3rd one so that we can centralize validation (if we ever add some)
oh. sorry. I'm missed the usage in the test. All good.
can we log a warn here - "failed to validate incoming join request from node [{}]" ? I think this should be visible.
I've been wanting a portfile for a few weeks now for other testing things. It'll really make BWC tests more clean to have this, I think.
oh boy, so this test sets a field called "index" to value "index" which is contained in the "_index" field. I must admit I would try to make it a bit easier to read using different values if possible.
maybe it would be better if each test had its own instance of TestTemplateService (better test isolation)? I think we shouldn't worry about performance or too many objects created here.
maybe we should have a constant somewhere with _source and _ingest? pretty sure we already use them somewhere else
I am starting to wonder if this additional code belongs to PipelineStore. seems like it's become much more than just a store. (referring to InternalTemplateService and the doStart above)
ok thanks for the explanation.
> But maybe we should do this in a different place then here? yes in the impl... :) sorry but I find this confusing.
are we sure we want to silently go ahead this way when templateService is null? Maybe we should fail so that we find out when it happens, unless it's situation that is actually expected to happen.
not sure about this default impl. Seems like the runtime has to plug in its own template service and template impls. but in the interface we assume that we always have mustache. I don't mean to make the engine pluggable, but I think we have to better figure out who needs to do what and how when the ingest library is used outside of the context of es core. I think for now this assumption would be better placed as part of the implementation.
you can use MustacheScriptEngineService.NAME
Yikes! I'm super glad we had these tests fixed!
Should assert that `e` is an instance of `TypeMissingException`.
You can get away with it right now because there is only one test, but this should be initialized once before the test suite, not once before each test in the suite.
It defaults to `false`. :)
You can just make this an anonymous ActionListener as you had before, capturing an AtomicBoolean to flag whether or not `onFailure` was actually invoked. I think that it will clearer if it's inline with the test, and there will be a little less boilerplate because you can drop the field and the setter.
Let's also make this a JUnit assertion instead of a Java assertion.
You can just use the literal boolean `false` instead of the string `"false"`.
Can you make this a JUnit assertion instead of a Java assertion? It's preferable to use a test assertion with matchers for this because then nice error messages are produced automatically when the assertion fails.
The `new HashSet<>()` can be replaced with `Collections.emptySet()` (and then you'll have an import to remove).
Since this is static, the name should be `THREAD_POOL`.
that will be hard to debug if it's false - which one misses? (holds for the one above as well).
we can open a new issue to add debug logging to SelectedType but this is really arbitrary here. I don't think this should be info - we have enough info going on and if we reset it we see it in the settings, no need to X different ways to get some information
> Would you do this just when it is overriden (ie not the default)? Or at all times? What about if they explicitly select netty in their settings? Anything but the first is not a one line change. Yep. Log all the time. > I don't think we should just willy nilly log things > I don't think this is useful. It saddens me that this takes so much effort and discussion. It is also sad that there is no constructive discussion but rather these yes/no statements. As I said, I find it useful, for the reasons I mentioned. That should be enough for this kind of change. I'm signing out of this now. I don't think it's constructive anymore.
cool stuff - I'd kill for these not being .class instances but I think this must be a followup :)
I think it's important to know that the transport system is replaced and that the settings have effect. This also has security implications, as plugins can add settings. I think this should stay an info log like it was. Adding something similar to the SelectedType is good but over there debug is the right call indeed - it may be used for many things.
oh, I see. It felt like a utility class but it's officially a bless thing. Thanks for pointing it out.
can we add logging to what transport is active.. We used to have : ``` logger.info("Using [{}] as http transport, overridden by [{}]", httpServerTransportClass.getName(), source) ``` The source will be hard to do, so I'm fine with dropping it.
how about preparing the scene and have two methods - registerRestHandler and registerCatHandler? the latter can accept things that extend AbstractCatAction and the first can assert the parameter doesn't inherit from AbstractCatAction and throw the right exception.
ok, sure. I thought you liked the separation. This works for me.
Intelijj seems to think this is not used...
Just catch IllegalArgumentException I think. Maybe assert something about the message but its not a big deal.
Yeah, you are right. Things like IllegalArgumentException should always check it the message. There are exceptions for which its not as important because they are more specific. Though most of the time its just worth checking anyway.
It is a big deal! Otherwise we have no idea if this test is hitting a completely different error than what was expected. Testing exceptions should always check something in the exception message to be sure the _right_ exception was caught (we use IAE all over the place, so the exception class alone is not sufficient).
this keeps bugging me :) we should something on the executor as well....
while you are at it can you remove the randomization on script engine service given that we have only one now? seems like this test is a bit outdated after recent changes.
can you undo this change given that we will soon ban wildcard imports? #15395
Nit: `);` can be on the previous line
Nit: "have change it's" -> "has changed its"
Nit: this can go on the previous line
Nit: `added [{}] the` -> `added [{}] to the`
Nit: this does not need to be on a separate line
A simple change to the message can make this assertion fit on one line: ``` diff diff --git a/core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java b/core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java index 5ccb541..ca172f8 100644 --- a/core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java +++ b/core/src/test/java/org/elasticsearch/index/seqno/CheckpointsIT.java @@ -54,8 +54,7 @@ public class CheckpointsIT extends ESIntegTestCase { IndicesStatsResponse stats = client().admin().indices().prepareStats("test").clear().get(); for (ShardStats shardStats : stats.getShards()) { if (shardStats.getSeqNoStats() == null) { - assertFalse("didn't get seq no stats for a primary " + shardStats.getShardRouting(), - shardStats.getShardRouting().primary()); + assertFalse("no seq_no stats for primary " + shardStats.getShardRouting(), shardStats.getShardRouting().primary()); continue; } logger.debug("seq_no stats for {}: {}", shardStats.getShardRouting(), ```
Typo: "`check point`" -> "`checkpoint`".
Typo: "`local checkpoint`" -> "`local checkpoints`".
Nit: Change the casing of `CheckPoint` to `Checkpoint` in the method name.
Assert that the current thread holds the lock on `this`? The results from `ObjectLongMap#indexOf` remain valid only if no one else is mutating.
Typo: "`These map`" -> "`This map`".
Nit: there is an extra space after the `&&` and before `inSyncLocalCheckpoints`
Typo: "`can not be update`" -> "`can not be updated`".
Nit: "seq no" -> "seq_no" (inconsistencies will make searching more difficult)
I see, it's scheduled from the constructor of `BaseAsyncTask`. I do not think `volatile` is needed here then, and this field can be marked `final`.
If your intuition is that these will be almost always needed, then obviously we should keep them.
I think I'd prefer if it were just a string/enum/whatever that made the logs and api spit out "true"/"false"/"unknown".
Sorry I am late, but I think this should go under debug logging, as by default, this will be logged each time a node starts, and I don't think it is important enough to log each time a node starts...
I'm doubtful. I wonder if we should do this or follow the proven path we have in the listener interface where we have a processed callback (changed or not). I'm leaning towards the later as it gives more options (with the burden of an extra equality check).
ok. I'm with you.
If it can be null we should skip it here.
I'm just used to `ClassCastException`s in tests being test failures. The cast is implicitly part of the test. I'm fine with keeping the explicit assertion but I'd prefer the other style so you get nicer error messages.
I know you've just copied but is it worth even asserting this? Also, if you do assert this maybe use `assertThat(o, instanceOf(Text.class))`;
s/has to be/has been/
++ on error
Are these debug level events? They all seem like bugs.
It feels squicky to ignore the exception. Personally I'd `assert false : "Exceptions not expected here."` and `logger.error` about it.
looks like a leftover
and here too ;)
this shouldn't be public, potentially private, otherwise package private if we call it from tests.
Maybe catch `Exception` instead. If there is a jvm error (out of memory and such) then we should just bail and not try to execute the onfailure processors.
after rebase you will have to get rid of any wildcard import, or the build fails :)
same as above, no need for try catch
If the search request is impacted too I'd rather have it in a different PR
> The boolean query throws an IllegalArgumentException when min should match is a negative value but only on some cases, for instance if you don't have should clauses then the query works. OK, I didn't know that, it's more complex than I thought. Let's merge this PR as-in then and work on another PR to make minimum_should_match validation more consistent.
do you know if there is a good reason to return 0 if we got a negative value, or could we just return 'result' directly? (if this doesn't make any tests fail, this is good enough for me)
good point. I didn't think about that. The value to append can be an json object too, so yes the exception should be replaced with logic to deal with that.
oh nevermind I thought it was a parsing method, but this is the simple Enum.valueOf
No more wildcard imports right :)
100kb also seems arbitrary, I know it was here before maybe @costin knows
you can call `bytesAsInt()` then you safe the cast and it checks that it's lossless
nit: `== false`
ok I would always pass down true then otherwise it feels like we should do something with it. Consumer<Void> would be better in that sense but then you need to provide null which is even worse to see...
as I said below on the REST action, I don't know if this is right. I think we should still register these CRUD actions, but don't register simulate. I think it all depends on the meaning that we give to the ingest flag. does it mean that the plugin is disabled completely or that data ingestion (pipeline/processors) is disabled? I think the latter...
also disabling put delete and get actions on the rest layer doesn't disable them on the transport layer, I think we should remove that part.
this makes me wonder: if a node has node.ingest set to false, for sure no processors should run, hence simulate should be off and IngestDisabledActionFilter should throw exception when a pipeline_id is used as it does now. But how about crud actions for pipelines? One has to go to specific nodes to store them, that have node.ingest set to true? this may not be needed, as those are just index, delete and get operations that any node supports...it's like making client nodes reject index requests, they can forward them to the proper nodes, no problem with that.
oh sorry I see that you don't register the transport action when ingest is off, sorry I had missed that.
so, if we fail to install the template, we just flag and continue working as usual? So ingest template is optimistic, and not a requirement.
yes, we can improve this in a follow up issue. I do think we should change metadata to be a map of maps and then get the entire exception into it (including all cause exceptions).
> Is this enough info from the error? I was expecting something more detailed like we do in ElasticsearchException#toXContent. Is that not needed? That makes sense to do, right now we may lose a lot of details. > One more thing, can it happen that we have multiple errors but we keep track of only one of them due to key collisions in the map? The exception is only available in the context of the current on failure processors. They need to act upon it.
Because a processor may also be wrapped by other processors with on failure definitions.
I don't think that the scripting/template service/engine should be json oriented by default. I think it should work with potentially any content type (json, yaml, plain text, or whatever). I wouldn't have a parameter called `escape_son`, but instead maybe have a `content_type` parameter? and expect the values of this to be actual content types (i.e. `text/plain`, `application/json`, `application/yaml`, etc...)? By default we should simply assume `text/plain` and `utf8`. Then it'll be the responsibility of the client of this service to provide the context (e.g. the search templates will add `content_type=application/json`).
XContentParser.Token.VALUE_STRING.isValue() always returns true
No, I don't think we do
nit: space after IOException
braces please. for the rest of the method too. (I realize you just tweaked this to be a lambda but it would be good to fix this as two line single statement `if`s are dangerous and evil).
You know, we can add a checkstyle check for those....
It would be good to have some context as to where the filter appeared. I would at least word it a little differently: `"Missing [type] setting for anonymous char filter: " + charFilterName`
this method should assume that `tokenizer` is a tokenizer name rather than trying to parse it as json
here too I think we should just do `this.tokenFilters.add(new NameOrDefinition(tokenFilter));`
same here, just `this.charFilters.add(new NameOrDefinition(charFilter));`
make this an atomicreference to throwable so we can see what it was in the failure message? (use assertNull)
make this Error? we alway want to use this.
can we call this `explainOrThrowMissingRoutingNode` ? the docs can read something like "a utility method to handle the case where a disco node can not be found in the routing table. Typically this would mean it's not a data node"
Callers of this method can just do `allocation.routingTable().shardRoutingTable(shardId).primaryShard()` these days (if we add an overload for shardRoutingTable which takes a shardId). I don't think it's worth having this utility method. (I know it existed before - we have progressed since it was written :))
same - please name it something like `explainOrThrowRejectedCommand`
stale logging message - use allow data loss set to true
create does an inline reroute, so you can check directly here that no shard is assigned. No need for timeouts
see text from other suggestion for empty primary allocation
maybe "all copies of " + shardId +" are already assigned. use the move allocation command instead".
maybe the reason can simply be "primary " + shardId + " is already assigned".
I see now that MoveAllocationCommand is not touched by the PR. I think moving to NamedWriteableRegistry is a good idea, but I'm fine with putting it out of scope for this PR
Most of the imports here are not used
nit: I know this was already the name, but can we call this ShardFailedClusterStateTaskExecutor? It will help also with it being a member of ShardFailedTransportHandler .
also, I wonder if we should just inline this method.. not sure it buys us much now.
nit - the old logging had the structure "componet][node][shardId] message " which we try to use for all shard related messages. I think we should keep it.
cool thanks for clarifying!
A description here on why someone would want a custom name resolve would be nice.
Nit: strictly speaking i think we need targetBuffer.remaining() , which is how many bytes we are reading.
nit: read lock -> just lock.
Can we soften this message? maybe "deleted previously created, but not yet committed, next generation [{}]. This can happen due to a tragic exception when creating a new generation"
use == false . Also where do we check that the total number of captured requests is what we expect? (to guarantee it doesn't contain the node the primary shard is on).
we have a utility method RoutingTable.shardRoutingTable which gives you what you want. Also IndexShardRoutingTabel is iterable so we don't need the shard list.
this can go away now...
sorry - got confused - I thought this code skips the primary. I think it will be clear if the if here would say `ShardRouting.primary() == false`
one too many new line? :)
This eats the stack trace which could be a pain.
the processor ids have no meaning on our side and are completely meta. So its fine. It is more of tag then it is an id, so others that are integrating with ingest.
+1 let's have AbstractProcessor instead
I am glad we simplified things by just renaming id to tag, that's a good choice
+1 on the rename, tag is much better than id
Can we remove `tag` as parameter here and make it part of the `config` parameter? In the end it is an optional argument and it feels that the this should be part of config, this way the signature of this method remains clean.
also, at the moment we don't check for ids uniqueness, which is probably fine given what we need ids for, just double checking that we don't rely on uniqueness anywhere
Should we keep in incrementing even for cases where we find the id? or only increment for cases when the id is not provided? I am not sure myself, just wondering, maybe not that important either at the moment.
Ok, just checking.
given how processorIdGenerator is used, it really shouldn't be AtomicInteger anymore, otherwise it is misleading, a simple int counter does it instead.
This method is kind of tricky now. I tried to come up with a different approach, but that didn't make it easier.
Maybe only invoke `processorIdGenerator.getAndIncrement()` when no id has been specified in a if statement? Otherwise there will be holes in the numbers generated if only some processors have a user specified id.
instead of this check just override `public void onRejection(Throwable t)`
Yes, redirecting is much better than failing. It has been discussed (it is a small not on the meta issue) and we improve this after the move ingest into core refactoring has been completed.
I don't see how the cluster state is not _always_ the right place to store configuration. Putting it in an index is a hack. I don't see the issue with "large pipeline configuration", but I also don't see how it would differ from how cluster state for a specific index is passed where necessary, eg mappings are not loaded for all indexes on all nodes.
missing error message
Fair enough. Its probably not worth it.
Personally I don't think that is an issue. Removing the extra transport header is the cleanest.
I think we need check whether pipeline has be specified on the bulk request or on any sub requests here. Right now we always delegate to the pipeline execution service that will execute on the ingest thread pool, which we should prevent if no pipeline has been specified.
+1 this makes sense. Then we can drop the `PIPELINE_ALREADY_PROCESSED` transport header.
This seems not to be the right exception message here (looks like cp'ed from term query).
The advantage of doing this via serialization instead of copy constructor would be that this is a deep copy then, this way we only copy the references.
Similar concerns as ValueCountAggregator.
I really wonder if we should just rethrow and get rid of TranslogException eventually
can we please not use `forEach` I think we should stick to for loops it just makes the code more readable and consistent
Hmm why are we ignoring exceptions here? You can consolidate those two `deleteFilesIgnoringExceptions` into one call and it will do the right thing if either path hit an exc while being deleted...
this loop is hard to follow. I think we can make this more elegant by using a sublist and make sure we process all of them. we can also peak at the first element and get it's generation to calculate the range directly.
++ on this. The ReroutePhase owns retrying across the board.
nite: can we add some java docs about the life cycle of this reference (closed when operation is done and replication have completed).
I think the it's also important to explain here is why we don't allow writes on RELOCATED
we can just use the enumSet.toString here (assuming it gives nice output). This will make sure we are never out of sync and I think it's OK to have a technical message here.
same here re enumSet.toString
can we link to IndexShard.updateRoutingEntry ? it's not trivial to find.
can we add an assertion that we actually removed something? I think this will also mean that we need a tighter control over what we remove. For example, if the primary is relocating, but we did not move to "relocated" mode, the currentNodeId should not be in the list (and shouldn't be removed).
can we check and stop if the background thread had any issues? o.w. will have to dig through more than needed.
I think we don't need the AtomicBoolean here. We should protect from double closing on the level below
++. Looks good.
can we make this an enumSet called writeAllowedStateForReplica (and the same for primary). It's getting out of hand :) also please right them in order recovering, post_recovery, started, relocated
we can remove the mutex here, but tbh this whole check can go away - It's just an extra safety mechanism. We can keep it simple
can we change this block to use the same flow as the doRun (it's equivalent, but I have to double check every time) ``` if (shard.primary() == false && executeOnReplica == false) { numberOfIgnoredShardInstances++; continue; } if (shard.unassigned()) { numberOfIgnoredShardInstances++; continue; } if (nodes.localNodeId().equals(shard.currentNodeId()) == false) { numberOfPendingShardInstances++; } if (shard.relocating() && shard.relocatingNodeId().equals(nodes.localNodeId()) == false) { numberOfPendingShardInstances++; } ```
we throw the exception and thus take care of the interrupt. We don't need to set it...
this is tricky - it leaves us in a potentially scenario where there are two active primaries - the target and the source. I don't have a clean solution for this. My suggestion is to fail the shard and let the master promote another replica.
I think we can fail the shard on _any_ failure here. failAndRemoveShard already does logging under WARN
can we replace these test with a simpler to understand test, paying the price of things being less targeted? experience have shown that this type of tests are very hard to maintain and often don't reproduce exactly what was intended anyway (because it's so hard)..
why do we need assertBusy here? I rather use two signaling methods - the one you have now and the one signaling the some/all the threads have acquired the ops lock. Note that now I think you have a race condition with the recovery thread can sneak in first.
this should be part of the tests for the replication phase - with random cluster state and all..
you can do : `internalCluster().getInstance(ClusterService.class, nodeA).localNode().id();`
Doesn't PipelineConfiguration deserve its own class file under o.e.ingest ? It's returned by the java api too.
I think this constructor can go away
I double checked and this has changed with this PR, we used to return 404 before.
Writeable#readFrom returns a new instance of the object, it allows to have final fields, but it requires to have a PROTO instance of the object to call readFrom against. I wish there was an interface to declare writeTo only though but we don't have it at the moment.
ok let me have a look then ;)
wasn't the plan to keep this one as part of the java api? Seemed like a nice to have, too bad you wrote it for nothing!
this needs to be tested expclitily in the PipelineStoreTests
yes and remove equals and hashcode from PipelineConfiguration too I think
nit: wondering if we can avoid parsing the map and rewriting it directly in json format. not sure there's a better way to do this.
given where it ended up being called, I think that removing properties from config is not that useful anymore here? maybe we should have two versions of the method, one to validate and one called here so we avoid the deep copy? in theory the pipeline should be correct at this point right? no need to validate it twice probably
I see why we have maps of maps, but if we have to do it this way, maybe we could have a PipelineConfiguration class that holds the configuration as a map and is Writeable and ToXContent etc. ? similar to the previous PipelineDefinition but this one would hold the configuration.
this one can be private
does this constant make sense to be here or in the fallback code should we just pass `new LoadAverage(-1, -1, -1)`. Maybe we should remove the ctor taking a single `double` as well, and pass `new LoadAverage(x, -1, -1)`. I had trouble following the logic but then it would be all clear what values are being returned in those cases.
We wouldn't have the confusing Double (it immediately makes me think, how/why can this be null). I expect we'd use -1 for the back compat case anyway? (e.g. just read a single double, and return { x, -1, -1 }).
Somewhat related to this PR: should we just remove this code? this "save/restore" is quite complicated, and I know for a fact it never happens (we don't have any OS we have ever tested on where thread cpu time is _supported_ but not _enabled_, and if such a situation were to present itself, we'd surely want to know what the implications of switching it on temporarily are). I know this because this method requires permissions that we don't grant `ManagementPermission("control")`, so it does not ever succeed. We don't have to do this on this PR though, but it definitely makes the code here complex.
we can... but it's important to not forget that some classes should not to depend on es. Packages don't enforce anything but may make you think about it (or maybe not). Anyways I think it's nice to keep separated the es runtime for ingest and the ingest classes that are independent from es core. There is a pretty big difference between these two sets of classes. As I asked above, maybe there are other ways to keep things clean, not sure.
Maybe for now we should move things into one package. When we want to make ingest a separate jar then this will be a separate project. There are other things that we will have to address once we move to a dedicated ingest jar.
In the case the cluster layout is the same then the sequence of nodes that will be picked during the first requests will be the same. I think this randomisation is good? Another benefit is that if we run in tests the test seed influences the sequence of nodes. This is good for test reproducibility.
I also wonder if we should only have `Task.java` and have different ctors and if you don't pass a parentID you are not a child task? and default parent id is 0 just like in linux etc.
`ifsmd.count = Math.max(ifsmd.count, blocksmd1.count);` would be a bit more clear to me.
Maybe a method on `StatementMetdata` to copy the appropriate metadata. It saves a few lines of code and forces you to name the operation.
You might want to have a superclass here just so common stuff is obvious like `source` and (sometimes) `read`.
Or `adapter.createStatementMetadata(blockctx, trysmd);` or `trysmd.substatement(blockctx);`.
`blocksmd.copyContext(trysmd);`? I know you use "context" to mean something and this might not be the right use of that word though.
Its clearer to you because you are used to it. For me, every time I see a `visit` call I have to think "why does this one get an increment/decrement and not that one?" and "do all calls to `visit` with this parameter type increment scope?" and things like that. That second question is why I asked if you could push the increment/decrement to the implementation. Making a method call for it probably wouldn't make it clearer though.
`clone` makes me think of the funky `clone` method. Maybe call it `copy`? Not in this PR, but a thing to think about.
Fair enough. I just see the increment/visit/decrement pattern a lot and it feels like something you could make more automatic/harder to forget/explicitly named.
These are awesome.
It might be worth adding a default implementation of it to whatever interface declares it. The vast majority of the time its a noop.
That used to be like one character in the antlr syntax....
don't we need to read the original indices anymore here? not sure, but if that's the case we may be able to remove that specific PercolateShardRequest constructor.
you are right, the original indices are read/written in the super class, good! You can then remove the PercolateShardRequest that takes shardId and originalIndices as arguments I think.
I think it's odd to have a public constructor for a test only... remove the OriginalIndices parameter at least? it's always set to null I think.
I think we should remove this method and insist on using the setScoreMode(QueryRescoreMode) method instead. This will make the API cleaner as only the permitted values (the ones in the enum) can be used. The parser can then use the fromString() method to convert REST request values to the enum but Java API users will have the safety of the enum
Why would a user set this setting to false ? It's not taken into account when the shard is closed due to relocations or when the index is removed so is it useful to have it as an updatable index setting ? IMO this could be a chance to remove the setting and force the value to true.
I like keeping changes like this purely mechanical though. If we should remove the setting we can remove it after I think.
Sure I was just wondering if there is a use case for this.
Starting these with `INDEX` is a bit redundant now I think.
I know this correct, but can we use `semaphore.tryAcquire(1,0,TimeUnits.SECONDS)` - It got me worried that we ask for 0 permits :)
not sure why we would have null here, but even if we had it, none of the following ifs are going to be true. I think you can remove this if then
We also need a simple rest test, testing integration like we have for the other processors
update java docs
Also, thinking about liveness, maybe `HANDSHAKE_ACTION_NAME` should be included in the defaults for `transport.tracer.exclude`
I just fiddled around with it some more and see what you mean about periodically checking. Your arguments sounds good to me.
I guess these two requests are really the only requests where you can't call `getVersion` on the stream.
The "liveness" request sets the type to "state" but I imagine that doesn't matter here because there is only a single channel open anyway. Actually it looks like the default is `REG` which I guess is more accurate for this. Speaking of the liveness request, why do we need it if we have this? I guess the liveness request can use the version but it feels like this request has all the information that the liveness request has.
Is the version needed? I don't see it being read here.
Rather, I only see you serialize it but I don't see it accessed.
``` java || this.secondariesStorageSettings.isEmpty() ``` :crying_cat_face:
Ah. I see.
Typo: `thoulands` -> `thousands`. Same typo [here](https://github.com/elastic/elasticsearch/pull/15998/files#diff-091a93c5c6d4170159d3170b6c085be2R95).
indent and extra new line
We should not make these changes until we remove site plugin support. This part right here, does not play well with this change at all.
it's done in refreshInternal as well.. so I'm probably missing something w.r.t. lucene. Looking to learn ;)
do we want to unify this code with the refresh method by making this get a manager to work on? (+ a string description for failures)
no you can remove this - it's called when needed in onSettingsChanged
this seems to bypass the IndexSearcherWrapper defined in IndexShard (and passed via the searchFactory)
This is going to be 512 Unicode code units, but I think we should do bytes.
> Or "no longer than 512 bytes" +1
Or "no longer than 512 bytes"
I think we lost this debug message. I think it's fine to log once in both the shard failed and shard started cases.
Yes, I think the builder should operate on the field type.
maybe s/VS/?/ since we don't know yet if the valuessource is applicable
maybe wrap in a RemoteTransportException? will communicate better why this is here...
Hrm, why this switch of java imports to the bottom? I had just changed by import logic to match eclipse which puts them at the top...
Safe because ~~our~~ we know
not was indeed my point - we don't need to task what shard allocation does with the shard failures, we just need to test we told it to fail the right shards. But this is nit picking :)
do we want to check that the shards were actually failed? Ideally we would just check that the allocation service was asked to fail them. Mockito maybe helpful? I don't want to hold you back for this though...
I think you can leave it in one line, no need to break after 80 chars or sth (unless I missed that :-)
looks more tedious than just filling an array, but I didnt do an awful lot of java 8 stuff yet :-)
`== false` here, as we do this in most other code as well
you mean providing the size of the array I guess? cause I don't see a constructor that accepts an array in ArrayList.
creating two ArrayLists may not be the best thing to do. Lets do this instead: ``` List<String> splitList = new ArrayList<>(); Collections.addAll(splitList, oldVal.split(separator)); ```
@mike can we just do this as follows: ``` Java List<Throwable> exceptions = new ArrayList<>(); try { for (Closeable c : Arrays.asList(unreferencedReader, () -> Files.delete(translogPath), () -> Files.delete(translogCheckpointPath))) { try { IOUtils.close(c); } catch (Throwable t) { exceptions.add(t) } } //... ExceptionHelpder.rethrowAndSuppress(exceptions); ```
This this can be simpler with org.elasticsearch.ExceptionsHelper#rethrowAndSuppress - we just need to keep a list of throwable and deal with them in the end.
This should be: ``` ^(?:[-\\w]+[.])+$ ``` This assumes that (a) at least one level is required and (b) there is nothing after the group key (so it is anchored to the end of the string)
I think this should be: ``` ^(?:[-\\w]+[.])*[-\\w]+$ ``` - non-capturing groups (`(?:..)`) are more efficient - `\w` already includes `\d` and `_` - a `-` inside `[ ]` should appear first, otherwise it indicates a range (at least in pcre)
Oh I thought it aleardy did! Then maybe a better way would be to make this method pkg-private? (I'm just trying to keep the API to a bare minimum)
beware that the build now bans wildcard imports
I'm wondering that maybe we should not expose SearchAfterBuilder at all to the client API and make it internal to SearchSourceBuilder? So SearchSourceBuilder would still hold a SearchAfterBuilder instance, but the two above methods would look like: ``` java public Object[] searchAfter() { return searchAfterBuilder.getSortValues(); } public SearchSourceBuilder searchAfter(Object[] values) { this.searchAfterBuilder = new SearchAfterBuilder(values); return this; } ```
can you use indexRandom? this will randomize insertion order and add some deleted docs to the index
The code in SortParseElement also seems to support `reverse`, which seems kind of the same as order. Was that deprecated along the way? I'd be in favour of that, seems redundant.
In BaseTermQueryBuilder we convert BytesRef back to String in the getter, we could do here as well, otherwise client setting a String gets something different back here.
Add short-curcuit return if this == other.
I think `writeGenericValue` handles null values, so you could omitt the surrounding check.
Curious about what happend to this parameter.
If `elementName` is always the (target) fieldName for the sort, can we call it that way? Also, this only serves to create the FieldSortBuilder, maybe there's a way of passing in the blank FieldSortBuilder instead, making it clearer what that argument actually represents? Not sure though if that would be possible for the other builders as well though, so just an idea.
As mentioned above, maybe we don't need this here.
Wondering if it would be possible to create the builder first, then call all these setters in the parsing loop above already. Not really that important though.
Maybe throw error here it `nested_filter` is the only allowed option here.
Sure, thats fine.
Yes, sorry for the confusion, I remember the discussion now. Maybe just rename then, although `elementName` is also fine.
maybe also add a protected constructor that sets the tag? This allows us to make the field final too.
I am fine with making the field final, I didn't like the field repetition everywhere, that's it. calling super is fine ;)
I think the logging in this class is unneeded. if you debug it you can add it back
is this really needed we already have sooo much logging everywhere
ah the new `declareObjectArray()` on `ObjectParser` did the trick here.
+1 this is much nicer. I like the fact we can tell ObjectParser to just work with ArrayList. I kind of thought that a custom POJO was always needed...
Maybe package private if only needed in the tests like at the moment? Or move there? No strong opinions though.
Looks like `FieldSortBuilder` is the only impl of SortBuilder that actually supports `missing`, so I'd try to delete this from the interface and move it there.
Maybe add check for object reference equality.
Note to remember: while this is kept as a QueryBuilder internally, I think we need to make sure to call `toFiler()` on it once on the shard (e.g. in the new build() method, doesn't seem to be there yet)
I see, so parser always sets both "order" and "mode", regardless of whether they are set by the user. But what if we only go through the java api, use a plain builder and set "reverse = false". Translated to json this should give us "mode = MIN", but only if not explicitely set by the user otherwise, no? Sorry, haven't got a good solution myself so far either.
Hmm, doc says `The order defaults to desc when sorting on the _score, and defaults to asc when sorting on anything else.`, so maybe having the default in the enum is missleading.
Nit: I might get something wrong, but seems trappy to me since it goes to Object equals now (and does the right thing), but if any other superclass (like SortBuilder) would implements it's own equals() this would short-circuit everything. Of course there are tests for it now, but I'd do an explicit check for reference equality here.
Maybe you could put the validation removed from toCContent here. (point.size > 0)
Were the `params` here a mistake before? If not, with non-null params this would have rendered to an array, which I think the parse would not accept. Looking at older commits (e.g. 10ec2e948a2f1426a5058ca3b2c2e39952f141d8), this looks like once where the `builderParams` but this was mixed up with the script params at some point.
I'm not 100% familiar with suggesters at this point, but from looking at the docs, I think `name` here refers to some custom name for each suggest entry in the request (like "my-location-suggestion" etc..) and its used amongst other things to name the results in the response. The NamedWritable#getWritbaleName() should provide a unique name for the type of writable object. In this case I think the abstract class shouldn't implement this, but the three implementations (Suggest/Term/Completion) whould return how they are identified here.
I'd try to avoid this, expecially since it looks like the name and suggester field should actually be mandatory. They are used in toXContent and if not set this will cause NPEs. Instead, each implementation of SuggestionBuilder#writeTo() should create the correct instance with name and suggester already read from the stream.
I mentioned the NamedWritable#getWriteableName() above, I think "term" here should be a constant that identifies the type of SuggestionBuilder this is. Then we don't need it as an argument in the super constructor anymore.
We didn't serialize or use the defaultType of ToXContentToBytes so far, neither in the QueryBuilders nor in any other elements of the SearchSourceBuilder. I think it is safe to ignore this, because the only thing it does is to determine which type of xContent the object is written to when `buildAsBytes()` is called without an argument. It doesn't really change anything about how the object behaves on the shard.
I don't think we need to change this here.
I think the sub-type (e.g. TermSuggestionBuilder) should read the name and use the constructor).
I think we can get rid of this field in the abstract class, as far as I see it can be "term", "completion" or "phrase", those should be the NamedWritable NAME constants in the subclasses, so the superclass won't need to store it anymore.
I think we can use the unbounded wildcard here since SuggestBuilder will just add to `List<SuggestionBuilder<?>> suggestions` anyway.
Sorry, didn't see that, you're right.
We would need a call to super() somewhere here. Or, like we did with the query builders, have the superclass declare two abstract doEquals/doHashCode methods that it then calls and the subclasses need to implement.
This should be immutable I think, it looks like its mandatory.
Oops, sorry, brain fart. :)
Can you add a check for reparsing (ie taking a settings that have been run through archiver and using them in another settings builder) the settings works? ie the setting stays archived and doesn't disappear.
I also think that this should log the input instead of the normalized input.
Boxing of primitive `boolean`s always leads to the same references as this is guaranteed by the JLS so the explicit call to `Boolean#valueOf` is not needed (however the OpenJDK implementation of the boxing conversion here is to just insert a call to `Boolean#valueOf` but that's just an implementation detail). > If the value `p` being boxed is `true`, `false`, [...], then let `r1` and `r2` be the results of any two boxing conversions of `p`. It is always the case that `r1 == r2`.
haha! I was pretty sure you were going to answer that :)
this should be `public static final Setting<TimeValue> INDICES_CACHE_REQUEST_CLEAN_INTERVAL = Setting.positiveTimeSetting("indices.requests.cache.clean_interval", TimeValue.timeValueSeconds(60), false, Setting.Scope.CLUSTER);`
this should be `INDICES_CACHE_REQUEST_CLEAN_INTERVAL.get(settings)`
I tried to suffix all settings with `_SETTING` to make it's type clear
this can be: ``` Java public static final Setting<List<URIPattern>> ALLOWED_URLS_SETTING = Setting.listSetting("repositories.url.allowed_urls",Collections.emptyList(), URIPattern::new, false, Setting.Scope.CLUSTER); ```
can this be: ``` Java public static final Setting<URL> URL = new Setting<>("url", "http:", URL::new, false, Setting.Scope.CLUSTER); ... URL url = ...; if (URL.exits(settings) == false && REPOSITORIES_URL.exists(settings) == false) { throw new RepositoryException(name.name(), "missing url"); } ```
s/! Strings.hasText(path)/Strings.hasText(path) == false/
Would it make sense to create a new method in `Randomness` with the following signature: `public static Random get(Settings settings, Setting<Long> setting)`? Now we "unpack" the key, just to get the settings in `Randomness` the old way again.
I think I was not really woke up. I thought it was a test assert... ``` java assertThat(Discovery.REFRESH_SETTING.getKey(), is("5s")); ``` Stupid me! :)
I've already tried that too. It has to be a compile time constant.
Or just reference it as `PhraseSuggestionBuilder.DirectCandidateGenerator` instead of its FQCN
> Also, for my understanding - what is the difference in terms of class loading between running that static `NettyTrasnport.DEFAULT_RANGE` that was before? Accessing static compile-time constant variables will not trigger class initialization, but any other access to a class member will trigger class initialization immediately before the first such use (the JLS provides very strong guarantees about [when](https://docs.oracle.com/javase/specs/jls/se7/html/jls-12.html#jls-12.4.1) and [how](https://docs.oracle.com/javase/specs/jls/se7/html/jls-12.html#jls-12.4.2) this occurs). > This is so easy to slip without anyone noticing. Yes. The [suggestion](https://github.com/elastic/elasticsearch/pull/14549/files#r44182190) for a test for this has come up before.
yeah again, its definitely bad the way it is now, easy to break. if we initialize netty classes too early before security kicks in, we might not notice anything, then suddenly users jvms are crashing (e.g. because of some bug in unsafe/native usage, or whatever). with a netty module things would get way better: one advantage is, netty isnt on the classpath anymore, instead we load it explicitly with `URLClassLoader.newInstance` in PluginService, followed by `Class.forName` and so on with the registered plugin class from the plugins configuration file. So it would be well-defined exactly when these classes will get loaded, and its all after security and everything else is fully initialized.
> Last - if there is any way to determine the list of classes loaded at the point security kicks in, Yeah, you can do something like this: `JAVA_OPTS=-verbose:class bin/elasticsearch`
Shouldn't this check instead be: `if (!(obj instanceof SuggestionBuilder))` Because sub-classes of this class will call its equals to test the equality of its specific private fields.
My bad - didn't see this actually went against the feature branch. Sorry.
Not one of my creative days today. Maybe in the org.elasticsearch.test package, no idea what the classname could be :)
+1 to moving this to its own file
is this somewhere on a todo? I'm afraid we'll loose it
Thank you @brwe :)
this means the mutable ShardRouting will have their hashes re-calculated every time . Can you double check the impact this have on a huge routing nodes and the balancer (your favorite code :)).
depends (which is why I asked). If it's about API bwc I think we should break it and be clear about the impact of the version. Which is also makes me think we should not render the field if we don't have a version (because we removed it)
can we make that -1 a constant and use it in all the relevant places? it would be easier to remove it once we go to 4.0
> The allocation id has only been added to ShardStateMetaData in v3.0.0. Good point. I'm fine with leaving as is
fyi - this gives you double [[]]
Sure, it was just an observation. :)
I see `.get()` more than `.execute().actionGet()` in newer tests and I use it myself just because its less typing. I'm just curious why you changed.
I _think_ you can drop this one.
+1 to .get()
Got confused. It actually likely the previous routing node has an older cluster state in which we should override the existing value.. nevermind
maybe - failed to find primary but cluster state version [{}] is stale (expected at least [{}]).
maybe better to say "failed to find primary despite of request being routing here. local cluster state version [{}]] is older than sending node (version [{}]), scheduling a retry... "
I see where you are going with the minimumClusterStateVersion here. I would prefer being more explicit and naming it `routedBasedOnClusterStateVersion` or something that implies when it's set. In your original suggestion this was a hard requirement, but now we only use it on failure to resolve a primary.
can we sometime just rely on the wrong allocation id? (and have a valid node)
can we move the catching logic to `constructPipeline(...)`? then we can create unit tests for this and remove the java integ tests in `IngestClientIT`.
maybe just return null if all is ok? then we just need to do a null check at line 158.
we should include `e` here, otherwise we lose the cause of the configuration error.
nit: IMO `sort must not be null`
Also, we will need the oposite conversion from the ES enums (e.g. TermSuggestionBuilder.SuggestMode) to the Lucene enums (org.apache.lucene.search.spell.SuggestMode) used in the DirectSpellcheckerSettings later anyway, so rather than having `fromUnderlying` better turn the direction around to each enum knows how to produce the corresponding low-level enum.
I don't think we should allow null's to be passed into Builders as this can easily lead to NPEs. If this settings are not required then the user should not set them rather than setting them to null.
Why do we need a concrete and an abstract method for each test where the caller only calls these methods anyway? I think we should just make `testReadFrom` and `testWriteTo` abstract.
We should also have tests for testing the ordinal value of each enum value (`Enum.ordinal`) and for checking that reading a random invalid ordinal throws an exception. See ShapeRelationTests for examples of what I mean
Good point, but my personal opinion here is that if the value is optional we should allow null. It would be different if this can overwrite a default settings, but thats not the case here. I'm not a big fan of the annotation though.
Can you put the read/write logic for the enums into the respective class? That way it's easier to see how these enums are sent over the wire.
I think we don't need to make the global SuggestBuilder implement NamedWritable, simply Writable should be enough. This is the only implementation (and will likely stay so) so we don't need to differentiate it from others on the stream.
I like the succinctness of this checks, wondering if java 8 offers anything to avoid having to introduce the utility class for this though.
Either leave it or throw an error here, so someone takes a look at the test.
what is unused is unused :) I have no idea how it became unused though.
I think NamedWriteableAwareStreamInput was introduced just a bit later. Only my guess.
A value if 0 seems to be okay here. At least there are some tests in SuggestSearchTests (unfortunately in the lang-mustache module) that use that and now fail with an exception.
nit: `accuracy` instead of `Accuracy`
These should all have sensible defaults rather than being null, as we have done with all the other builders. This also means we can remove @Nullable from all of the methods and add checks in there that throw an exception if null is passed in to make it safer. The defaults we currently use can be found in DirectSpellcheckerSettings.
nit: `maxInspections must be positive`
Wherever makes the most sense really. In this case I would put the default constants in `DirectSpellcheckerSettings` I think
nit: IMO `suggestMode must not be null` sounds more explicit.
nit: `if minDocFreq is greater than 1, then it must not be a fraction`
nit: `if maxTermFreq is greater than 1, it must not be a fraction`
nit: maxTermFreq must be positive
minDocFreq must be positive
nit: `stringDistance` must not be null`
nit:`maxEdits` instead of `max_edits`
minWordLength must be greater or equal to 1
This test is not really testing what we want to be testing here. The reason that it's not is because the cache key for a file named `".hidden_file"` is not `"hidden_file"`, but rather it is `""`. A file named `".hidden_file"` never would have been processed by the compilation engine because it doesn't have an extension. So this will ultimately throw, but not for the right reason.
This is gone from master now, you'll have to integrate master and rework.
I think this should go before the previous check because otherwise a trace log will read that the script file is being loaded, and then that it is being skipped.
I don't think so, that's relying too much on an internal implementation detail (that there is a cache, that file scripts are compiled and put into the static cache, that the key is the prefix of the filename, etc.). The purpose behind the PR is to get the script service to ignore hidden files, and that is what needs to be tested. I haven't looked too closely, but I suspect that you're going to have to hook into the script service or maybe the resource watcher and possibly refactor a little bit to expose the pieces needed to in fact make this assertion. Let me know if that's enough to get you started, I'm happy to take a closer look if needed. :)
can you remove this line break? :)
`expectedType.cast(e)` should remove the need for the unchecked suppression.
is this a leftover? I don't see where this is used outside of tests? and even there I think it's a huge overkill. Can we please remove this entirely. If you really need stuff like this for testing then look at `ThreadContext#setTransient` which you get from a threadpool
I think this is a mistake here you didn't want to pull in some guice class here right? `+import org.elasticsearch.common.inject.Provider;` Can't we just make the privder be a string instead? and if necessary it should be a `Supplier`
oh nevermind I know why it's Streamable since it's send as a request...
can we please unpack the tuple right away instead of using v1 v2? just easier to read
I also think we should not catch the excep here.
make it an IllegalStateException please
make it an IllegalStateException please
can we rename this to `boolean isCanceled()` and then instead of the exception just return a boolean? I think it would be more intuitive and we really don't need yet another exception
Copy and paste leftover.
Er, probably not. But a bit confusing name because it looks like a typo.
Copy and paste leftover.
Copy and paste leftover.
The great irony is that these are really hard to read in the font that chrome uses for plain text files so I have to squint to make sure these are right.....
Snuck in but it doesn't matter.
Just personal preference, but I prefer the `Setting.groupSetting` version of this since it's explicit that the method is static just from looking at it in the code context, but that's probably too personal-preference-y for this :)
This is a different indent than the standard, Intellij and the .dir-locals.el indent these to be opening `(` for class constructors (personally I don't like it, but Intellij  _ )
Another personal preference, but we should be able to avoid empty opening `(` here
oh that is true - wasn't always the case... wonder what happesn if you run in an IDE? not sure if need to support it.
either way that's fine
I think we should use an ExtensionPoint instead and be able to register the FsProbe to use. I'll have a look at it but it would be nice to have something similar to `Transport` were we can register different transport modes but use one specifically.
do we really need to walk these directories, can we just do what `getFSInfoIgnoringQuota` does? I really don't think we should walk the direcotries
I don't think we need the filtering on the node. How about using: ``` shardRoutingTable.assignedShards().stream().filter(as -> as.isSameAllocation(failedShard)).findFirst().isPresent() ``` If true, we keep the shard failure, o.w. we clear the iterator...
same here re iterator.remove()
we should remove the iterator in this case. I would just do: ``` if (indexRoutingTable == null) { iterator.remove(); continue; } ```
just do `registry.getIndexSettings().getIndexVersionCreated().onOrAfter(Version.V_5_0_0)`
the "efficient" way to consume binary doc values would be to do ``` java BytesRef queryBuilder = binaryDocValues.get(docId); if (queryBuilder.length > 0 || bits.get(docId)) { queries[docId] = parseQueryBuilder(docId, queryBuilder); numQueries++; } ``` This works because binary doc values guarantee that docs without a value would return an empty BytesRef.
actually I'm wondering if we should use an IntObjectHashMap: given that the percolator works on a type, I'm afraid users have a lot of data in other types too so creating a Query[maxDoc] could cause an OOME on some of them. By using an IntObjectHashMap we will be more on the safe side: it will une more memory if all docs have a query but much less in the sparse case.
ok sorry I understand now that I see the rest of the PR
I think we should keep it in 5.x and remove in 6.0.
hmm actually I think we should load deleted queries too
Can we make getHighlightFields always return a non-null value? (using Collections.emytyXXX if necessary)
+1 to a follow-up
I think I'd rather stay on the safe side
Since we use `Type.fromString` only here maybe should change `Type.fromString()` so that it throws the right exception? We would have to change the signature to accept a tag.
well it isn't very nice... but I think better than throwing an excepting and then one method higher up in the stack catching that exception and re-throwing it.
right, I see. It is just that the caller will never see this exception, the node processing this exception just won't ack the cluster state update. Also the if there is an issue during pipeline creation the failure is likely to be caused in the put() method as we construct the pipeline to see if something is wrong before submitting a cluster state update. The only time I think we end up throwing an exception here if when an ingest plugin isn't installed on all nodes. Some nodes will fail the processor factory for a specific type. If this would happen then it isn't a good situation to be in, so we should think about how to best tackle this problem and I think we should open an issue for this.
I don't think this catch logic is needed here? We can just let any exception bubble up here, because this logic is executed when each node receives a new cluster state and is processing the pipeline config.
I don't have a complete design here but I feel like phase should be called something more generic (like stage for example), moved all the way up to the task level and have a common expandable dictionary. The stage description is a bit tricky, but it looks like we have several stages like INITIALIZING, SENDING_CHILDREN_REQUESTS, WAITING_FOR_CHILDREN, CANCELLING, DONE, FAILED, etc that will be shared among many tasks. But then we will have some stages that will be important but very task specific. So, a few designs with pros and cons comes to mind. It can be an enum (to describe common stage) + string (to optionally describe a specific phase), or a class extending some common Stage class, or some ever-extending enum where tasks will add their specific stages (the simplest but not going to work for plugins), or maybe just a string and an utility class with a bunch of constants defining common phases.
The task management is opt-out for now. We can change the default implementation to make it opt-in, but I think that to get the most benefit we should enable it for most if not all actions. We discussed the getDescription handling in other PRs and the main problem with the current default implementation is that while we indeed evaluate it lazily, we also hold the reference to the entire request for longer than otherwise needed, which can be also bad for large requests. So, I am considering making the default implementation to be just a string instead of a string provider (which should have been supplier and not provider anyway) with default value of `""` instead of `TransportRequest#toString()`
I didn't realize we are always creating a task (since the task management infra supports null tasks, I gathered this is opt in). I had a more careful look and the description is indeed evaluated on demand. Benchmarks also look OK. Thanks for explaining.
This can just be `System.out.print(msg)` now I think. Thanks for removing the formatting from these print apis! I think that was the real problem, its trappy for a `printf()` to be named anything other than `*printf()`. And in this case the caller can just always `String.format` themselves.
I don't like doing this "try a url and on error do something". Can we instead add an extra condition to the clause, so in addition to having 3 coordinates separated by colon, it does not contain `/`? All URLs will have a slash. Then we don't have to worry about this code trapping a malformed URL and attempting as maven coordinates, and the URL handling below can handle malformed urls.
Good catch @javanna.
I think that it's cleaner to write this as: ``` ElasticsearchParseException ex = (ElasticsearchParseException)ExceptionsHelper.unwrap(e, ElasticsearchParseException.class); assertNotNull(ex); assertThat(ex.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]")); ```
Is this deletion related to the refactoring or just something to do in general? Just asking because I don't get the details here.
+1. Maybe this helper so far is limited to the query / suggester tests only, but then again it might be general enough for ESTestCase.
+1 on pulling this out, I'm sure this can be used in other places, although in many of the tests I'm thinking about right now we have NamedWriteable things under test, but only the copy method would need to be slightly different.
nit: Everything okay, but I found this (and the following same constructs) a little hard to read. Can I suggest writing the boolean flag first without conditions (e.g. `out.writeBoolean(fuzzyOptions != null)` and then have the if-block only? But either was is fine.
I think this should be done via IndexShard#failShard (which can be exposed via indexShardReference ). Will be cleaner and faster (it's local fail first, then notify the master)
I think we should fail the local shard on any unexpected error. Seems like the "safe" thing to do..
this is already logged in the shard state action
I think we can do simpler by just returned a retryable exception to the reroute phase that started all of this. It will do the same thing. Also - I miss the annoying "request" reset (we should open an issue to remove it). I guess it's still coming..
Also it seems really silly that we have 2 properties here...
I think changing any property names we use to just pass info from the launch script to ES should just be under a different prefix.
If it is just trying to avoid adding a "foreground" setting, then I think the solution is to not use `es.` prefix? Then it won't be put into settings.
and so the registry returns to the scope of the factory! I am a fan of the `ProcessorRegistry` cleanup
unused import now, no? https://github.com/elastic/elasticsearch/pull/16432/files#diff-208398fdbe888b55ad36dd4f161fdf48L22
just leave that here for consistency? the other tests have it as well
Simon added a fancy resolveIndex method
args are not used.. we should either remove those from the method sig (which will be consistent with the `println` methods) or pass `String.format(text, args)` to `doPrint`
This method is not necessary. With the code as is, we would be extracting the entire zip, but only using the one directory from it. Instead, we should do checks when extracting, see the `unzip` method. There we can have a prefix check like: ``` if (entry.getName().startsWith("elasticsearch/") == false) { // only extract the elasticsearch directory continue; } Path targetFile = target.resolve(entry.getName().substring("elasticsearch/".size())); ``` This will unzip everything in the `elasticsearch` directory directly into the temp installation directory, and all the other plugin cli installation code can work as-is.
This was not a typo
I thought it was a typo as well until I read https://en.wikipedia.org/wiki/Luser :p
I don't think this is a usage error (it's not something the user did wrong, it is something wrong with the plugin). We also already get a FNFE when the descriptor is missing, is this really necessary? I don't think this message, vs FNFE, will give any better indication to the user that the plugin is broken.
zips do not use the filesystem separator, they always use `/`. I don't think this is necessary. Below we should just use `substring("elasticsearch/".length())`
I think you're changing the import order from our standard here...
+1, rather catch them early than with transport weirdness later on
Do we need this one? It should be covered by the check in clearProperties.
I know that this code did not change in this PR but couldn't we just expose an endpoint setting and have the user set it instead of deriving it ourselves? This would also save us some maintenance effort.
I _think_ you can use `Setting.groupSetting(DISCOVERY_EC2.TAG_PREFIX, false, Setting.Scope.CLUSTER)` here instead of just a string.
Duplicate from the `discovery-ec2` module. Do we have any chance of sharing this code across modules without too much hassle? Otherwise we might forget to update the settings here in case we change them in the other module.
This triggers a checkstyle error regarding star imports. Reproduce with `gradle precommit`. Note that there will be a few other checkstyle errors too from other classes (line length > 140; star imports).
I wasn't entirely sure either but good to know! :)
By that I mean a switch or something similar to what SuggestUtils#resolveDistance() does.
Also please `Locale.ROOT`
nit: we almost never use `Locale.US` exept for some number formating. While I think it doesn't make a difference for the enum names in question here, I'd suggest going with `Locale.ROOT`
I don't think raising en exception to save a few lines of code here is a good idea, please change this back to how it was before.
ok let's leave it as is for now.
I think it's in the right place here.
We can simply add responseSupplier to the constructor of TransportChannelResponseHandler (same I did in #17752). We can then remove the static methods in that class (one of which should be obsolete anyhow by the change here w.r.t. master).
Nit: `"call back"` -> `"callback"`
Nit: `"call back"` -> `"callback"`
Nit: `"call back"` -> `"callback"`
This is logic that I think should go into ReplicatedOperation.
This is logic that I think should go into ReplicatedOperation.
I wonder if this case distinction should be part of ReplicatedOperation.
I've never been a fan of implementations where every derived class _must_ call the super method as is the case here. With this change, I would much prefer that `resolveRequest` be made final and delegate to a protected `doResolveRequest` that the derived classes override. Either that, or some infrastructure is put in place (tests, or something else that breaks the build) to ensure that the derived classes always call the super method and we are getting the behavior that we want here.
The corresponding method resolveAndValidateRouting for deletes is a static method on TransportDeleteAction. For consistency, we should either make this one a static method on TransportIndexAction or move the one in TransportDeleteAction as instance method on DeleteRequest (my preference). Same for resolveAndValidateRouting in TransportUpdateAction.
I think that this too can be private.
Nit: there is an excess blank line here.
nit: `"it's"` -> `"its"` (or maybe even `"a"`)
Nit: `"call back"` -> `"callback"`
There is an excess `;` on this line.
I think that all of these members variables except for `finalResponseListener` can be `private`.
Just wondering if in the future we could use IndexShard as a point to synchronize recovery and replication. An IndexShard representing a primary could also precalculate the shards to replicate to. The code here would then just have to ask IndexShard where to replicate to.
nit: I have nothing against being defensive here with `pendingShards.decrementAndGet() <= 0` but I would like the assertion to capture `== 0` check. Preferably on the same variable as pendingShards is modified concurrently. This means: ``` int pendingShardsCount = pendingShards.decrementAndGet() if (pendingShardsCount <= 0) { assert pendingShardsCount == 0; ... } ```
Does it make sense to pull out `SmoothingModel` & co to an upper level class? think that will simplify `PhraseSuggestionBuilder`
IMO, it would be cleaner to have `QueryShardContext` and `Suggester` as params in the ctor instead, since they are mandatory params? I have taken a stab at what I mean [here](https://gist.github.com/areek/afda767f37263558c4a6)
Thanks for fixing this!
I know it was already the case, but can we make this allocationFound parmater actually count the number of allocation found (i.e. all shards with an allocation id, matching or not) Later we can refactor this maybe and remove it completely but for now let's keep it consistent.
I think it should rather be an illegalargumentexception since the problem is with the request (so that it would be a 4xx code instead of a 5xx code).
3 more indentation issues above
Thanks for the clarification @nik9000.
I'd remove the "well". To me it sounds too informal here.
I'd remove the "well". To me it sounds too informal here.
The interface design is odd at first but makes much more sense with the explanation.
You're missing "create" requests here (not sure if you want to support them, just wanted to make sure you knew)
> I'm not sure that negative numbers actually break vint. Yeah, they don't break it, they are just more inefficient, using `writeVInt` to me is kind of an indication that you never expect a value to be negative (from the code-side), so it's nice to encapsulate what you expect for the intended values. Could always change this to `writeInt` instead
Same, copy-paste error here
This looks like a copy-paste error
`Objects.hash` is going to cause an array allocation; if `Object#hashCode` is not going to be called on these objects, that's fine, but if it is and it's going to be called frequently, then we should think about whether or not we should really be using `Objects.hash` here and in the other places in this pull request.
And we could then just leave an assert here.
why is `<QB extends QueryBuilder>` needed? it hides `QB` that's already defined in this class.
good point! I think we need to iterate over the filterFunctionBuilders and rewrite their corresponding filters
This one makes FunctionScoreQueryBuilderTests fail with this seed: 5FB367958E3049EF:EB5C14A8F6674F6F . that's because although you set allow unmapped fields to the context, function score needs to have the field mapped (see DecayFunctionBuilder#187).
additional square bracket :)
remove this additional line break? :)
yields always true
Thanks for the explanation. Makes sense now, yes. And I learned something too.
this always yields true
weird to declare vars after the constructor
maybe `== false` just so we don't typo it in the future
small typo, 'saving'
may want to annotate with `@Nullable`
@javanna would it make sense here and for all JSON parameters below to use a `ParseField` object instead? For example, here you could define a `QUERY_FIELD` of type `ParseField` and make the conditional here `parseContext.parseFieldMatcher.match(currentFieldName, QUERY_FIELD)` This would also allow you to specify camel case variants like you are checking for some of the fields below.
I think it's simpler to just keep equality, but I'm good with you prefer it this way.
This was added as a protection against a failure during the rename, leaving the shard in a corrupted state (#10053) . We later added better checks in another place ( #11269 ) . I think it's OK to remove.
same question as above re no active allocation ids.
OK. So it's not strictly needed, just an improvement. wanted to make sure I didn't miss anything.
can we use package private methods and have unit tests for this.. an integration seems like an overkill.
I'd prefer warning :smile: Mentally I reserve error for issues like "oh no I'm going to lose data" or "oh boy I have to crash now" rather than "sorry, I'm busted and can't do what you wanted". That feels like solid warning territory for me.
Incredibly minor: should match the value used by the `return` line so that if the method ever changes, we use the same value.
to me we only have node and index settings, the settings that are dynamic are updated in the cluster state so we are already there
iff == if and only if so this is not a typo
Can you shrink-wrap this try clause? (Pull the `map.put` out after it.)
Hmm I just realized we are doing too much work here? We should only be visiting the files belonging to this segment, but we are instead visiting all files in the directory (at least, in the non-compound-file case)? I think instead of `directory.listAll()`, you could use `segmentReader.getSegmentInfo().files()`? This should return all file names that this segment uses ...
I think we need to check the incoming version and have back-compat here, i.e. if the version is < X (X = first version when we add this feature), we read the old (only memory) stats.
It's expected that some files will sometimes be gone here, because `IndexWriter` is actively changing the index while this reader has a point-in-time snapshot open. Maybe we catch `FileNotFoundException` and `NoSuchFileException` and don't log (even trace) those ones? Any other `IOException` we should log.
Actually, I think IW should not remove files that are being used by NRT readers (we fixed this a while back in Lucene), so I think we should continue to log the exception, but can you increase log level to WARN? Also, can you shrink wrap the exception handling? Put one try/finally around the `listAll`, and another on the `directory.fileLength`? And include in the exception message which directory (`directory.toString()`) and which file caused an exception.
you could just do `this.ranges = new Range[ranges.size()];` I think.
I think this is tricky to do here without any backwards compatibility. Users were able to do that in the past so on upgrade they will get an exception and they will not be able to fix it. I think we have to move this into where we validate our settings such that it only applied to new indices and old indices should still have this ability? I think we need to add something to the settings infrastructure to prevent this from being added but we should fail here too if the index is onOrAfter 5.0 so this means we should make this: ``` Java if(BUILT_IN.containsKey(name) && indexSettings.getIndexVersionCreated().onOrAfter(Version.V_3_0_0)) { throw new IllegalArgumentException("Cannot redefine built-in Similarity [" + name + "]"); } ```
this looks great! Can you fix the indentation to 4 spaces and also assert that the key has a `.` in it ie `s.indexOf(".") > 1` I think it's ready then. Thanks for fixing this great job figuring it out!
typo: specila -> special
Ah. I get it. You are checking that the first string is always a constant. Sometimes it makes for weird things like this but that should be ok.
You've changed the format of this slightly. Probably best to just remove `info:`.
Oh, you're right. Then it makes sense as is.
:heart:, this is one that I've wanted to do for a long time.
can we name that `getWorkaround`
Missing a space here after `id`
It looks like BASE_PATH can be removed as well.
I think we should support it for individual repositories. Not really sure what would be a use case of supporting it globally.
`Objects.requireNonNull()` either returns the first argument or throws an exception so you could simplify this to: ``` java this.lifecycle = Objects.requireNonNull(lifecycle, "lifecycle must not be null"); this.logger = Objects.requireNonNull(logger, "logger must not be null"); ```
I am not sure whether the log message is too specific, i.e. the subclass must not necessarily be a service.
Nit: There is a typo in the method name. The probably intended name is `findOverlappingSetting` (the "i" is missing)
`BrowserCompatHostnameVerifier` is deprecated in later Apache HTTP client releases. Additionally, it does the **wrong thing** for wildcard certs. I think we should use the non-deprecated strict hostname verifier
These non-immutable classes :(
Instead of creating the setting here, it should be a static setting defined in the class, similar to the way settings are defined in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/allocation/DiskThresholdSettings.java#L37
We don't really use the `Settings.get` method now, it should instead be `TEST_SETTING.get(settings)`
Oh I see it called above, nevermind, instead, this should be: ``` public String getTestConfig(Settings settings) { return TEST_SETTING.get(settings); } ```
I mean maybe instead of declaring the key and the default we should instead declare a `org.elasticsearch.common.settings.Setting` and use it. Like, for example, AutoCreateIndex#AUTO_CREATE_INDEX_SETTING. That might mean moving the setting out of this configuration class and into some other place to feel more natural, but that is the whole point of the jvm-example plugin - to have a working semi natural plugin.
I honestly think constants like this are more confusing than putting the string in the code. I see this and I can't tell what it is for until I scroll to its usage. The `Setting` class does a good job of making it obvious what the parameters do because we know the order.
with changes above, this can become tribeService.start()
Yeah, I was just wondering why we need a dedicated `DiscoverySettings` class. In other places we just pass `ClusterSettings` around.
Can we explicitly set the blocks here? Advantage is that - no need for TribeService to depend on DiscoveryService - no need for newly introduced method `removeInitialStateBlock(int id)` as we know exactly which blocks we previously applied. Even better would be to also set the STATE_NOT_RECOVERED_BLOCK block for GatewayService here. We could then not set these blocks in the first place if `tribeService.getNodes().isEmpty() == false`.
Ok, I hope we get to see a follow-up :-)
same here re type of equality.
Please don't remove this method here, it would make this a breaking change in that it would break any plugins that relied on watching for ClusterChangeEvents where the blocks have changed
can we add a note saying this is an reference level equality and not a true equal.
Wildcard imports will break the check-style build (ask me how I know :D)
General thought for these various `Double.NaN`'s .. should we be returning `null` instead (and subsequently returning a `Double` instead of `double`)? E.g. because some fields may actually have `NaN`'s as real values, and lacking a field isn't quite the same as not-a-number? Not sure, probably needs several more opinions :)
This name won't work as I can specify multiple scripts of a single type (e.g. `inline`) and the `ParseField` name for them all will be `"inline"` so they will overwrite each other. I think we need to go back to the parser here and parse a `Map<String, Script>` so each script can have a name. In the request this would look like: ``` { ... "script": { "my_script_1": { "inline": "script contents", "lang": "expressions" }, "my_script_2": "script contents", ... } ``` Note that scripts can either be a JSON object or a String. The `Script.parse()` method handles both cases.
I'm not sure we need to send this over the wire though.
Err, the next one a TimeValue, this one a DateTime or something? Its not required, at all and I just thought of it but a TimeValue would make runningTimeNanos easier to read I think.
I believe we've been just using the string version of field instead of these lately.
Probably makes more sense given where we are going. Sorry for making more work for you!
This line will break our `precommit` checks because it violates the 140-character line-length limit.
This swallows the `numberFormatException` but it should instead be wrapped by the `ElasticsearchParseException`.
I think we should only have one client method here. So I would replace the previous line by the new method.
s/`Strings.isNullOrEmpty(publicKey)||`/`Strings.isNullOrEmpty(publicKey) ||` (space missing)
Ok that makes sense to me.
Missing space. Should be: `String publicKeyBase64 = getValue`
right, simulate api won't forward the request to an ingest node but always execute locally, forgot about that. Then this PR is good to go , unless we want to revise simulate.
I think I see the value of validating pipelines for ingest pipelines etc. yet it comes with some cost, meaning every put pipeline means going to all nodes to check what processors they have. That said, put pipeline is not an api that will be called all the time, so it is probably ok and I don't see a better way to validate things at the moment.
BTW I am adding the forgotten filtering to DiscoveryNodes as part of #16963 anyways.
we could potentially select only the nodes with ingest set to true, but we'd have to add support for this filter to `DiscoveryNodes#filterNodesIds`
I think we can just do this: ``` Java if (value instanceof Number) { return Long.toString(((Number)value).longValue()); } ```
hmm can we make source a local var and only assign it once we are successfully rewritten and exited the loop
I wonder if we should only do this if the cache is enabled. for all other operations its unneeded and might be overhead? - I mean not the rewrite but the setting the field stats provider. We check this with `IndicesService #canCache(ShardSearchRequest request, SearchContext context)`
alright, lets just keep it that way for now.
also unnecessary boxing `((Long)value.longValue())`
can we do this `((Long)value).longValue())` no boxing needed here
I'm happy we made those exist queries fast. :)
can we add some assertion here that ensures we copied everything? I think we should have a method that does the clone that we can use to assert that we didn't loose anything something like this: ``` Java assert serialize(this.shallowCopy(queryBuilder, postQueryBuilder)).equals(serialize(this)); ```
can you add some inline docs explaining what the hack we are doing here? :)
same here ``` Java if (value instanceof Number) { return Float.toString(((Number)value).floatValue()); } ```
``` Java if (value instanceof Number) { return Double.toString(((Number)value).doubleValue()); } ```
I think we can't do that this way, for the query cache to work we have to pass in the `Searcher searcher` that we acquire in `SearchService` just above the creation of `DefaultShardContext` otherwise we will be subject to refreshes and the cache will have broken values.
I think we have to have a test for this, I suggest that we use a single node test that we can control that refreshes after we created the context with a new doc in it matching the query and ensure we are still rewriting to match all / none and then check if we have a cache hit? something like this...
that sucks, sorry :) We can't build stuff that is dependent on the order of how the listeners are added! Can we find a better way of doing this? I think each listener needs to have priority or so and every prioritoy can only be added once? Maybe we don't need this to be a list at all? This stuff is so fragile we have to iterate until it's safe
for instance in RestoreService we use `addLast` at runtime which messes with this assumption, that entire order thing is broken and error prone. The best thing I can come up with so far is to add defined stage like this: ``` Java enum ApplyStage { NewClusterState, NodesConnected, StateRecovered, ShardsStarted, RepositoriesCreated, NodesDisconnected; } ``` where listeners can be registered but I am not too happy about it...
ok however, I think we need to come up with a better model here after all maybe register all stuff up-front and make the registered listeners mutable and more tailored to their usecases and remove and add patterns
didn't see the `isHeldByCurrentThread` ++ thanks
it's really taste I guess so fine with me
we are generally moving away from gazillion packages and classes I am not a fan of all these service and they make things more complicated than they need to be today. I have a hard time to understand what feels wrong here and where you draw the line
we have to have some assertion that this is executed before we are started. And it should also be synchronized as well as doStart()
It is no concern at all, it only prevents deadlocks and performance issues :) Don't use parallel streams with the default pool, specify a pool instead (in the same or child thread group): http://blog.krecan.net/2014/03/18/how-to-specify-thread-pool-for-java-8-parallel-streams/
instead of having this setter I think we should do this in `ClusterService` and `TransportService` ``` Java @Override public void start() { throw new UnsupportedOpertationException("use start(DiscoveryNode) instead"); } public void start(DiscoveryNode node) { this.setLocalNode(node); // private call super.start(); } ``` WDYT
any time we measure deltas (how long something took) we should use nanoTime. currentTimeMillis can be reset by ntp ... if the bulk api uses it for this purpose, it should be fixed...
I think it's possible the ingest phase will take genuinely take 0 millis (i.e. <1ms)? in that case we want to report. I would suggest using a negative value to indicate "ingest never run" and suppress rendering in that case alone.
you can sue ZLong for small values (FYI)...
can we make a constant out of the -1L it's easier to see where it's used :)
Are there any calls to this version of findTemplateBuilder with matchType `string`? Or `findTemplate` below? very confusing how we have so many public variants of this method...
You don't see the resemblance between the two files!? :)
Can we please not show an example of such a silly use of hamcrest matchers...this should just be assertTrue(true)...
we need transport stuff too, for example, TransportSettings#PUBLISH_HOST (but there are more) . I wonder if we should just pass through everything prefixed with network and transport.
I'm doubting here too, though I see the value in a controlled environment. The true long term fix here is testing so we know what we need. Do note that one can set ANY setting on the tribe.\* level. This is about defining inheritance.
I have no idea what else we may need also, it is not the first time we forget about some needed settings... looking back I am not sure that the approach of selecting settings is correct, judging from how many problems it caused to users. It may be cleaner but the truth is that it is easier to define what settings don't make sense in a tribe client node and exclude them rather than having to include all of the settings that people may need passed through. My 2 cents.
nit: why not just use the setting objects? we could then use a the exists methods instead of check for null values.
+1 it looks like there are some remainders of the boost field mapper that we removed a while ago (eg. there is also ParseContext.docBoost).
I think we are missing includeHidden here
can you remove these? :)
Background tasks run in background periodically, but they shouldn't run for very long time. I guess what I am trying to say is if for some reason a fault detection ping or a stats operation started by a ClusterInfoService runs for 5 minutes it's probably also a leaks, even though it doesn't have corresponding REST request that started it.
I'm confused because I thought you were implying there are cluster level tasks always running in the background, that are simply part of the cluster operating normally. A leak is a leak, and we should catch and reject it.
Somehow we need to distinguish "background" tasks for the cluster from those started by rest actions.
This is a Request and setters in requests don't typically have "set" prefix and return the request itself. I don't like this notation, but this is the notation that is used in most of the existing requests including this one. Check detailed setting above.
I am not sure that it warrants "WARN" logging level. It's perfectly fine for some of the tasks to be running in a working cluster. This includes node and master fault detection pings for example. I feel that INFO level logging would be more appropriate here.
You probably meant `return task instanceof TestTask && super.match(task)`.
I am fine with the new way, but then let's fix all other setters and getters to match this one.
What do you mean by a "section", something in the rest test language? If so then I'm in favor of that. But then we should fail hard in rest test cleanup if there are any running tasks (ie the test isn't correctly waiting like it should).
I think everything inherited from BaseTasksRequest should be converted.
Yes, I meant that we should add this method which does not exist. It could be done in another PR.
I think we should remove all these boolean and pass an set of flags as a vararg... It becomes less and less readable (not in this PR)
Can we remove the "ingest_" prefix here? To me this structure should be similar to what we have for: ``` GET /_nodes/stats/indices?filter_path=nodes.**.indices.suggest { "nodes" : { "1S6tILi0QtKdOBVBMXml4Q" : { "indices" : { "suggest" : { "total" : 0, "time" : "0s", "time_in_millis" : 0, "current" : 0 } } } } } ```
Shouldn't this be `return writable.readFrom(this);` ? Otherwise we return the ref retrieved from the supplier, which can be IngestStats.PROTO
Oh right, sorry for the noise.
I think we could change the output a bit here to make it more coherent with the format of others stats. Maybe something like: ``` "ingest": { "total": { "count": 10, "current": 10, "failed": 10, "time": "10s" }, "pipelines": { "my_pipeline" : { "count": 10, "current": 10, "failed": 10, "time": "10s" } } } ```
can we mock TP here? This should make this test easier. Something like this: ``` ThreadPool threadPool = mock(ThreadPool.class); when(threadPool.executor(anyString())).thenReturn(Runnable::run); ```
if we reduce the publish timeout to 0 (which will make the test faster), we need to use the same assertBusy technique on masterNode2 to make sure it has process the change as well.
this is usually a bad sign. We should use sleep anywhere. Sometimes it's needed but we try give all the utilities to make sure no one used it explicitly. In this case we have assert busy: ``` assertBusy(() -> { final ClusterState currState = internalCluster().clusterService(masterNode1).state(); assertTrue("index not deleted", currState.metaData().hasIndex("test") == false && currState.status() == ClusterState.ClusterStateStatus.APPLIED); }); ```
I would reduce this timeout to 0 - we don't care - we check with assertBusy later for execution. Make it fast. Also reduce the publishing timeout by setting PUBLISH_TIMEOUT_SETTING to 0.
I think we can also get rid of the try/catch here, from the test setup we know its not date field if `queryBuilder.fieldName().equals(DATE_FIELD_NAME) == false`.
can we have only one return statement in this method and make this if a `if (changed == false) {`
Yea, the idea was to create a somehow fair movement. That was before we had things like throttled allocation and even the balanced algo, so it might not be relevant anymore.
I like this way more anyway
The `Math.max` calls here and throughout are unnecessary; `System#nanoTime` is relative and will not go backwards.
I think there is confusion about how this stuff works, please, start up elasticsearch and look. try different heap sizes. there is more or less a fixed cost, you know mapping in all the files the jvm needs, libraries all this. on my machine its around 500. but from there on, its all about file handles. and its a fraction of the number of file handles.
it does not make sense to check for 2^16 file descriptors but then 2^18 maps! also by default, we only use mmap directory for certain files so that people dont have to touch this. If we are going to do this check, then we should revert that and go back to full lucene performance (pure mmap!)
Nit: In other instances, we say "will be removed in the next major version" but if nobody else raises an objection, I am also fine with this wording.
Nice! Fine by me leaving the test as is.
Looks like there isn't an ExecutebleScript equivalent for search scripts anyway - ignore this.
Talked with @cbuescher in a chat - since these are just copied from their old place they should probably just keep their implementation in this PR. Moving to test framework is still possible in this PR.
And I was wrong again! It looks like if your NativeScriptFactory returns something that extends AbstractSearchScript then you can indeed use native search scripts....
Its cool. I don't think it is required for the PR. It would cut 50 lines or so - you implement NativeScriptFactory instead of ScriptEngineService and you implement AbstractSearchScript instead of LeafSearchLookup - just less code I guess.
Fine by me.
Maybe a name like "PlusOneMonth"
I think it'd be nice to have this in :test:framework so others can use it.
Probably worth putting an explanation in here.
Maybe explain that it is used in places where you want to make sure that scripts are valid but don't care about the specific script and this is the easiest way to do that.
What is the reason for not just using the timeout on the observer? ``` diff diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java index c8eb8a4..29a11d9 100644 --- a/core/src/main/java/org/elasticsearch/node/Node.java +++ b/core/src/main/java/org/elasticsearch/node/Node.java @@ -336,14 +336,12 @@ public class Node implements Closeable { @Override public void onTimeout(TimeValue timeout) { - assert false; + logger.warn("timed out while waiting for initial discovery state - timeout: {}", timeout); + latch.countDown(); } - // use null timeout as we use timeout on the latchwait - }, MasterNodeChangePredicate.INSTANCE, null); + }, MasterNodeChangePredicate.INSTANCE, DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings)); try { - if (latch.await(DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings).millis(), TimeUnit.MILLISECONDS) == false) { - logger.warn("timed out while waiting for initial discovery state - timeout: {}", DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings)); - } + latch.await(); } catch (InterruptedException e) { throw new ElasticsearchTimeoutException("Interrupted while waiting for initial discovery state"); } ```
The message doesn't really make sense because it's missing the "discovery state" part, right now it says it timed out waiting for initial timeout :)
This should be `== false` right? otherwise we will warn if the latch _found_ a state
Its sad that we have that problem but the log statements make sense then.
Also not gonna make java 7 very happy.
This looks like merge conflict leftovers.
(Not that you have it, it's a matter of preference)
This could be collapsed into the previous `if` statement: ``` java if ((searchResponse.getShardFailures() != null && searchResponse.getShardFailures().length > 0) || searchResponse.isTimedOut()) { .... } ```
(in case that wasn't clear) "bass" -> "pass"
OK :) thanks for checking.
> On the other side if the rejections occur because somebody misconfigured the generic thread pool or something like this the shutdown message will be very confusing. Fair enough re not saying shutdown. Note that @jasontedor is working on making it impossible to configure the generic pool in a way that violates what we expect of it - never reject unless we're shutting down. > trace level though. I think this will cause unneeded noise because when we stop the transport the node is shutting down and any outbound concurrent request may hit this..
can we also log the action name and the fact that we're shutting down? Also, I think this can be trace? (it will flood the logs ...)
I took a quick stab at sth. that avoids the Tuple and replacing the test builder [here](https://gist.github.com/cbuescher/88531fe7c2abd38936ef), but it still looks a little bit strange to me. EDIT: Doesn't work, equals-tests break with this little hack. Sorry, nevermind.
Nevermind, I just got to the part where this is overwritten in CompletionSuggesterBuilderTests. Quiet tricky though, I wonder if this could be made somewhat easier to understand, haven't completely wrapped my head around it yet.
I don't understand the use of this helper method, it only seems to be used internally and by introducing it I think the code gets harder to read. Might just be me though.
Request vs Reuqest Funny. The typo comes from the old IT test :)
OK. That seems to be a bug in javac :-) Maybe we should open issue!
I would have tried: ``` java assertBusy(() -> { assertThat(internalCluster().getInstance(GatewayAllocator.class, master).getNumberOfInFlightFetch(), equalTo(0)); }); ```
Can you use `== false` here...the `!` is almost hidden in all the other text around it...
ta -> to
ta -> to
And the same for TextFieldMapper, although that is a little different since it uses parseTextField...
te -> the
Wouldn't the definition of "upgradable" for string to text/keyword mean the norms setting fits with what is allowed? As this is now, it would mean eg keyword fields could have all of the old settings right, but they would be deprecated...that is just really weird for a new mapper type.
I think that would be better. IMO new code shouldn't be bogged down with backcompat behavior. It would only send the wrong message (eg seeing a deprecation warning when using norms.enabled on a text field, when it should be an error so the user looks at docs and sees "ah, it's just norms now").
Can you make this `== false`? It took me a second to see the `!` :)
caller does: ``` if (indexMetaData != null && indicesService.hasIndex(indexMetaData.getIndex()) == false) ``` so it doesn't care if indexMetaData is null...
this feels weird to have this here (concerning whether we should delete data of closed indices , on master nodes - I feel this should be made higher up). It is a different change though... (and has nothing to do with your change).
we can now remove this localUUID crazyness.. yay!
I think it'd be nice to add an error message here so it is more clear that this is an "index not found" thing.
I think I'd prefer `"failed to remove index " + index` here and get the toString for `Index`.
I don't understand why we need the extra loop.
I think this can become an assertion now? we never expect it to happen...
nit: let's just log the index here (so we get the uuid and such with Nik's latest change)
I wonder if we should make these Map<Index, IndexService> - it will remove all these index.getUUID() calls..
on master we don't have shard locks. I was thinking that for example pending deletes can come back and delete an folder already created for a new index. This is already an issue so we can leave it as is and it will be fixed by making index level folder unique
I see, it became a writable...
looking at usage of this Text index, I think it can be removed and replaced by sharId.getIndexName()
this the part I meant about skipping indices that do not exist anymore. We can accumulate a List<Index> todoList here and pass that to applyRequest, so we don't need to do another resolving round there.
we can just add `index` to the message and be done. the only situation we have where the uuids are not the same is when `indexService(index).indexUUID() != index.getUUID()` in which case things are seriously broken. just cleans up the code a bit - no big deal - was just sharing happiness ;)
this continues our discussion from yesterday (folded away by github). Later on in applyRequest we now have: ``` for (String indexName : request.indices()) { Index index = metaData.index(indexName).getIndex(); ``` but metaData may not have the index in question because we ignore that case here by doing `indexMetaData != null`.
I don't think we need to add logging to the builders: this only applies to users of the java API that would already get compiler warnings because of the deprecated annotation
Since ParseField already logs when using a deprecated parameter, an easier way to do it would be to create the ParseField with something like `new ParseField("ignore_unmapped").withAllDeprecated("unmapped_type")`
Not sure which junit version we are on. Came across the ExpectedException rule here https://github.com/junit-team/junit/wiki/Exception-testing recently. Just mentioning, no need to change this code right now as the above is the pattern we use all over the place.
this applies specifically to generic() threadpool. As this is a more general test case now, we can increase the bounds to (expectedmin, MAX_INT).
on second thought, this might be easier after adding proper settings (with defaults).
can we move this in a sep method and make this `private boolean balance()` only have a single return statement, all these returns are hard to read
that is great! I really appreciate the cleanups here
same here `if (shard.state() != RELOCATING) {`
can we invert this and move the `meat` into the if statement? /me doens't like returns and continues ;) it's easier to read IMO that we skipping the entire body if it's in the if block
same thing with 'even make' here
s/even make/eventually make,
@nik9000 Note that because of 5bbb1312b1b752a87d8ab1721042fad3f2133a7e this code in a slightly different place in 2.x (for the backport).
This is very close to the new check. I'm not sure we need it.
I think we want it to read "received state _with_ a local node that doesn't match the current one"
I think that node provider interface can go away (now that's were on java8 - function refs FTW)
Can you configure your IDE to stop re-ordering imports? I think we already settled on the standard so we should stick with it so PRs don't flip-flop imports depending on who's working on them
(or reverse them, so that a new `TimeValue` is only constructed if a readable rawTime is used)
there's no need to duplicate the functionality here, this can call: ``` java timeValueField(rawFieldName, readableFieldName, timeValue.millis(), TimeUnit.MILLIS); ```
this is unneeded - we just iterate of the list...
can we log the full index object, includeing the index uuid? also it would be good to have the current cluster uuid and the one in the index meta data state.
we should probably rename this method to indicates it searches for dangling and deleted indices.
Oh oh! deleteUnderLock should be called when you hold the lock! instead we should use IndicesService.deleteIndexStore
I think this will result in a double info logging - we already logged at info level when discovering this.
I know we already checked it but I think it's important to make sure all data deletion goes through the same place where we do (and will extend) all the required safety checks. We might even want to restrict access to the delete methods in NodeEnv via forbidden API.
we should remove `IndexMetaData#FORMAT`and `IndexMetaData#FORMAT_PARAMS` because they are now unused because of this
why `if (shardRouting.primary()) {` ? Directly put the assert there.
should have been **cleaned up** by
maybe put this check before the primaryTerm check
// must use exception that is not **ignored by** replication logic. (also 2 more occurrences of this in IndexShard)
Returns the primary term **THE** index shard is on
Is this somewhat duplicate functionality that is also in `applyCleanedIndices`? Maybe combine both of them here? It is weird that `applyCleanedIndices` removes shards.
maybe replace `shardRouting.isRelocationTarget() == false` by `shardRouting.relocating()` (simpler)
I'm with fine with doing that after the review is done. The advantage for rearranging the methods is that in future PRs it is easier to see how changes to two or more of these methods affect the overall behavior as the changes are then displayed in Github UI in the order in which the code is executed.
I believe we can safely expand the semantics of `isRelocationTargetOf` to also incorporate this case. This means changing `isRelocationTarget`, `isRelocationTargetOf` and `isRelocationSourceOf` such that it not only accounts for `INITIALIZING` state. (It is a safe change as a STARTED shard can never be a relocation source and RELOCATING shard can never be a relocation target). For example, replacing ``` public boolean isRelocationTargetOf(ShardRouting other) { return this.allocationId != null && other.allocationId != null && this.state == ShardRoutingState.INITIALIZING && this.allocationId.getId().equals(other.allocationId.getRelocationId()); ``` by ``` public boolean isRelocationTargetOf(ShardRouting other) { return this.allocationId != null && other.allocationId != null && other.state == ShardRoutingState.RELOCATING && this.allocationId.getId().equals(other.allocationId.getRelocationId()); ``` Can be a follow-up, just something to think about.
This method is only used in applyDeletedShards to iterate over the shards... That method is called every cluster state update for each IndexService, which might be wasteful in terms of resources. We can simply use iterator instead.
I find the boolean condition quite hard to read, maybe negate the whole logic to get rid of all this weird `&&` and `== false`: ``` if (newPrimary.unassigned() || newPrimary.isSameAllocation(oldPrimary) || (oldPrimary.relocating() && newPrimary.isRelocationTargetOf(oldPrimary))) { // same primary term } else { // incrementing the primary term ... } `` ```
Can you add a textual description (makes it easier to understand)? I was wondering for example at first why we don't increase primary term upon full cluster restart (then I noticed we do, as isSameAllocation yields false if oldPrimary is unassigned primary).
This exception will be treated as ignore replica exception. :wink:
This exception will be treated as ignore replica exception. :wink:
This exception will be treated as ignore replica exception. :wink:
I guess this can be reused to replace `routedBasedOnClusterVersion` (#16274). Yay :+1:
If you use ImmutableOpenIntMap instead (as for activeAllocationIds), you get cluster state diffs on it for free.
Maybe we could fold primaryTerms and activeAllocationIds into a single object. Both currently store values that are indexed by shard number. Could be sth like IndexShardMetaData.
I think this check is wrong. When we have relocation going on and relocation source is marked as relocated (i.e. we call executeRemotely in TransportReplicationAction), then we have primary relocation target replicating back to primary relocation source (see also ReplicationPhase).
I wonder what our system invariants are w.r.t primaryTerm in IndexShard. Primary term has for example a direct impact on the reroute phase. A new primary term means that we need to route to a new primary location. The big question is: What is the relation between primaryTerm that we use for routing to primaryTerm in IndexShard? Maybe we could require both to be the same and otherwise abort primaryOperation? Other invariants to think about: - If IndexShard gets created as primary shard (that is not a relocation target), then it should have the same primary term until its closed, right? - If IndexShard gets created as primary shard (that IS a relocation target), then it should switch its primary term only once, namely when it gets activated. I wonder how can we better capture these invariants in the code.
@return needs to go into new line..
OK I see the point. Thinking about this some more though, I think the following variation of the condition will make it clearer what we try to prevent: ``` if (shardRouting.primary() && shardRouting.active() && state != IndexShardState.RELOCATED) { // must use exception that is not ignored by replication logic. See TransportActions.isShardNotAvailableException throw new UnsupportedOperationException("active primary shard cannot be a replication target before relocation handoff " + shardRouting + ", state is [" + state + "]"); } ``` It precisely states that we cannot replicate into an active primary as long as it hasn't been relocated.
This bikeshed is going to be very multi-colored by the time we are done with it. :smile: +1 for @ywelsch's proposal, but I do not like `UnsupportedOperationException` here but `IllegalStateException` would be fine. I think of `UnsupportedOperationException` as being reserved for cases where the receiving object does not support the invoked method, but `IllegalStateException` as the receiving object rejects the invoked method because of the current state. :angel:
I prefer my way but have asked @jasontedor to chime in.
Test is called "testPrimaryOperationLocks" and then most of the code is there to check replicaOperationLock ;-)
`indexMetaData.getIndex()` -> NullpointerException!
We can push this down to `IndexShard.onSettingsChanged()`. No need to touch `IndexService`.
would you mind adding the same for allocation ids? :+1:
This test checks essentially the same thing as the previous unit tests? To check more in this test, we could as well verify that indexShard has the right term (i.e. that the cluster state update was properly applied to IndexShard).
Maybe distinguish whether no index with that name exists or whether index with that name has different UUID and have a different error message? Could be useful for debugging purposes.
not used (I was so much looking forward to more usages of this)
maybe put these checks into validate() method? The checks are then all in one place (the disadvantage is that exception happens later).
should we here or in the superclass fail if the cluster has not fully upgraded to 2.3? just as a safety guard I think that would be a good check in several places otherwise I can see us debugging weird issues `DiscoveryNodes#smallestNonClientNodeVersion()` has a neat method to check.
I wondered in the past whether we should add something to the transport registration logic indicating which action were added on what version, so we can automatically disable (new) actions if any of the nodes in the cluster is too old. Feels like this kind of things belong in a generic infra so action implementers won't need to worry about them...
Maybe `files_total`? We already break things, so...
Great seeing these replaced with ParseFields.
And this is required so the tasks request has something to target.
Answering my own question: you build the array to simulate the task being on all of the simulated nodes. I don't know that that is required here but doesn't hurt anything.
"Simulate a task that attempts to execute only on filterNodes. We are testing that this works."
we test that shard 100 can not be deleted because it's not there _when_ shard 0 is removed. We don't test when shard 0 is present .
This is what I meant (not a biggy): ``` assertFalse("shard is allocated", indicesService.canDeleteShardContent(shardId, test.getIndexSettings())); assertFalse("shard that was never on this node should NOT be deletable, even if there is another shard from the same index allocated to this node", indicesService.canDeleteShardContent(notAllocated, test.getIndexSettings())); test.removeShard(0, "boom"); ```
let's add java docs here, it becomes non trivial :)
I'm afraid this won't work - you only want to decrement this when someone responds to the channel... it needs to be built into the RestChannel..
you need to generate two errors for this to test what you want :)
this is not good... you rely on the base class implementation (that only these two parameters for it's state). You have to proxy all method implementation to the delegate
we can make this catch throwable and remove the catch from https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/RestController.java#L175
do we need this? it's like the base class (as we never have a cause)
we don't really need this here any more.. (it needs to be higher up)
can we clean up the other try catch? it's not needed now (as we catch things here too)
++ the the plan to move it here.
argh. Missed the grey method start line in the github diff.
To me I think the rule should be: we override this method if you have a specific rest http status code that's associated with the exception. I this case we just use a generic code that's not really indicative of the error. However, looking at alternative, I suggest we use 503 Service Unavailable
The reason why I suggested AutoClosable is that I don't want close to be idempotent, but rather throw an exception, see Closable docs: ``` If the stream is already closed then invoking this method has no effect. ``` I'm fine with not implementing any interface at all and just implement the close() method and call it on sendResponse
I think we need to make all the methods synchronized here? we don't have any concurrency guarantees right now
0 seems more intuitive to me (-1 means unset in many places)
it's not really arbitrary is it ? :)
please wrap on StaticCircuitBreakerService , so it will be easy to read
OK for now, but I guaranteed someone will use this and be surprised. But sure, we can wait.
I meant that one indeed. GitHub is playing tricks - it didn't show me that change in the same commit (but now it does). Sorry for the noise.
I could be wrong (not that familiar with the code in that area) but I think that in-memory data structures for mappings are not created by the `createIndex` method. These are merged later (see e.g. MetaDataCreateIndexService:325). We could check here as well that all is good on the mapping level.
yeah, looks wrong.
can we pass a reason to this method and mention it here? I always to scroll to find out whether this is a "true" index or just one that was created when importing/creating one.
that logging message is useless - it is also relevant for the case where the state was fetched from enough nodes, but some index isn't present on requiredAllocation - which is extremely rare. IMO we should just remove all index level "selection". We elect a global state and that's it. I'll do this as a follow up.
Sorry, `MetaDataCreateIndexService` was a bad example. Still, the method `MapperService.merge` which does mapping validation is (AFAICS) not called by the `createIndex` method. This means that `verifyIndexMetadata` does not run the mapping checks in `MapperService.merge`. We check these however when we run `MetaDataIndexUpgradeService.checkMappingsCompatibility` which is called by `MetaDataIndexUpgradeService.upgradeIndexMetaData` when we start a node.
You can remove the `hasResponseFromRequest` method - it is not needed anymore.
This change should not be necessary. We already have the ability to plugin in a filesystem in tests. See `PathUtilsForTesting.installMock()`
I still dont' think it is necessary. The mock FS needs to be installed before creating the Environemnt, and any calls to PathUtils.get will work correctly, including the one done for tmpDir.
> it will for example cause tests that are running on a filesystem that handles paths differently to fail But we shouldn't be testing every FS. We shoudl be testing the logic in plugin installation, which is either is or is not posix. So you can use posix for testing the stuff works, and "basic" for testing it skips the extra code gracefully for non-posix FS.
typo: two "if"s
"if if" again
You should try using `expectThrows` :)
Ok. I'm just worried because this non-obvious logic is already present in DateFieldType and FieldStatsProvider.
In other cases like this we went for reducing the number of classes, so here too, I'd go for adding these two simply as abstract methods.
After reading this distinction for the third time - maybe generating a new FieldSortBuilder or ScoreSortBuilder could be factored into a private helper method like so: private SortBuilder generate(String fieldName) { ... }
In a followup PR we should merge SortBuilder and SortBuilderParser, I think. The latter one was only introduced as an intermediate step to avoid having to refactor all builders at once. Not sure if we can add the interface ToXContent there as well then.
Totally up to you (whether you want to or not), but maybe `setRequestsPerSecond` should handle the null case so you don't have to worry about checking it post-parse? What do you think? (I'm fine either way)
As I mentioned above, I do think we should support both 0 and -1 for no throttle to be consistent with our other "disabling" APIs
Cool, I think we should allow the -1 to it conceptually makes sense :)
Same here with naming, we have `delayedPrepareBulkRequest` and `delayPrepareBulkRequest` which are confusing when vars and functions are named so similarly
This might read better if it were renamed to `hasRun` initialized with false and then it could be ``` java if (hasRun.compareAndSet(false, true)) { prepareBulkRequest.run(); } ```
in -> is Might be good to specify also whether it's called _before_ cancellation, or _after_
I like the fact it is reusing the current convert types!
you also have this variant `org.elasticsearch.common.xcontent.XContentBuilder#timeValueField(java.lang.String, java.lang.String, long, java.util.concurrent.TimeUnit)` which you can use without changing TimeValue
There is a single field here, `tombstones`. If the `IndexGraveyard` instances are reference-equals, then the `tombstones` fields will be reference equals. But `Object.equals` method already does a reference check before deferring to a logical-equals. Thus, just deferring to `Object.equals` on the `tombstones` field does not save anything. Hence, I _do_ think that only the equality check on the `tombstones` field is needed (after, of course, checking that `obj` can be cast to an instance of `IndexGraveyard`).
There is because it becomes one very simple line: ``` diff diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java index 6220c4d..5604bf7 100644 --- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java +++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java @@ -89,15 +89,7 @@ final public class IndexGraveyard implements MetaData.Custom { @Override public boolean equals(Object obj) { - if (this == obj) { - return true; - } - if (obj == null || getClass() != obj.getClass()) { - return false; - } - @SuppressWarnings("unchecked") - IndexGraveyard that = (IndexGraveyard) obj; - return Objects.equals(tombstones, that.tombstones); + return obj instanceof IndexGraveyard && Objects.equals(tombstones, ((IndexGraveyard)obj).tombstones); } @Override ```
on the move now, so can't really check, but I'm confused. timeValueField always output the raw value and that should be used for parsing. the human version can be ignore. Not sure follow how this influences the Index.Builder class.
> I don't have a strong opinion on this one either way I do because less code that is easy to understand is easier to maintain than more code. 
If we use Collection<Tombstonre> we can return an unmodifiableCollection() which doesn't copy stuff..
although under the hood this ends up being efficient (the ArrayQueue copies it's array to another array which is used by the ArrayList<>) I think all these conversions between collection types making things unneededly complicated. I think we can use ArrayList<> explicitly in the builder and here. The only place where dequeue is potentially useful is in the purge method, but there we can use ArrayList.subList(), which is even more efficient (and it all becomes simpler, which is the important part)
removed can just be a count. We always remove from the beginning of the queue.
can we make the units clear in the name? (it's also not used now) getDeleteTimeInMillis (we typically use time)
oh, the boxing horrors :)
here we could use sublists again - just scan to the place you need. No need to reverse then.
I think that since you're deferring to `Objects.equals` anyway all the other checks above can be removed.
And the array allocation!
Drop the string constant. :)
You're right, I confused myself.
newQueue -> newTombstone
can we keep the same semantics of other metadata parts and no write the key here (but rather in metadata class)
this method can the max size and min expiration window as parameters, which will make it easier to test and have those be settings,
always do these things with nano time. Also the logic here is the reverse - we want to keep up to 10K items, UNLESS they are too recent. Which makes me think that the default _minimum_ expiration time can be 48 hours.
can we move all purging to the purge method, which will always be called by the builder? The down side will be not having logging, but if people want that they can put the cluster service in trace logging and see the changes.
Oh I see why - there is no builder. Can we follow the same pattern as in other places - make this class immutable and add a builder? All purging and such can be done at the builder level.
can we make those statically configurable ? (no need for dynamic settings)
can we add the serialization logic we need to the Index object it self? we're likely to use it in other places.
This is what I meant: ``` Index: core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java IDEA additional info: Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP <+>UTF-8 =================================================================== --- core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java (date 1459873883000) +++ core/src/main/java/org/elasticsearch/cluster/metadata/IndexGraveyard.java (revision ) @@ -37,6 +37,7 @@ import java.util.EnumSet; import java.util.List; import java.util.Objects; +import java.util.concurrent.TimeUnit; import java.util.function.BiFunction; /** @@ -290,12 +291,14 @@ final public static class Tombstone implements ToXContent, Writeable<Tombstone> { private static final String INDEX_KEY = "index"; - private static final String DELETE_DATE_KEY = "delete_date_in_millis"; + private static final String DELETE_DATE_IN_MILLIS_KEY = "delete_date_in_millis"; + private static final String DELETE_DATE_IN_KEY = "delete_date"; private static final ObjectParser<Tombstone.Builder, Void> TOMBSTONE_PARSER = new ObjectParser<>("tombstoneEntry", Tombstone.Builder::new); static { TOMBSTONE_PARSER.declareObject(Tombstone.Builder::index, Index.INDEX_PARSER, new ParseField(INDEX_KEY)); - TOMBSTONE_PARSER.declareLong(Tombstone.Builder::deleteDateInMillis, new ParseField(DELETE_DATE_KEY)); + TOMBSTONE_PARSER.declareLong(Tombstone.Builder::deleteDateInMillis, new ParseField(DELETE_DATE_IN_MILLIS_KEY)); + TOMBSTONE_PARSER.declareString((b,s) -> {}, new ParseField(DELETE_DATE_IN_KEY)); } static BiFunction<XContentParser, Void, Tombstone> getParser() { @@ -381,7 +384,7 @@ builder.startObject(); builder.field(INDEX_KEY); index.toXContent(builder, params); - builder.field(DELETE_DATE_KEY, deleteDateInMillis); + builder.timeValueField(DELETE_DATE_IN_MILLIS_KEY, DELETE_DATE_IN_KEY, deleteDateInMillis, TimeUnit.MILLISECONDS); return builder.endObject(); } ```
I think we could persist a time-to-live in the cluster state, and on each cluster state update the current master can subtract the offset between the current `System#nanoTime` and the `System#nanoTime` from the last time the master updated its local cluster state and persist the maximum of the new time remaining and zero. Now the tombstone will live exactly as long as there has been an active master and we do not have to worry about any crazy time issues.
> Is there a different way we can handle this altogether? That's what I'm going to be thinking about.
we should validate we started on a start array
For the default value, you can more safely use `TimeUnit.DAYS.convert(14, TimeUnit.MILLISECONDS)`.
everything in here just uses the state maybe we only pass the state to the method instead of the ClusterChangedEvent
++. PS - We shouldn't optimize for refresh mapping as it's going away.
Sure. This one is more than good enough :)
in a different change - since were talking about it - if we inline pending delete processing with Indices service, where I grow more to think it naturally belong, this indexSetting juggling will go away
wondering if we need recoveryState.isPeerRecovery() to simplify these lines.
nit: can you assign `event.state()` to a local var
oh nevermind - I see why 
I think `requiresIndexMappingRefresh` is very misleading - it should be called `updateMapping` as it does update the mapping (and returns a boolean for refreshes)
future note - once refresh mapping is gone, we should inline this with index creation.
applyDeleteIndices -> deleteIndices
in the case of createIndices, this should send shard failed for the shard routing that triggered the index creation, but it doesn't because the indexService is empty. I think the interaction with shard failures is too brittle/tricky. My suggestion would be to just return an exception on failure and let the caller deal with it in the right way. PS - maybe this signals a testing gap as well..
I think they can stay here, until we start using them in other places.. but I'm good with moving if people want to
should this entire method go into `MapperService`? I think this would make more sense here
I actually think we can get rid of this entirely. We start the `IndicesService` before `IndicesClusterStateService` and stop it in the reverse order in `Node.java`. I think we can just rely on the state of `IndicesClusterStateService` and don't add this to the interface at all.
we can rather make it private and throw an exception like `ensureStarted()` and call it before we go anywhere that violates the condition but even that is best effort.
I didn't look at other users of that method, but +1 ! this boolean annoyed me for a while :)
since we have the AllocatedIndex here can't we get the IndexMetaData from the `AllocatedIndex#getIndexSettings()`
we should also have messages here in this assert
s/to list of/to the list of/
Nice, I like it.
good point.... we should be able to get rid of it.
ok can be a followup
alright let's maybe make it a TODO then, just so we know the plan is to make it go away
I don't like that settings need to be passed into every check method. I'd prefer the specific setting needed be passed into `MinMasterNodesCheck` when it's constructed (look at the `MlockallCheck`).
Unrelated formatting changes: :cry:
There's a `BootstrapCheck#check(boolean, List<BootstrapCheck.Check>)` override that is visible for testing exactly so that the test can be written without having to rely on passing in a setting the triggers enforcement.
Okay, that's probably not a bad test to have; I think it'd be better to make `BootstrapCheck#checks(Settings)` package visible (and final) and check that `checks(Settings.EMPTY)` contains an instance of `MinMasterNodesCheck` (it's a more direct test of what you're trying to test).
Yeah, let's the keep the tests just focused on whether or not `MinMasterNodeCheck` does the right thing based on whether or not `discovery.zen.minimum_master_nodes` is set and we can think about broader tests for the default checks from `BootstrapCheck` itself in a separate pull request.
That makes sense, thanks for the explanation!
I like this style. I think I'm going to steal it.
Maybe implement Writeable and a reading constructor? That seems to be the way we're doing things more and more.
I think we've started to use `setShard` style here? I'm not totally sure.
Can you change this to assert `e.getMessage(), contains(the text)` while you are here? That'd make the error message so much more useful.
Fine by me! Can you make an issue explaining it so we don't forget totally? I'd do it but I don't know the problem well enough.
I _think_ that you can get away with just letting the exception bubble up and RestController will send it to the user. You won't get the error log but I'm not sure logging an error on 400 level parse errors is a good thing in the long run anyway. I try to usually run requests with `error_trace` on them so we don't eat the stack trace....
Null initialization feels wrong here because you will always set it. Not initializing `req` feels more right.
`info *is* unavailable` I think.
These are all so nice!
Probably should remove the leading space here.
Can probably be `writeOptionalWriteable` or something like that.
It feels weird to have a call to `.value` at the same level as all the calls to `.field`. I presume this works because UnassignedInfo writes its field name. Maybe call `unassignedInfo.toXContent` then? You are more of an expert on this stuff than I am though.
Probably should also be getAssignedNodeId.
> the code in ObjectParser looks scary at first glance! That's no good. Can you please keep notes as to what made you feel that way? it should be the go to place of "no problem, I can parse json settings in a breeze and with confidence"
I don't think that this is the right place for this. Since #13086, we already do duplicate settings validation in the `XContentSettingsLoader` and the `PropertiesSettingsLoader`, and this kind of check should sit right along side those checks (rather than having these checks spread out). If we do add this check to `XContentSettingsLoader`, this pushes the check as far down as it can go, and enables us to fail as early as possible. As a bonanza, we can give an error message that includes the line number that the failure occurred on. This is as user-friendly as we can get here. I didn't realize that you had opened this pull request, but I already opened #17310 that does exactly this.
We call them "master nodes" everywhere else. :frowning:
master_election.filter_data [{}] is a leftover I think
grrr you are right, thanks for reminding me, my bad!
seems a little risky... but we should have tests for serialization that catch these things!
I'm not a fan of overriding the class under test, but short of mocking IndexReader and MapperService I'm not sure how we can test this here.
The other problem with doing this is that we can't test the logic in `getRelation(QueryRewriteContext) here.
Can we keep the line numbers in the test assertions? I think they are important to maintain.
do we do this in other places? I mean using read/WriteByte for enum ordinals? I think there's many many places where we have VInt instead
with vint we save a cast to byte I guess, that's it :)
oh boy, I see that we were using VInt for writing and Byte for reading.... thanks for fixing...
go for it, you don't break anything, it's a new method added in master only anyways
when it is possible, I would do it...not a huge deal but still ;)
right, I forgot about the exception.. let's keep it, but private ;)
Maybe a TreeMap if you are just going to build it an then iterate over it. Probably doesn't matter.
Maybe call it `topLevel` or something.
I just go with a TreeMap automatically if I know I'm going to iterate on it. Not required though.
I'm not a fan of `_` in any java method names. I don't that much though.
So long as you go through the builders I don't care how you do it. I'm happy with something in PercolatorQueryTests.
And I want you to go through the builder just because I think tons of confusing stuff happens in there and its easy for things to look right at this level but not work properly in practice. I think the builders are more likely to catch things.
I see because it is used in tests it has to be public for now...we really need to fix this isolation of classes from there tests, which require so much to be public. But that's unrelated to this change. :)
oh no! I thought we broke rescorers extension point, not this one too!
I didn't think about this, maybe we should do the same with IndicesQueriesRegistry and get rid of the buildParserQueryRegistry method.
but if you are in strict mode you get an exception so you don't get back false :)
I really think this should not be part of this query class. The fact that it's only used here doesn't mean that some other components will not be able to use score functions one day, which are also pluggable. The idea is: 1) plug custom function through SearchModyle 2) if you need to lookup a function, inject the registry. This last thing I think shouldn't have a dependency on the query. Sorry for making your life hard here, I hope you see what I mean.
is it an option to make this method package private? Then it would become more of an internal thing. Thanks for addressing this!
I think it still causes NPE, IIRC it's because a search context is not set... so `SearchContext.current().indexShard()` still blows up. The TODO is still valid as a whole... and is the reason why we don't test this function as part of our function score tests
oh I see what you meant now in the other PR :) if Tuples don't pollute the method arguments, I am ok with this, actually it simplifies synchronization issues between the two maps otherwise, I will update my PR to do the same.
if we only use all names to put things in the map we lose all the deprecation warnings that we might have etc. we should rather keep track of the original ParseField and call ParseFieldMatcher#match.
If we don't want to address this as part of this PR, let's add some TODO or norelease. I think we should do the same that we do for queries: make the parser a functional interface and use ParseField for parsing.
we may need to go over all the callers of the new NamedWriteableRegistry#register and see whether they are compliant to the decision that we have yet to make in #17458 :)
I think this has the same problem as in #17458 as it uses the first parser name to register the named writeable.
readFromStream? :) not that important...
the fact that the parse field matcher matches is a requirement, I think it should be an assert instead. It's really a our bug if it doesn't and we should catch it differently compared to "no function found"
deprecated names match too. match should always return true, or rather throw an exception in strict mode if you use a deprecated name. I think it should be an assert. We had the same discussion with Colin in IndicesQueriesRegistry I think :)
mmm I think I don't see why we need a functional interface here. Seems like having a registry class makes it more clear and is more consistent with what we do with queries. But it's my personal opinion, not sure what others think.
> If we felt like testing fromXContent without SearchModule we could implement a super easy mock implementation That we can easily do with a little registry too I think? fromXContent won't depend on SearchModule anyway, that is the point of having a separate registry for only the pieces that fromXContent needs (registered score functions). In the end there isn't a huge difference between the two solutions. there is and will be a lookup method somewhere, but instead of being a method reference as argument, it will be an explicit registry. I find it more readable this way, but I do get how this is just a matter of opinions.
I have a question: what is wrong with java classes in their own .java file? :) Along with IndicesQUeriesregistry I would call this ScoreFunctionsRegistry probably.
thanks a lot! should we have a test that leverages this extension point for score functions? I thought we had one already but not sure anymore
the lookupScoreFunctionParser method allows to use SearchModule as the registry using method references. So effectively we need and already have a registry, but this solution saves us one class as we don't have a separate registry class for score functions.
Like Simon suggested, I would just do: `if (allowedTotalFields < totalMappers) {`. If someone needs to have many fields anyway and understands the issue, (s)he can still set `index.mapping.total_fields.limit=1000000` for instance.
nit: `oder` -> `order`
nit: `an` -> `a`
I am now wondering if we still need this queryShardContext instance member if we (almost) always create a new one.
are we losing the STRICT bit here? it's important that we use STRICT here, so we make sure that we never output deprecated stuff ourselves. and we test deprecations separately.
no I like creating a new instance, seems to reflect what we do everywhere at this point. But we should clean things up a little bit more, to make sure that the set search context is consistent at least.
thanks a lot, looks great!
same here, might be that we are good, but let's make sure we don't lose the STRICT one
I wanted to remove the `allowCommit.set(false)` here with an ensureOpen at the beginning of the method. Only saw later it's already there. No doubles.
this is now redundant. openTranslog commits when openMode == EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG
maybe replace this with ensureOpen in the beginning? feels cleaner to me
hehe. There is already ensureOpen. so this can go away... (simpler state management --> easier to understand). but I'm good if you want to keep it.
I wonder if later on we are able to get rid of QueryRegistration and register stuff straight-away rather than when calling buildQueryParserRegistry. we will see later though!
I'd probably write validate's results to a variable and reuse it.
ObjectParser has another constructor version where it takes a provider that can be used to build the request. You could save a whole line if you used it!
it should rather save the XContentType detected from the bytes (XContentFactory.xContent(bytes)) above and re-use it.
the bytes are not the same right because we shuffle the keys? I guess it depends on how we compare bytes references.... would be nice to get around it so we don't need this odd method at all :)
the ones returned here are the fields that can be shuffled or that should not be shuffled? Not sure if the name is consistent with the behaviour
I think you have to sort before shuffling, otherwise it will not be deterministic, cause the order in the list still depends on the order in the set, which depends on the jvm :)
Quick question (unrelated to this PR) for clarification: the goal was to get rid of the Prototypes, but this will happen in later PRs? Is there already a proof-of-concept type of PR I can look at how this will work, I just played around with it a bit wasn't quiet sure how this will work with the common `boost` and `query_name` fields.
I've just noticed in the parsing code that option 1 above is already present (you can specify the `type` in the body of the query). I think given this we should definitely deprecate the `match_phrase` and `match_phrase_prefix` names since you can at the moment do something wholly confusing like: ``` { "query": { "match_phrase": { "my_field": { "query": "foo", "type": "boolean" } } } } ``` The above query looks like a `match_phrase` query but will actually override the `type` and run as a `boolean` match query.
s/it's/its I think
Personally I think the camelCase and snake_case handling should absolutely be done with ParseField as it then makes things really easy to do when we remove camelCase support since we only have to deal with changing ParseField. The Match Query seems to be the only query that has multiple names (ignoring snake and camel case variations) and actually we are abusing the name anyway by using it to switch between different features in the query. I think we should either: 1. Add a `type` parameter to the match query that has values `boolean` (default), `phrase` or `phrase_prefix` and deprecate all the query names except `match` 2. Split `MatchQueryBuilder` in to 3 classes which all use the same underlying class called `MatchQueryBuilder` (for the `boolean` type), `MatchPhraseQueryBuilder` (which extends MatchQueryBuilder but sets type to `phrase`), and `MatchPhrasePrefixQueryBuilder` (which extends MatchQueryBuilder but sets type to `phrase_prefix`). Another advantage of this approach is that we can then make sure that only the parameters allowed by a particular variation (boolean, phrase or phrase_prefix) are used. Both of these approaches mean that all query builders will only have one name which makes sense since we really shouldn't have anything in Elasticsearch called by multiple names as its just confusing for both devs and users
that is a good point Christoph! if we really want to keep varargs otherwise we could validate that the array has length >=1 , I have no strong preference on how we solve it though
The previous builder also supported "multiMatch". I'm late to the party, but this is one problem I see in removing the alternative `names()` method from the builders, it moves the parser names (and alternatives) far away from the builders themselves. I guess this move is part of makign QueryParse a functional interface, but couldn't we leave the `names()` method and use it even though it will not be part of the QueryParser() interface? Or add it to AbstractQueryBuilder instead? Having all these string constants here in this class without a connection to their builder is a source of errors IMHO.
I had the same thought in the beginning, but then I thought it made sense to have the parsing code in one place, and the names it's registered against as part of the registration code. Is this source of error just because we are moving the code around and we may forget things/ not properly review? Or is there more to it? There should be failing tests for these problems anyways!!! We should work on those I guess ;)
I think we should make sure that the String that is used as Id in serialization gets treated in a special way, also in the builders themselves so they are not accidentally changed between versions. Thats why I liked the old `getWritableName()` (that seems to be on its way out). Maybe we should even have an own class `QueryId` which simply wraps the NAME string constant but forces us (and users implementing their own queries) to think about this as a special case. We could change the existing `String getName()` method in `QueryBuilder` that currenty just forwards to `getWritableName` to do this. This is just some thought for discussion, nothing to block this PR though.
Having all the query names ony here in `SearchModule` also makes it hard to test them for accidental changes. We are not doing that yet, but I think we should, and for that they should stay in their respective classes.
The idea is to replace calling readFrom against the prototype with calling a new constructor for each query that takes a StreamInput as argument. Depending on where those common fields are read/written they may have to be moved, which is the part that breaks serialization as a constructor can only call super from its first line.
this really really feels like ParseField. What does it buy us over ParseField? Sorry I may be missing it!
I am not totally sold on how we test this. Besides using reflection that hurts my eyes :) we have a NAMES constant that we use for registration and we wanna make sure that we register the query under all the names. Maybe we should move this test to SearchModuleTests and check against the queryParsers map instead? Cause in the end we simply want to enforce that we did use the constant in this test. One other thought I have (yes, this is paranoid) is that if one removes one query name from the NAMES constant everything will still be green. We may be fine with this though, cause the only way to solve it is duplicating the names in our tests.
ok if it stays as it used to be, leave it! I was afraid it was a bug introduced here
haha I remember making the same mistake at some point :)
oh boy this PROTOTYPE was totally useless :)
I think the variable name `http_port` is misleading. In RFC3986 this is called [authority](http://tools.ietf.org/html/rfc3986#section-3.2) but I think `host_port` or something along those lines is also fine. Btw, I checked out of interest and all your logic works also fine for IPv6 addresses.
Hmmm, I do see what you mean. Personally I would still prefer the register method in the registry but I think this is a personal preference thing rather than a substantial concern so I'm happy to yield on it :smile:
To me, this logic should really be in `IndicesQueriesRegistry` so we construct the registry with just the `Settings` object and then call a `registerQuery(ParseField, QueryParser<?>)` method which unpacks the `ParseField` and adds it to the registry map. That was the registry is dealing with how to internally structure the data and the internals can be easily changed later without affecting outside code.
I dropped QueryRegistration in https://github.com/elastic/elasticsearch/pull/17458.
Sounds good. I was also wondering if we need the QueryRegistration object since I think we should be able to directly register the query with the named writeable registry and the parser registry in the register method
It would be nice if we could somehow get the parseField from each parser, but I guess the problem is that QueryParser needs to be a functional interface? Maybe this can be added later.
It doesn't _need to_ be a functional interface. It makes it easier to work with, I think.
also reflect the deprecations on the java api to the corresponding parse fields too? (slop etc.)
I think the following if is not valid anymore in fromXContent: ``` MatchQuery.Type type = MatchQuery.Type.BOOLEAN; if (parseContext.parseFieldMatcher().match(parser.currentName(), MATCH_PHRASE_FIELD)) { type = MatchQuery.Type.PHRASE; } else if (parseContext.parseFieldMatcher().match(parser.currentName(), MATCH_PHRASE_PREFIX_FIELD)) { type = MatchQuery.Type.PHRASE_PREFIX; } ```
this would allow to remove the two parse fields above I think
replace match here too
you are right, sorry
I think we should keep this second test though and use ParseFieldMatcher.EMPTY, then test that it indeed breaks with the STRICT one.
max_expansions may still be used along with fuzziness, so it may not be ok to deprecate. Can you double check? This is why splitting MatchQuery would help. It is hard to figure out what happens when.
thanks for doing that Colin ;)
same question as above
oh right....unless we split the lucene query too. gotcha
an alternative would be parsing the proper version of the query, calling toQuery against both and checking out that they are the same via equals checks. Also we could add here a quick serialization check using assertSerialization
BTW, instead of the above to wait for the nodes to come up, could something like this be done? ``` client().admin().cluster().health(Requests.clusterHealthRequest().waitForNodes(Integer.toString(3))).actionGet(); ```
could use 1. `UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey()` 2. `IndexMetaData.INDEX_NUMBER_OF_SHARDS.getKey()` 3. `IndexMetaData.INDEX_NUMBER_OF_REPLICAS.getKey()`
nit: not sure of the preference, but I find it helpful to have method arguments `final` if they will not be changed.
nit: ditto for `final` method args here
I'm fine we keeping the test of canceling from the runnable task, but can we also have a test for external canceling where post cancel call the runnable is a runned at most once? Note that to do this you will again be in that situation where you wait for something that shouldn't happen, causing the test to be slow - I'm fine with a direct check that will sometime cause false positives.
if we use double futures (one that you have already and another to pass back the results) we won't have to busy wait here.
I'm not a fan of this. we effectively always wait. Can we just rely on the `assertNull(throwableRef.get())` at the end of the test? it may have some false positives but if we get something wrong, we'll know soon enough
can we fix this into a boolean before we start? will be easier to reproduce / know what happened.
can we add a word about the implications of using SAME? i.e. this will run on the schedule thread, of which there is only one and should only do super light weight things.
can we use AbstractRunnable ? this will give us handy onAfter and onFailure methods.
Works for me. Maybe just have an interface called Cancellable - it seems CancellableTask is taken.
can this executor? name is what we often give the tasks..
strictly speaking, we can log the reason here as we don't know why people called us. We need to either pass a parameter (reason) or remove the last part.
yeah, this is tricky. I instead I think we can run a test that cancels from the running task and makes sure it is never run again? we can then decide whether to cancel immediately or after a few iterations
schedule first run should be called twice, regardless of cancellation right? can we check for that using an assertion? we don't really have to test it as there is no way for the user to achieve this when the class is not exposed.
cancel that :) I figured it out.
non volatile multi thread assignment/read - dangerous territory , but we're good here since we only over calculate in this case (working with the JVM will be crazy if would expose half baked objects in this case).
Then +1 too, but why do we DiscoveryNode#toString anyway? (just asking, not saying it should be different)
I'm not convinced that: > we basically always use DiscoveryNode#toString anyway. re: > Out of curiosity, did you look at the places in ZenDiscovery where this is used? I scanned it , as you say, it's hard to find. All the places I saw, including the ones you mention are things that happen every once in a while, relate to a very small subset of the nodes in the specific cluster state instance. I would prefer not to calculate the toString all the time. I'm ok with the optimization...
then +1 for computing it in the constructor if we will use it anyway
> I think this change should be reverted! ++
should we have a `registerValueFormat(String name, Reader reader)` method so plugins that add a new field type can register the value type? If we do add that method, the built in value formats should be registered using that method too.
since there is no natural order for filters, I would rather not make KeyedFilter implement Comparable and provide a comparator to Collections.sort? also I'd prefer if we copied the list instead of modifying it in place
I think you should pass in the request here instead of empty params
I see. Maybe we should move the test? I am fine wither way though, up to you.
I would probably make this package private
oh so this is not new stuff? If it was already there this way leave it, please!
++ thanks for doing it this way, I had thought it was new stuff in the beginning. looks good as is.
A thought for a follow up: perhaps we could enforce valueForSearch is tested for all field types, by having a test in the base test case which "round trips" a value using indexedValueForSearch and valueForSearch? This would require having an abstract method on the base test case to provide a sample value that is appropriate for the field type.
maybe add the type that is found in the error message with fieldType.typeName()
this will only ensure that there is one percolator field per type. To check at the index level, you need to iterate `fieldTypes`.
it will allow to use a percolator field across several types, but I think it is fine, and it plays well with the fact that we are progressively divorcing types from mappings
in a follow up PR we should try to migrate this test to expectThrows
I think BuildFactory should be allowed to throw a ParseException since subclasses should have the ability to throw it if there is a problem with creating the builder at this point
Yes but I think we should restrict this parameter to only numbers since we can't parse any string into a value of sigma, we should also be consistent with how sigma is parsed in the `extended_stats` aggregation which checks for `XContentParser.Token.VALUE_NUMBER`.
Actually looking more at the changes you have made I see that you have made the code more like the other aggregation parsers so I think I would like to keep these changes and implement the above in another way in the future.
This reason for using the leftover map here is so we can provide all the incorrect fields in one error message rather than reporting one error at a time and making the user potentially have to iterate multiple times to fix all their errors. Would you be able to revert these changes here so we again have these errors all reported in one go? In the future I would like to extend this to the entire request and all parsing errors so we report all problems with the request at once.
well, I'm not sure we can assume that all addresses are by default "public". I tend towards saying implementers need to make this call.
Just dug into it and found we use `TestRuleAssertionsRequired` from the lucene-test-framework jar to do this check
++ thanks Nik
We have a check on the test setup for all tests that makes sure assertions are turned on and fails the test if it's not (not sure exactly where it is but if you try running a test without assertions turned on you'll see it)
ok I remembered some CI randomization around `-ea` in the past, maybe that has changed in the meantime.
I don't think that we need to mark a private method as deprecated. The advantage of leaving this annotation off is that then you can move the two `IndexModule.Type.DEFAULT.match(storeType)` checks in `FsDirectoryService#newFSDirectory` to a separate private `isDefault` method (the JVM will inline it anyway) and thus move the suppression on `FsDirectoryService#newFSDirectory` to just that method which reduces the scope of the suppression.
I think you can leave null here. this init empty value will never be used as we throw exception when queryFound is false. Otherwise there are two places that can cause empty and that may be confusing.
doesn't this beak bw comp as opposed to other changes in this PR? I thought we'd have to bubble the empty up in this case as well.
indentation looks off here. btw shall we output no array or an empty array? :) what were we doing before? not sure this change is needed.
perfect, thanks for checking
I think isn't needed as the registry is closed via the pipeline store.
should we implement SearchContext.toString? This way it would also be easier to look at SearchContext objects when debugging
I think it is fine: we only build one search context per request per shard.
does not seem harmless, but not sure we should sneak this in here. Also it may be worth discussing what we do when things break here and what we should do instead.
one too many line break
here too, sorry :)
yea sorry....... should have been "seems harmless". Yet we may want to change what we do here and whether we need logging or not. And not sneak it in among all these other changes.
yes. This is too low level - I think we will fix it at the higher levels. I dont' think deleteIndexDirectoryUnderLock should be lenient
do we need a dedicated test for this? can't we randomize the number of nodes in the normal deletion tests? O.w. we will need to test this situation with both closed indices and open. It's a shame IMO
++ on adding this.
I don't think we need to get into this now. For now we can remove the test and just keep a test where we have some nodes with no shards assigned to them.
bodies in both cases are the same, why not write just: case "script": case "template": .....
Actually, ignore this, the rest actions are actually just forwarding to the transport actions
Not sure this should be "Query" 
Not sure this should be "Query" 
I don't know what it looks like in query registration, but in all the other register methods we have, we put the "key" first. ie here the agg name should be first? I really don't understand why we are taking ParseField instead of String too...seems we will never get rid of camelCase alternative fields if we keep extending support for it.
yeah, prefer top-level there as well.
I was not talking about moving the registration of the defaults but the `registerAllocationCommand` method.
I don't like that we're making `StreamInput` a God class.
The reason used to be not to make writeNamedWriteable public. I think it is still the same. Design decision we made with Simon when we added the NamedWriteable abstraction. We didn't want e.g. plugins being able to serialize just anything by using this generic write method.
as an alternative to my suggestion above, `registerAllocationCommand` can already now be moved to `AllocationCommandRegistry` as we expose the registry here.
if we need to have this discussion I'd suggest to have it on a separate issue and not hold this PR because of this.
we can remove the bound (AllocationCommand) and make this a top-level class `FromXContent` (dual to `ToXContent`). There are also other places that could implement this new interface (e.g. ContextMapping, MetaData.Custom, ...).
typo (of <-> if)
So many extra changes to this file :(
nit: I wonder if we should rename this to "fromXContent" like we do in most of the queries, suggesters etc... No strong opinion here though.
Nice. I was not aware of this class / feature.
go randomized algorithms :)
I'm good with latest. It's also probably a good idea to change testJoinElectedMaster_incompatibleMinVersion as well
Looking at this again, I think we can remove the node settings as updateDelay / getRemainingDelay only depends on index settings.
just noticed this uses `metadata.settings()` (= correct) whereas `updateDelay` in `AllocationService` uses node Settings (which are not updated).
I think this method is not quite right yet. A few observations: - if (unassigned) primary, then finalExplanation / finalDecision should be influenced by staleness of copy. In case of a replica, however, staleness does not influence allocation decision. - if replica, then we can still show that store copy is corrupt / has an io / error without influencing final decision / explanation - for the primary / replica shard allocator, corrupt / io_error is treated as no data. I think we can first calculate store copy (which just represents the status on disk) and then influence finalExpl / finalDecision based on that. Here is my go at it: ``` public static NodeExplanation calculateNodeExplanation(ShardRouting shard, DiscoveryNode node, Decision nodeDecision, Float nodeWeight, IndicesShardStoresResponse.StoreStatus storeStatus, String assignedNodeId, Set<String> activeAllocationIds) { final StoreCopy storeCopy; if (storeStatus == null) { // No copies of the data storeCopy = StoreCopy.NONE; } else { final Throwable storeErr = storeStatus.getStoreException(); if (storeErr != null) { if (ExceptionsHelper.unwrapCause(storeErr) instanceof CorruptIndexException) { storeCopy = StoreCopy.CORRUPT; } else { storeCopy = StoreCopy.IO_ERROR; } } else if (activeAllocationIds.isEmpty()) { // The ids are only empty if dealing with a legacy index // TODO: fetch the shard state versions and display here? storeCopy = StoreCopy.UNKNOWN; } else if (activeAllocationIds.contains(storeStatus.getAllocationId())) { storeCopy = StoreCopy.AVAILABLE; } else { // Otherwise, this is a stale copy of the data (allocation ids don't match) storeCopy = StoreCopy.STALE; } } final FinalDecision finalDecision; final String finalExplanation; if (node.getId().equals(assignedNodeId)) { finalDecision = FinalDecision.ALREADY_ASSIGNED; finalExplanation = "the shard is already assigned to this node"; } else if (shard.primary() && shard.unassigned() && storeCopy == StoreCopy.STALE) { finalExplanation = "the copy of the shard is stale, allocation ids do not match"; finalDecision = FinalDecision.NO; } else { if (nodeDecision.type() == Decision.Type.NO) { finalDecision = FinalDecision.NO; finalExplanation = "the shard cannot be assigned because one or more allocation decider returns a 'NO' decision"; } else { finalDecision = FinalDecision.YES; if (storeCopy == StoreCopy.AVAILABLE) { finalExplanation = "the shard can be assigned and the node contains a valid copy of the shard data"; } else { finalExplanation = "the shard can be assigned"; } } } return new NodeExplanation(node, nodeDecision, nodeWeight, storeStatus, finalDecision, finalExplanation, storeCopy); } ```
My (hopefully) last corrections here (in this case I'm correcting my earlier self ): - We should also add allocatedPostIndexCreate() to be more in line with PrimaryShardAllocator. This means that this condition and the subsequent two become `shard.primary() && shard.unassigned() && shard.allocatedPostIndexCreate(indexMetaData) && storeCopy == ClusterAllocationExplanation.StoreCopy.STALE` - We should also handle `StoreCopy.NONE` in the same way. In case of assigning an unassigned primary, having no shard data is also a NO.
From the top of my head no, but I will keep it in my mind to change those when I see them.
I am fine with doing it in a follow-up PR if that works better for you
no need for `? true : false`
beware that wildcard imports will cause the build to fail
if all sub queries are term queries with the same boost, it would be nice to build a SynonymQuery instead
QueryBuilders should always set the defaults since they can be constructed without ever passing through the parser (Java API)
QueryBuilders should always set the defaults since they can be constructed without ever passing through the parser (Java API)
QueryBuilders should always set the defaults since they can be constructed without ever passing through the parser (Java API)
Nit: Almost ;-) To be or not to be, and also where, that is the question.
Why a block per? You can often get away with something like ``` java Exception e; e = expectThrows(RuntimeException.class, () -> BootstrapCheck.check(true, Collections.singletonList(check), "testHeapSizeCheck")); assertEquals(e.getMessage(), "words"); e = expectThrows(RuntimeException.class, () -> BootstrapCheck.check(true, Collections.singletonList(check), "testHeapSizeCheck")); assertEquals(e.getMessage(), "words"); e = expectThrows(RuntimeException.class, () -> BootstrapCheck.check(true, Collections.singletonList(check), "testHeapSizeCheck")); assertEquals(e.getMessage(), "words"); ```
Like, probably with new lines between each one or something.
+1 on that reformatting.
So as far as I see is this is the crucial change in this PR? I was wondering if might have undesired effects that we allow more field value types to be serialized/deserialized than before by using `writeGenericValue`. What would happen for example when fieldValue is a GeoPoint. It would have caused the serialization to trip previously, now it will be okay (and I guess it might cause error later). I guess switching to `writeGenericValue` is a good tradeoff here but would like to hear your ideas about that.
I had the same thought, but I guess the point is to properly validate things when they get set to the builder, so that non supported values will never be serialized. I assume that we do that already, otherwise we should.
I think the better option is to add the '@SafeVarargs`annotation to`NettyHttpClient#post()`(which requires the method to be`final`but I'd prefer to make even the class`final` as it is not intended for inheritance).
We no longer support `aggregations_binary` (already listed as removed in the release notes for 5.0) so we can just remove the `aggregation_binary` code below which should solve this issue? (it looks like I missed this bit whilst doing the aggregation refactoring before.
I usually find it cleaner to make a variable of type Runnable in cases like this. Doesn't matter though.
Can you use `expectThrows` instead? I think we should use it rather than this try:fail/catch:check_msg pattern for new code.
nit: one too many line break? :)
nit: can we call it getParseFieldMatcher? now that we have an interface it should be easy to rename all the existing impls at the same time. If you feel like it should be a follow-up, I am fine with that.
Fine with me.
very minor and I know this wasn't your code anyway but we could move writing the size out of the if block since it's the same on both branches
expectThrows works nicely here.
can we still do this under and assertion? (and yes, make it return true if all is OK). The reason is that we want to communicate that this is a sanity check and not really something we expect can happen here and want to guard against here
nit: you can just let the validate throw it's exception - this will fail the test and give us more info
instead of accumulating validations under RoutingTableValidation, we can throw an exception immediately upon encountering an error. This will remove one more class and will simplify things further.
same here. I think we should throw an exception when running into an error. No need to accumulate things and return them
don't try to fix it, you just moved code around, but this catch block worries me :(
I've never liked using the constants like this here in tests. The reason that I don't like is because if someone has their IDE open and accidentally inserts a character into one of the settings keys (e.g., `indices.memory.index_buffer_size` -> `indices.memory.index_buffer_sizes`) then the tests will continue to pass and so we've silently broken things. I would prefer the string literals just be copied and pasted here: `"indices.memory.index_buffer_size"` instead of `INDEX_BUFFER_SIZE_SETTING.getKey()`. Break the build early and often. :smile: (This applies to other similar uses in these rest of this diff.)
> probably we should make such constants (e.g. the ones in IMC.java) private in the future. +1
Maybe "units" or something.
ok, talked to David about it. We will add a note to breaking change docs.
We looked at the usage places. Currently only used for specifying thread pool sizes. If a user specified that he wants queues of "5b", then it SHOULD break :-)
afaics kb is not supported for SizeValue. it is "k" or "K"? Same for the other entries.
nanos and d (for days) are missing here as well.
The use of `#` might make queries confusing since it is also used for filter clauses.
Since the purpose is to replace eg. range queries, I am wondering that we might want normalization to behave like range queries so that scoring is the same (that would come ootb if we extend ConstantScoreWeight and don't extend getValueForNormalization/normalize). I realize this is not really related to this PR since we already have the issue today in master since we use Lucene's MatchNoDocsQuery which also returns a norm of zero, so maybe it would best be addressed in a follow-up PR? (do as you prefer)
Why do we register `S3Repository.Repository.*` settings here? Those are extracted from the repository settings when it is created/registered, but I don't think we need to register a global `compress` or `throttle_retries` setting...
Something like `settingsModule.registerSetting(S3Repository.Repository.KEY_SETTING);` registers a global setting named `access_key`: once done, nothing else (core or another plugin) can register a setting with the same name. And it laos registers it as `Filtered` so I guess every setting with the same name is filtered out from every places? That seems wrong to me because the plugin only need to retrieve the `access_key` from repository settings and to filter out this setting from the repo settings. I think we should find another solution but not register this as a global setting like it is in the code.
I don't like leniency. Can it be `"true"`, `"false"` or `null` with the former parsing to the right `boolean` and null giving the default? A typo of `"tru"` will parse to `false` and that makes me :cry:.
And I think that's bad because with `"tru"` the user meant `"true"` and the system silently swallowed their error. I want `"true"` -> `true`, `"false"` -> `false` and `null` -> `false` and all other values are rejected.
> I think we're talking about two different sets of leniency :) ++ :smile:
No, that allows `0`, `off`, and `no` for `false`, and `1`, `on`, and `yes` for `true`.
oh I was hoping that was gone already. seems like parsing booleans is very complicated for us....
The worst is how `on` and `no` both parse to legitimate values, very dangerous for transposing typos.
In case this is left as a set, can we add a check to throw an exception if the set already contains the ThreadContext
No, I still think that it should not be a method on the `Strings` class.
I don't think this should have been changed
There is a type called `DeprecationRestHandler`. That's what handlers passed to this method get wrapped into. But you don't and can't pass one of those to this method yet that's what the name `registerDeprecatedHandler` suggests you can and maybe should do. The method does not register deprecated handlers, it registers plain handlers as deprecated handlers. I think `registerAsDeprecatedHandler` is fine though. :smile:
Can we fold this into `writeTo` and add a boolean to the signature (maybe `includeRequestHeaders`)? It seems like it just requires adding a if/else block
I think it would be simpler to have the node remove its threadcontext from here on close
> I thought I was getting at that without making it too wordy. The key is that you're not registering a `DeprecationRestHandler` as the name `registerDeprecatedHandler` implies. Instead, you're registering a handler that gets wrapped as a `DeprecationRestHandler` before registration. I guess `registerAsDeprecationHandler` would be slightly less wordy than `registerHandlerAsDeprecationHandler` but the point still remains.
can this be `requestHeaders`? I find it a bit confusing as response headers should also be persistent
Same idea here as with StreamInput: ``` java @FunctionalInterface public interface StreamOutputWriter<T> { void write(StreamOutput t, T value) throws IOException; } public <T, R> void writeMapOfLists(Map<T, List<R>> map, StreamOutputWriter<T> keyWriter, StreamOutputWriter<R> valueWriter) throws IOException { writeVInt(map.size()); for (Map.Entry<T, List<R>> entry : map.entrySet()) { keyWriter.write(this, entry.getKey()); writeVInt(entry.getValue().size()); for (R v : entry.getValue()) { valueWriter.write(this, v); } } } ``` The caller would use it with: ``` java out.writeMapOfLists(map, StreamOutput::writeString, StreamOutput::writeString) ```
this is just personal preference so feel free to ignore it, but I like the name `registerDeprecatedHandler`
The name makes the method sound generic and I think the method may have more uses if it is generic, like: ``` java public <T, R> Map<T, List<R>> readMapOfLists(StreamInputReader<T> keyReader, StreamInputReader<R> listReader) throws IOException { int size = readVInt(); if (size == 0) { return Collections.emptyMap(); } Map<T, List<R>> map = new HashMap<>(size); for (int i = 0; i < size; ++i) { map.put(keyReader.read(this), readList(listReader)); } return map; } ``` The method above is not tied to string keys and string lists. The caller here would use: ``` java in.readMapOfLists(StreamInput::readString, StreamInput::readString); ```
Is this right? Shouldn't it be `validHeaderValue`? And I don't think this should be an assertion, but a hard failure (assertions are disabled in production and if we are sending bad headers something is seriously wrong and we need to die hard).
> Though I do prefer that it fails fast instead of lazily later. ++
Everything else is `Deprecation` so I think this should be too. Also, I think that `registerHandlerAsDeprecationHandler` might be a clearer name since we are really wrapping a handler.
Yeah, it's relatively new but it's the clear path forward especially with JUnit 5 coming with built-in support for the same.
One more question: What about `DiscoveryNodes.delta()`? It does not detect if node has been removed and readded. It also does not take `equalsIncludingMetaData` into account. This means that other code that relies upon the nodesDelta API in ClusterChangedEvent to detect if node has restarted might miss a restart? One such example is possibly `SnapshotsService.removedNodesCleanupNeeded`.
can you throw an exception in the else clause, eg. "All queries must extend AbstractQueryBuilder but ..."
similar concern about modifying the value that is set
Not sure we need this. For instance we don't allow to remove sorts on a SearchRequestBuilder.
good lets do that
Not for `Streamable`. If you switch it to `Writeable` then yes. But that might make sense to wait for another PR? I don't know, I haven't been following this one very closely.
hmm can't this just be replaced by ``` Java return new ShardRouting(shardId, in); ```
can we just pre-compute the hashCode in the ctor instead? I can already see this going bad with concurrency and we might be just lucky we didn't hit that yet? Either we do in the ctor and make it final or we need a volatile read here
can we have some messages for these asserts, they are a pain if they trip without a message
Just another terrible diff.
At this point we really should just be passing the string to the method. I expect this change is purely mechanical so that isn't something you want to do here.
let's reference "1m", "5m" and "15m" directly
There is an open PR #17811 that will kill `DummyTransportAddress` anyway.
nit: space missing between cast and value
This is a terrible diff.
This is confusing is what this is.
I think it is more normal to do `getRequest.realtime(request.paramAsBoolean("realtime", getRequest.realtim());`. Just so the default is on the request rather than in the API.
Should we test script usage here, or does the IP agg not support scripts? (I didn't see anything to validate that scripts are not being used in the code)
we may want to rename match_formats as well here, can do in another PR though.
thanks, that explains it.
oh I see now. without moving things around it'd be clearer
I've been moving these up, right under the other constructors and moving writeTo under it.
I'd move the writeTo implementation under this constructor so that they can be on the same page. It helps so much with eyeballing them for errors!
I'd remove this method and just have callers use the constructor.
`readFrom` is going to be removed from the `Writeable` interface, maybe in a few hours. Right now it has a default implementation telling you not to use it. So I'd just skip this bit.
finally! its gone!
Needs a guard.
Sadly, I think that these need to be guarded (e.g., Windows does not have POSIX support).
Needs a guard.
Needs a guard.
Needs a guard.
I think this should just be a plain Java `assert`.
I think this should just be a plain Java `assert`.
I think this should just be a plain Java `assert`.
I still feel like there ought to be a way to make these methods look less copy-and-paste-ish. They just set off my copy-and-paste blindness even though they aren't copied and pasted.
Answering my own question - no - we didn't need it before. I should read more closely next time.
`"auto_expand_replicas": 2` instead? That way you don't end up with a yellow cluster on one node and you end up with a fairly large amount of paranoia if you have three nodes? I'm ambivalent for the 2 vs 1 replica thing but I like `auto_expand_replicas` here.
This one is `false` but the one above is `true`. I'd prefer both to be `true`.
Why not `true`? I think it'd be nice to have the detailed info. Actually I pretty much always want it all the time.
s/listener/delegate/? I read this and immediately thought "infinite loop!" because this thing already **is** a listener. I know it is silly though.
Optional: darkon has this style that I like where you start a new block every time you startObject or startArray and it makes these much more readable!
These two methods feel kind of copy and paste-ish. They aren't really, but when you scan them they look similar.....
the code that exists now, doesn't bother me :)
ow I see this PR is WIP, so I'm guessing that you'll add it :) Anyways this change does LGTM so far.
This is especially important in the case of `TransportReplicationAction` where the request sizes can get big. So we definitely want to count those bytes, but still exclude these requests from the checks.
Without digging into it it sounds like `addWithoutBreaking` is what we want for those requests? They still take up space so why not count them? I get that this could push us past the limit but that seems, if not a good thing, at least appropriate.
> Even in that case I'd go so far as to call it `canTripInFlightRequestCircuitBreaker` I'm fine with this, but do prefer my less-wordy suggestion. 
The name `isCheckSizeLimit` is not clear what it's for, but it's really used only to check whether or not the request being handled can cause the circuit breaker to trip so it should be implementation specific.
Nit: this blank line is extraneous.
Nit: this blank line is extraneous.
Perfect, just what I had in mind. Thank you!
I've also started removing them in places I touch. Each one costs us an extra class in the classloader that never gets unloaded. If the constant is used in exactly one place, I do not see the point. If the constants are needed, I don't think they need an extra separate class.
As written this isn't symmetrical with the write method. I would prefer that it be written in a symmetrical way for ease of comparison.
I believe it is fine to just remove this class and use the strings. That is what we've been doing with new code. If we want field constants the Fields class is really nice. But I don't think the constants buy us much.
It'd be "more normal" to declare this as `Writeable` and use `readOptionalWriteable` and `writeOptionalWriteable`. You've done plenty in this PR so it can wait though!
> It sounds like I'm going to shift to List from arrays anyway, and thereby drop these methods altogether. +1, that should shrink the added code in this PR by a lot.
> I'm going to add the static method, but I do want to note that the method name is toInnerXContent rather than toXContent, so it's not overridden by any of the child implementations. Sure but it _can_ be overridden, if it is overridden it must be called, and it has to be called by the `toXContent` methods on the inheriting classes that do implement `ToXContent` The typical pattern to address this is to make `toXContent` final and have it delegate to an abstract `doToXContent` inner method that the inheriting classes must override. But the reason that I do not like that solution here is because not all of the inheriting classes will implement `toXContent` so I do not think this method should be on the super class at all.
I'd prefer to replace implementations of Streamable with Writeable or some other hack like having Streamable extend Writeable. I'm not sure what the right hack is though.
StreamInputReader already does this.
What about readList? I get that sometimes you have an array and not a list but this seems like overkill.
I think that the `ResponseContainer` abstraction is completely unnecessary. You can just add an abstract protected method to `TransportNodesAction` that receives the collected responses and failures. Take a look at `TransportBroadcastByNodeAction` where a similar mechanism is employed (the grouping is handled in the base class, and then delegated to an abstract protected `newResponse` method to handle creating the correct response object).
If you separate out the array creation into its own line, the suppression can be applied to that line only rather than the entire method.
I don't like super methods that when they _are_ overridden, they _must_ be invoked. I also don't like that this method is on the base class and thus inherited by response objects that do not implement `ToXContent`, I think that is misleading. Lastly, I think that we should include total/successful/failed counts in the response. My specific suggestion is to add a method to `RestActions` similar to `RestActions#buildBroadcastShardsHeader`. I think it can look something like this: ``` java public static <TNodesResponse extends BaseNodeResponse> void buildNodesInfoHeader( final XContentBuilder builder, final ToXContent.Params params, final BaseNodesResponse<TNodesResponse> response) throws IOException { final TNodesResponse[] responses = response.getNodes(); final FailedNodeException[] failures = response.failures(); final int successful = responses != null ? responses.length : 0; final int failed = failures != null ? responses.length : 0; buildNodesInfoHeader(builder, params, successful + failed, successful, failed, failures); } public static void buildNodesInfoHeader( final XContentBuilder builder, final ToXContent.Params params, final int total, final int successful, final int failed, final FailedNodeException[] failedNodeExceptions) throws IOException { // nocommit: add a constant to Fields builder.startObject("_nodes"); builder.field(Fields.TOTAL, total); builder.field(Fields.SUCCESSFUL, successful); builder.field(Fields.FAILED, failed); if (failedNodeExceptions != null && failedNodeExceptions.length > 0) { builder.startArray(Fields.FAILURES); for (FailedNodeException failedNodeException : failedNodeExceptions) { builder.startObject(); failedNodeException.toXContent(builder, params); builder.endObject(); } builder.endArray(); } builder.endObject(); } public static <TNodesResponse extends BaseNodesResponse & ToXContent> BytesRestResponse nodesInfoResponse( final XContentBuilder builder, final ToXContent.Params request, final TNodesResponse response) throws IOException { builder.startObject(); RestActions.buildNodesInfoHeader(builder, request, response); response.toXContent(builder, request); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } ``` And then `buildResponse` call sites can just look like `return RestActions.nodesInfoResponse(builder, request, response);`
I'm still not following you? What's wrong with the `static` reference? Whether you use a constant string field (`static final String` assigned from a literal string) or just use literal strings in all the places that you would use the constant, the effect is the same: a single `String` is put in the constant pool and that constant is pushed onto the stack when needed using `ldc`. The java compiler effectively interns all literal strings, and this is the same effect as using a constant string field.
Did you push the change that added it? I don't see it.
I'm not following you? The bytes for the string have to sit somewhere and they always have to be there. When the compiler needs to use a string from the constant pool, it just emits a special instruction `ldc` to load a reference to the string onto the stack.
> I do think that these things actually add value when the field is used more than once in the class Having a `static final String` constant is fine (if you refer to the constant, or refer to a literal string, it doesn't matter they will both end up in the constant pool for the class), it's adding an _extra_ class to hold it that is unnecessary and just adds extra classes to be loaded.
Note that we also use this pattern for shard responses which are in the same situation.
As far as inconsistencies, some classes even have a blank line between the opening and the body, but not the body of the class and the closing.
Now I wonder if you should make readFrom/writeTo final and make an abstract method that is just responsible for reading the response. That pattern was used by the query builders until they switched to constructor based reading and it seems fairly a appropriate. I don't think it should block the PR though. It isn't really important I think.
I usually just do ``` /** * Read from a stream. */ ``` The input and output are relatively obvious. Same for the IOException. The IllegalArgumentException isn't, but honestly it isn't a big deal I think. Also not a big deal, more a matter of my personal style preference which I don't want to force on anyone.
They are just hedging their bets.
Nit: I'd just use the string rather than make a constant. We are slowly removing these objects.
Yes. Renaming that is a good thing.
You can also assert that there are no bytes left to read I believe. Some of the other tests in this file do it and it is a good idea I think.
It's from a [suggestion I made in an earlier pull request](https://github.com/elastic/elasticsearch/pull/17804#discussion_r59959473). The codebase is not consistent but there is definitely a tendency towards having a blank line between the opening of a class and its body, and its body and the closing of a class. It's not a requirement, but I think that it looks nicer this way. 
I prefer it without the blank lines. No wonder we aren't consistent.
Because not all of the inheriting classes implement `ToXContent`.
I think I missed the discussion but why isn't all this (this method and the next two) part of BaseNodeResponse's toXContent implementation? It can declare an abstract method that the subclasses can override for their own xcontent? We use that pattern pretty frequently with things like the query builders.
we only have index name (and not index uuid) in this one, wonder if we need snapshot uuid here...
again, this should be resolved outside of the loop. Also we have to be careful about when we are resolving the names (here we are using generic thread pool).
Snapshot Name - repository name + snapshot name ;-)
maybe only catch `IllegalStateException` to avoid hiding bugs? It seems to be the exception that the text field throws when fielddata is disabled and numeric fields when doc values are disabled
what I mean is just do it at the end of the method so waiting for refresh will be concurrent to the fsync (just a minor optimization - not really something worth complicating flow paths)
I think it's confusing that the WriteReplicaResult flow is different than WritePrimaryResul. i.e., `finishWrite` is called in the constructor for one and in the respond method for the other. We should try to make them the same as far as possible.
I see how you solved the multiple inheritance issue we want here with an interface and default implementation. Neat. I do however think it can be simpler with a static method. I prepared a patch to show what I mean - "simpler" is subjective :)
my thoughts too :)
can we add some docs as to what we mean with ephemeral - most notably when it changes, and it's implication to identity
I prefer `assertEquals` in cases like this. `assertThat` is great if you need to take a matcher or want to assert something complicated, but I like `assertEquals` for equality.
I'd also remove `deploymentSlot` field and change the code to: ``` if (deployment.getDeploymentSlot() != Discovery.DEPLOYMENT_SLOT_SETTING.get(settings).slot) { logger.debug("current deployment slot [{}] for [{}] is different from [{}]. skipping...", deployment.getDeploymentSlot(), deployment.getName(), Discovery.DEPLOYMENT_SLOT_SETTING.get(settings).slot); continue; } ```
I'd write: ``` if (Discovery.DEPLOYMENT_NAME_SETTING.exists(settings) && !Discovery.DEPLOYMENT_NAME_SETTING.get(settings).equals(deployment.getName())) { logger.debug("current deployment name [{}] different from [{}]. skipping...", deployment.getName(), Discovery.DEPLOYMENT_NAME_SETTING.get(settings)); continue; } ``` and remove `deploymentName` field.
I think this is okay though, it checks if the current `zeroTermsQuery` is the same as the default, which is ZeroTermsQuery.NONE.
same here, "Missing types in exists", maybe beter sth. like "Missing types in 'exists' query" or something like that.
Can we do this instead (throw an exception)? And just tell the user to use a cidr mask? Supporting "fuzzy" queries on ip addresses seems crazy, and I don't think we should continue this.
I was curious about how new ingest processors are implemented, so looked into this PR. Regarding requiring "sort": what if lastType is "sort" but it was sorting another field? Something that might be caught by pipeline validation perhaps.
Nah, if it hasn't hit yet we'll handle it if it does in the future, don't worry about it.
Could always make this a class var so it doesn't have to recalculate the number of nanoseconds in a second every time (but I guess it will be JIT-ed anyway)
I think it might be nice to have it behave the same for the Java API, where it needs to be explicitly set, but as you say, this can be a separate PR
Since this is a backport, this needs version checking
This should probably be `< 0` in case someone passes -2 as the requests per second
These need more version guards
Same here for version checking
How do you feel about not setting this and forcing a user of the Java API to be explicit about the rethrottling amount? Otherwise someone can accidentally un-throttle because they forgot to call `.setRequestsPerSecond`
Hope this doesn't bite us for really slow (read: Windows) CI servers...
Wouldn't this potentially be larger than Float.MAX_VALUE, be cast to a float and end up negative, causing an exception? (it could be I don't know what happens in Java numerics for this too)
Okay, don't forget the versioning when this is backported to 2.x!
This goes away in favour of the version in #18042 I guess.
Fyi, you could use ESTestCase#randomValueOtherThan() for this I think.
Like above, I'd simply use randomFrom(SortMode.values()).
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
Nit: I think you can leave out ESTestCase here.
The assertions here need to stronger than this.
There are other methods in InstallPluginCommandTests that could potentially be useful if we wanted to share them.
We will need stronger assertions here too.
There should be a newline here between the brace closing the method and the brace closing the class.
This also needs a test for the exceptional case (exception during reading).
there is a test helper method that can create plugin property files
I think there is a bug here. What is `\\`? I guess Windows? You need to take caution for different filesystems.
I think that the both kinds of tests (verbose and non-verbose) should specify exactly what the output is. Note that as written the non-verbose tests would pass even if the production code was changed to always output the verbose output. So, the output would be wrong but the test would not fail.
`Description` -> `Descriptor`
This lambda does not need to be a statement block.
This lambda does not need to be a statement block.
Maybe should be called something with `dirty` in the name because all the methods refer to that name.
you could rewrite this as ``` ElasticsearchParseException e = expectThrows(ElasticsearchParseException.class, () -> factory.create(config)); assertThat(e.getMessage, ... ); ```
++, it's a java 8 only idiom, which is not a problem for ingest, no backports
That's true, the Hamcrest matcher doesn't handle recursive matching, so it won't really simplify this class. A very general question - why didn't you use Jackson to do the equality check directly on the actual and expected JSON? Because the assertion errors would be frustratingly non-specific? Or because you are only testing certain elements of the JSON response for equality? Thank you, this is a very neat enhancement!
I'd probably keep the old signature of this method and do the set construction above.
Nit: it isn't a jsonBuilder - it is whatever kind of xcontent was passed in. Nit: maybe only set prettyPrint if the original had it set? I don't know if you can tell though. Neither are a big deal.
So maybe you don't need to handle the null case at all and just expect people not to pass null because it is a vararg. And it isn't passed as `@Nullable`.
Oh no! The build failed because of tabs! You can catch these sorts of things with `gradle core:precommit` which is much faster than `gradle core:check` because it doesn't run all the tests. I'll run the tests before pushing but for something like this it ought to be safe for you not to run them.
The second one, implement toString with the utility. On May 8, 2016 9:28 PM, "Johnny Lim" notifications@github.com wrote: > In core/src/main/java/org/elasticsearch/action/get/GetResponse.java > https://github.com/elastic/elasticsearch/pull/18102#discussion_r62442944 > : > > > @@ -168,4 +169,17 @@ public void writeTo(StreamOutput out) throws IOException { > > super.writeTo(out); > > getResult.writeTo(out); > > } > > + > > - @Override > > - public String toString() { > > - try { > > @nik9000 https://github.com/nik9000 Are you suggesting not implementing > SearchResponse.toString() but using Strings.toString() in application > code, or implementing SearchResponse.toString() via using > Strings.toString()? > >  > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub > https://github.com/elastic/elasticsearch/pull/18102/files/c5f0c73b8b0f9c57500656081005aa64e28f509b#r62442944
I mean to say that I think you can just call `Strings#toString`.
I think this is the same as `Strings.toString(this)`? Otherwise I think this looks fine.
Do we actually use this anywhere? It seems to only be assigned into.
Should we also have tests for the case that some intermediate mappers already exist? For instance above you are testing to index a field called `foo.bar.baz`, so it would be interesting to check that everything also works if `foo` already exists.
I would also like it better if the side effects of this getDynamicParentMapper were limited to just dynamic field creation, and path management stayed local (so that we keep both the add and remove together in the same method).
Do we really need a tuple? Shouldn't it always be `paths.length - 1`, ie anymore more than one path piece means we had to get that many extra parent mappers? Alternatively, I had also considered adding a serialized form, ie do a join of the first `paths.length - 1` elements of path and add those as "one thing", so you are always just removing a single element at the end. I think this would work since all path serialization does is add dots between elements.
I was only talking about the context _path_. But what you have is fine for now, the entire class really needs a rethink. :)
btw I've also had my confusions with this `processExistingRecoveries` method before ;-) At some point you don't even look at method names anymore, you just expect the code to be in that place.. :-)
`allocation.hasPendingAsyncFetch()` will always return false here. The field that is used to determine this value is set by Primary/ReplicaShardAllocator. Even if this field were correctly set here, it would still be the wrong value to determine whether the shard can be allocated or not. The primary/replica shard allocator is only interested in knowing whether there are still pending fetches for the targeted shard id.
Looking at the overall code of `calculateNodeExplanation` again, I think we should also improve the message in case where the shard can be assigned but we have throttling. Can be a follow-up.
I think this should explain why shard fetching is preventing allocation (otherwise this message will be too cryptic for the user). For primary shards it is to determine which nodes have a non-stale copy of the data. For replica shards it is to determine which nodes have a copy of the data and which copy shares the most data with the primary to speed up recovery.
We should take allocatedPostIndexCreate into account here (see PrimaryShardAllocator / ReplicaShardAllocator), i.e. ``` if (shard.unassigned() && shard.allocatedPostIndexCreate(indexMetaData) && hasPendingAsyncFetch) { ``` For replica shards (`ReplicaShardAllocator`) it's even a little bit more complex. Shard fetching is only relevant if not all deciders say YES (i.e. throttling leads to shard store data not being fetched) . We should thus include the condition `nodeDecision.type() != Decision.Type.YES` for replica shards to be exact. ``` if (shard.primary() == false && shard.unassigned() && shard.allocatedPostIndexCreate(indexMetaData) && nodeDecision.type() != Decision.Type.YES) { finalExplanation = "the shard cannot be assigned because allocation deciders say " + nodeDecision.type.name(); finalDecision = ClusterAllocationExplanation.FinalDecision.NO } else if (shard.unassigned() && shard.allocatedPostIndexCreate(indexMetaData) && hasPendingAsyncFetch) { ... shard fetching } ```
Wouldn't it be better to not call setScorer at all? I suspect most collector impls do not expect a null Scorer.
> extending AbstractQueryBuilder does it for you Awesome!
This is why it had the generics. OTOH I'll bet if we always return the subclass of QueryBuilder anyplace interesting and have each of them implement this method by returning their own type then no client code will have to change. We should probably add something to the docs on this method telling implementers to return their own type, not QueryBuilder.
on the reading side we use inputstream, i find it strange we add callback methods for writing (in various ways: inputstream, bytesReference, etc), versus say the ability to open an outputstream and write whatever yourself. maybe just a good TODO to investigate as a followup. If there are less "producers" of this api (e.g. snapshot) than there are "consumers" (e.g. various cloud storage plugins etc), it may make things simpler as then the different implementations have less to do (e.g. copy loops and so on).
it makes it too easy to call delete when its not necessary.
affect -> effect. as far as all the delete methods, i am again curious if this is the right approach. deleting a Collection of files or prefixes won't be atomic anyway, so maybe the impl should only have to delete a single file at a time? This would prevent them from having complex or buggy exception handling. the caller could just use e.g. listBlobs/listBlobsByPrefix and call delete on each one. Separately: does delete really need to be lenient if the file does not exist? Otherwise it leads too easily to `delete + write` which is the same as truncating data :)
And here just call `readOptionalWriteable` at the call site. You might could also do `in.readOptionalWriteable(SnapshotInfo::new);` which is a bit more concise if it works.
While you are here why not remove the whole `Fields` class and just use strings? That is the "new style". For things that are used in more than one place we'll typically use constants - and if those things are using in parsing the constant will typically be a ParseField, but otherwise we've started to not make these `Fields` classes.
Can you drop this method and just call the constructor instead? I don't think it buys anything.
I think for Longs, Doubles, and GeoPoints it's safer to use List (List < def > ) so as an example Longs would be copyStruct("Longs", "List", "Collection", "Object"); The reason being is that if anyone ever casts to one of these then we probably want to be able to handle the Object coming out of it without requiring a second cast. ((Longs)longField).get(0) is now a def type instead of an Object type. Really rare that this would ever happen, but I think for consistency this could be important.
Oh nevermind, I see the problem now, the field name is not used to calculate equality so they can stomp on each other even if they have the same name :(
And again, looking at the code, they _do_ use the field name in equality, but for the query, you can't have multiple _different_ queries even with the same name (which is why the hashset doesn't work), I'll stop confusing myself now.
Right, what I meant was, that `containsKey` value is only used if `value != null`, so why not get it only if `value != null`
But `value` is coming from the system property and `setting` is come the settings object? He's ensuring that he takes it from one of the two, but not both.
You don't need `containsKey` here, you can put this line of code below the check for `if (value != null)`
Ah, I misinterpreted that line because I never use that syntax. :)
Probably worth noting this exception to the rule in the PR description (since the description suggests that `es.*` causes a failure when this clearly shows that it won't).
I call those "leftovers".
Fewer type parameters is always a win!
I'd probably just pass the map rather than close over it.
I wonder if you can remove the `Response` type parameter here too!
I think I'd just let deleted and updated come out in the toString rather than worry about that. I think it was a mistake that I did it. I don't think it is worth the complexity of the map. It is just a couple of fields that we can ignore.
Instead of using params maybe the caller can do this? Like, the REST endpoint knows about it. And it isn't a big deal to have the extra field in the task status.
This are actually just plain wrong now since we no longer have individual plugins, but unrelated to this change (I'll fix in a followup).
maybe I am missing something, but `.getSourceAndMetadata()` returns a mutable Map? here is an example: https://github.com/elastic/elasticsearch/pull/18193/files#diff-4e27382bea1f95bce321ce30c5315e98R42
this `readStringProperty`? it is not parsing script code, it is an ingest-node config reader
Yikes! I'd really prefer not to do this. This kind of thing is hacky when you like guice and it is super bad when you are trying to leave guice. I'm not really sure the right thing here though.
I wonder if it makes more sense to push the default into the engine rather than a fixed switch statement. Like, plugins could declare that they should be on by default or off by default.
Isn't `js` the common extension? At least, that's what is in RFC 4329.
Maybe make this two declarations? It is really funky to read like this.
I wonder why we're mocking this simple class. Maybe we can fix the tests? :-)
why did you remove this? What if we have multiple pattern that match multiple snapshots? we now add these multiple times. For example: `foo*,*bar` will match the snapshot `foobar` twice.
IDEA says that this method is not used... maybe just remove it.
we can make this `public Entry snapshot(Snapshot snapshot)` The only place where there is an issue is in the tests. There, we can manually iterate over the entries...
I also wonder if we should make `entries` a set (follow up).
Let's use `SnapshotId` instead here.
space missing after catch
we should probably expose snapshot directly as I see that usage points create a new Snapshot(..) again based on the data here...
Remove if/else by `in.readOptionalWriteable(Snapshot::new)`
Pass SnapshotInfo directly to this method? See `TransportSnapshotsStatusAction`. No need to revalidate...
this method is dangerous as it wrong usage leads to re-resolving all the time. Maybe just remove it and do the `resolveSnapshotId` in TransportDeleteSnapshotAction
use the constant defined in SnapshotId...
you probably intended to write "alias:id,snapshot".
The listeners in SnapshotService and RestoreService could just themselves implement this matching logic. The API would then be ``` snapshotsService.addListener(repository, snapshotName, new SnapshotsService.SnapshotCompletionListener() { ... }); ```
that's not whats happening here. If a snapshot with the name exists in the repository only the snapshot in the repository is deleted. The running snapshot is not canceled. We should do both imo.
We can use a set here instead (it's sorted at the end anyhow). ``` final Set<SnapshotId> snapshotIdsToIterate = new HashSet<>(snapshotIds); // first, look at the snapshots in progress final List<SnapshotsInProgress.Entry> entries = currentSnapshots(repositoryName, snapshotIds.stream().map(SnapshotId::getName).collect(Collectors.toList())); for (SnapshotsInProgress.Entry entry : entries) { snapshotSet.add(inProgressSnapshot(entry)); snapshotIdsToIterate.remove(entry.snapshot().getSnapshotId()); } ```
There are a few issues here. First I think we have a race condition here. Moving this check to the the repository level will not remove it right now, but will tighten the loop. Hopefully, we will solve it there at some point of time with tightening the blob store api and adding locks. The second issue is that it can be done more effectively on the repository level.
Maybe simpler just to do it the oldschool way. ``` final Map<String, SnapshotId> allSnapshotIds = new HashMap<>(); for (SnapshotInfo snapshotInfo : snapshotsService.currentSnapshots(repository)) { SnapshotId snapshotId = snapshotInfo.snapshotId(); allSnapshotIds.put(snapshotId.getName(), snapshotId); } for (SnapshotId snapshotId : snapshotsService.snapshotIds(repository)) { allSnapshotIds.put(snapshotId.getName(), snapshotId); } ```
It only checks those persisted in the repository...
I prefer that we make this explicit by passing `UUIDs.randomBase64UUID()` here and removing the overloaded constructor.
This does IO in the cluster state update thread  Super dangerous. We should probably put assertions in BlobStoreRepository that we are not the cluster state update thread to help prevent mistakes like this.
This leads to resolving the snapshot name on the repository for each entry in the list, which is very wasteful (We query the list of snapshots n times and thus have n separate network requests on S3/GCE etc.). This method should be rewritten so that we query the snapshot names on the repository once and then get all the snapshotInfo elements for the entries that we requested in a subsequent step.
there is just 1 node
lets put this into a unit tests class
I have seen this comparison in several places. Maybe, it makes sense to move inside Snapshot and have something like `boolean hasName(String repository, String name)` method where it will take place? Not sure if `hasName` is a good name for this method though.
I think this check should go into initializeSnapshot or repository.
maybe call this "resolveSnapshotNames"? I would also prefer to use `List<String> snapshotNames` as parameter to bring it closer to the return type.
This reads a bit funny: "If any .. are not found"...
Simpler: `snapshotIds = in.readList(SnapshotId::new);` No need to create lambda :-)
As this is just a public wrapper for new Snapshot(...), I prefer that we make the constructor public and get rid of this method.
I wonder if we should use something like `"_na_"` similar to the `INDEX_UUID_NA_VALUE`. That is more an explicit value.
make class final
did you mean norelease? otherwise it will fail the build
We have something similar in mappings, and I would advise against it. It becomes too easy to just add some essentially global state, which turns into a monstrosity with 14 members. It is much better for methods to explicitly take what they will use...my 2 cents.
can we use `writeOptionalWriteable/readOptionalWriteable`
can this be done in the base class with ``` Java return getClass().getSimpleName() + "[field=" + field + ", id=" + id + ", max=" + max + "]"; ```
acceptDocs will be checked _before_ these bits are checked anyway
it should be used in the hashcode too
we also need unittests for these queries!!!
you can ignore acceptedDocs here I think.
I think the reason whi Jim did so is that this is the method that is implemented by all fielddata impls regardless of whether they use numerics, binary or sorted doc values under the hood.
not related to this PR, but chaning -> changing
Fine with me.
if you do `extends ToXContentToBytes` instead of `implements ToXContent` you get that for free
I think an absurdly high limit could still be helpful? (in a follow-up PR)
ok I wasn't sure, perfect
minor nit pick `return useTermQuery ? new TermsSliceQuery(field, shardSlice, numSlicesInShard):new DocValuesSliceQuery(field, shardSlice, numSlicesInShard);`
s/Long.hashCode/BitMixer.mix64/ ? otherwise we might still have the issue with doubles given that Long.hashCode is a bit naive
I think we should do this even if we use docvlaues? I think we should have consistent slicing no matter how it's done!
we should use `org.elasticsearch.common.xcontent.ObjectParser` for this instead of parsing all the stuff ourself
we might want to rehash the value before calling contains. For instance if the numeric field is a double and all values represent integer values, the lower bits will always be zeroes.
Thanks. I will merge.
My first thought was that the regex `^https?://` is probably faster. A simple benchmark showed it to be around 40% faster. My second thought was it's shame that we have to create so much garbage here (compiling a regex, and creating the substring). But we do not need to compile the regex every time, we can just create a static regex: ``` diff diff --git a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java index 4eb6323..2c8a575 100644 --- a/core/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java +++ b/core/src/main/java/org/elasticsearch/http/netty/cors/CorsHandler.java @@ -33,6 +33,7 @@ import org.jboss.netty.handler.codec.http.HttpResponse; import java.util.HashSet; import java.util.Set; +import java.util.regex.Pattern; import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.ACCESS_CONTROL_ALLOW_CREDENTIALS; import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.ACCESS_CONTROL_ALLOW_HEADERS; @@ -131,10 +132,12 @@ public class CorsHandler extends SimpleChannelUpstreamHandler { .addListener(ChannelFutureListener.CLOSE); } + + private static Pattern PATTERN = Pattern.compile("^https?://"); private static boolean isSameOrigin(final String origin, final String host) { if (Strings.isNullOrEmpty(host) == false) { // strip protocol from origin - final String originDomain = origin.replaceFirst("(http|https)://", ""); + final String originDomain = PATTERN.matcher(origin).replaceFirst(""); if (host.equals(originDomain)) { return true; } ``` The same benchmark showed this to be around 250% faster and creates one less object.
It was removed from ParseField in #17933. It also shouldn't be added here.
Probably for another PR since it's unrelated but I wonder if `scriptable`, `formattable` and `timezoneAware` should be properties of the Builder object rather than/ as well as the parser so the builder can ensure an IllegalArgumentException is thrown if e.g. an unscriptable agg has the script method called on it? /cc @jpountz
I'm pretty sure camelCase shouldn't be supported any more.
`fields = in.readList(StreamInput::readString);` I think
Really the attributes for the super class should be read and written by the superclass to ensure they are consistent across all implementations
+1 but I would check with @clintongormley whether we are happy making this breaking change in 5.0. However, we haven't actually formally removed camelCase in ParseField IIRC so adding this extra name here might be unnecessary anyway
I don't like these being in ParseField because ParseField is broader than Aggregations and is used all over the codebase. I would rather this was moved to a constants interface called `CommonFields` as an inner interface to AggregationBuilder (much like we have `InternalAggregation.CommonFields` for the response side).
Ok great, thanks for the correction @rjernst
I think the accepted convention now is to read the stream in a Constructor that takes StreamInput as a constructor arg
I'm pretty sure this just throws an AssertionError so it wouldn't work either. I don't suspect it'd be very likely and I think the test would fail spectacularly on an InterruptedException anyway, so maybe just log an error? You could also make some list outside the runnable to accumulate the result. I bet we have some useful thing sitting around for this if you wanted to do more than log though.
Got it. LGTM
What about : ``` json "retries": { "bulk": 0, "search": 0, } ``` Note: I tend to like JSON inner objects since clients and parsers can skip whole objects while parsing...
It's super minor, but our log standardization is usually all lowercase
We have code in `ExceptionsHelper` for this (`unwrapCause` I believe)
I think it'd be cleaner to put the `finishHim(e);` into an `else` statement than to return early in this logic
and I think we should leave periods out of log messages too
This is a breaking change now so this PR needs to be marked as "breaking" and a note added to the migration guide
Same here with period and lowercase
I like `hasSize` better for this because it gives a nicer error message on failure.
Yup. I just don't know it well enough. Disregard.
I'm not sure this method name is right. Maybe I just don't know this code that well though.
this one is sneaky :)
I expect to be tested as it's static, but I don't see test? I think its good to have one. Also, it seems it can be package private.
can we randomly test another similar path where a shard from another index becomes unassinged and have a shorter delay? this should reschedule as well..
that's OK because of the fact that this run by a single thread, but it will be easier on the eye to use: ``` existingTask.cancel() ``` instead of removeTaskAndCancel()
it's different because applyStartedShards and applyFailedShard are doing something that isn't done in the reroute. Also this method is called when we are 99% sure that something is going to happen. I don't think it's worth the extra code path.
we don't need the thread pool anymore
Ahh, sorry. You are 100% correct.
Perfect! Thank you.
If you take a look at line 245 of your modified file: `actual = read ? last.after : definition.voidType;` I think this needs to be separated out for the two cases: for the ADefLink case -- actual = read ? expression.actual : definition.voidType; for the regular case it should be the same, but moved into the else block actual = read ? last.after : definition.voidType; This way the actual type for the EChain in the def case will be the expression.actual value that ends up duped onto the stack.
@uschindler -- Does your solution fail for `int x; def y = 2.0; x = (int)y` ? If it does I think this line `if (last instanceof ADefLink && this.expected != null) {` needs to be `if (last instanceof ADefLink && this.expected != null && !explicit) {` Otherwise, we will not properly cast in the case of explicit casts because the expression.actual will match the expression.expected already.
@uschindler Sorry again! I need to quit providing you with incorrect examples. I mean in this case -- `double x, def[] y = new def[1]; x = y[0] = 1`. the EChain for y will leave painless to believe a def is on the stack, but the duped value will be an int!
You're solution is the best one. After thinking about this some more, I realized that my solution doesn't actually help at all because in the case where it would help it's going to be def anyway due to the promotions. You do need the 2-passes to get all the information necessary here.
Shoot. Sorry for the confusion... I meant this.actual (the EChain's actual value). I believe if you have something like double x; int y; x = y = 1; will blow up? (Or at least do bad things.) The duped value on the stack ends up being viewed by painless as a def type rather than an int.
Sorry, I overlooked the null check. This is good!
This is close, but I think this needs to, unfortunately, be placed in an overridden cast method. this.expected isn't guaranteed to be set until after the analyze phase because of possible promotion. Something like the following: ``` @Override AExpression cast(final CompilerSettings settings, final Definition definition, final Variables variables) { final ALink last = links.get(links.size() - 1); // If the load node is a DEF node, we adapt its after type to use _this_ expected output type. // Note we avoid doing this if the cast is explicit because we want to ensure that the cast // will do primitive narrowing operations and upcasts at runtime without throwing an // exception. We also avoid doing this if a store is being done since the value from // the store is what will be on the stack. if (!explicit && expression == null && last instanceof ADefLink) { last.after = this.expected; this.actual = this.expected; return this; } else { return super.cast(settings, definition, variables); } } ```
I believe for chained assignment like int x; int y; x = y = 1, expression.actual needs to be set to be the storeValueType since that's what will be duped onto the stack. expression.actual is set a few lines down from here, but it's set to last.after which is no longer correct in this case.
Yes! Thank you.
ugh, that's a little annoying. Nevermind
Personally repetition makes me blind to any small changes but I can live with that.
Fair enough. Can you set up the mocks in a `for (String generation: new String[] {"young", "old"}) {` loop or something? There is enough setup for each and they _look_ the same. If it were in a loop any place where it isn't the same setup would pop out.
"marker" interface is also fine. I made it as a class for the special cases we might introduce when we have more "def" types propagation. For now interface is also fine. It would spare to repeat the ctor!
I had something similar before, maybe revert back to using ALink.after for that - I was not sure if the operand should always have the same type like the after value. In any case I would keep the superclass ADefLink to make them easier to "detect" in those if statemments, although the ADefLink class is empty afterwards.
You can skip these two and just create the index. It ought to just automatically create.
It is more common to just call `.get()` now. This is a pretty old test!
I'd set refresh to true on the index request instead of using a separate refresh.
Since you don't care about the body of the source maybe use something like `setSource("foo", "bar")`.
`assertNoFailures` is more common in newer tests and much shorter.
I like `hasSize(1)` for this kind of thing because it makes a nicer error message.
channeling someone by saying `primary() == false` :)
nite: line length..
I think this tryIncRef is only there to make sure shouldCancel can called without things changing on it. I think we can just call cancelRecovery and see whether it returns true.
we lose some concurrency control here where we only cancel the recovery if it's not done. I'm wondering if we should use `cancelRecoveriesForShard` and if it returns true, we then remove the shard. Also - In all cases where the the source of recovery changes due to a primary failure, the master cancels the allocation and changes the allocation id, meaning we don't get here. I wonder if we should also catch the case where a primary relocates on the master and not have to worry about all of this here. I think it will be simpler all in all
can we change to use isRelocationTarget() instead of relocatingNodeId() != null ? I think it's confusing that a relocation source will return true here.
isn't this what peer recovery means? should we just check currentRoutingEntry? at this point we know it has to be the same allocationId, which will means it's safe.
> can be cancelled just because primary relocation completed before shard was activated by the master node yes. I'm aware of that - I'm thinking that with seq# fast recovery it wouldn't matter much as a ready shard will quickly re-recover. However, seeing how the new code looks like with the cancelRecoveriesForShard + shardRouting.isPeerRecovery changes, I think it became much simpler. I'm good. We can see how things develop later on and potentially move some logic to the master (which will simplify this class) or not.
can we change to use isRelocationTarget() instead? i.e., `isRelocationTarget() || primary == false`
I'd remove this again. Because this is done in original ASM, just to prevent incorrect stack on voids
ok I remember now. The point of IndicesRequest and CompositeIndicesRequest is to return the indices that a request works against. when a request is composed of multiple operations, it should implement CompositeIndicesRequest. In this case delete by query reads from some indices as part of search, and writes as part of deletes. But what indices would it delete from? It is not possible to create a DeleteRequest that points to multiple indices, yet it is hard to predict all the deletions that will be performed as part of the request execution. I doubt that this request should implement CompositeIndicesRequest then.
Shield uses it as a first pass. The actual security is rechecked on the search and sub-bulk requests so this is just more of a pre-flight thing. It makes sense to return both requests, yeah.
I think what gets returned by singletonList is already immutable, no need to wrap it with unmodifiableList too.
Sorry for the back and forth, I had to do some more digging. CompositeIndicesRequest is not really about the type of operation (read or write etc.) but more about each set of indices and their corresponding indices options that are used to resolve them. Think of _msearch for instance, each inner search has its own indices, which get resolved based on each individual indices options. That said, I am not sure that delete by query should be seen as a composite request, as the indices get resolved once when doing the search. Then it should simply implement IndicesRequest and return the set of indices that the inner search request points to.
s/`that are currently can't be allocated`/`that can't currently be allocated`/
unrelated but maybe we can clean this further up and rename `matchedQueries` to `setMatchedQueries` and make it use a List instead of a String array.
Nit: `No` -> `no`.
I wouldn't reuse that MetaData parameters here but use new parameters specific for this class.
Sorry, nevermind: both of those queries also subclass `GeoPoinInBBoxQuery`, so this check is sufficient!
I'd just make one CancelTests or something and have it check all three actions.
One thing that has come up with reindex being in a plugin: users of the Java API have trouble finding it. I don't know what, if anything, we should do about that. This will have the same problem.
So that seems to confirm that they are interchangeable.
can we not read the indices from the cluster state response instead of resolving them manually again? I guess I don't get the order problem...
ok, while using cluster state output elements are ordered in a non deterministic way? I did not know what.
Thank you for formatting these. It makes them so much easier to read.
I usually try to write a test that fails if the version is 6.0 to ensure we don't miss this, like a simple unittest
Do you think we could have something like: ``` java bulkRetry = Retry.on(EsRejectedExecutionException.class).policy(BackoffPolicy.wrap(backoffPolicy, task::countBulkRetry)); ``` I find it easier to know what's going on on bulk retries.
Maybe this was already covered somewhere, but is `GENERIC` the right threadpool for this? (I don't have a better suggestion, just asking)
Yeah, less classes is much better
sure, or just make it `[foobarbaz/0/mynode]` or something, `[foobarbaz//]` if there is only one or something
its -> hits
Can we create the exponential back off here and provide a package protected getter for the `BackoffPolicy`? It avoids to jump to another method just to see what kind of policy is created
I don't think we should force `https` if a password is used. It's unfortunate but someone might want to use auth without encryption
Ahh okay, that makes sense, I missed that
For readability, could we have ``` java List<SortBuilder<?>> sorts = mainRequest.getSearchRequest().source().sorts(); if (sorts == null || sorts.isEmpty()) { mainRequest.getSearchRequest().source().sort(fieldSort("_doc")); } ```
This seems weird since `retries` is an iterator for TimeValue, what is this going to print? the log message makes it seem like it's expecting a plain number for the number of retries
Maybe rename `myself` this to _local_ like in `network.host` setting? At some point we should support port ranges too, but it can be done in a follow up PR.
This can be removed now
Could initialize this with the size of the hits list to prevent resizing
Can you reverse this, the negative makes it harder to read
super minor, but indentation is off here
It drives me bonkers that this is called "scroll" everywhere instead of "scrollId", but it's a matter of taste, no impetus to change it if you like it :)
It's minor, but we usually lowercase exceptions and elide ending punctuation
If you change the previous method, you can make this return `void` because it doesn't need to return the string
I believe you could replace this function body with: ``` java for (String iOrT : array) { checkIndexOrType(name, iOrT); } path.append(Strings.arrayToCommaDelimitedString(array)).append("/"); ``` Which also then doesn't leave the `path` stringbuilder in a bad state when an exception is thrown
also we usually lowercase all the log messages and exception messages too (minor nit)
`SearchResponse` implements `StatusToXContent`so you can use its `status()` method instead of forcing `OK`. Something like: ``` java SearchResponse response = item.getResponse(); builder.field(Fields.STATUS, response.status()); response.toXContent(builder, params); ```
`status` must be declared in the static inner `Fields` class like `Fields.RESPONSES`
ok, fair enough
nit pick why not: ``` Java } catch (ScriptException e) { throw e;// its already good! } catch (Exception e) { ... ```
any chance we can shard this code with AllocationService
We should also get rid of this local variable (The map is not needed as `DiscoveryNodes` has a method `get()`). We could write `this.nodes[i] = clusterState.nodes().get(nodesIds[i]);` a few lines below. If `clusterState.nodes()` is too verbose, we can extract that one into a local variable.
maybe we should just get rid of this local variable and write the next line: ``` nodesIds = filterNodeIds(clusterState.nodes(), resolveNodes(request, clusterState)); ```
please remove the additional `getNodes()` here :-)
I don't know for sure but I suspect the generated query will not match anything.
this line can be removed
Ok, I understand that. I also don't like the added complexity of having this removal code in another place...
this should probably read: `we can only shrink to 1 *shard* so far!`
`initialRecoveryFilters.match(node.node()) == false`
We could remove this setting once all shards have been activated (in `AllocationService.updateMetaDataWithRoutingTable()`).
do we really need to extend this class? I wanna get rid of it? We should set things up in a ctor and impl. Closeable IMO
I think this patch could work just fine? ``` DIFF diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java index 8a1df50..94a0ab1 100644 --- a/core/src/main/java/org/elasticsearch/node/Node.java +++ b/core/src/main/java/org/elasticsearch/node/Node.java @@ -98,6 +98,7 @@ import org.elasticsearch.search.SearchService; import org.elasticsearch.snapshots.SnapshotShardsService; import org.elasticsearch.snapshots.SnapshotsService; import org.elasticsearch.tasks.TaskResultsService; +import org.elasticsearch.threadpool.ExecutorBuilder; import org.elasticsearch.threadpool.ThreadPool; import org.elasticsearch.threadpool.ThreadPoolModule; import org.elasticsearch.transport.TransportService; @@ -210,12 +211,12 @@ public class Node implements Closeable { throw new IllegalStateException("Failed to created node environment", ex); } final NetworkService networkService = new NetworkService(settings); - final ThreadPool threadPool = new ThreadPool(settings); + final List<ExecutorBuilder<?>> executorBuilders = pluginsService.getExecutorBuilders(); + final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0])); NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry(); boolean success = false; try { - final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool); ModulesBuilder modules = new ModulesBuilder(); modules.add(new Version.Module(version)); modules.add(new CircuitBreakerModule(settings)); @@ -223,6 +224,7 @@ public class Node implements Closeable { for (Module pluginModule : pluginsService.nodeModules()) { modules.add(pluginModule); } + final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool); modules.add(new PluginsModule(pluginsService)); SettingsModule settingsModule = new SettingsModule(this.settings); modules.add(settingsModule); diff --git a/core/src/main/java/org/elasticsearch/plugins/Plugin.java b/core/src/main/java/org/elasticsearch/plugins/Plugin.java index 1efc151..695a255 100644 --- a/core/src/main/java/org/elasticsearch/plugins/Plugin.java +++ b/core/src/main/java/org/elasticsearch/plugins/Plugin.java @@ -23,9 +23,12 @@ import org.elasticsearch.common.component.LifecycleComponent; import org.elasticsearch.common.inject.Module; import org.elasticsearch.common.settings.Settings; import org.elasticsearch.index.IndexModule; +import org.elasticsearch.threadpool.ExecutorBuilder; +import org.elasticsearch.threadpool.ThreadPool; import java.util.Collection; import java.util.Collections; +import java.util.List; /** * An extension point allowing to plug in custom functionality. @@ -80,4 +83,8 @@ public abstract class Plugin { */ @Deprecated public final void onModule(IndexModule indexModule) {} + + public List<ExecutorBuilder<?>> getExecutorBuilders() { + return Collections.emptyList(); + } } diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java index f373da6..bb22854 100644 --- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java +++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java @@ -40,6 +40,7 @@ import org.elasticsearch.common.settings.Setting; import org.elasticsearch.common.settings.Setting.Property; import org.elasticsearch.common.settings.Settings; import org.elasticsearch.index.IndexModule; +import org.elasticsearch.threadpool.ExecutorBuilder; import java.io.IOException; import java.lang.reflect.InvocationTargetException; @@ -261,6 +262,14 @@ public class PluginsService extends AbstractComponent { return modules; } + public List<ExecutorBuilder<?>> getExecutorBuilders() { + ArrayList<ExecutorBuilder<?>> builders = new ArrayList<>(); + for (Tuple<PluginInfo, Plugin> plugin : plugins) { + builders.addAll(plugin.v2().getExecutorBuilders()); + } + return getExecutorBuilders(); + } + public Collection<Class<? extends LifecycleComponent>> nodeServices() { List<Class<? extends LifecycleComponent>> services = new ArrayList<>(); for (Tuple<PluginInfo, Plugin> plugin : plugins) { diff --git a/core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java b/core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java index 31f3f31..61e5141 100644 --- a/core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java +++ b/core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java @@ -30,7 +30,7 @@ import java.util.List; * * @param <U> the underlying type of the executor settings */ -abstract class ExecutorBuilder<U extends ExecutorBuilder.ExecutorSettings> { +public abstract class ExecutorBuilder<U extends ExecutorBuilder.ExecutorSettings> { private final String name; diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java index 0b564b2..1d641aa 100644 --- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java +++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java @@ -151,7 +151,7 @@ public class ThreadPool extends AbstractLifecycleComponent<ThreadPool> { return Collections.unmodifiableCollection(builders.values()); } - public ThreadPool(Settings settings) { + public ThreadPool(Settings settings, ExecutorBuilder<?>... customBuilders) { super(settings); final Map<String, ExecutorBuilder> builders = new HashMap<>(); @@ -175,7 +175,13 @@ public class ThreadPool extends AbstractLifecycleComponent<ThreadPool> { builders.put(Names.FETCH_SHARD_STARTED, new ScalingExecutorBuilder(Names.FETCH_SHARD_STARTED, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); builders.put(Names.FORCE_MERGE, new FixedExecutorBuilder(settings, Names.FORCE_MERGE, 1, -1)); builders.put(Names.FETCH_SHARD_STORE, new ScalingExecutorBuilder(Names.FETCH_SHARD_STORE, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); - this.builders = builders; + for (ExecutorBuilder<?> builder : customBuilders) { + if (builders.containsKey(builder.name())) { + throw new IllegalArgumentException("builder with name: " + builder.name() + " already exists"); + } + builders.put(builder.name(), builder); + } + this.builders = Collections.unmodifiableMap(builders); assert Node.NODE_NAME_SETTING.exists(settings); threadContext = new ThreadContext(settings); @@ -190,10 +196,6 @@ public class ThreadPool extends AbstractLifecycleComponent<ThreadPool> { this.estimatedTimeThread.start(); } - void add(ExecutorBuilder builder) { - builders.put(builder.name(), builder); - } - @Override protected void doStart() { final Map<String, ExecutorHolder> executors = new HashMap<>(); ```
we can maybe have an assertPathHasBeenCleared variant? then people will hopefully see both and choose :)
is this always used in an assertBusy context? wonder if we should add it here. This can be suprising...
That should probably go to TaskInfo, especially parser that should be definitely somewhere close to the corresponding toXContent method that generates this json.
It might be a good idea to store a version of elasticsearch that generated the task somewhere on the top level. So, we could filter out incompatible versions in the future and don't have to support old persistence formats forever.
We should make TaskInfo final then.
clusterName is not needed here.
Could you explain the reason for this change? It seems to be against the common pattern that I have seen pretty much everywhere else in the code.
This might be really confusing. We don't support any parameters that are provided by `BaseTasksRequest<GetTaskRequest>` so the only reason to use it would be to continue using TransportTasksAction which is overkill here anyway since we know the node that we need to get the task from. So, we can base this directly on TransportNodesAction instead.
Hmm, not sure how I feel about that.
You are throwing away the stack trace here. Just have this method throw Exception, and the tests that call it as well.
You can use expectThrows()
Same concern about reproducibility as in the other PR.
I think println should be rewritten to use this? ``` public final void println(Verbosity verbosity, String msg) { print(verbosity, msg + lineSeparator); } ```
I'd vote for removing this constructor and change the caller to provide explicit arguments since it is not obvious what the defaulh parameter values should be
maybe update the docs to say this is a terms query rather than a bool
clause.isScoring is also true for MUST clauses, so I think the naming is confusing
Oh I see. I like it better the current way better then. I was confused by the fact that you could have both ALWAYS and PARTIAL in the same doc, maybe we could add an assertion that it never happers.
or maybe even better: I think you can simlify the logic below by using Lucene.asSequentialAccessBits (that we also use in function_score and filter(s) aggs) to check whether a doc matches the scorer.
should be clause.getOccur() == SHOULD
I know previous behavior was to spawn a thread every time. I think it will be cleaner to return a boolean from the deletion methods to say whether the index deletion was succesful or is marked as pending and only spawn the thread for the latter.
nit: can we move the look up of the primary to the callers that pass null? this method is hairy enough :)
ok fair enough
yes that is true for primaries not necessarily for replicas. The replicas might not be a light recovery
Yeah, I think we can collapse both deciders into one here - it will make things simpler. Call it RecoveriesAllocationDecider that is incharge of all recovering shards (replicas and relocating primaries). It's good to do it in a different PR imo..
can it be unassigned too? I am still trying to get back into how this works but I wonder why :)
> Makes sense? It does not make sense. Having try/catch like this means the test doesn't really know what it is testing.
Yeah, this is really tricky, but I think you did the right thing here, I don't see a better way.
Typo: `afllowed` -> `allowed`
I'd rather just `new ConcurrentLinkedQueue<>()`.
Nit: `accross` -> `across`
Nit: `parallel` -> `concurrent`
Nit: `currentParallelSearches` -> `currentConcurrentSearches`
Nit: `maxParallelSearchRequests` -> `maxConcurrentSearchRequests`
Nit: `maxParallelSearchRequest` -> `maxConcurrentSearchRequests`
Nit: I think that name of the request parameter should reflect concurrency rather than parallelism.
Nit: I think it should the name should reflect concurrency rather than parallelism.
I think that we can do better than this. If I'm reading this correctly, this means that we wait until an entire batch is complete before submitting another batch of requests. Thus, a slow request can hold the next batch and thus the response. I think instead we should try to maintain as many requests in the queue as possible, up to the concurrent request limit.
+1 to: ``` List<DiscoveryNode> nodes = this.nodes; if (closed) { throw new IllegalStateException("transport client is closed"); } ensureNodesAreAvailable(nodes); ```
Yeah, I'm not advocating for removing the mutex. That isn't going to work. My thought came from vague notions of "can we just set the list to null when we've closed the client?" kinds of questions.
Wouldn't it be good enough to write: ``` List<DiscoveryNode> nodes = this.nodes; if (closed) { throw new IllegalStateException("transport client is closed"); } ensureNodesAreAvailable(nodes); ``` as we know that the close() method first sets `close` to true and then empties the nodes list.
Nit: "doens't" -> "doesn't"
no need for this, that's the default..
no need for these local variables, they're only used once...
more reformatting in this class everywhere, revert all this...
what happens when indexing completes, relocation happens and then (and only then) the global updater kicks in. I _think_ we still have an issue, if true, let's just relax global checkpoint for now. I'm good with doing all of that as a separate fix in the interest of getting this in.
we might make this pkg private once we moved all logic into IndicesService
I think we should have a dedicated method for this in IndicesService. ``` public FieldStats<?> getFieldStats(Engine.Searcher searcher, String field) { // do the caching in here and also lookup the MappedFieldType again! } ``` this way we don't allow everybody to cache whatever on our request cache!
I'd just return null if the field doesn't exist
alright - didn't see it immeditately
I wonder if we should default it to the number of shards :)
I think we should be very conservative here by default
this should go away here I think it's impl details of a bwc layer in S/R
why do we need this here? I think this entire `hashAndLengthEqual` can and has to go away
if this is for tests only then don't register it by default. Rather register it in `InternalSettingsPlugin.java` and install that plugin in the relevant tests
what about `requests`, `templates`, `metrics` ? :)
also here I'd go for `params` and `ratings`
I really wonder if we should just expect a full blown query in there instead of templates. It would simplify a lot and users can still put the template in the clusterstate to run stuff against if they use a template query. then they just put pass the params with the query as a template query and we can be our own plugin / module without the dependency
secure iff not reproducible (sentence is a bit mixed up)
Maybe use `getSecure()` so that it's closer to the other methods in this class (`get()`).
put [ and ] surrounding the blobPath, so `"File [" + blobPath.toString() + "] ...`
Instead of making up our own exception, why not use just use Files.delete? This will give you a better exception message. https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#delete(java.nio.file.Path)
@gfyoung @rmuir's suggestion here is correct. If you can fix that, I'll merge the PR; sorry I was a bit delayed today in responding.
> We should not catch the `SecurityException` at all. Let it propagate. Precisely.
We should not catch the `SecurityException` at all. Let it propagate. We should not have even gotten to this point if the security manager did not give us access here, but in any case, its not an exception we should handle at this level. It should just be propagated.
I don't think that a security exception should be re-thrown as an `IOException`.
Please add this to the package-info as well.
I don't think you can use this method because it won't necessarily store the type correctly since we do the slots ourself to avoid trash being on the stack with variables scopes and such. Instead you'll have to use writer.visitVarInsn(asmtype.getOpcode(Opcodes.ILOAD), slot);
Rather than the `noinspection`, I'd prefer: ``` diff diff --git a/core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java index 7ec47ca..4db70ed 100644 --- a/core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java +++ b/core/src/main/java/org/elasticsearch/cluster/service/ClusterService.java @@ -407,8 +407,7 @@ public class ClusterService extends AbstractLifecycleComponent<ClusterService> { synchronized (updateTasksPerExecutor) { List<UpdateTask> existingTasks = updateTasksPerExecutor.computeIfAbsent(executor, k -> new ArrayList<>()); - for (UpdateTask existing : existingTasks) { - //noinspection SuspiciousMethodCalls + for (@SuppressWarnings("unchecked") UpdateTask<T> existing : existingTasks) { if (tasksIdentity.containsKey(existing.task)) { throw new IllegalArgumentException("task [" + existing.task + "] is already queued"); } ``` because it's more obviously correct. :smile:
I wonder if the input parameter should just be typed as an `IdentityHashMap` then? Note that currently this will silently ignore duplicates in the input `tasks`.
And so does my proposal, by typing `existing` more strongly than the compiler can do (because of erasure) but is clearly correct and eliminates the suspicious call inspection.
Nits: `set` -> `batch` and `task` -> `tasks`
Yes, I confused myself.
I think you can remove that method now and inline the body above. It was just added to make the code in `SSource` not rely on internals here.
Nit: I realize your IDE probably did this, but would you mind switching it back to `org.elasticsearch.painless.Definition.Type;` and list out the `org.objectweb.asm.Type instead`? It matches with the other imports already from Definition.
I think this map should also be unmodifiable like the outer one. It is not so important, because the whole thing is private, but for consistency. Alternatively remove the unmodifiable from outer map, too.
OK. It was just a suggestion. UweSays: Switch statements are very non-java like and must die :-)
Definitely for a different PR, but I think we can fix this by making the String constant in ALink an Object constant instead.
I think that suggesters are just less far along than queries. It is fine though.
It might make sense to replace this class entirely with ParseFieldRegistry. I'm not sure.
I think we've been registering default stuff in the module to keep it all in one spot.
Also I use sometimes (in workshops) file:../relative/path
I used to use file:/path/to/file.zip instead of file:// Not sure if it respects standards but it works.
Fair enough. I know it used to work in previous version but I'm fine with this implementation. And even better it will be consistent with Kibana plugin manager which also checks that `://` exists.
should we check here that the totalShardWeight is not negative. I was just thinking if somebody uses the number of docs per shard and we overflow? I really wonder if we should put some upperbound into the setting to ensure folk don't go crazy? They should use some log scale rather than actual numbers? maybe we use `1<<16` as the upper limit for now? and move totalShardWeight to a long and use doubles elsewhere? I really just wanna protect us form going negative :)
I'm OK with floats here, eg the jump from 1 to 2 is quite big, I can imagine people wanting to use `1.5` here
Nothing to do with this PR, but I don't like that we have this `NumberFieldTypeTests` which randomly chooses a type to test. We should be testing the mapper/fieldtype for every numeric type on every test run...
alread -> already
Maybe call this `getNativeScripts()`? I think that makes it more clear what is returned, vs just following exactly the fact that native scripts are implemented through a factory.
Too bad we don't know if this task should be persisted in the first place. That would have simplified the whole thing to start with.
also static method `matches` in Pattern is no good. otherwise it all seems fine.
Alternatively I think we should whitelist entire `java.util.regex` api properly with all exceptions, interfaces, etc. Just exclude Pattern.compile! Then tests can simply do stuff like: ``` assertEquals(Pattern.MULTILINE, exec("/foo/m.flags()")); ``` The constants in that file and helper methods like `split` seem useful. Also `pattern()` seems really useful for tests, especially if we want to test nuances of escaping. I can do it if you want, i know the whitelisting stuff is not the most fun...
I think the most direct tests are preferable for unit tests. If we want them in the docs as examples lets put them there and have them tested there instead. As far as Pattern, it would be really beneficial. whitelisting it should improve things as it would allow stuff like: ``` for (def foo : /foo/.split(xxx)) { ... ``` as well as hooks into function stuff like Predicate/Streams.
we have `TestThreadPool` that makes it simpler
if you catch IR you have to reset the interrupt status otherwise we keep on blocking
use `ThreadPool#terminate` here otherwise you will get lingering threads etc.
maybe just implement `Iterable<IndexShard>` here and inlcude the primary... would make the streams below simpler and we can simply write: ``` Java for (IndexShard s : this) { s.refresh() | s.flush(); } ```
can you add the `@Override` back? (here and in all the other subclasses of `Discovery`)
don't prettyprint please we don't test this here
same here don't do the pretty printing please
maybe tell the engine of a global checkpoint and see that it is persisted? now we don't test this as global checkpoint stays the same.
maybe just call it `createBigArrays`? also make the method protected.
Maybe "you should use time based indexes or cron a delete-by-query with a range query on a timestamp field"? Or something that mentions time based indexes....
I usually use `Map<String, Object>` though I suppose it doesn't matter.
Sneaky tests, testing all the things!
These two indexes are the same now? That seems wrong....
I see now. `idx` gets some docs which automatically sets up some mappings but `idx_unmapped` is empty.
use assertBusy instead.
I wonder if we should just let the default be 1 instead of confusing the user with some sort of QUORUM semantics that does not exist in our replication model.
no need to pass ThreadContext here. Just do `threadPool.getThreadContext()` when you create the observer.
The time span sampled here is from before we submit cluster state update until acking completed. There are two timeouts however that come into play when submitting cluster state updates, ackTimeout and masterNodeTimeout. I would make things simpler here and proceed in the same way that we do in other places: just do waitOnShards() with masterNodeTimeout.
There is a superclass ReplicationRequest where we should put these methods (as they apply to all the replication requests).
I think we should add ThreadPool as dependency to MetaDataCreateIndexService instead.
Why is it not possible to specify 0? I might want to create an index without waiting for any shard of that index to be active.
ok, I see now that we create one monitor per index creation. Is that necessary? We could just have a single IndexCreationMonitor instance as the class is pretty stateless.
I wonder if we should put this into MetaDataCreateIndexService instead (so we don't need to duplicate it into the two transport actions).
we should clearly state that these are the shard of the new index
can also expose the index level ack, or neither? this is confusing imo.
we should soften the language here. We can return before these are active (with a time out flag))
and.. looking at the parsing logic this is indeed internal and we don't accept none from strings. Sorry for the noise.
@clintongormley mentioned that NONE doesn't have many external usages (we only use it for index auto creation) so we might want to drop the special naming and use `0`. I will keep the object reuse in parsing.
ok. I see the parsing code now reuses those object instances in it's parsing logic. I personally think this is overly engineered but not enough to ask you to change it if you prefer it this way. Up to you and @ywelsch then.
This method could take an IndexMetaData object as parameter instead. This would let us get rid of exceeds method as well.
exception messages should start with lowercase (for consistency)
I think this check comes too early. Templates have not been applied yet. I suggest doing this once IndexMetadata has been created.
never mind, I saw them later on
ok sounds good. maybe add the `case 1:` then.
we should check here that acked == true but shardAcked == false
This is covered by assertAcked? (same for other occurrences in this class)
I you decide to go this route you should also remember to replace the reference equality checks (`this == ActiveShardCount.NONE`) by equals checks or by looking at value (`this.value == 0`).
remove "or timing out".
maybe call this `onFailure`
we "special case" NONE here but not ONE, maybe it's simpler just to remove this method as well as the `validateValue` one and use `new ActiveShardCount(...)` in the two places it's currently used (and also ad ``` if (value < -2) { throw new IllegalArgumentException(...) } ``` to the constructor.
ensure green has the nice reporting if we fail to get to green. Can we have an equivalent utility method that takes a create index response and check for shard acked, and log things? maybe a variant of `assertAcked`
never mind, I saw them later on
make these parmaters to this method? then it doesn't really need to know about index creation requests.
I prefer to encapsulate this in a class instead of floating magic values around (-1 and -2) making it hard to ensure they are properly used everywhere.
I get that, but given that this going to be used in the indexing logic as well (we should share the code) - I rather avoid the boxing. We can add request level/serialization validation that the values are proper.
let's assume that if the method parameters are not marked as @Nullable that they are non-null. Otherwise we clutter the codebase everywhere with these checks.
I would give this method the same name as the method above: `enoughShardsActive`. One operates at index, the other at the shard level. To make things even more explicit, the above method could have parameters (IndexRoutingTable, IndexMetaData) and this one (IndexShardRoutingTable, IndexMetaData). This would also leave it up to the caller to deal with `indexMetaData == null`.
half the shards? please update
no need to set timeout, the default is good enough
I would prefer that we don't use a threadpool in this class, it's supposed to be side-effect free. I prefer to set request.waitForActiveShards() to NONE and make a shortcut in MetaDataCreateIndexService for NONE.
I think if you are going to go to the trouble to call `requireNonNull` you should have an error message. Either we expect it to throw and NPE in the parser or stick a message on the exception.
this whole block here looks pretty much the same in all invocations. Can we make this even simpler? Maybe create a method `createIndexAndWaitForActiveShards` in `MetaDataCreateIndexService`. I've implemented it here: https://github.com/ywelsch/elasticsearch/commit/6e67ecabbfa5cc2568c0c987401e3ea521c7a330
remove this line and use `updateRequest.index()` instead below.
active shards cannot be started. They are started by definition of being active ;-)
to make this simpler, I think we should add a method `addCustomFields` to the `AcknowledgedResponse` object instead and just call `response.addCustomFields(builder)` in `AcknowledgedRestListener`. As a follow-up we can then make AcknowledgedResponse implement `StatusToXContent`.
why do you pass the response to this method? `this` already has all information.
> just let the default be 1 instead My rational with going with half as default is that I think that adding replicas should change the behavior - if someone runs with 6 copies , it's probably not a good default to let of them (but one) go away before signalling alarm. I chose the word "half" in order to avoid a loaded word like "quorum" which implies stuff that aren't part of our model (i.e., quorum reads). I don't mind if we round up (i.e., `(size() + 1) / 2`) or down (i.e. `size()/2` ) as long as it's not `size()/2 + 1` .
> 6 shard copies? That's rather useless in a system like ES We do have people using more then 3 shards so that lead to the idea of having the default scale with it. Thinking about it more I think the main usage for having so many copies is auto-expand-replicas-like usages, where you want to have shard copies available on all active nodes. In those case I think you mostly care about the data being on everything thats up and not be bound by durability guarantees. In that case I would be fine with waitForActiveShards default to 1 and allow people to set things differently on the index level if the want different default (when we do that change). > This setting is of limited use anyhow as it does not provide the guarantee that most users are after Correct - this setting is meant to be used to limit the scope of events that will be indexed into less than a given number of shards. It should be coupled with a check of the response of each write operation.
These tests will now, right? Because of the empty on_failure processors? I think we should expect an exception here instead.
We have `readStringArray` in the reader so maybe we should do `new ArrayList<>(in.readStringArray());`. Now is the time to break wire compatibility if we ever do.
I see it now. Sorry!
I think you can drop the null check. It returns an empty array instead.
While you are here can you move this constructor (the reading one) below all the other constructors? That way it can be _right_ next to `writeTo`. It is nice to be able to fit reading and writing onto the same screen.
Oh. You need to explicitly close the op.
it seems that the latch is useless? or maybe you wanted to enable the recovery half way through the "indexing operations"? Also, this suffers from the same racing condition - if the ops that are now in flight when the relocation comes in are not first in the onLockAcquiredActions list, we will have a deadlock.
If you prefer, in the indexing method we can do tryAcquire first out of lock and go under lock and try again, if failed.
I think this can move to before the threads start - will be nastier
I'm fine with removing it - it's useless . I'm ok with just checking it further after the PR is done.
++ on removing this catch. Not true any more
forcing execution should be a parameter for now imo - I know we want/maybe/potentially change how we deal with replicas and queues, but for now I rather not change semantics and have primary ops non-forced and replicas ops forced.
++, one less releasable to remember to free.
form -> from
it's a shame we have to wrap the call back here, but I don't see how to simplify this without doing something not nice instead, like making the PrimaryShardRefernce having a non-final releasable field...
should be ThreadedActionListener<>
I this this can simplified even further : https://gist.github.com/bleskes/0bf520c969eaa9542b1deefb4a8e1d5a (also note the test fixes, which are unrelated)
executes already checks for cancel. No need.
I wonder if we should move it up above the IllegalIndexShardStateException, as we don't want to retry in this case (although it's not that bad).
I wonder if we want to add an `@After` rule that checks that all semaphore permits are back.
this causes a deadlock when the operations that are blocked by the block are first in the future list while the last part of the future list prevents the block from being acquired. See the gist I made.
maybe try with 30 seconds first? 60 is a lot, 30 is a lot already as well....
one `[` too much
there is a word missing in the sentence above. also triple stars.
s/will by/will be/
uhm, this line does not belong here. I wonder why this did not break any tests....
I think we can now remove this condition as the client can not be null because we throw now `new ElasticsearchException("Unable to configure Azure compute service", e);` in the CTOR
I think we should stick with calling these getters like `getCharFilters` because it is the char filters that the plugin has, they aren't "extra" in the context of this one plugin.
++ to just `get*`
All of the `*Plugin` interfaces we have added so far have used `get*`. I think we should be consistent.
Yes, that is what I meant.
This directory is optional now, so the logic should be changed? ``` java if (Files.exists(userAgentConfigDirectory)) { if (Files.isDirectory(userAgentConfigDirectory) == false) { throw new IllegalStateException( "the user agent config path [" + userAgentConfigDirectory + "] isn't a directory"); } PathMatcher pathMatcher = userAgentConfigDirectory.getFileSystem().getPathMatcher("glob:**.yaml"); try (Stream<Path> regexFiles = Files.find(userAgentConfigDirectory, 1, (path, attr) -> attr.isRegularFile() && pathMatcher.matches(path))) { Iterable<Path> iterable = regexFiles::iterator; for (Path path : iterable) { String parserName = path.getFileName().toString(); try (InputStream regexStream = Files.newInputStream(path, StandardOpenOption.READ)) { userAgentParsers.put(parserName, new UserAgentParser(parserName, regexStream, cache)); } } } } ```
Creating an `InputStreamReader` without specifying a charsets isn't allowed. Instead use this: ``` java new InputStreamReader(UserAgentProcessor.class.getResourceAsStream("/regexes.yaml"), StandardCharsets.UTF_8) ```
Maybe use `expectThrows(...)` instead? It is much cleaner and safer than try-catch blocks: ``` java ElasticsearchParseException e = expectThrows(ElasticsearchParseException.class, () -> factory.create(config)); assertThat(e.getMessage(), equalTo("[regex_file] regex file [does-not-exist.yaml] doesn't exist (has to exist at node startup)")); ```
Should `TYPE` here reference the `NAME` constant? so `private final static Type TYPE = new Type(NAME);`
I'm suggesting to just round it to the nearest `long` with `Math.round`, the output will always be between 0 and 100, and thus can be indexed as an integer.
Nit: `return` -> `returned`
No matter what precision you pick will have that problem, for example rounding `89.99` to one decimal point will round to `90`, and so on.
I'm good with one decimal point with the caveat that this endpoint really should not be being indexed.
I think we should maybe make it even simpler and add this signature: `````` Java public synchronized void verifyIndexMetadata(final NodeServicesProvider nodeServicesProvider, IndexMetaData metaData, IndexMetaData metaDataUpdate) throws IOException {``` and just call `IndexService#updateMetadata()` if it's not the same? ``````
if we do what I suggested aboce this can go away
I don't think you need to divide at all - randomDouble is between 0 and 1, I believe.
As long as we make sure plugins don't try to overwrite any of our handlers, I think we will be fine. They pass back a set, so if they decide to insert the same class twice, it's their own silliness.
I think we should just use Set? The ExtentionPoint class was added at a time we thought that would be the new plugin model. But I don't think we should use it anymore.
Can we have a small note on what this test does? link is good but description is better. Same thing with method name: `testNoRegionReturnsEmptyList` or something like that
Nit: you could use `Collections.emptyList()` here.
oh god please get rid of it!!! thanks!
maybe ${stashedKey} alone should return an object then? Does that complicate things? Calling toString makes sense when the stashed thing is part of a string, otherwise returning the object sounds better.
we end up supporting both `$stashedKey` and `foo${stashedKey}bar` ? we may want to move to the latter once all the clients runners implement this feature, to have a single way to get stashed values.
Maybe we should just do the version bump? Technically we don't need to keep disc compatibility with the alphas so something like this isn't required but I like having it for history's sake. Maybe leave a note about what can go in 6, i.e. everything but size_field_type with doc values.
There's an extraneous blank line here.
Does this really need to be generic? We certainly don't care about any of that on the consumption side.
I would have something in the name here about this being for aggs (and for the other agg related methods).
I'd probably write an `assertFieldStats` method for these three asserts just to cut down on the copy and paste-ness.
I'd remove the whole `Fields` class. They have fallen out of favor. I'd just use the string at the call site, especially if it is just used once.
Cool, I can see how this can complicate things, was just hoping that this code reuse would be a low hanging fruit.
Just a question: would it be possible to extend from `LongFieldMapper`? Would be nice to have some code reuse.
Hmm good point, I had forgotten that this class could actually be returned to the user (masked behind an API interface).
Can you add the `<String, Object>` (@nik9000 style) or `<?, ?>` (@jpountz style) to the map? Again, I don't know if you should have a default here.
If you think lots of places will use the empty definition then I don't think it should be abstract. I thought only a few script would use the empty definition though.
I'd make the string here `"junk"` or `"not interpreted"` or something. If you don't read the file carefully it looks like the script is run because that is a valid looking script.
Thanks for fixing that...
Should this be abstract? It feels weird to use it without actually configuring any scripts. I think maybe in `StoredScriptsIT` just extend it and return `emptyMap` there. That seems like a special case of using this.
Thanks, these are easier to read.
makes sense - treat whatever I wrote as a draft that can be adjusted as more metrics are added
Yeah, I think this is a good idea.
makes total sense - initial ordering was arbitrary
I think this is fine for now. While I do think expert users will want to tweak exactly which metric they use for evaluation, I guess it's ok to not make this plug-able right from the start.
This question reminds me of an interesting larger topic that I believe could be related: At least for precision IIRC one can compute a micro-averaged or macro-averaged version. Maybe it makes sense to let users decide which of the two (or both) they want? No idea how this plays out with the other quality metrics you looked at.
> fail if the annotation was unnecessary Yeah, that is very important. It would be nice to be able to annotate at the exception level. Much cleaner if not for those nasty problems. I still think we shouldn't allow the annotation on classes at all and should force them to make a static method call if they want to swallow. But I'm not so against it that I'd block this whole PR over it.
Yeah. I just don't like it! I'll live with it and maybe think of a way to make it less painful later.
Yeah. I was more thinking with my fingers.
One mindless way to solve the problem with methods that look like this is to forward them all to `Strings.toString` and just annotate that one with `@SwallowsException(reason = "We don't want a broken toString to bubble up")`. I'm not sure that is a **good** reason, but it is the reason the `toString`s all do this.
These are expected, even required when implementing `ActionListener`. Either we should create an `AbstractActionListener` that does this for us the same way we have a `AbstractRunnable` or we should add a permanent hack to allow this specific construct in `ActionListener` subclasses. The former seems like a better choice but it'd mean more work before we can get this merged.
This is another place where we should probably have a common superclass that does the forwarding to the listener.
I think allowing this on a whole class is too broad. Is there a use case I'm not thinking of? I just figure it'd almost always be better to have it on a method or constructor.
This one should probably just include the exception as the cause.
I don't know that we should fix this now, but I think failures of this test will miss the gradle reproduction steps, right? I've been thinking of pulling those into a tiny shared project without dependencies just so we don't have trouble with stuff like this but I haven't looked into it deeply enough to be sure.
It is probably better to use nanoTime here so you don't get clock skew giving you weird numbers. Not like it matters a whole lot here though.
Please do not drop stack traces; `Throwable#getMessage` is an abomination.
I think we'd do better logging the first 10 levels or something. It'd be fairly painful not to have any stack trace.
time for a base class for all these recovery requests? we can remove a lot of boilerplate in RecoveryTargetService by have the recovery resolving shared. As a different PR , of course.
I think we should make newRecoveriesHanlder be: ``` private final Function<String, Releasable> delayNewRecoveries; ``` and then all of this becomes: ``` try (Releasable ignored = delayNewRecoveries.apply("primary relocation hand-off in progress or completed for " + shardId)) { final long currentClusterStateVersion = currentClusterStateVersionSupplier.get(); logger.trace("[{}][{}] waiting on {} to have cluster state with version [{}]", indexName, shardId, request.targetNode(), currentClusterStateVersion); cancellableThreads.execute(() -> recoveryTarget.waitForClusterState(currentClusterStateVersion)); logger.trace("[{}][{}] performing relocation hand-off to {}", indexName, shardId, request.targetNode()); cancellableThreads.execute(() -> shard.relocated("to " + request.targetNode())); } ```
maybe, "the primary shard has been relocated while we copied files. This means we can't guarantee any more that any operation that was replicated during the file copy (when the target engine was not yet opened) will be present in the local translog and thus will be resent on phase 2. The reason is that operation replicated by the target primary are sent to recovery target and the local shard (old primary) concurrently, meaning it may have arrived at the recovery target before we opened the engine and is still on inlight on the local shard. Checking the relocated status here, after we opened the engine on the target, is safe because primary relocation waits for all ongoing operations to complete and be fully replicated. Therefore all future operation by the new primary are guaranteed to reach the target shard when it's engine is open. "
can we move this to ShardRecoveryContext and pass request and shard to addNewRecovery ? this way won't need all the supplier fun. Might as well rename the add method here to addNewRecovery too? or maybe `registerRecovery`? (I don't mind much about the names, just a suggestion)
maybe just : ``` final ShardRecoveryContext shardContext = ongoingRecoveries.computeIfAbsent(shard, s -> new ShardRecoveryContext()); return shardContext.addNewRecovery(() -> createRecoverySourceHandler(request, shard, shardContext)); ```
we use this implementation a lot - maybe we should make utility base class for it (different PR). a default method implementation will be tricky because of the local node. Maybe it should be a parameter to the onClusterServiceClose. Food for thought.
nit: coordinating node sends..
nit: finishes running on the node
could name this `getUUID` to be consistent with other usages of UUID in the code base
I think we could also have tests for (unless I missed it): 1. master changes while task executing (but before trying to send notification) 2. master changes while sending the response on the response node 3. master changes when task is marked as completed in cluster state but before the response is sent on the PublishingFailureListener
the suppress warnings could be right on the line of code doing the cast instead of the whole method
perhaps a different name for this listener, as it doesn't only handle failures but also successful response publishing
Ditto here with `getUUID()`
ok now i can see that it is null when removing the task, sorry for the noise
this class could be made `final`
I think a better name would be `acknowledgeResponseReceipt` or something similar
could be `final`
same for here, not sure if the full Objects.equals needs to be called
I'm wondering if we need to use `Objects.equals` here which would be quite heavy-weight on the entire CockroachTasksInProgress... in this case, for example, all we care about is if there are new task entries whose executor is the local node id... in that case, maybe we can have a method on `CockroachTasksInProgress` such as `hasNewEntriesForExecutor(localNodeId)`
I think you want to use `notVisitedTasks` here instead of `runningTasks`
I don't believe this needs to be `volatile`, as it is only assigned from `CockroachActionExecutor#startTask` once per task id
could be `final`
nit: extra empty line
its not clear to me when `listener` could ever be null here (and also below in `clusterStateProcessed`)? if it is possible, then the method should have @Nullable annotation for listener
This could return an immutable list
class could be `final`
The exception is not really used in this class? So far it's only used in the tests? If it's only used by the test then I would prefer we just match on the message of the IllegalArgumentException and not introduce this exception.
If I see this correctly, the source still appears in pending cluster state tasks, which is why it should still contain the node name, i.e. `"zen-disco-node-left(" + node + ")"`
that awfully sounds like two voices for debug.... your turn, @jasontedor.
yeah, my guess is (long ago :)) that the idea was to reduce unneeded noise - this whole node leave business is to prevent ugly messages like "pinging failed to this node" and have a cleaner shutdown experience. So if a node sent the leave request to a master but got it wrong, it's not a big deal. I'm fine with going either way, error or debug but I tend towards the later (debug)
right! oh the confusion :D
but `electMasterService` is passed to the ZenDiscovery constructor? I don't follow, sorry.
I think that the idea behind that was not to log on error level when node was no longer master (as node leave don't matter in that case anymore).
final, not volatile...
Maybe just rename the method to `snapshotShard` or something. I know it takes the IndexShard, that that really helps.
Is this implementation essentially unchanged when moved? I mean, modulo the obvious stuff.
Forget the two interfaces idea. Just renaming the methods'd be good enough for me.
It'd be more helpful to me if these notes were on the method if we're going to mix them together like this. Another option would be to have two interfaces implemented by one class. I'm not sure that helps at all.
I have no idea how we get these weirdly aligned lines....
Same deal with `.get()`. I try to only do this when I edit the line so it doesn't blow up the diffs, but this is a good opportunity to do it here I think.
Can you `s/.execute().get()/.get()/`? I try to do that if I'm going to be touching the line.
sure sounds good. I thought the enum contained all the possible codes :)
not terribly sure about this. we could still recover the right status code although there was a problem reading the body... not sure which exception should win :)
shall we make some of these protected? sounds quite wrong to be using this exception without providing a status. Would be nice to remove some of these variants but I guess they are needed by EsSecurityException.
some adjustment is needed here once you merged master in
yes I am working on that
@jaymode what do you think? I brought it up but I don't have a good answer for this.
I don't think `4xx -> 400` and `5xx -> 500` is good enough. Sure we are against building new exceptions but then we are ok with losing info about errors here? I personally think we should make this work properly, not sure what others think.
> Would using `ExceptionsHelper#convertToElastic(...)` helper method in `ConfigurationUtils#newConfigurationException(...)` or similar here be sufficient? +1
@jasontedor Would using `ExceptionsHelper#convertToElastic(...)` helper method in `ConfigurationUtils#newConfigurationException(...)` or similar here be sufficient? I think the excepting being thrown if the mustache template couldn't be compiled is a `MustacheException`, which gets wrapped by `GeneralScriptException` in `ScriptService#compileInternal(...)`, which is what also should be fixed. Instead the lang-mustache should catch `MustacheException` exceptions with `ScriptException`.
We can't safely say that all such exceptions will extend `ElasticsearchException` (e.g., a bad `NullPointerException`), but I like your idea of wrapping the ones that do not extend (as long as it's not wrapping it in an exception that sounds like the user can do something about it).
> why would the user not be able to correct these exceptions? Because I'm specifically referring to exceptions being thrown because of bugs in the compilation explicitly not due to the user's template.
Please don't lose the original exception. It's already difficult enough to debug script exceptions without them being swallowed.
There's a stray semicolon here.
Nice, I like the randomization on the thread pool.
A more meaningful name would be nice here too, for example `TestRequestHandler` is okay.
Nit: Can we give this a more meaningful name instead of an abbreviation? I'm fine with `TestResponseHandler` for example.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
> It wouldn't prevent misconfigured nodes from joining the cluster. We can still check that the `cluster.name` key is present in the settings object, which means that it has been explicitly set, instead of letting the default override things. That would prevent unconfigured nodes from joining the cluster
I think that'd be a little dangerous. I mean, not as dangerous now that we only do unicast discovery, but still a little silly. It wouldn't prevent misconfigured nodes from joining the cluster.
Can we somehow factor out these bytes w/ e.g. `Checkpoint.java` so that if we change the header of the checkpoint / translog files, this tool is fixed too? Otherwise if we fail to do this, a user could run this tool and it makes a corrupt translog instead!
I think you should fsync after writing these bytes, e.g. `out.getChannel().force(true)`, like the translog code does? Would be horribly unlucky if the user ran this tool then their box crashed and it left the translog corrupt.
> At the same time though, acquiring the write lock would be good, because even though there is a warning that this should not be run when ES is running, trying the lock seems like it would be a good idea Definitely, +1
> If there are multiple commits, what does IndexWriter.getCommitData() return? I am guessing it reads the "latest" commit's data? Yes, the latest.
It won't always be the case that there will be one index commit, sequence numbers will change this assumption.
Instead of using `IndexWriter` here, you could use `DirectoryReader.listCommits` (there should be at most `IndexCommit` returned) and then call `IndexCommit.getUserData()` instead. Seems safer since `DirectoryReader` cannot do any writing on the index, doesn't acquire the write lock, etc.
The language in this sentence isn't clear, perhaps change "that is replacing" to "and it is replacing"
Can you move all that code to AwsEc2ServiceImpl.getEc2Attributes instead? Potentially we could unit test this method.
Could you move the unchecked stuff to a smaller method? Like just make a little method that does the unchecked work so this whole thing isn't unchecked.
Thanks. I'll take the yellow squiggles until we remove guice.
It feels weird that this is empty.
right, that makes sense. so if we deprecate a query we have to manually create its `ParseField` and register it through the other method by providing the `ParseField` instance. That doesn't happen often though. Still wondering if these two register methods are helpful or confusing. I am on the fence :) up to you.
can't we simply return `SearchPluginSpec` instead of having yet another class for this? Maybe call it `SearchExtensionSpec` instead
what is the difference between MatchQueryBuilder and the others? Why are we keeping the ParseField constant there but dropping it for a bunch of other queries? Trying to understand if it makes sense to have two ways to register queries.
Although this change is correct it is not necessary. An inline script with no parameters can have two forms: ``` "script": "scirpt contents" ``` or: ``` "script": { "inline": "script contents" } ```
This part I like since it is growing based on the number of aggregator instances, which is not accounted today.
It is also fine to use this 1024 constant, even if overestimated. Aggregations that hurt us most are those that create many aggregator objects so it sends the right incentive.
I'm a bit torn on this. I like the similarity to the other Aggregations and this does also leave it open for pipeline aggregators to have multiple result readers if they need to, but on the other hand at the moment it's not currently needed. I'd say I'm slightly leaning towards keeping it like this but I am happy with either way so coders choice. 
I think if you collapse `readIndexGen` into `getRepositoryData` you will be able to remove otherwise unnecessary isLegacyFormat flag from RepositoryData.
I have a feeling that this might cause some issues. We don't control the underlying storage, which might change on us without our knowledge. I would like to run a couple of scenarios by you when you have a chance to see if we should/can make it more robust.
Does it need to be Writeable? It looks like we only serialize it using JSON.
I was confused by this name for quite a bit, but I cannot come up with something that would be concise and at the same time describe this class better, which made me think that maybe we actually need two maps - one from name to IndexId and another from name to List of snapshots. I think we should brainstorm on this when you have time.
That seems redundant since we only call this from snapshot initialization.
Might be good idea to update folder name on this level as well to reflect the fact that we are using uuids now instead of actual index names.
I think this needs a bit more explanation, as well as what null means. Honestly it'd be best if we could avoid it being part of script altogether because most scripts don't have a content type.
Needs explanation here too because it isn't obvious what this means to most scripts.
well well if you were an external contributor I would have run the whole suite that takes 45 minutes. But in your case, trust won. I should have totally counted the characters of that line as part of the review. P.S. we java folks are the first ones pushing without running tests at times (WAT?), so no biggie. I will check closer next time.
This has a line-length violation. I pushed 575fa4e00a8be31a54859adf06f39c7280691040.
I think if you don't have the Java build stuff setup you should make javana do the merge  On Jul 14, 2016 8:21 PM, "Honza Krl" notifications@github.com wrote: > In > test/framework/src/main/java/org/elasticsearch/test/rest/support/Features.java > https://github.com/elastic/elasticsearch/pull/19436#discussion_r70905675 > : > > > @@ -34,7 +34,7 @@ > > */ > > public final class Features { > > - private static final List<String> SUPPORTED = Arrays.asList("stash_in_path", "groovy_scripting", "headers", "embedded_stash_key"); > > - private static final List<String> SUPPORTED = Arrays.asList("stash_in_path", "groovy_scripting", "headers", "embedded_stash_key", "yaml"); > > Thank you @jasontedor https://github.com/jasontedor, I don't have my > env setup for java so I missed this. > >  > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub > https://github.com/elastic/elasticsearch/pull/19436/files/d53406b8d3919c5367c1bb574fd365fb2af7110e#r70905675, > or mute the thread > https://github.com/notifications/unsubscribe-auth/AANLotcG0AVdYcNe3YtwU1U545rSEKT2ks5qVtJ2gaJpZM4JMfjQ > .
> We should never rely on the ordinal for any enum anywhere We've been relying on ordinals for serialization for a while, asserting that the ordinals do not change in the tests.
You don't have to be sorry. I'm just saying we do it all the time. If we shouldn't do it we shouldn't do it.
I'd replace this with `get()`.
Maybe change this to "waiting for replicas to be assigned" or something.
this is really the job of TRA to test this? it's an index meta data thing. Whis is it here? I think testing here should be very minimal and just check that the TRA funnel index level setting to a request object _IF_ the request has it's method set to default. All the rest of the logic is a `ReplicationOperation` thing.
can we inline this - it is ionly used once
we need to remove this from onGoingRecoveries and only call cancel if it was found, otherwise we may leave a lingering recovery id, pointing at a cancelled recovery.
nit: now that we folded the close and performRecoveryRestart into `status.resetRecovery()` we don't need the success pattern anymore. This can be: ``` if (onGoingRecoveries.replace(id, status, resetRecovery) == false) { resetRecovery.cancel("replace failed"); throw new IllegalStateException("failed to replace recovery target"); } ```
should be `'norms': False`
should be `'norms': True`
ensureSearchable is calling ensureGreen, so for that to be the same as waiting for shards on index creation, on the prepareCreate("index") call, we would have to setWaitForActiveShards(ActiveShardCount.ALL)
The method name implies yellow though. I bet there are places where we don't _need_ green.
I see what the difference is now but I think the name needs to be changed here. Can we called this method `addPipelineAggregatorReader` or something like that since this is actually for registering the serialisation method for the PipelineAggregator itself not for the result. It's not analogous to the InternalAggregation. Likewise the `addBucketReader` method should be renamed to `addResultReader` since this is not about serialising a bucket but about serialising an InternalAggregation the same as in the metric and bucket aggregations
Actually we probably don't need a method to add the reader for the pipeline aggregator as there should be just one reader for each pipeline aggregation type so we could just pass it into the PipelineAggregationSpec constructor
let's save a few lambdas: `context::nowInMillis`
maybe add assertion here for `(min == null) != (minAsStr == null);` and same for`max`.
Is this method used anywhere? `assertVersionSerializable` below operates on `Streamable`.
wow, just wow
I wonder if we should introduce a method to InputStream/OutputStream to read maps. We have similar methods for lists. Something like: ``` <K, V> Map<K, V> readMap(Reader<K> keyReader, Reader<V> valueReader) ``` I can think of about a dozen places we could use it.
Pre-size the buffer? ``` java final String index = getIndex(); final String type = getType(); final String id = getId(); final StringBuilder location = new StringBuilder(3 + index.length() + type.length() + id.length() + (routing == null) ? 0 : (9 + routing.length())); ```
Can you just break these all onto one line per? This was also changed last week and if that was done then, the change set would just be the additional line.
would it be possible to eagerly initialize the clients and make them final? I don't see why not, but maybe I am missing something.
not sure, but should we make the location available to java api users too? Transport client is still a thing in 5.x and this way one has to build the location by passing in the routing value. Should the location rather be a field in the response object? Not sure though as it becomes a header in the final rest response. maybe it's ok this way.
+1, now I see what you're saying (I read that section of the spec. the same way).
perfect thanks. sorry for the confusion.
was the answer yes? :)
no worries, this one seems indeed a good candidate to be tested via unit test :)
alright I can't see a reason why one would have the two clients using different protocols. Then reading from settings becomes overkill. I am sorry, I think I'd go back to the method that you had before then. It was a good one, I just needed to see it gone to realize that :)
right, I forgot about the skip part. then we also end up trying to validate that there's only one version, otherwise skip won't quite work. if we really want to run this thing against a multi-versioned cluster, we should rather take the lower version and lose the validation. But for now this is ok as-is.
I guess you would have to carry the routing around in the response, serialize it etc. maybe it is not worth the trouble.
great! thanks @dadoonet
This means the size of the token would always be `end - start` too.
can you just call spliStringToSet with true? why do we need another variant when the method this calls is public
My understanding was sort of the opposite: use it in places where it doesn't not make sense. Whenever you don't care, use it. At least, that is what seemed right to me. This is fine though.
I think it'd be nice to throw an UnsupportedOperationException here just to catch any weird usage quickly.
Note for anyone else: this is just copied from SuggestSearchTests which was removed below. It looks right to me.
Moving this to `SearchTemplateIT` means it'll run while the REST test cluster is running because it is in a module. We usually don't do that for subclasses `ESIntegTestCase`.
Makes sense to me. I only point it out in case you hadn't noticed it and it could save you any code.
Any reason not to just call `indexRandom(true, youstuff)`? Are you sure you want `ensureGreen`? Now that creating an index waits for all primaries to be assigned stuff like this should be the exception rather than the rule.
that `!` fucked me up can we have `== false`? 
I don't think we need to copy headers here, we do it in `RestController#dispatchRequest` in a controlled way after we stash the context so I think this is obsolete
should this be final and maybe called `isScroll` or `isScrollRequest`
please assign suggest.filter(CompletionSuggestion.class) to a local var so we can reuse it below when we iterate it a second time.
I also wonder if we should move this part in a different method such that this method doesn't get too big to be inlined if needed but I guess it's not hot enough anyhow.
can this just be an `Iterable<ScoreDoc>` I think it's only used as a such so we don't need to make a copy of it.
this code and the code in `SearchPhaseController#sortDocs` is almost identical. I think the only difference is the assignment of the shard ID. Maybe we can factor this out into a static method and use it in both places. It would be good to redcue the duplication on such a level and it would increase the test coverage. I would be in favor of that.
the generic here are aweful. I regret it so much that I added them. We should really clean this up it makes stuff so complicated for no good reason
can we rename this to `sortedShardDocs`
maybe we can factor out a method `boolean hasHits(QuerySearchResult result)` it's used in two places an a complex condition
if so, I think we should have a signature like: `public ScoreDoc[] getLastEmittedDocPerShard(ScoreDocs[] sortedShardList, int numShards, int offset, int length) {` it's more flexible I think
can we implement this in a non-functional way? I have a hard time to understand that
hmm is that true? i wonder if this is equivalent to a match all? I could be very wrong.
ah nevermind I see you do the `hasScoreDocs` thing
this only works if we have entries? shall we check? Also can we just do a for loop here instead
maybe call this `setHit`
> For the record, the JVM code itself seems to be using AssertionError for impossible branches. Weird. I can live with it but it is weird.
Yeah. It can and should wait.
I think if you throw a subclass of RuntimeException here it'll get bubbled back to the user and we'll get a bug report if it shows up. AssertionError's problem is that catching it and forwarding it around properly gets us dangerously close to catching OOMs. We probably could make an exception for catching AssertionError but I'm not sure it is worth it when we can just throw a RuntimeException of some sort instead. Also it just feels weird to throw an assertion error from outside of assertions! Its icky 
can we doc the new methods here too, since they are on an interface, they represent a contract.
This is a big antipattern: here we have two methods, both named interval(), just with different signatures. One is actually a getter, whereas the other changes state! But you have no idea reading source code, what is going on, e.g. its just a method and you have to follow the source code to figure out what is happening. Please change to getInterval() and setInterval() [there are other instances of these in these files, obviously those should be fixed, too]
this new file is missing some kind of explanation about what it does...
Feel free to tell me I'm totally wrong - but do we feel confident enough that a mistake here (in code) is a good enough reason to cause the [node to restart](https://github.com/elastic/elasticsearch/pull/19272) ? I'm worried about endless restarts, where a single mistake on a specific API can cause problems on an entire node.
I think `InternalDateHistogram.NAME` would be a bit better. Now that type isn't used for serialization I'd like to remove it entirely one day.
Man this feels like a mess compared to ObjectParser. We can't do anything about it in the middle of this PR though. Just makes me sad.
Can you throw something else? It just makes me uncomfortable to throw AssertionError.
Why remove it? I was adding them because I thought it was nice to mark the constructors for anyone unfamiliar with Elasticsearch. It'd help them get their bearings.
Er, well, it doesn't work like that. Ignore.
I think you can drop the `(long)`s because these are `double`s now.
You can use mockito here if you like, I imagine.
import not needed.
imports not needed.
Same deal as the last `toString`.
I'm tempted to remove `isCreated` and just use `getOperation` everywhere. Might make more sense to do in a followup PR because it is a lot of small mechanical changes.
I see it now - I think how you've got it now is the most right thing then.
Since it is defined on the superclass it'll still be there, just not a delegates-to-superclass override.
I wonder if with this change you can remove `UpdateHelper.Operation` entirely and just use `DocWriteResult.Operation`. I'm not sure it'd be clear to use `CREATE` instead of `UPSERT` in all the places though.
Thanks for removing this.
Again, I wouldn't pull out the ternary.
I think you can remove the whole override.
Ooooh - maybe we should just delete to `getOperation().status()`? We could probably move this whole method up to the superclass then.
If you always want to use the lowercased version of the constant maybe just implement `toString` to return lowercased? I'm not sure what the right thing is here but it isn't a big deal either way.
I don't think you need to keep the `found` part of the `toString`. I'd just nuke this line.
I'd just leave the ternary operation there.
nit: missing space
I think you mean `fromOperation`? I think it'd be slightly better to have `Operation` implement `Writeable` and call this method `readFrom` and have it take the `StreamInput` instead. At least, that is how we've been doing these enums in the past few months.
These `Fields` classes have fallen out of favor in the past six months. Rather than add a new constant I'd just add a new string below, similar to what I did when I added `forced_refresh`. If it doesn't bloat the PR too much you can remove the whole `Fields` class and replace with strings at the call sites.
Same feedback as the last `toString` - I think you can remove `created` and might want to look at `Operation`'s `toString`.
I think you can just blast the entire method in this case.
you could use the `assumeTrue()` method to ignore a test if a condition is not given... not too sure if this increases readability, to be honest
Depends where you want to through the exception I guess. I think throwing it right in the listener is going to make the failures more clear but both should be fine.
I'd likely do an `AtomicBoolean` and `boolean alreadyFired = hookWasFired.getAndSet(true); assertFalse(alreadyFired);` just for extra paranoia.
Now I get it! I had to dig around a bit in `ScriptService` to understand what was up. Now it makes sense.
I think the old indentation was a bit better than this.
perhaps this check should be for versions before 5 instead, otherwise we'll need to fix this for 6.0 as well
nit: space after the second percent sign (on all these lines)
at this point you don't need the restClient variable anymore
ah right I am familiar with this, I had the same too... you could do System.out.println(client).... just kidding.
Personally I'd do this with `assertEquals` as well but you can absolutely keep this if you prefer it. We certainly don't have a standard for when hamcrest is a good idea. I expect there are folks in the Elasticsearch project who prefer hamcrest all the time so they can have consistent argument ordering. I don't, only using it for more complex assertions. But, yeah, if you like hamcrest then keep it this way.
I'd prefer `assertEquals(DocWriteResponse.Operation.INDEX, updateResponse.getOperations());`. I prefer equality here because it is a stronger test. If we have the information we may as well use it. I don't like using Hamcrest for strait equality because it can get overly complicated and I don't mind switching the argument order to match normal junit.
Same here, I think the ctor should contain the mandatory arguments and those should be immutable later. Again, this means for parsing we need ConstructingObjectParser.
I think you need to return the BufferedHttpEntity in the response for this to work. Building it here will consume the underlying entity, but you still return that entity. So it'll be empty. I'm honestly not sure why the tests pass.
I didn't know you could mutate it!
I think we catch it if it happens with the assertion no? so we are handling the null values at the source, plus we cover the case where we missed something. that should be enough.
indicesClusterStateService should not have changed, so no need to refetch from the map.
maybe you should pick a node that has actually a shard of the index assigned to it. This allows to check that the node actually had in-memory structure of the index before it was removed. ``` DiscoveryNode nodeWithShard = state.nodes().get(randomFrom(state.routingTable().index(name).shardsWithState(INITIALIZING)).currentNodeId()); ```
instead of "simulated change 1" can you provide a text that describes the actual change? e.g. "index created" or "cluster uuid changed".
just `for (IndexMetaData indexMetaData : state.metaData())`
isn't this the default? just omit in that case.
If we separate `updateNodes(state, clusterStateServiceMap);` out of `randomInitialClusterState`, there is no need to have a `clusterStateServiceMap` in this test.
or just leave it up to the caller to pass the `AllocatedIndices<? extends Shard, ? extends AllocatedIndex<? extends Shard>> indicesService` in.
maybe make this a parameter on `randomInitialClusterState` (i.e. `Supplier<MockIndicesService>`)
not sure exactly what you mean with "client". This is used both when this node connects to another node and when this nodes receives a connection from another one.
we may still leak things on exceptions. How about: ``` while (isOpen.get()) { Socket incomingSocket = serverSocket.accept(); MockChannel incomingChannel = null; try { configureSocket(incomingSocket); incomingChannel = new MockChannel(incomingSocket, localAddress, profile, workerChannels::remove); //establish a happens-before edge between closing and accepting a new connection synchronized (this) { if (isOpen.get()) { workerChannels.put(incomingChannel, Boolean.TRUE); // the channel is properly registered and will be cleared by the close code. incomingChannel = null; incomingSocket = null; // this spawns a new thread immediately, so OK under lock incomingChannel.loopRead(executor); } } } finally { IOUtils.closeWhileHandlingException(incomingSocket, incomingChannel); } } ```
Right. I figure we should have a test for it or explicitly make the min for that parameter `1`. We have other ways to turn off dynamic scripts if we want so maybe it'd be better not to have two ways to do it.
I'd mention the name of the setting that controls the limit in the error message. I can imagine needing this desperately. I'm honestly really hoping no one sees this for the first time in production, but I know some people will.
Maybe something like: The bucket can "overflow", in which case the excess capacity is just "spilled", rather than saved up. So it never holds more than a minute's capacity.
It'd be nice to have a note about why you have to handle `null` here and what null means. It looks like `null` means "this cluster state change didn't change the setting".
I think it'd be better to check before and decrement the counter after. We're in a synchronized block.
It'd be useful to describe the how here I think. I guess this works like a bucket of water slowly filling up. You get to compile a script if there is enough water in the bucket, otherwise you don't. If the bucket overflows then you don't get more water.
Er - if you are going to log something then it doesn't matter which order you do it I guess.
ah - now I see what you did it :)
this also means that we can fold setting the primary term on a request into the ReplicationOperation - we currently only assert that it was set.
can we add some trace logging here? I can imagine it will save some WTF at some point.
can the aid matching be the implementation and the rest just assertions ? it should be enough
Using 0 as a non-value (because we serialize using `out.writeVLong` ) seems brittle to me. I rather use -1 and serialize using normal `out.writeLong`. With 0 we will always be asking ourselves if it's a valid value. We can also potentially have two methods here - failReplica and failShard, where failReplica requires a valid primary term and failShard doesn't take one (but uses -1 internally) so users won't need to know about this implementation detail.
+1 to explore that option. Although, I always doubted whether we should set the primary terms on the ReplicationOperation level, I didn't do it because it wasn't needed and reluctantly opted to keep things as simple as possible and left it at that assert we have now. With the move the primary term based failures we bring the terms into scope, which makes me more inclined to also set them. As said, I'm still on the fence about it myself so I will go along with not doing it, if you feel differently.
this made me go and check that this is tested in ShardStateActionTests , which it is, but we miss tests there, like double sending the same shard before it's processed and failing a shard of an index that doesn't exist
maybe call this pendingTasks or resolvedTasks? I got a bit confused by the valid notion - some of the tasks are marked as successful but are not valid :)
same here - we need move double starting and such to ShardStateActionTests
Ok. great. Thanks for doing the research.
nit: `>=` -> `>` (equality is not an option :))
Sorry, I meant `o.e.common.regex.Regex#simpleMatch` specifically. It is more like shell globing than a regex and we use it all over the public API. So, yeah, I mean behavior consistency. Also I think `simpleMatch` is less prone to stupid backtracking issues than arbitrary regular expressions are. Which is a small plus in any API.
`thread_pools_patterns` does not match the name of the parameter registered in the RestController
I think that'd make it consistent with the other APIs.
s/same id is/same id but it is/
It's more of an offset than a number of directories
This doesn't feel right to me, adding a parameter to the bootstrap check infrastructure that is specific to a check. I think that we should try to find a different approach (I'm happy to help brainstorm about this, but I have not yet done so).
This seems wrong to me: this class is abstract and contains tests that should work with any type of repository. We should not force the repository type to `fs` but rely on the `createTestRepository(repoName)` method instead like it is done in the other test.
maybe add which type of section it was, so that it's even safer to get rid of the double exception? That is the only useful bit I find in the original stacktrace that would otherwise get lost.
ok I am fine with that, not a huge deal
yea I was looking at the stacktrace and wondering what is important in there, not sure.
My motivation is both making it so there is one obvious way to calculate distance (I was reminded recently of this beatiful mantra from the Zen of Python: `There should be one and preferably only one obvious way to do it.`). I also think not having instance methods will allow us to play more with the underlying field access so we dont need an intermediate object, GeoPoint).
I tend to like expectThrows better for doing this.
Exception e = expectThrows(Exception.class, () -> doSomething()); assertEquals(e.getMessage(), containsString("bla"));
Maybe just a single `read` method instead of `readChecksummed` and `readNonChecksummed`? They do the same thing? In the future, we'll have to also pass in the version to `read` so it can read any version-specific things ...
Oh nevermind, `CodecUtil.checkHeader` is already doing the real check. You really do not trust anyone ;)
maybe its simpler to do this: ``` Java try (FileChannel channel = factory.open(checkpointFile, options)) { IndexOutput indexOutput = new OutputStreamIndexOutput (resourceDesc, checkpointFile.toString(), java.nio.channels.Channels.newOutputStream(channel), BUFFER_SIZE); // ... } ```
oh yeah I missed that :/
Thank you for this test, much appreciated.
Awesome to see this TODO go away!
Not sure if that makes any sense at all, but somehow my inclination would be to write counter < 1 instead of counter == 0 here.
... as discussed f2f: Add to the TODO to collect errors and return only when all requests are done, I'd do the actual implementation in a separate PR though
put the message as the first arg on assertTrue so we know what it was when it fails.
maybe expectThrows would be easier.
is this really testing the right thing? This test does not seem to call `canForceAllocatePrimary` as `TestAllocateDecision` returns THROTTLE and not NO for `canAllocate`.
we can have three methods here, but all of them could just share their code by calling a common method `getNoDeciderWithForceAllocate(Decision)`
does it make sense to replace `new TestAllocateDecision(Decision.NO)` by `new TestAllocateDecision(randomBoolean() ? Decision.YES : Decision.NO)`.
I'm not sure we should be adding more references to the injector. Either we should make the `NamedWriteableRegistry` a package private member of the client or we should just serialize the thing somehow.
But why does SearchModule need to create the SearchService? I find keeping all these "modules" around post guice just keeps things confusing (hard to know where something belongs) and am leaning towards everything being in Node (yes it would mean Node will be big, but it really is, right now there are arbitrary and inconsistent lines drawn). You could move the single line binding SearchService out of SearchModule and add it to the other bindings done directly in Node.
Hrm, I actually meant just `createSearchService()`? I don't think we need tests to override the entire SearchModule...
I don't get it, it seems the test provides no guarantees whatsoever? Why not be fast then :)
Sure, but you get that also with a 5-10ms sleep. Basically I don't think we should continue to go the path of long sleep-based tests, we know this is not good...
I opened #19880 with an alternative which does not use sleeps.
Can limiting this to only one snapshot cause some race conditions on repo? The only scenario I can think of is if restore lists index files and just before reading the file it needs delete creates a new generation index file.
this should be alpha6 now that alpha5 is out
Maybe point out that this is actually the place where we modify the valid input query by adding a new object level to it.
This loop tries to insert the closing bracket at the same level as the newly inserted object, right? Also maybe add a short reminder about that in front of the loop so its a bit quicker to figure that out again later.
I wonder if those should all be sets? and must be non-null
I guess it would be good to assert that the returned delegate is not another instance of `JsonGeneratorDelegate`
to me these should be sets and required to be non-null
this makes the executable script not that simple anymore ;)
This is outside the scope of the PR, but we should look into requiring vars to be non null in the future
thanks for killing this IllegalAccessError: i added this on accident when debugging long ago!
I think we should inspect the `AssertionError` and make sure that it comes from Groovy (by inspecting the stack trace and looking for `assertFailed` and other relevant Groovy calls).
You see InvokerHelper at the very top, that is the bad guy that throws it.
Sure, but that was before we knew that we would need to add a permission for `InvokeHelper` too? So maybe we should just not add it since it will not help with getting asserts to work? Sorry.
Right, but the point is that the `InvokeHelper` is right at the top of the stack trace. I do not think we should be descending in case the top of the stack trace is from an assert failing elsewhere outside of Groovy.
I wonder if we should either check for `Function.identity()` or allow the function to return `null` for "no change needed".
Please no `null` for no change needed, returning `Function.identity` is clear, and there is no need to make an optimization check.
This is not the way to get the custom upgraders passed around (we should not be passing around PluginService). See other examples in Node: you should collect all the upgraders, and put them into a container that will then be injected (and eventually when GatewayMetaState is deguiced, we will just have it there as an arg, instead of via injection).
This can be `UnaryOperator` instead of `Function`.
I notice this pattern in every implementation. Perhaps this should be a Map instead of Collection (keyed by the custom type name)? Then the map can be copied, and keys replaced, removed, or added easily, without needing to have logic for the other custom metadata that the plugin does not care about.
I would say something like "the node should not start" instead of the part about stopping (since this is during startup, while stopping implies the node is already started).
this last line is redundant I think, we already check the same above
ignore this, sorry for the noise :)
I like the variable name changes, makes it clearer to understand the purpose of the object.
Also, note that this refresh call can come from, for example, `InternalEngine#writeIndexingBuffer` from `IndexShard#writeIndexingBuffer` which has a general catch block which will swallow the `AlreadyClosedException`!
This catch will now swallow an `AlreadyClosedException`!
I think this is an ok way to handle `BoostingQuery` given that we can't modify the boost on some terms if other terms are found but it doesn't model how it works really. I think it is worth leaving a note comparing how `BoostingQuery` actually works and how you've modeled it and why it is ok.
Maybe this should be `boost * boostingQuery.getBoost()`? I think that is closer to the way that the boosts are applied.
Believe it or not, it's fine to pass `null` to `IOUtils.closeWhileHandlingException` (it just skips them). We do that for cases where you might have N things that need closing from a large try block, and you don't know which are `null` and which are not (depends on where the exception was thrown). But for here I like the `null` check: less smelly.
We don't explicitly support upgrading between alphas. Can you test this with a 2.x version instead? I would use `VersionUtils.getPreviousVersion(Version.V_5_0_0_alpha1)`
If we publish all our plugins (including xpack) to maven central or any other maven compatible repository, then I'm fine with this change.
Because I expect the ZIP file to be in my .m2 dir so I can run mvn clean install anytime (even offline)
The previous format was super handy for maven users. GroupId/ArtifactId. I was able to download a plugin from maven then launch it within a ant script to make integration tests against a cluster (external node). I feel like it will be more difficult for maven users now.
Nevermind, I see, it was a remnant from the imports up top, just got shifted around
oh i see, the JsonStreamContext now doesn't implement `writeValue()`.
same question here if Strings.toString(XContentBuilder, true) could be used, and also for https://github.com/elastic/elasticsearch/pull/20011/files#diff-4dad2c5391932f86a402ee44b5160218R57
make this final
and this one
this can be removed now
it is a bit disappointing we have to add new methods here, are they all necessary? (might be yes, just checking)
you should pass fieldNames as an argument
oh nevermind I see there is a storedField method too
could we implement Writeable rather than Streamable? (Writeable is supposed to be a replacement for Streamable if I'm not mistaken)
I would just do ``` java public static final ParseField OPTIMIZE_BBOX_FIELD = new ParseField("optimize_bbox", "optimizeBbox").withAllDeprecated("no replacement: `optimize_bbox` became useless thanks to recent improvements") ``` and let ParseField emit deprecation warnings by itself. I don't think we need to only emit the deprecation warning for 2.2+ indices.
can you give it a name that contains `dateHistogram` to make it clear what it tests that the other tests do not test
tests should avoid using `now` as it can hurt reproducibility
When you run tests, a seed is automatically chosen and returned in case of errors. This way, you can re-run the test with this particular seed so that these random vars get exactly the same values. This would not be the case if `now` is used.
As this setting should usually be only set once, it is probably simpler to leave it non-dynamic (as @jasontedor suggested and as it was before this PR). In case where this must absolutely be updated on a production cluster, rolling restart (of master nodes) with config update is always possible.
I'm unsure if this setting should be dynamic.
this may get confusing since the feature will be allowed `today`, where `today` is some time in the future that someone will read this. maybe we can reference the PR here, and use more past-tense terms like `previously`.
is this really sufficient to describe a nested field relationship? e.g. `targetField = "abcde"` and `field = "abc"`
I see, ok
since this is done once. should we be more restrictive here to actually have a proper path format? just a `.` is pretty flexible and would allow ``` hello. .hello ```
Given the method's name I expected it to check the values too.
It'd be fine with me if it were one of the random options for `createSearchSourceBuilder`, yeah. We'd want to make sure we kept the assertion that the deserialized copy's bytes match the original's bytes which I don't think is an assertion we normally have in these round trip tests.
Sorry about these crazy incantations....
Thanks for this - it'll be a lot easier to debug failures than to look at two differing byte arrays and back into the difference.
I'd use `randomAsciiOfLength(5)` rather than fixed strings for this.
Personally I'd write this as `assertEquals(singletonMap("field", "value"), hitContext.hit().sourceAsMap());`. If you like it better the way you have it that is fine too.
Since you don't decrease this when you hit an `END_OBJECT` this isn't really `depth`. It is more like `objectIndex` or something.
I think it'd be nice to separate mutation generation code from the parse testing code. It is complex enough that it'd nice to have a test for it that asserted that it returned the mutations you expect it to return.
thank you! :)
maybe put the actual and expected length in the message
I think using a `LongAdder` would probably be more efficient than `AtomicLong`, since the value is not going to be inspected as often as its incremented
looks new. I like this update!
Ignoring that we need it for `network_types`, I kind of like that settings are now on by default.
What about the case when `request.fetchSource().fetchSource()` is false? Why do we even have a boolean there? Maybe we should just use null instead? It doesn't make sense to have a `FetchSourceContext` with `fetchSource = false` and `includes = ["something"]`.
Maybe just handle the boolean for this PR, but we should think about removing it....
Is it worth optimizing the case where `fetchSource` is true and `includes` and `excludes` are empty and the xcontents line up? Then you can just return the bytes you have like we do for search lookups.
ok. Considering `Setting` as a simple value class, it should probably have no logger at all (but that's a whole different story).
This line can just be removed
I wonder if we should throw an `IndexNotFoundException` here or just log the occurence and move on, because the intended result would be the same - the index is no longer in the cluster state. For example, it could be possible that just before someone executed an alias action with remove index, they executed a delete on the index. If we just log the fact that the index doesn't exist and move on, we would have achieved the same objective. I may be missing something though.
+1 much better than the while loops in the rest action
> There aren't any aliases to resolve if you are removing the index though. Right now it'd just NPE. I could probably hack it so it'd work but I don't think it'd be too pretty. Right, I see. > I think I should just move the switch statement up a level and move the for loop into the two arms that need it? Something like that. That could work too, but then we would end up with multiple for-loops? I don't think that would be pretty too, so maybe just leave it this way.
thanks for moving this to a unit test!
this should always be non null now
sorry for the confusion!
you know what? I thin that I was just being paranoid about the test succeeding despite the path prefix not being used. we check that it's used in the returned request line! Maybe we can just remove this method then and keep the following one, which makes much more sense.
I think I recall cases that work perfectly without the leading "/". e.g. https://github.com/elastic/elasticsearch/issues/19314
missing fail :) use expectThrows instead
if we make such change, can we do it in a separate PR please? ;)
and randomly append '/' at the end
same note as in the json processor PR.
Maybe rewrite this using `assertNotEquals`? It makes it symmetric with the previous line so that it's clearer, and it gives better failure messages from JUnit when the assertion fails.
Same suggestion here for `assertNotEquals`.
This can be outside the try/catch right? If there is a failure to create the pipeline, there is no pipeline to close.
Same question here about closing.
But that means the pipeline can never be used again (it is now closed).
And regardless of if the parsing exception is important, if closing is necessary, failure to close should be added as suppressed exceptions to the parse failure.
No, this shorthand version is still supported. We'd have to remove it if https://github.com/elastic/elasticsearch/issues/20122 goes in
Isn't this the old format we removed in 5.0? ie, not being explicit about it being an inline script.
@jdconrad You should be able to accomplish that by deprecating lang usage that is not painless in the various script using code (update, function_score, script search). If done, I'd recommend keeping the Python/JavaScript deprecation on construction warnings. Those will appear in the log at least in the event that they're used infrequently. I'd also add that to Groovy's engine constructor to get the same behavior.
I think it should be `greaterThanOrEqualTo(0)`... According to https://docs.oracle.com/javase/8/docs/jre/api/management/extension/com/sun/management/OperatingSystemMXBean.html#getTotalPhysicalMemorySize-- both `getTotalPhysicalMemorySize` and `getFreePhysicalMemorySize` should return a value.
can you update the `shouldAutoCreate` method so that it getl a local reference to the AutoCreate object as a first step and then uses this reference for checks? The motivation is to make sure all checks are done against the same instance in case there are concurrent updates of the auto_create setting
Maybe: ``` java Exception e = expectThrows(IllegalArgumentException.class, () -> RestAnalyzeAction.buildFromContent(content, analyzeRequest, new ParseFieldMatcher(Settings.EMPTY))); assertThat(e.getMessage(), startsWith("Unknown parameter [token_filter]")); ``` That way you don't have to assert the type of the exception.
I usually use a ternary for these sorts of random values. I don't usually like ternaries but for things like this I figure it is still pretty readable.
that retry sounds like a bug. looking at the places where we throw `IllegalStateException` we should never retry. I vote for changing that (and only retry on timeouts + unknown exceptions (with a warning))
let's remove fuzzyMinSim entirely from this class, it looks unused
I think it might be worth dropping the `timeUnit` for now? I'm just scared of it.
I'm not sure this is ever the right thing to do though - it'll spit out only the user readable version. Also I think `field(String, Object)` will need so work so it forwards to your new implementations. I think `value(Object)` will need some work as well. I think maybe it is right to just disallow `TimeValue` there. Make it only ok to use `TimeValue` with `field` variants.
Allowing this to be customizable is nice but it is going to break parsing pretty hard if this doesn't stay MILLISECONDS.
Yeah - it'd be super nice to be able to set the time unit on the URL and get it back - I'm quite happy at the idea. I'm just worried about the times when we persist stuff in XContent form and it lives for a while. We'd have to expand the places where we parse quite a bit. I think we could to it fairly easily for ObjectParser and friends but all the old places where we parse by hand are going to be a pain. They'll work now just because nothing ever changes this from millis. That is why I propose moving it out of this - if you just did the names cleanup that'd be a good thing but supporting more units is going to be a bigger change.
this key is different from the one specified above in https://github.com/elastic/elasticsearch/pull/20310/files#diff-9a1a4781a78e7e7a89657e8717f88997R61
you are the man! that is awesome!!! that should just work. I really wonder if we can build a BWC test index with a percolator that ensures we can read this stuff if would be awesome to have asuch a test
Hmm if info is null (`fieldName` is unknown) should we throw an exception here? `DocValues.getSortedNumeric` will just return an empty doc values.
update version to beta 1
update version to beta1
this must be updated to onOrBefore
No need to be fancy here, just: `logger.debug("got exception", throwable);`.
Nit: `reject` -> `rejected`
Nit: `reject` -> `rejected`
`String.format(Locale.ROOT, "%s operation term [%d] is too old (current [%d])", shardId, term, primaryTerm)`
nit: can we move this under the if? it's confusing now as we add the id but don't increment indexCounter
I'm guessing it made sense when read with some backwards compatibility code we've since removed? I don't think it applies any more so I'd remove it so no one else is confused by it.
I'd also appreciate some tests here for these methods and also for the settings that are supposed to accept percentages
++ to keep byteSizeSetting here
I don't think so, I think these should be bytes or size-value only.
should we say something about upgrades in the message? I suspect people with "uninstall" in their head will have a WTF moment. Probably good to say "delete manually if not needed".
same here just use synchronized methods
`Matchers.equalTo` or `equalTo` with a static import is more normal for this sort of thing I think.
While you are cleaning up maybe we should put these into their own class that `XContentBuilder` extends? That way all of the wrapper methods are all in one spot.
I guess we're doing `instanceof` not `getClass()`, but still, it sounds like maybe we should in some cases.
Yeah. I'm not sure either. I like that concrete classes are nice and quick. Lets drop this point for this PR at least.
When it gets long like this can you indent it like ``` assertResult(() -> builder() .startObject() .startObject("foo") .startObject("bar") .endObject() .endObject() .endObject(), ``` I know we `assertThat` has the matcher second, but maybe we should put the closure second for this? I think it is nice when the closure is second because it makes the code formatting prettier.
Can you assert something about the message? As it stands someone could break something to throw a **different** `IllegalArgumentException` and we'd not notice.
`assertEquals` and `assertSame` for these? I like to avoid hamcrest if it doesn't buy anything beyond "normal" junit asserts.
These asserts are totally fine (and helpful!), but I think as a future direction, it would be nice if we encoded the invariants about a decision into the data structures themselves, so we make it harder to create an illegal state. It's totally not work for this PR, only an idea for future direction :)
Perhaps annotate this one with `@Nullable` since it's the only one that can be null here
Question, do you think it would be helpful to copy the pattern we have elsewhere having a `*Safe` version of the functions? So something like: ``` java public Decision getDecisionSafe() { if (isDecisionTaken() == false) { throw new IllegalArgumentException("decision must have been taken in order to return decision"); } return decision; } ```
Additionally, I like calling these `getFinalDecision` in ClusterAllocationExplanation.java because it differentiated it from the node decisions, how do you feel about that? (It would also probably change `getExplanation()` to `getFinalExplanation()`)
Nit: `currentThreadNamme` -> `currentThreadName`
I think this can have an impact no? Even if we don't expect clusters with mixed alpha/beta/rc/ga versions of 5.0
Just a thought for the future (this is fine for now): so these arg lists don't get too large, maybe we could pass version through and let the index method have conditions based on that, like we do when generating mappings.
I think this is missing from hashCode/Equals as well
script seems to be optional, I get NPEs in some roundtrip tests for this.
this can blow up when there was no script writen to the StreamInput
nit: writeOptionalWriteable() would be a bit shorter
nit: in.readOptionalWriteable() would be a bit shorter
I recently used the Script.parse methof in the same way when refactoring the ScriptSortParser, Nik encouraged be to add a static 'parse' method to the Script class that does the wrapping of the potential IOException. This can most likely also be used here.
This logging should be removed.
nit: naming this "template" instead of script (although the argument is technically a Script) could communicate the usage of this setting better
I think we should turn this code in a util method on ScriptService in core module. There are now several places where we have code similar to this. This would fix the code duplication. Something like: ``` java public SearchSourceBuilder templateSearchRequest(Script script, QueryParseContext context) { .... } ```
asked f2f, we can probably delete logging at this point.
Can empty list be a valid value here so we can avoid the boolean? Not needed as part of this PR but it'd be nice.
Do you think we could get away with making this an assertion instead? A plugin developer would hit it during testing.
The indentation is off here and the rest of the way through this test.
This should say `higher cluster state version` instead of `higher id`.
Nit: `who's the a better` -> `which is the better`
The return value does get modified by a caller in `ZenDiscovery#findMaster`; I think there is risk of a modification while the caller is copying, so it's better to do the copying under the synchronized lock.
Nit: `cs version` -> `cluster_state_version`, please.
`that the array` -> `that the collection`
Nit: `candidate` -> `candidates`
can you please add the `clusterStateVersion` to the assert message
s/to an array/to a collection/
This can just be a plain old logging statement `logger.debug("[discovery-file] using dynamic discovery nodes {}", discoNodes);`.
Same concern regarding the leniency.
Same concern regarding the leniency.
can we not short cut return, but rather set the hostList to empty and continue the same execution path (with the same logging)
I really don't think we need this leniency, I'd like to understand why we're introducing it. I think we should just blow up the pings.
I guess you could make this a final class that takes a functional interface as its only parameter and runs it and you could still use lambdas for it. My instincts are that that is marginally better from a design standpoint (composition vs inheritance) and a little easier to read but I'm not sure.
Maybe mention that this amounts to a volatile read.
Can you remove the empty javdoc? We are going to fail the build on those at some point....
Is this any quicker if you use bulks? I tend to do that out of habit.
Probably worth asserting that they return success. I've never seen them not do so but if they ever start then this test won't have many segments and will fail in strange ways to anyone trying to debug it.
Does this happen because the scroll doesn't hit docs in that segment so it can't be blocked? Should canceling a scroll request nuke the scroll id? If you tried to pump it again would it be busted? Maybe something for a followup though.
I find that `equalTo` is almost always a bad choice. In this case I think `assertThat(cancelTaskResponse.getTask(), hasSize(1));` will do the same thing but have much better error reporting. That way you get to see all the tasks when there are too many. Same for the above assertion.
It'd be cool to be able to list the phase and/or which shards you are waiting for. You could put all kinds of cool stuff in here one day! But for now this seems like the right thing to do.
Maybe `createContext` should take the `SearchTask` as an argument? That'd make it very difficult to forget to set it.
should we be consistent with the number of `l`? there is only one here while you put 2 in `cancellation`? (feel free to ignore, it could easily a special case in english that I don't know about!)
I'm not a fan of the name (but I don't have better ideas) but this looks good to me.
nit: I'd prefer that we move the sendRequest call to the try block and remove the return from the catch block
It is worth benchmarking this because it adds a volatile read. If it is too heavy we can make the check less frequent I imagine.
This one is upper case and the one below is lower case.
And here is the clear. Cool.
Oooh! Yeah! That makes sense - scrolls contexts will outlast that task that spawns them and need to get a new task. I haven't read the whole PR yet, but I wonder if we should clear task from the context between requests? If we don't then we maintain a reference to it after the task manager and that seems a bit weird. Changes to it wouldn't be useful to anyone.
construction contractor? :)
Something like "This class assumes that the supplier is fast, with performance on the order of a volatile read." would give a lot of context to the decisions around how to use the Supplier.
Then could you do something like: ``` @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE) // Changes settings ``` That'd make it clear why it is needed so folks don't go copying it into their tests without knowing why... 
try finally here please since if close fails we don't release lock etc which can be missleading
`throw new AssertionError(e);` to get full stacktraces etc
I think it must be abstract if it's named `*TestCase`
well we use a dummy lock so I guess it's fine
extra new line
Great to see mutations added here as well.
Thanks for fixing these docs along the way.
I think after closing #19917 we can remove this todo
Nit: `OP` -> `Op`
This is exactly what I had envisioned. Thank you!
I think it will be cleaner? not a biggy though
> Why do you think that? Does having a hard reference to an object guarantees all weak references to it are kept around? I was worried the GC might decide to remove a weak reference just because, in which case we will create another marker. Again - not saying this is wrong, but looking to learn.
can this be `PARSER.declareBoolean(SearchTemplateRequest:: setExplain);` instead
can this be `PARSER.declareBoolean(SearchTemplateRequest:: setProfile);` instead
You probably meant geoData.isEmpty() == false
long live java 8
thanks @nik9000 ! @elastic/es-clients is this ok? I guess all the client runners will have to be changed accordingly.
Fine with me :) I'm already wiping the repository itself after each test, so this shouldn't have much effect (I don't think).
Nitpick: I wonder if it would be better delegating this assignment to the constructor above.
As soon as external JSON names are stable we should use the same names inside for variable names I think. Can be done in a separate PR though.
Would you kindly add some line feeds here to make it look like json instead of a wall of text? It'd be so much easier to read.
man `== false` reads so much easier :)
Nevermind, just saw that `searchSourceBuilder.indexBoost` is now randomized by default in `createSearchSourceBuilder()createSearchSourceBuilder()` so I guess thats the reason why. Correct me if I'm wrong.
Has this test moved somewhere or was it deleted for any particular reason? Just had a quick look but can't find it.
I think it is good enough to call `output.bytes().steamInput()`.
Can you use a word other than `clone`? It makes me think of the `clone` method which this isn't. I tend to use `roundTripped` or something.
`Map<String, TransportFactory<Transport>>`? Then `TransportFactory<Transport>` can be an interface. It has too many parameters for a sane person to use it like a `@FunctionalInterface` but it'd be nice not to have to override the ctor and stuff.
The nice thing about making `NetworkModule` take a list of plugins is that these register method can become implementation details/non-public/go away entirely. You'd need to make a anonymous inner class here for the plugin but it isn't a big deal I think.
Could you make the plugins a ctor argument like the other modules? Making the modules immutable makes them much easier to understand. We could do it in a followup/not at all if that is too strange for this module.
Now that we have `ESRestTestCase` you can write the test in java if you like. It is up to you if you want to do so, but I like having stuff like loops available. Yaml is nice if you want to make sure the language clients can run the tests but that really only applies to the tests in `rest-api-spec`.
Do we need this on this one? It seems like these test suites are very small and any of them taking 40 minutes is grounds for making someone look at the VM at a minimum.
I think it can fit on one line
I think it could be null instead of `"*"`
could be just `if (ancestors.add(value) == false)` when moving to a set
and remove the below null check
otherwise I would be happy to just return in the else block and remove the instanceof check in the while loop below
This should probably be an IdentityHashSet.
Fine be me.
nit: how about simply using the ordinal of the enum? we would have to be careful with the order if we add more units, which we are not likely to do. Also, in any case we would have to be careful with be comp anyways.
This is totally funky and the parsing code is crazy, but I verified it works with weirder timezones. ``` assertDateMathEquals("2014-11-18", "2014-11-18T00:00:00-09:30", 0, false, DateTimeZone.forID("Pacific/Marquesas")); assertDateMathEquals("2014-11-18", "2014-11-18T23:59:59.999-09:30", 0, true, DateTimeZone.forID("Pacific/Marquesas")); ```
I hope jit takes care of this to be honest
just a nit, can we move the `initialRecoveryFilters != null` first since it might be able to skip the lookup then
you should replace the curly bracket with a square bracket here.... :D
is it possible that somebody doesn't provide the suppler but has ignoreUnkonwnFields set to false? in that case we wouldn't get here anyways right? just checking... :)
What does that check actually help us verify? I don't think smoke testing should care about what particular hash was used to build, it should care about whether the artifacts it is told to download work? I view the release as a blob. That particular check requires us to know the "release hash" as you called it here (other unified release stuff calls the version+hash the build id, note that it is not a git hash of any repo), _and_ the ES commit hash, but I think it should be easier to run smoke tests. We can do that check (did we put the hash in the jar we built correctly) in tests in ES directly.
Why isn't this replacing `--hash`? The release hash here should be the only thing needed to download the artifacts.
Out of date doc.
Extra ` `s
can we assert that ignoreReplicaException is false? it will be weird to have something fail a shard but not reported.
Fall back to _old_ behavior
Capturing what we discussed f2f: I don't know how to best fix this - I'm ok with putting this in with NORELEASE for now, but this definitely needs either an equals implementation that doesn't rely on serialisation or it should be moved to the test code.
I think it'd be nice to do: ``` java Exception e = expectThrows(UnsupportedOperationException.class, () -> { RestTable.getRowOrder(table, restRequest); }); assertEquals("blah blah blah", e.getMessage()); ``` It is just nice to assert something about the message so you know _which_ `UnsupportedOperationException` you get.
Ah, you'd need to box anyway to do the sorting so far as I know so this isn't worth it I think.
I'd do something like ``` boolean reverse = false; if (columnOrdering[i].endsWith(":desc")) { columnOrder[i] = columnOrder[i].substring(...); } else if (columnOrdering[i].endsWith(":asc")) { columnOrder[i] = columnOrder[i].substring(...); } ``` or something like that. I think we should complain if it ends in something other than `:desc` and `:asc`.
nit: we usually add a space here. We don't have anything that enforces this style but we usually do.
Same deal, I'd just convert it to nanos.
What about just converting to bytes and comparing? The way you have it now this isn't consistent with `equals`.... Also the _cat API we call `toString` which doesn't really use the unit anyway.
Yeah, looking at it again, that makes sense!
I'm not sure the length comparison buys you much. I'm fairly sure `String#endsWith` already checks that super duper early.
Finally getting back to this! On this look I noticed that this would make hashcode and equals inconsistent which is yet another thing to fix. I haven't really thought through the right way to make them consistent, but there ought to be a way!
I think we should complain if we don't find the header name.
Sorry for all the trouble with this! It was a mess before and you are only making it better and this is really just another thing to do to make it better but hashcode and equals being consistent is one of those cardinal rules of java....
Rather than `- 5` I'd do `- ":desc".length()` just to make it super clear what is up.
I wouldn't move everything within the catch, just the part that you moved out as part of this PR. We use the same cluster state throughout the execution of the whole multi get transport action, so deleted indices once the cluster state has been retrieved won't be seen by it. What will happen is that the shard action will fail as the shard is not there anymore, hence that specific get item will fail.
If it is really important that these docs end up on separate shards can you add an assertion that they are on separate shards? That would explain the `true, false, builder` thing.
Ah! I see the static reference now. I think this method is fairly confusing then? Like, it _looks_ like it is just about the version that you are calling it on but it does stuff with the _current_ build.
are we saving some newlines here? 
I guess this reads "an active shard must stay active"... ;-)
Nit: `active.size()-1` -> `active.size() - 1`
Nit: `initializing.size()-1` -> `initializing.size() - 1`
I think this one is fairly lite - typically there only a few and very specific rules
+1 to leaving as is then.
nit: I prefer the following check: ``` if(clusterService.localNode().isDataNode() || clusterService.localNode().isMasterNode()) { clusterService.addFirst(this); } ```
Can you collapse the check into a single line, it's awkward to read ``` java explain ? "..." : null ``` Since it splits the check and the result, so maybe ``` java return ShardAllocationDecision.no(AllocationStatus.NO_VALID_SHARD_COPY, explain ? "shard was ... in the cluster" : null)); ```
Same here for the line splitting
Whoa this is even stranger to read since it spans 3 lines
NullPointerException here (decision can be null)
I wonder if this method should return NO instead of null. This also makes the line ``` UnassignedInfo.AllocationStatus allocationStatus = decision == null ? UnassignedInfo.AllocationStatus.DECIDERS_NO : UnassignedInfo.AllocationStatus.fromDecision(decision); ``` obsolete in another place.
sorter being a field now, it does not need to be pass around anymore, see e.g. `private String[] buildWeightOrderedIndices(NodeSorter sorter)`.
I think something like `randomSearchSourceBuilder` would be a more consistent with other random builders.
Did you mean to push some commit? I don't see any change here
so we can assert _that_ we can still use it
shall we check that we get some results back? Just making sure that we actually search against the alias. If it doesn't exist we could get back empty results without any error...
rather than introducing a generic setting for this (that people can abuse independently), I would prefer we explicitly check for tribe settings. `TribeService` checks this by using `settings.getGroups("tribe", true))`. We could make `tribe` be a static string and use it here or have a static method on the `TribeService` called `isTribeNode`.
nit: should be "commit_ting_ writer with commit data"
I think it will be simpler to just capture the local and global checkpoints before we start and build one map here. This way we don't need two maps.
Checkstyle is unhappy with this.
Checkstyle is unhappy with this.
Checkstyle is unhappy with this.
The parentheses around `t` are unnecessary.
created version should not be able to be null. We have numerous checks that depend on it in the base FieldMapper.
Our build does not allow the use of the `@Test` annotation and instead we have the convention naming test methods starting with `test`. Could you change this test method to have a name beginning with test? Also it might be a good idea to run `gradle precommit` after making the change to find out if there are any other issues.
The name is correct as-is, it adds spaces.
This is fine as-is.
This is fine as-is.
I find this a lot easier to read as it's currently written.
I wonder if this should be `Exception`. see also #20659
can you remove this method override here? No need to override empty method with empty method.
Definitely a hero I've never heard of :)
It should say greater than zero, 0 is not permitted.
It should say greater than zero, 0 is not permitted.
Maybe pass the `Executor` in to the ctor instead of `ThreadPool`? You could keep this method so you can still override it in tests to throw.
do you understand this if block? I suspect it made sense before your change, to make it possible to refer to existing indices or aliases that started with `-` rather than treating the name as a negation. That said, I can't quite follow why it makes sense in some cases for `result` to be `null`. This was already there, not your doing but I wonder if it's related and may be cleaned up.
I wonder if we should set this to true only if options.expandWildcardsOpen || options.expandWildcardsClosed . Otherwise wildcard expressions don't get expanded...
Also `-test1,*test2*,-test20` or something along those lines? :)
I'm afraid we need to rely on the order if we want to be able to distinguish between negations (applied when a wildcard expression appears before the negation) and referring to indices that start with `-`. We will be able to get rid of it in 6.0 only when we will be sure such indices are not around anymore. I opened #20962. Can we also have a test where the wildcard expression is not the first expression but still before the negation? e.g. `test1,test2,index*,-index1`
At this point I wonder if the order should still matter as it used to. Maybe negations should just subtract from the rest of the indices mentioned in the expression, with the requirement that an expression cannot be a negation only, otherwise we interpret it as an explicit index name that contains `-`. Should `-test1,test*` resolve to the same indices as `test*,-test1`? What other edge cases am I missing here? I also wonder why we support `+` as it is implicit anyway when missing.
I think it'd be more accurate to call it `testSuccessfulBuilderContext` or something like that because it isn't just testing with the settings - it has the `ContentPath` too.
I'd probably use `checkNotNull(indexSettings, "indexSettings is required")` here instead. Usually I don't think it is needed but since this used to be nullable it'll be nice to have the assertion in production code.
I'd never really thought of this but it makes sense.
This isn't a hot code path so I'm quite willing to try and write the most readable code possible and let the optimizer try and optimize out the extra OR operation if it is capable of it.
I think this was more readable the old way.
I wonder if `PARSER.declareString((b, v) -> b.sortMode(SortMode.fromString(b), SORTMODE_FIELD);` is better? I kind of prefer it because then you don't need to think about `ValueType`.
Same deal here, it'd be cool if there was a static `parser` method or `PARSER` member that we could use as a parser here.
This parser used to ignore arrays of stuff with any name. Now it ignores them. Awesome.
Ah CONTENT_TYPE I see. Sorry for the noise ;)
Maybe better to use MapperService.isMetadataField since "_" is a valid prefix for a user field name.
I don't think these should be `IOException`.
Yeah, don't feel bad, that's what reviews are for. :smile:
oh come on don't feel bad Tanguy :)
I'm fine with `IllegalArgumentException`, in all the places of course. :smile:
I've started to appreciate ``` Runnable r = randomFrom( () -> builder.setFuzziness...., () -> builder.asdfadfasdf ); r.run(); ``` That way you can't forget to change the upper bound when you add a new random option.
actually we could even skip the unmodifiableList wrapper since the wrapped list is already unmodifiable
This will need serialization protection when backported to the 5.x branch
Same here about the serialization protection
actually there should protection on 6.x too since we have not given up the idea about multi-version clusters yet
I'd probably add an `else` clause that sets `splitOnWhitespace` to the appropriate value just to be super clear.
I think it would be easier to read if we use the string representation to create these: http://docs.oracle.com/javase/7/docs/api/java/nio/file/attribute/PosixFilePermissions.html#fromString(java.lang.String)
That is much easier to read!
This should be changed to `tracerLogExclude` as well.
And one more...they seem to be all over, I presume a mistake in regex find/replace.
just use `assertFalse()`
I don't think a `finally` block is enough though. If we crash hard between the creation and destruction of the file the finally block won't do anything. Say we lose power or something. Sure, these are exceptional situations, but there isn't really any reason why we can't recover in this case and move forward. Not recovering here means that you have to read elasticsearch's code to figure out to delete the temp file.
Let's move this to a `finally` block.
And anyway, moving it to a `finally` block does help because if the create fails because the file already exists, the delete in the `finally` block will clean it up so we only fail in this way once.
I'd prefer to delete the temp file first as well but it looks like to be consistent with #19036 we should do the delete in a finally block. That way if the file exists we'll nuke it in the finally block as we abort startup. I guess this has the advantage of failing once for the user so they know it is a problem.... I'm not sold on it. The bottom line is that what we have now is wrong and if you move the delete to a `finally` block I can merge this without further debate. We can then open an issue or another PR to discuss deleting the file first. That way we have _a_ fix in and we can more leisurely decide what the right solution is.
This includes both of the fixes but to make the change uncontorversial it should just include the `finally` part, not the checking if the file exists part. Personally I think removing the file up front is the right thing to do but I'd like to separate that out into a separate PR because I expect other folks to object to that way of doing it (see the linked issue) and I'd like to get the `finally` block portion of this fix in.
No tests will ever be executed concurrently. Removing the thread context is enough, I suspect that this is how we will test deprecation warnings once we remove strict parsing, which we currently use only for testing.
It would be nice to find another way to do this other than replacing the thread context. I had a look but didn't find other ways though :) I guess you have tried as well.
does this need to be protected? I think it is safer if it is private and the only way to access it is via checkWarningHeaders
shall we move to using `@ConditionalIgnore` here. If I understood correctly in that case before and after wouldn't be executed for ignored methods, hence you wouldn't have to keep track of whether the test ran yourself
me neither, I did some googling :)
I think that we are leaking a thread local here? We should close the current threadContext before overriding it.
I'd also like a test for the case that either a single bound or none of the bounds are specified (even if that means checking that an exception is thrown depending on the decision we make).
(same question for FLOAT)
If we plan on keeping the time zone, I think we should add it here.
should we round the lower bound and the upper bound like DateFieldType.innerRangeQuery? Also I think you could use a `nowInMillis` impl that just throws an exception since `now` is not supposed to be used at index time.
you need a version check here too
I'd rather make tem live in RangeQueryBuilder and import them from the RangeFieldMapper. Or duplicate.
+1 to support it and also +1 to not do it now
To keep things simple, I would not support null values for now.
Given the timing of this PR, this will rather be V_5_2_0
this new indentation is a bit inconvenient as the conditions of the if statement are at the same level of indentation as the code in the block
we have `ignoreMalformed` on numerics for historical reasons, I'd be in favour of not having it on range fields
this method seems to be only used at indexing time, so I don't think it should accept `nowInMillis` since index dates need to be concrete dates rather than math expressions
you should protect it with a version check
Can you make this a full `if` statement, it's really easy to miss the `return` while reading in this format
Strings.EMPTY_ARRAY could be used too (if you want)
can we make sure not to delete a file that wasn't created and hide the original exception? we typically do this: ``` bool success = false; try { do something success = true; } finally { if (success) { clean up } } ```
You could probably avoid this by making the linux check a method that you stub out in OsProbe.
Ok that was fast :D
I'm fairly sure `Location` is supposed to be a valid URI. I'd be ok with catching the exception and logging a warning for it unless we expect this to fail frequently. If we _do_ then we shouldn't be using `URI`.
Could you add the exception to the log statement? If we really don't expect this to happen I think it should be a `warn` level. If someone is able to reproduce it then they can silence it with the update settings API before we work around it. Now that I think about it, it'd be nicer to throw the `URISyntaxException` out of the method and then catch it when we go to write the location, warning there instead of here. That way we don't add the `Location` header with an empty string.
I think I was thinking that it'd be better to just throw the exception and catch it in the caller. That way the caller can log the warning and not add the location header.
This is _much_ better.
Actually, I think this logger can be removed now (thankfully).
I'd use `expectThrows` here. I also like assertion *something* about the exception's massage just to make sure I caught the one I expected.
This shouldn't be here. You should use ESLoggerFactory instead.
I wondering if we should do it on validate? feels more natural there, as we won't need to wonder if there if we got all the paths to set version types
I think you know I'm not a huge fan of lambdas :), but I fully understand why you do it here, since this is probably cleaner than another interface.
Also, I realize this was probably copied from PSubBrace which has the `if (!write)` guard, but it's actually not necessary. I think it got left over from PSubBrace accidentally when I was working on the EAssignment refactor, so it can be removed there too. You can see in setup method for these classes that the prefix node is appropriately called while skipping the write method altogether for assignments on the final lhs node.
Yes! Please test this and maybe a couple others like x[-1] += 1 with numbers or something with Strings.
I'm not sure what you mean here exactly, since all the subclasses that use this are final, except the one that _has_ to be extended.
I think this will succeed even without the change in this PR? I'm not sure what is exactly tested here.
removeIndex might be good enough here.
assertBusy uses by default 10 seconds, no need to specify it here again
the description does not match what the test does.
Pick node with THE primary shard
waitForCompletion returns SnapshotInfo
to the same node
I guess that's not encapsulated into the message, so it's hard to check huh..
solely based on names, hard to distinguish from `testRebalanceNotAllowed`
`noAllocationDeciders` and `throttleAllocationDeciders` can be used instead here
I usually do this: ``` assert xContentBuilder.generator().isClosed(); return true; ```
10000000L seems arbitrary, maybe `Long.MAX_VALUE` would better reflect what you are trying to achieve here? Or maybe we can make `-1` to work as `unlimited` or check for `unlimited` string constant.
If these privileges are only needed for loading static definitions, then this should be done in a static block when the plugin is loaded, instead of on every invocation of the script.
I think we can do this without adding an interface? Users should not need to do this? A script cannot realistically be used for both search and executable. I know this is more a problem with the existing scripting apis, but I think here we can just implement executable, and search should throw UOE.
If this jolly library really only works with context classloader then I think we should find another example...we should not be promoting in any way setting the context class loader.
this is tricky TTF will not respect deletes, I think you need to fetch the doc though. It's not much overhead compared to this.
Actually I think it should be `int index = Math.round((percents[i] / 100.0) * (data.size() - 1));`
I'd really like to see either the response type changed to `ByteSizeValue`, or the name changed to `getBufferLimitBytes()` (preferably the first one)
I also think it'd be good to have `HeapBufferedAsyncResponseConsumer` take a `ByteSizeValue` instead of an `int` of bytes, much less chance of misinterpretation.
I really don't think we should capture assertions and rethrow, it leaves us really open to typoing and swallowing test failures because we accidentally forgot to rethrow. I assume your intent here was to give more info if one of them failed? In that case, I think ordering them by most to least information would be better (though optional, I mostly care about removing anything catching asserts): ``` java assertThat(e.getMessage(), containsString("Remote responded with a chunk that was too large. Use a smaller batch size.")); assertSame(tooLong, e.getCause()); assertFalse(called.get()); ``` And then removing the try/catch altogether.
Ahh I totally missed that this was client only, `getBufferLimitBytes` makes sense
this is not the intended way to use this method. Why should `null` be accepted? I would rather throw NPE earlier than accept it and convert it to something else.
yes that would be perfect thanks
Definitely nice to add this at a later time since even with the docs I bet people will run into this and be confused. But, yes, for another PR :)
Since setup is only used during writing and writing is illegal, this can also be the same create error as store.
Oh never mind. I misread the grammar. The elvis operator applies to the call, not the var. Sorry, trying to think about too many casts at once :)
I swear I'm missing something. I thought that top-level variables wouldn't be working with this based on the grammar changes I see...
This'll fail the build.
There's an extraneous blank line here.
Below is what I get when I try it out. As you can see that log message is drowned in many other log messages that don't mention the index name. A lot of this is guice and we're working on fixing it, but I think the easiest is to make sure that the index name is mentioned in the exception for now? people won't see it otherwise. ``` [2016-11-03T00:08:29,112][ERROR][o.e.g.GatewayMetaState ] [ePegTxb] failed to read local state, exiting... java.lang.IllegalStateException: can't archive mandatory setting [index.number_of_shards] at org.elasticsearch.common.settings.AbstractScopedSettings.archiveUnknownOrInvalidSettings(AbstractScopedSettings.java:528) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] ... a terminal screen worth of stack trace here ... Caused by: java.lang.IllegalArgumentException: Failed to parse value [2000] for setting [index.number_of_shards] must be <= 1024 at org.elasticsearch.common.settings.Setting.parseInt(Setting.java:533) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] at org.elasticsearch.common.settings.Setting.lambda$intSetting$12(Setting.java:504) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] .... [2016-11-03T00:08:29,144][ERROR][o.e.c.m.MetaDataIndexUpgradeService] [ePegTxb] [foo/TYJyxhVDRjGIMDrHomQciw] failed to process index settings: can't archive mandatory setting [index.number_of_shards] [2016-11-03T00:08:29,146][ERROR][o.e.g.GatewayMetaState ] [ePegTxb] failed to read local state, exiting... java.lang.IllegalStateException: can't archive mandatory setting [index.number_of_shards] at org.elasticsearch.common.settings.AbstractScopedSettings.archiveUnknownOrInvalidSettings(AbstractScopedSettings.java:528) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] ... another terminal screen worth of output [2016-11-03T00:08:29,161][ERROR][o.e.c.m.MetaDataIndexUpgradeService] [ePegTxb] [foo/TYJyxhVDRjGIMDrHomQciw] failed to process index settings: can't archive mandatory setting [index.number_of_shards] [2016-11-03T00:08:29,161][ERROR][o.e.g.GatewayMetaState ] [ePegTxb] failed to read local state, exiting... java.lang.IllegalStateException: can't archive mandatory setting [index.number_of_shards] at org.elasticsearch.common.settings.AbstractScopedSettings.archiveUnknownOrInvalidSettings(AbstractScopedSettings.java:528) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.archiveBrokenIndexSettings(MetaDataIndexUpgradeService.java:171) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:81) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] ... another terminal Caused by: java.lang.IllegalArgumentException: Failed to parse value [2000] for setting [index.number_of_shards] must be <= 1024 at org.elasticsearch.common.settings.Setting.parseInt(Setting.java:533) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] at org.elasticsearch.common.settings.Setting.lambda$intSetting$12(Setting.java:504) ~[elasticsearch-6.0.0-alpha1-SNAPSHOT.jar:6.0.0-alpha1-SNAPSHOT] [2016-11-03T00:08:29,229][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main] org.elasticsearch.bootstrap.StartupException: org.elasticsearch.common.inject.CreationException: Guice creation errors: 1) Error injecting constructor, java.lang.IllegalStateException: can't archive mandatory setting [index.number_of_shards] <--- THESE IS REPEATED 4 times at org.elasticsearch.gateway.GatewayMetaState.<init>(Unknown Source) while locating org.elasticsearch.gateway.GatewayMetaState for parameter 4 at org.elasticsearch.gateway.GatewayService.<init>(Unknown Source) while locating org.elasticsearch.gateway.GatewayService ... another terminal, this time full of guice information Caused by: java.lang.IllegalArgumentException: Failed to parse value [2000] for setting [index.number_of_shards] must be <= 1024 at org.elasticsearch.common.settings.Setting.parseInt(Setting.java:533) at org.elasticsearch.common.settings.Setting.lambda$intSetting$12(Setting.java:504) at org.elasticsearch.common.settings.Setting$$Lambda$183/2092885124.apply(Unknown Source) at org.elasticsearch.common.settings.Setting.get(Setting.java:312) at org.elasticsearch.common.settings.AbstractScopedSettings.archiveUnknownOrInvalidSettings(AbstractScopedSettings.java:525) ... 47 more And this ^^^ is the last message on the screen. ```
Also- I traced the code and as far as I can tell, in the case of `index.number_of_shards` we never say which index was problematic? I think it's important to add that info..
Nit: I think we need to renamed the method + docs - it now also rejects mandatory and invalid settings.
People don't know what archiving setting means. They just upgrade and their end up in an illegal state. I think it will be cleared not to mention the "can't be archived" part. But as said - just a nit, not a big deal.
@s1monw I'm sorry that I didn't take any time to reply last night. The situation with the response parameters is quite complicated. Look for example at `Settings#toXContent`. The situation here is that the `flat_settings` parameter is consumed there, but the signature `ToXContent#toXContent(XContentBuilder, Params)` is a general signature, we can't just go and add a boolean parameter for flat settings to the interface because it doesn't make sense in all situations. It is for this and similar reasons that I ultimately handled response parameters the way that I did. Barring a redesign, I would prefer that we remain consistent for now. > It's just yet another place we need to maintain and look for params. Right now it is how we handle output parameters.
This isn't where I would expect it to be consumed since it affects the output only, not the request handling.
This is one way to do it, but I'm wondering why you opted to do it this way instead of using the infrastructure that exists for handling response parameters? Namely, override `AbstractCatAction#responseParams` (being sure to include the response params from super).
I'm not sure about this, it is a usage error.
To be clear, I don't think we should change this to an I/O error. I think I'd be okay with a configuration error though.
Yeah, exactly, and I think usage should really be reserved for incompatible or invalid arguments, for example. This is more a state thing, so now I think I'm convincing myself that configuration is apt.
Empty array is a thing that the jvm is very good at optimizing here. It is a very common case and they've worked hard to make sure it is quick.
We should still throw an exception here though.
I'm fairly sure I have the wrong generics incantation there....
Is it not ok to have an empty array anymore? That seems weird actually.
Does this still support numbers? Having a test for numbers would be good here too.
I might use an empty array here or switch the IdsQueryBuilder work with lists.
`AbstractObjectParser::boost` should work here, same with `AbstractObjectParser::queryName` below. One less lambda for the JVM to reason about...
It might be worth eventually making some method that takes an `ObjectParser` with the appropriate signature and returns a `QueryParser`. Eventually....
We might should move these last two declarations to a common spot something like ``` static <T extends AbstractObjectParser<? extends QueryBuilder>> declareStandardFields(T parser) { parser.declareFloat((builder, value) -> builder.boost(value), AbstractQueryBuilder.BOOST_FIELD); parser.declareString((builder, value) -> builder.queryName(value), AbstractQueryBuilder.NAME_FIELD); return parser; } ``` and then we can declare them when we're initializing the object.
The other `BUFFER_SIZE_SETTING` (~ line 111) must also be updated. (I hate those settings overrides...)
not sure if this should be pretty printed as we put in the exception message. we'll have to see an example to be sure. Alternatively we can log a warning line like we do in ensure green. Also, you can consider using Strings.toString().
We tend to prefer `false ==` over `!` because it is harder to miss the `!`.
any chance we can use `org.elasticsearch.common.xcontent.ObjectParser` here instead of the old style way of parsing stuff.
Maybe add "for example"
Fine by me.
Could you move `writeTo` up here? It is easier to compare them if they are together.
I like this much better!
I _think_ you can do `XContentParser::mapStrings` above instead of having this method.
I'd try and more this suppression just to the variables that need it that way it isn't super wide.
I wonder if this is actually easier to read than making a building with non-final fields and using that. If you think it is I'm happy to accept that but it might be worth writing it out the other and having a look just to make sure.
I'd move the extractions all to the top.
I think it'd be clearer if s/throw/check/.
I see why you did this with the map here. I wonder if this is more of an argument for a builder because you can set the lang on the builder before you do the parsing.
I think maybe just "return Math.log(doc.popularity) \* 100;", you don't need the "<idOrCode>" bit.
Maybe this one too, I'm not sure.
I think s/lang/defaultLang/
We did used to do this but we've moved away from it. So you'll see examples in the code when we had this but it is no longer the preferred way.
Have a look at `ConstructingObjectParser` which is designed for those cases where you have required parameters for the constructor. Alternatively you may end up making an object that has writeable parameter and then building the script from that. Whatever you think is more readable.
5.1+ I think, actually. Have a look at `VersionTests#testUnknownVersions` and the note right underneath `Version#CURRENT`.
super nit: we don't need this static variable, it's only used in the static block, so it could become a local variable there.
instead of the assertBusy, maybe use a future above with setWaitForCompletion(true).
nit: we can change this to greater than or equal relationship and check on both statuses.
there is a waitForStatus variant - should be simpler.
+1 to this plan
Personally I'd make this an interface that has a static method that returns an implementation that makes the `HeapBufferedAsyncResponseConsumer`. I'm not super comfortable with the no-arg ctor for `HeapBufferedAsyncResponseConsumer` because it sort of pushes you not to think about the buffer's maximum size.
For now, the `ObservedState` contains the same info as `ClusterServiceState`. I also think anything we add in the future will likely also be relevant for the `ClusterStateObserver`, so I'm wondering if we need to maintain the `ObservedState` class and can just use `ClusterServiceState`. I don't believe its a big issue though, so if you prefer to keep it as is, we can do that and revisit later if needed.
I also wonder if we should pass in `ClusterStateService` instead of `ClusterState` and `ClusterStateStatus` separately. This will make it easy to take the full `ClusterStateService` object into account in validation methods.
typo? oracle corp isn't the jvm version..
I think we can remove this
maybe call the concrete indices "index1" and "index2", otherwise one may think they are aliases :)
I find it odd that we modify the original source builder with the resolved indices names and replace what we originally had. Would it be possible to transport the resolved indices differently? I think we should serialize this new info separately and carefully handle bw compatibility around that.
maybe we should be more explicit and initialize indexBoost in an else branch here, rather than above when declaring it. I think de-serialization is the only scenario when it may not get assigned.
if the list is already initialized like I suggested above, this if goes away too
I think that we can unconditionally assign indexBoosts.
Should we initialize indexBoosts to new ArrayList<>() straight-away instead and remove this null invariant? This if would go away too.
no there was no misunderstanding, I just didn't realize the implications of this check
I see why it's 1 and not 0, and I think it's correcy. my question was rather why we don't pass it in like in the other constructor, I think it is because the apis that use this constructor don't support indices_boost right? just trying to understand
I think that we can remove the query boost setter and make the instance member final.
Can we switch to using the method that returns `Index[]` and use the index uuid rather than the index name as key in the map? That will help with the upcoming cross cluster search feature ;)
I think that it can rather be empty, hence we always print out the array, which sounds ok to me. I think the null check can go away.
I am sorry, I didn't think this through. The two sections have the same name, so the json wouldn't even be valid if they were both there. I think we should not check it explicitly, otherwise we would have to do it for every single field in our parsers, which we don't do. Relates to #19614
good point, I think it's ok if it is configurable. and then it should do by default the same as the rest of the same search request does.
same for the other constructors too
no, I think we should print out what the user sent, not our own resolved objects.
can we move the resolution method out of `SearchSourceBuilder` too and also transport the info out of it as well? Like we do with aliasFilters map for instance? Does it make sense? Trying to keep `SearchSourceBuilder` as POJO as possible and have logic outside of it.
I think that we have a problem with backwards compatibility here. Most likely our bw comp tests are going to fail (would be nice to verify that actually). I think indices_boost would get ignored in a mixed version cluster (e.g. 5.x and 5.1): - 5.x coord node does the resolution on the coord node, serializes search source builder to 5.1 node (nothing changed there) which doesn't know how to resolve indices names yet, hence it reads indices boost from the original source, it works as it used to. That's ok. We can't do better than this. - 5.1 coord node doesn't resolve indices boost on the coord node, serializes source builder to 5.x node (nothing changed there) which knows how to resolve indice names, but it doesn't get them through shard search request as 5.1 node didn't send the info. Hence the indices boost are always going to be 1, ignored. We should fix this. I think the way to fix it would be to detect based on the version whether we have to read the index boost from the source builder of from the shard search request.
thanks a lot for adding this!
I feel like we implement this pattern enough times that we should make a helper for it at some point. No need now, but at some point.
fancy pants :)
nit: flip this to true? easier on the eyes
this will be easier to read now explicitly setting it to indexRequest
too bad ..
could you please add the curly brackets around this statement? we tend to use them although not needed around a single, for better readability.
I wonder if it's nicer to append the random uuid.
I don't think we should special-case this. Depending on underlying blobcontainer, a different exception might be thrown.
`super.readBlock` instead of `readBlock` to prevent double `maybeIOExceptionOrBlock`.
can be specified as `catch (SnapshotCreationException | RepositoryException ex)` in Java ;-)
sorry, forgot to add here that we could randomize between atomic and non-atomic move.
This API call is forbidden and fails the build. `random().nextBytes(randomBytes);` is fine though. I'll fix it before I merge.
This line is too long, it is causing checkstyle to fail. We have a line limit of 140 characters that we started enforcing a few months ago but only on files that didn't have any violations. We're super super slowly fixing the violations as we go. I'll fix this before I merge.
> disruption tests do not use mock discovery They don't use it because of the MockZenPings - if we make that an optional feature of MockZenDiscovery (via a setting or whatever), the disruption tests could use MockZenDiscovery, turning mock zen pings off.
well, we can call it `ZenTestDiscovery` :) - my point is more that then we can move the `getZenPing` to the test only variant and not have it public on `ZenDiscovery`. There were also times where we considered mocking elect master service where this test-only discovery would have been useful.
After giving this some thought, what if instead of making a new node, the pieces were built up in the Walker? I think this might end up being easier to maintain since the nodes have already had some testing. Something like the following: ``` AExpression condition = AExpression expression = (AExpression)visit(ctx.expression(0)); AExpression value = AExpression expression = (AExpression)visit(ctx.expression(1)); AExpression comp = new EComp(location(ctx.expression(0)), Operation.EQ, lhs, new ENull())); return new EConditional(location(ctx), compare, new ENull(), value); ``` This should take care of all casting issues since null will naturally become object for the promotions. Let me know if I'm missing something here, which I very well could be.
I don't understand here what you mean by synthetic variable. If you mean the two ENulls, the analysis and writing would be contained to only compile-time.
Two test requests: What happens when you have something like `Integer a = Integer.valueOf(0); int b = a ?: 1;` `Integer a = Integer.valueOf(0); int b = a ?: Integer.valueOf(1);` I believe these are expected to be ClassCastExceptions where Integer cannot be cast to int, but I'd like to be sure.
@nik9000 What do you think about this proposal for now? What if for the null safe operator (?.), if the guarded type is a primitive we disallow it, and then for the elvis operator (?:) if the lhs is a primitive we disallow that too. I honestly don't think anyone will notice because currently there is no way to get a primitive out of a field from a non-static type (nothing exists in the whitelist afaik), and I would argue in the case where you want a primitive out of a call, it doesn't make sense to have the receiver be a boxed type. You would still have to check to see if the receiver was null afterwards anyway. For most cases the type will be def and the auto-boxing will just happen anyway. Both of these operators can be very useful for def types without needing to have them do magic for primitives. I would hate to not have them because of boxing issues when it's improbable that users would run into them. I would think the average use case would be something along the lines of list?.list?.map?.get(value)) in which case this operator is awesome.
Painless is neither Groovy nor Java. We maintain similarity for convenience of new users being able to jump right in. The only reason the boxed types are even whitelisted is so that other whitelisted methods that require the boxed types can be used. One bonus is that we allow unboxed types to call boxed type methods. But otherwise, there is no reason I can think of to use a boxed type. Eliminating boxing support for these types other than that situations I listed above has removed immense amounts of complexity from both casting and promotion. I strongly stand by this decision and will continue to do so. On the boxing situation for the elvis operator I would make it work exactly as the regular conditional does right now. If a regular conditional had `Integer a = Integer.valueOf(0); int b = a != null ? a : 1;` I would expect it to throw a CCE. Divergent behavior between the two operators will be even more confusing, not to mention the odds of someone actually hitting this is pretty low considering the def type should still work. `def a = Integer.valueOf(0); int b = a ?: 1;` should work without any issues. Another option would be to disallow primitive types from working with the elvis operator at all.
@nik9000 Robert and I had a long conversation about boxing early during development. We decided to eliminate it as much as possible because of serious complications involving promotion and casting (which as you know is already very complicated). There's only a couple of places auto-boxing happens -- arguments to methods because it would be hard to force a user to cast something to an object to add to a list and with anything related to def type. Otherwise, there is no auto-boxing in painless. Perhaps, this should be the same for consistency? Sorry, I sort of missed this yesterday thinking about the cases, but def should work anyway already, otherwise primitives don't make sense here since we don't allow Integer to become an int anywhere else. With the def type we deemed auto-boxing to not be necessary anymore, and ideally something Java would've hidden from the user to begin with. It also happens that users can call boxed methods on unboxed types to further eliminate the need to ever have a boxed type.
should rather -> must
is different -> are different
I _think_ you could just pull the `PositionLengthAttribute` and if it has a value greater than 1 on any token, it is a graph.
Hmm, why did we need to fork this class from Lucene? Is it only for the added `getBufferSize` public API? If so, I think we can make this change directly in Lucene. I'll do that ...
> Ok this doesn't work because the standard synonym token filter also sets the position length attribute Arrrrgh, you are right. Here's maybe another idea: why not always treat things as if they were a graph,since a single linear chain of tokens really is just a graph. And then, when the graph enumerates to just one path, it should naturally "become" what already happens today? I.e. don't break out any special treatment for "graph" vs "not graph".
can we make the environment variables passed on to this configurable ie. as ctor arguments? I also wonder if the variable should be prefixed with `ES_`
lets call `stdinReferences.clear()` after we closed all of them. I also think you should use `IOUtils.close(stdinReferences)` instead, it will close all references even if one close call throws an exception.
or rather pass it to `spawnNativePluginControllers` and change it's signature to `spawnNativePluginControllers(Path pluginPath, Map<String,String> env)` I don't think we need to depend on `Environment`
please use `assumeFalse(Constants.WINDOWS);`
maybe like this: ``` Java try { IOUtils.close(() -> processes.stream().map(s -> (Closeable)s::destroy).iterator()); } finally { processes.clear(); } ```
the reason why I suggested to make it configurable is that we could pass in our own values in tests that's all... not a big deal
we can make this a function to `List<MergableCustomMetaData>` - we alreay check with instance of in the implementation of it.
if we do so, I think this becomes a simple stream reduce
Answering here - I would prefer keeping the logic the same for all nodes i.e., any custom change is processed based on the latest cluster state of those nodes. No need to special case the current node as I don't see what it buys us but complexity? without it it can be a simple stream
I don't think you need an if else structure here? you can set the first and iterate over the rest? This can be simplified
this can be out of if now.
this method became useless I think? the whole try catch can be removed as it's handled by the caller in the `ClusterService`. I think we should break applyUpdate into several methods each updating a different parts. i.e., updateNodes, updateIndices and now the now updateCustoms
nit: can we have this ugliness in a getClusterService(Node node) method? we use it in `doStart` as well.
It seems like it would make more sense if the setting being `false` meant that term queries were not cached. Right now just looking at the name, it seems like setting it to `true` would mean that term queries **are** cached.
Hang on: this is index throttling, which still exists and is different from store throttling? Index throttling is when ES forces incoming indexing to be single threaded because merges have fallen too far behind ...
This condition will never evaluate to `true` as we'll get an NPE when dereferencing a `null` instance of type `ExecutorHolder` in the line above.
typo: direct**or** -> direct
you need a long, or this could overflow. Or even not store it at all, and use `termsEnum.ord()` to know the ord of the current term
This is true. The alternative way I was thinking about would be to pull a terms enum from the sorted set doc values, and set bits in the bit set based on the hash of the values rather than their ordinal. This way, the partitioning would be based on the value but the terms aggregation would still be able to leverage ordinals to do the bucketing. The drawback is that it requires to compute a hash on every term of the field.
+1 for the 0-based partitions, this would be consistent with the `from` and the `slice`.
Check that partition >= 1 && pof > partition ? It's only checked when we (de)serialize from json.
+1 for the terms enum pulled from the sorted set. It should be fast to enumerate the terms since it is a sequential reads in the term dict of each segment. The hashing will be slow though, but again it's great if we can have a way to exhaust a term aggregations consistently even if it's not as fast as we would like ;).
To me this is the part of the PR that needs attention. The way it is implemented here means that the same partition could match different terms across refreshes. I am unclear whether it would be an issue. Maybe it would not, like pagination of the search hits.
I think we should. For instance in the case of a date field that only stores dates with second precision, all values would be multiples of 1000.
you can have a look at SimpleQueryStringBuilder.VERSION_5_1_0_UNRELEASED to see how to do it
you could call `BitMixer.mix64(value)`, which should have the same quality but might be a bit faster
I would use `Math.floorMod(term.hashCode(), incNumPartitions)` to avoid issues in the case that the hashcode is `Integer.MIN_VALUE`
can you throw an exception here too? I'm worried of users providing the partition numbers as strings, which would be ignored
this can be collapsed into `assertTrue(foundTerms.add(bucket.getKeyAsNumber()))`
I think the version should be 5_1_0
I think it'd be a bit cleaner not to do this for Byte, Short, Integer, and Long.
nit: it's just other nodes - we are pinged by any node in the previous cluster.
nit: can we add the timeout value here.
I think we can get rid of the ResolvedHostname abstraction - what am I missing? ``` diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/UnicastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/UnicastZenPing.java index 61bf1cc..3d3495e 100644 --- a/core/src/main/java/org/elasticsearch/discovery/zen/UnicastZenPing.java +++ b/core/src/main/java/org/elasticsearch/discovery/zen/UnicastZenPing.java @@ -242,40 +242,36 @@ public class UnicastZenPing extends AbstractComponent implements ZenPing { throw new IllegalArgumentException("resolve timeout must be non-negative but was [" + resolveTimeout + "]"); } // create tasks to submit to the executor service; we will wait up to resolveTimeout for these tasks to complete - final List<Callable<ResolvedHostname>> callables = + final List<Callable<TransportAddress[]>> callables = hosts.stream().map(hn -> lookup(hn, transportService, limitPortCounts)).collect(Collectors.toList()); - final List<Future<ResolvedHostname>> futures = + final List<Future<TransportAddress[]>> futures = executorService.invokeAll(callables, resolveTimeout.nanos(), TimeUnit.NANOSECONDS); final List<DiscoveryNode> discoveryNodes = new ArrayList<>(); // ExecutorService#invokeAll guarantees that the futures are returned in the iteration order of the tasks so we can associate the // hostname with the corresponding task by iterating together final Iterator<String> it = hosts.iterator(); - for (final Future<ResolvedHostname> future : futures) { + for (final Future<TransportAddress[]> future : futures) { final String hostname = it.next(); - if (!future.isCancelled()) { + if (future.isCancelled()) { + logger.warn("timed out after [{}] resolving host [{}]", resolveTimeout, hostname); + } else { + assert future.isDone(); // guaranteed by the invokeAll try { - final ResolvedHostname resolvedHostname = future.get(); - if (resolvedHostname.isSuccess()) { - logger.trace("resolved host [{}] to {}", hostname, resolvedHostname.addresses()); - for (final TransportAddress address : resolvedHostname.addresses()) { - discoveryNodes.add( - new DiscoveryNode( - idGenerator.get(), - address, - emptyMap(), - emptySet(), - Version.CURRENT.minimumCompatibilityVersion())); - } - } else { - final String message = "failed to resolve host [" + hostname + "]"; - logger.warn(message, resolvedHostname.failure()); + final TransportAddress[] addresses = future.get(); + logger.trace("resolved host [{}] to {}", hostname, addresses); + for (final TransportAddress address : addresses) { + discoveryNodes.add( + new DiscoveryNode( + idGenerator.get(), + address, + emptyMap(), + emptySet(), + Version.CURRENT.minimumCompatibilityVersion())); } } catch (final ExecutionException e) { final String message = "failed to resolve host [" + hostname + "]"; logger.warn(message, e); } - } else { - logger.warn("timed out after [{}] resolving host [{}]", resolveTimeout, hostname); } } return discoveryNodes; @@ -289,17 +285,11 @@ public class UnicastZenPing extends AbstractComponent implements ZenPing { * @param limitPortCounts the port count limit * @return a callable that can be used to submit to an executor service */ - private static Callable<ResolvedHostname> lookup( + private static Callable<TransportAddress[]> lookup( final String host, final TransportService transportService, final int limitPortCounts) { - return () -> { - try { - return ResolvedHostname.success(transportService.addressesFromString(host, limitPortCounts)); - } catch (final UnknownHostException e) { - return ResolvedHostname.failure(e); - } - }; + return () -> transportService.addressesFromString(host, limitPortCounts); } @Override ```
Actually plugins can implement `Closeable` and they will be closed when the node shuts down.
I wonder whether we should use `unicastConnectExecutor` for this and keep it contained (and throttled).
nit: it looks like we can just capture the version here and forget about the entire cluster state
Rename the method to something about captures I think now that the utility is gone.
Sorry for not noticing this sooner (I was blinded by the `Tuple`), but I don't think that we need this method. Most of the time we do not need the primary term (it's only used to resolve conflicts in the sequence number, and we can just load both separately then) so I think that we can safely drop this (and thus drop the wrapper class).
Nit: `primary term` -> `_primary_term`
Same thing, I don't think this variable is necessary.
random drive by question - why is the primary term part of the index result? it's already part of index and index result is supposed to capture the dynamic things that the engine has assigned.
another drive by question - should we do it here, or separate this into two, more explicit, flows - one we when we create an Index operation on a replica (where we set the seq no number based on incoming request) and one here when we operate as a primary? If you guys feel the current version is more intuitive, I'm good but I wanted to get confirmation this was considered.
I see this was already like this, but this can go on a single line.
`seqno` -> `_seq_no`
Nit: `seqNum` -> `seqNo`
Can we please not use `Tuple` here? I'm fine with a wrapper class, anything but `Tuple` (plus, with a wrapper class, no boxing). :smile:
Nit: `seqnum` -> `seq_no`
If I'm reading this correctly, the `context` variable is never used so can we just assign to `leaf` directly (one less thing to think about).
I see. The seqNo and the term do not necessarily always go together. the seqNo is the location of the operation and the term is the authority to put it there. I like the fact that the result object only contains the things that the internal engine creates / changes. Seq# are owned by the engine (on a primary). Terms are owned by the shard. I would prefer to remove the term. At least in the example you gave (`Translog.Index#Index(Index, IndexResult`) it's readily available from the index operation.
I prefer it the way that you have it.
Should we just do `String text = mapper.fieldType().valueForDisplay(textToHighlight).toString();` all the time? This looks correct to me and is more future-proof? If that does not work, feel free to merge the PR as-is.
would it work if we called MappedFieldType.valueForDisplay instead? I'm concerned some fields are stored as bytesrefs too but do not represent utf8 strings, like ip addresses
did you run into this being a problem? how can we open an index that was created before 5.0.0 and never had insync replicas but does have allocationId? the only thing I can think of is a node network issue during shard initialization. I'm wondering if we need to optimize for this and no keep this code simple (i.e., demote shards with a lock exception)
same request for assertion.
nit: too many newlines
you can fold this into the previous `if` block, so it reads: ``` if (in.readBoolean()) { this.nodeDecisions = Collections.unmodifiableMap( in.readMap(StreamInput::readString, NodeAllocationResult::new)); } else { this.nodeDecisions = null; } ```
maybe expand the explanation to "shard cannot remain on this node but throttled on moving to another node"
shard can remain
yeah, that was what I meant
the node where the shard should move to
this reorders the output. I think that the StreamOutput/StreamInput can call super() but that the toXContent should be completely done by the subclass so that we can render output in the optimal way. I would for example prefer to see the can_remain_decision **before** the node decisions where to allocate.
or maybe "shard cannot remain on this node and is force-moved to another node"
still don't like the "final" ;-)
I prefer to use the word "force" somewhere in the message to make it clear that this is not rebalancing based on weights.
look into `StreamInput#readMap`
`client().prepareIndex(...` is more normal now.
This shouldn't be needed anymore. By default we wait for the index to be created now.
`.get()` is more idiomatic now.
maybe we can try to log the stacktrace in a try-finally block in hopes that we can get the full stacktrace? In the finally we can throw the Error
I would stick with a separate exception simply because it's easy for the user to tell the difference between something he/she caused rather than an actual internal error.
I think it'd be nice to have an assertion on the text of one description just so we can look at it.
good call on the typo. I did see the issue that originated this change, thanks for the heads up ;) Just trying to understand if yours is a special case that needs a custom timeout (which could be set), or whether the default timeout really has to be increased that much.
yes I would at this point. it's a remote connection to another cluster that may be at a different location etc.
How does the volume of data relate to the slowness of the search api in the remote cluster? We are using scan search which is pretty fast and doesn't do deep pagination, and e.g. socket timeout set to 10 seconds doesn't mean that the whole response has to come back in 10 seconds, the request only times out if no data comes back for 10 seconds at any point of the request execution.
I hope I'm not splitting hairs, but there's also a typo in the field (deault is missing and 'f'). (This PR resulted from a question I asked here; thanks for all the awesome work you invest there!)
should we call the field `socket_timeout` ? Will we want to add `connect_timeout` too in the future? I think timeout is very generic so we may want to be more specific.
Can you only do toxcontent for ScriptException, but still log the entire stack trace for other exceptions? eg, if we have an NPE because of a bug in painless, I think we should still see the full stack trace.
I think the message here should be different? Otherwise it is confusing to see the same message, with different endings (one being xcontent, and the other being a java stack trace).
funny, we were testing the setting and the test succeeded although the setting was ignored? :/
right thanks for the explaining, I should have known, having worked on the search refactoring :)
good point, I think I went for pretty as default there because it's called by `toString`, and that made more sense in most of the cases, at least for queries and java api builders. Those methods are there solely for logging and debugging at this point. It does make sense to pass in false then in this specific case.
too many shards [%d] already allocated to this node, [%s=%d]
too many shards already allocated to this node for index ...
ShardRouting#getIndexName() is good enough, the index uuid is just distracting here
and 2 more occurrences below
same here as suggestion above
too many shards [%d] allocated to this node, [%s=%d]
fair enough not sure anybody needs to access this list :)
we might be able to make this pkg private - it's only used for testing
shall we stop it first? we might get false positive as the node threads move an acquire shared resource and trigger the block detection.
Nit: `numConnection` -> `numConnections`.
I think that this should be an `IllegalStateException`.
Nit: `getNumConnection` -> `getNumConnections`.
we do this kind of parsing in several places maybe a util can help at some point? not sure just an idea
one assert per member is better then you see what's not null :0
this part has to stay here - we want to extend our pinging as we learn of new nodes
can we unify the resolvedDiscoveryNodes logic with the buildNodesToPing ? They are similar in the sense that they don't change during a ping cycle.
You can add the return value of `resolveDiscoveryNodes` to the HashSet being built
we typically don't like using tuple return values. In this case I think we can just return a HashSet.
Yeah, let's do that (expand the range to the entire valid port range, and add assertions etc.). Thanks @jaymode.
This predicate can be simplified to `(count, limit) -> count > limit`.
can you undo all indentation changes, it adds noise to the diff
I wonder about the cases where this can happen as the `close` method on `InternalTestCluster` that calls `shutdown` on the executor is synchronized (same as this method). Have you observed this exception being thrown? If we don't expect this to occur under normal operations, I would prefer not to swallow the exception here.
Can the `Async` interface also be removed? I see it's an interface defined inside InternTestCluster.
did you mean `MapperParsingException`? ;-)
nit: you could use writeOptionalWriteable() here
Just a question: this means we are failing the whole rank_eval request here, no? I think this is okay, as long as this is what you intended.
nit: possibly use readOptionalWriteable here
I don't see why we need to restrict types here. This should work fine for stored scripts as well, and they should be validated too.
I don't think the compile script should be stashed here. Instead, just check that the script can be resolved with the script service (don't just check inline; for example, it would then also ensure if they use a file script it exists).
Since we have to specify a default, I think it might be cleaner to just write: ```diff diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestNodesStatsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestNodesStatsAction.java index 9309484..7fee930 100644 --- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestNodesStatsAction.java +++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestNodesStatsAction.java @@ -180,8 +180,8 @@ public class RestNodesStatsAction extends BaseRestHandler { if (nodesStatsRequest.indices().isSet(Flag.Indexing) && (request.hasParam("types"))) { nodesStatsRequest.indices().types(request.paramAsStringArray("types", null)); } - if (nodesStatsRequest.indices().isSet(Flag.Segments) && (request.hasParam("include_segment_file_sizes"))) { - nodesStatsRequest.indices().includeSegmentFileSizes(true); + if (nodesStatsRequest.indices().isSet(Flag.Segments)) { + nodesStatsRequest.indices().includeSegmentFileSizes(request.paramAsBoolean("include_segment_file_sizes", false)); } return channel -> client.admin().cluster().nodesStats(nodesStatsRequest, new NodesResponseRestListener<>(channel)); ```
This isn't the right fix, it's not a response parameter (it controls the request that the node client sends). Rather, this parameter needs to be consumed (and parsed as a boolean).
should we allow null here? I think it must not return null but the raw handler instead? maybe I am missing something
I can try and play with it afterwards, no big deal
if we'll need this in other tests, we should probably try to shorten this setup part of test by re-using what we have in our java api, that allows to provide `Object... source` , but we also want to be able to randomize the xcontent type, which is why we need to adapt it a bit
we should totally not have this method, one more reason to not implement the interface.
We now have 3 places we can express a timeout - URL param, Action line and Search body - feels like 2 too many to me but I'm not familiar with the reasoning for wanting all these. Either way, this code only parses and sets an Action-supplied setting if there is also a URL-param supplied setting which is surely wrong? It think this PR needs some tests to catch stuff like this and to demonstrate the correct support for parsing the various time-formats.
I'm thinking of the case that the previous cluster is the same as current (and thus has a master) and I basically the same question here - do we have a case where the cluster state has a master but also the no master block? if not, maybe we should an assertion to that matter as an else clause of `if (newClusterState.nodes().isLocalNodeElectedMaster() && previousClusterState != newClusterState)`
It seems that the normalizer can be changed when the field is updated ? It should be prohibited because we apply the normalizer at index and search time. Though the check is done on the index analyzer only and here we set the search analyzer so it won't prevent the normalizer to be changed at runtime.
Got it. Thanks
this is what you get from the crazy syntax 
Please revert this change.
None of these changes are necessary, but if we are going to get in these places and make changes, let's do it the preferred way which is with method references (where possible).
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
Please revert this change.
This function is used to resolve index constraint on field stats. I am not sure if we should implement index constraint on a geopoint field but if we don't then we should throw an exception here.
not sure what happened but these are now needed in this class
I'm wondering if we should adapt the whole message and say "Recovery failed on " + targetNode , if there is no sourceNode
We tend to use `expectThrows` for this when we write new tests. It isn't required but it is a bit cleaner to read.
(others tend to have a preference for `== false` so I'd recommend using that)
Maybe we could avoid generating intermediate garbage because of boxed objects by directly creating the primitive array, eg. something like: ```java int[] v = new int[values.size()]; int upTo = 0; for (Object value : values) { if (hasDecimalPart(value) == false) { v[upTo++] = parse(value, true); } } if (upTo != v.length) { v = Arrays.copyOf(v, upTo); } ```
There's a nice helper for this kind of random value creation in `ESTestCase#randomValueOtherThan(T input, Supplier<T> randomSupplier`). The second argument is a function that generates a random value, the first argument is a value that is supposed to be excluded from the range of possible random values. This might simplify this mutate code (and others in this PR) quiet a bit.
Some more candidates for `ESTestCase#randomValueOtherThan` here.
same here and in the rest of this method
I'd prefer using `IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> prez.setRelevalntRatingThreshold(-1))` here like we do in many other tests and get rid of the @Rule
again: ESTestCase#randomValueOtherThan might shorten this quiet a bit
I'm happy we have all these tests. It is also another data point to move in the direction we discussed - i.e., failures should mark things as stale.
I'm wondering if we should just never remove if there are no active copies? it's a simpler edge case to understand and it covers this scenario. I think it's even equivalent (i.e., the last active shard must be a primary).
Okay, glad to hear that; thanks for the clarification!
It is in.
ver -> version
The 2 `if-else` conditions can then also be simplified to just: ``` if (lowestVersionSeen == null || replicaNodeVersion.before(lowestVersionSeen)) { lowestVersionSeen = replicaNodeVersion; candidate = shardRouting; } ```
I think that the activeReplica method could also be expressed very concisely using Java 8 streams :-)
now that this method does not just select any active replica, maybe we should rename it to something that better reflects its purpose
`return ingestNodes[Math.floorMod(ingestNodeGenerator.incrementAndGet(),ingestNodes.length)];` ? I know it's just copied but while we are at it
this seems like it could create a lot of garbage since we do this for every request. Can we maybe hold a version of this per clusterstate version and invaliate it once the clusterstate has changed...
can this be final
can we maybe remove the dependency to ClusterService from this class it might be cleaner
I think assert makes sense because it is really a bug. I wonder if it makes sense to actually validate our own responses while parsing them. I think we should rely on tests for that, that our responses are correctly printed out using toXContent.
I wonder if this will cause problems down the road, the fact that toXContent doesn't output a valid json object per se.
if you save the xContentType randomly chosen above you don't need to guess what it is from the bytes here. we use auto-detection in so many places without knowing, we should not do that.
++ to that
maybe, but I am also asking questions, answers and opinions are welcome :) what I suggest doesn't always make sense
oh right we do have the mismatch here, like we expect a start_object that is never printed. It's the expected state of the parser when passed in, as highlight field is not a valid object per se. Odd. Not sure what we can do about it.
I understand the problem, and we will encounter this in many more places. The question is whether this is going to cause problems down the road, which I guess we will find out shortly.
Supporting multiple versions is hard, in order to support that we should have versioning support in our fromXContent methods, knowing which version we got the message from, like we do for serialization. I would note down this problem and not address it now.
TestShardRouting automatically provides some of the randomness that you explicitly do here.
why these changes? I think if we want to clean it up we should rather make it implement `Writeable` etc.
I think it might be nice to move this in `TcpHeader`
I'd probably stick the parser helpers at the bottom of the class.
We have a "isPartitionBased()" helper method to check this mode
I think s/" + "//.
nit: IllegalArgumentException ;-) You're checking the `settings` argument here, not the state of InternalTestCluster.
space missing between ) and {
Same here with regards to the test.
I think if the currentName is null we have a bigger problem. why do we do the comparison even? have you tried passing in fieldName set to <null> ? if the current token is a field name, currentName should not be null ever. I would treat that corner case differently.
If they are essentially the same thing then I wonder whether we should eliminate one of them completely? Anyway, I am fine with the current state.
Nit: `while(` -> `while (` (whitespace)
Nit: too many newlines here
nit: space after `connectTimeout`
Nit: there is extra whitespace in the `getVersion` invocation
Okay, I'm cool with that.
I think this should never happen; maybe: ``` final TransportReponseHandler handler = pendingHandshakes.remove(requestId); assert handler == null : handler; ```
Nitpicking - unused imports were added to this and other IndexActions.
Should we still log something here, in case `terminal.println` throws an IOException? Or the impossible becomes possible (hey, you never know with JDK9)
And by "log", I guess worst case scenario is `System.out.println("The impossible happened: " + ...);`
`rm` takes multiple paths, why not use that? It handles exceptions from any of them.
method refs ftw
Is this done already? Maybe specify what "sanity checks" means otherwise.
Maybe make "match_template" a constant for this test since its used in several places.
I'd apply this to all these methods.
add a private constructor so it is not possible to instantiate it. Maybe even make it final, although it is redundant with the private constructor.
can we pass in the supplier here too? No need to provide the whole parser.
Why do you need to remove this `.defaultOperator(Operator.AND)`. It looks like a test use case.
I am not sure about this one. I feel like if it's part of `RepositoryData` it needs to be in json. Otherwise it cause things like `fromXContent` with additional parameters. Would it be a huge deal to pass gen id alone as an additional parameter instead of adding it to `RepositoryData` itself. I feel like it's not really a repositories concern, if you see what I mean.
Discussed this with @abeyad more. It looks like it should be part of RepositoryData after all, otherwise the index generation abstractions is getting exposed on the Repository interface layer, where it makes even less sense.
You should also test `nextUp(-0f)` ?: ```` assertEquals( NumberFieldMapper.NumberType.DOUBLE.rangeQuery("field", -0f, null, false, false), NumberFieldMapper.NumberType.DOUBLE.rangeQuery("field", +0f, null, true, false)); ````
shortcut: VersionHandshakeResponseTransportResponseHandler handler = pendingHandshakes.remove(requestId);
Nit: `if(` -> `if (` (whitespace)
OK, thanks for the clarification
haven -> have
if `addToRoot` is true then maybe fail if target field is specified? Not a big a fan of silently ignoring this.
Use IllegalArgumentException for checkVersion functionality as well. It fits better than IllegalStateException I think (the node does not need to look at current state to reject serializing the message but just at the arguments).
I wonder if we can have a REST test that checks this exception. This could be done with a skip section of the form `5.2.0 - ` so that the test is only active in BWC tests where the minimum es version is < 5.2.0.
not true anymore
we can still verify that that node has the primary and fail if that's not the case.
why prevent it on primary? Sure it makes not much sense but it does not hurt
`current_node` might suffice (see reroute API)
indeed it is not happy about it
or maybe we have other tests in this class that already use other formats, in which case it's ok as is
I am adding a util method for this in XContentHelper, maybe worth replacing this later (should not block this PR)
Question - why not just get the connection from the transport service right here? does the supplier buy as something? maybe we can even try to get it first and create a new temporary only if it fails ``` Transport.Connection existingConnection = null; if (transportService.nodeConnected(nodeToSend)) { try { existingConnection = transportService.getConnection(nodeToSend); } catch (NodeNotConnectedException e) { // it's ok we'll create a temp connection } } if (existingConnection != null) { sendPingRequestToNode(existingConnection, sendPingsHandler.id(), timeout, pingRequest, latch, node, nodeToSend); } else { ```
I called these methods test*ToAndFromXContent() as they effectively do one complete roundtrip: toXContent -> fromXContent -> toXContent
I don't believe this is only kept for BWC. You use this to parse `_source` above.
I guess it could be renamed to isFalse() / isTrue() now
I think `"Failed to parse value [" + value + "] as only [true] or [false] are allowed."` is a easier to read.
nit: missing a space after the first comma
I don't think this is `@since` anything.
I think you use it indirectly through `parser. isBooleanValueLenient`.
Word wrap again.
Old indentation was better because it made it obvious that the conditions weren't part of the body.
Please rework this word wrap too.
Seems strange to split the string here rather than moving `Mapper.etc` to a new line
I think `value.length() == 0` here should be `String.hasText(value)`, so it checks for `" "` and uses the default value for it.
Does this need to be public, or can we make it private to this class and force everyone to go through the String version of the `parseBooleanLenient`? (I did a cursory glance and didn't see usages, but it's possible I missed some)
It's super minor, but I think we usually don't include punctuation at the end of exception messages (the `.`)
Also minor, but I think I'd prefer `node == null ? null : node.toString()` because it requires less negative-resolving in my brain, up to you though.
Maybe an additional arity version that didn't require the version and automatically looked it up from itself, that could save you some lookups (if you want)
You could add an assert that it's not ES 7.x here so we know to remove it
Ah nevermind, I see where we use it :-/
yep. it's just before the shuffle. got confused. all good.
I presume this is still in progress? (which is fine)
you evil person :)
++. Maybe also add a sanity check that a get on the doc at the end gives us what we expect? (deleted or found)
I usually prefer avoiding lambdas when it is possible, in that case that would give something like this: `Collections.sort(this.filters, Comparator.comparing(KeyedFilter::key));`
This worries me a bit as this is inconsistent with the filters and ranges aggregations.
I think we should return `null` if the key is not an instance of `String` rather than converting it
it is not pkg private
`Arrays.asStream(response.pingResponses)` would not materialize it
use `localNode.equals(incomingObject)` we know that `localnode` is non-null
if you wanna be more explicit you can use `Function::identity` to make sure it's not a typo
do we need this trace log here and if so can we fix it to say `temporarily` or something like this
just flip it then you don't need to negate
or when some docs match the query but do not have a value
you should run `gradle precommit`
this header is wrong
I don't like having both doEquals and innerEquals. Maybe just override equals to also include the format and still expect sub classes to implement doEquals? (Same for the hashcode)
We do this check in RestoreService.isRepositoryInUse. I am not quite sure what's the reason to repeat it here.
ok that should repro the previous behaviour without assertions enabled you mean? I dunno, given that we are touching how we read and write stuff, I would be happy to have proper bw comp unit tests, especially around negative numbers that we don't cover anywhere in our test infra I guess.
Oh, I'm fine with symmetry, I just wanted to make sure that I was reading it correctly.
I'd add something like `Prefer {@link StreamInput#readNamedWriteable(Class)} and {@link StreamOutput#writeNamedWriteable(NamedWriteable)} unless you have a compelling reason to use this method instead.`.
do we still want to warn here? Supposedly this only occurs when you uninstall a plugin that uses customer state, right? I think it'd be cool to log a warning.
Can it have a reading ctor? Those tend to do nice stuff like let you declare more things final.
I see the symmetry of having these two methods. I still wonder if it'd be as readable if you removed them and used the ctors directly.
do we need to handle NONE? I'm wondering this could happen if there is a segment that does not have any values for that field
I wish the API was more in-line with things like collectors and comparators, ie. `LeafCollapsingDocValuesSource CollapsingDocValuesSource.getLeafSource(LeafReaderContext context)`
Can we return 0 when the value count is 0 to be consistent with the singe-value case, or throw a proper exception? I am concerned this code could raise a weird error message otherwise.
Here's another reference check.
I don't like these reference checks to the empty list.
I'd rather we used a boolean flag to indicate closed instead of a special reference.
Could you explain why this needs to be public now? I think we should try to keep this package private if possible
Could you explain why this needs to be public now? I think we should try to keep this package private if possible
I wonder if it would be better to move towards supporting only `_key` for the time/term ordering since to the use this is actually what its doing, sorting by the key of the bucket? We could do this in a backward compatible way by using the`ParseField` and having `_time` and `_key` as deprecated values for the order. We could also solve the mismatch in `byte id` values by checking the version in the writeTo/readFrom methods and if the version is <6.0 making the code read/write the old id values and otherwise read/write a consistent response. This should mean that we can move towards only having BucketOrder and not the individual implementations.
actually I don't fully understand why we can't just do `this.order = order` all the time
I think it's fine this way. This PR is already good incremental progress.
`((NamedDiff<?>)value. getMinimalSupportedVersion()` should avoid the raw cast warning and be perfectly safe.
I think it is a bit weird to declare this as nullable but assert that it isn't null in a getter. Maybe it'd be better if you put a message here and on the other assertions like "getMinimalSupportedVersions should only be called on the write side but looks like it was called on the read side because minimalSupportedVersion is null".
But that day can wait.
It'd be nice to be able to change this to only take `ToXContentObject` one day....
Can we do all of these things (stripping last char, backslash escape and quote escape) in one pass with a string builder? Right now 3 copies are made of the string.
It is a bit odd that we are testing here the `ShardId#fromString` method. In fact I am not even sure this method is generic enough to belong in `ShardId`...
here, we should also have a case for "ca-central"
oh seems like you also removed some ParseFieldMatcher usages, nice :)
thanks for the help here too <3
We use `assumeFalse(Constants.JRE_IS_MINIMUM_JAVA9);` elsewhere, I don't think this is right since the version string no longer has the leading `1.`.
I worry that this may end up hiding some cross-shard issues for other queries. Perhaps this should be controlled via the tests that need it (possibly using DFS if that works?)? Since I'm not familiar with what Lucene's Graph query is even doing, I just wanted to put this out there.
can we add some java docs? the name to functionality transition is not trivial anymore
nit: the word "event" is no longer relevant
I wonder if we should just fold this into a single iteration: ``` for (final DiscoveryNode node : currentNodes) { if (discoveryNodes.contains(node) == false) { <-- probably should rename discoveryNodes to nodesToKeep or something like it) // remove logic } } ```
I'd switch the order of these so it matches the declaration order.
I think it needs another `else` block with `throwUnknownField(currentFieldName, parser.getTokenLocation())` here
It can be complicated to rebuild the object, how about doing something like: ``` assertEquals(parsed.getCause().getMessage(), "Elasticsearch exception [type=parse_exception, reason=" + originalMsg +"]"); ```
Instead, can we build what we expect given the exception? Isn't it just wrapping what we have into an ElasticsearchException with the same message? Or does it get too complicated? I think Tanguy did something similar in some other test.
shall timedOut also be a Boolean? maybe not because it's always returned while terminatedEarly is not? trying to understand the difference between the two
ok can you add alls these infos into this class
annoying :) I'd be nice if we could share it somehow
yeah I can see why I was just asking to put this info on the class so folks see immediately why we duplicate code
`like` -> `likely`
`like` -> `likely`
You're inconsistent in your use of periods (I prefer not having them but I don't mind either way).
The sequence number backwards layer won't be needed after 7.0.0.
Asserts are better for this. 
yes. I should have been more precise - the usage of SequenceNumbersService.UNASSIGNED_SEQ_NO is only relevant due to a BWC aspect.
A tiny typo in the test name : should be `From` instead of `Form`
@javanna Yeah, that's exactly what I have in mind.
given our direction (REST client vs transport client) I think we are fine rejecting `-1` on the REST layer. But how about keeping the `int`, using `-1` as default, and rejecting negative values in the setters? That's how we changed some of the request validation as part of the search refactoring too. This way the java api would be rejecting -1 too, which is nice, especially cause we are reusing the same requests for the high level rest client.
This method isn't needed since I think the boxing change should be reverted.
I think that this change that introduces boxing should be reverted. This means that we need a value to represent unset, and -1 is the idiom within Elasticsearch for that.
A better wording for the error is: `[from] can't be negative` Changing that will require updating tests
A better wording for the error is: `[size] can't be negative` Changing that will require updating tests
There is a problem with this test setup that I just found: the xContentType that is used for parsing here is not necessarily the same as the one that is used int randomUpdateResponse(). So the expected values might be off, e.g. if in randomUpdateResponse() SMILE is used and here xContentType is Yaml.
you are right thanks a lot for catching this
While I think a common methods would be great, I'm not sure this distinction (small vs. large longs) is needed very often. In the parsing tests its good to have some more "smaller" values to catch cases where they might be parsed back as int. I wonder how a common method would be called (randomNonNegativeLongButOftenSmall?), which range to sample the small values from (that might vary between tests) and where to put it. But maybe @tlrx has some suggestion.
++ I like that solution here
I think that `removedChannel != null && removalChannel` is more defensive (here we are relying on either always putting `Boolean.TRUE` in the map (which we do), or the fact that a boxed boolean always boxes to `Boolean.TRUE` (the JVM guarantees this) but I prefer to not rely on these fragile/subtle things).
That's even better.
s/The tasks has/In case the task has/
After seeing the mess that these kinds of "constants your children set" have caused in aggregations I'd prefer to have `shouldCancelChildrenOnCancellation` be abstract.
It is probably also worth mentioning when the runnable is called.
It feels weird to turn the nodes list in the cluster state in set of strings and then turn it back into a node on `sendSetBanRequest`.
But you are just fixing a typo so you can skip it.
`assertThat((List) filteredMap.get("array"), hasSize(1))` has better error messages.
This should be a `USAGE` error, not a `DATA_ERROR` (and the period dropped from the exception message).
I didn't mean to revert the changes made after SImon's review, I meant to revert the variable rename from key to header.
would you mind reverting the variable name change, at least temporarily? it confuses the review
this name is not good. there is no *error* involved. I think you should maybe name it `generateFailureXContent` or somethign like this
I don't like this. the reason is that when you grep for `es.index` you find nothing. I think we should never do this.
I would try and rename this one as well, I find it annoying that is has the same name as the ordinary toXContent.
I find it confusing with the `toXContent` coming from the `ToXContent` interface. Arguments help but I think naming is better now.
@jasontedor Thanks. I think `:` is a reserved char on Windows and if used in logging.yml but no node name is configured then it might fail the creation of the log file. But I don't think there's something we can do.
nit: shall we call this resultOnVersionConflict? also - it would be good to do the same in innerIndex
Nits: `settings` -> `setting` and `not to trigger manny` -> `to not trigger verbose` (it doesn't have to be exactly this, but at least fix `manny`)
Nits: `settings` -> `setting` and `not to trigger manny` -> `to not trigger verbose` (it doesn't have to be exactly this, but at least fix `manny`)
Nits: `settings` -> `setting` and `not to trigger manny` -> `to not trigger verbose` (it doesn't have to be exactly this, but at least fix `manny`)
It is all good!
I think it'd be easier to read if this were `ObjectParser` stuff.
after all, what would the parsed output of `SearchPhaseExecutionException` look like? I'm asking because that subclass prints out an array and potentially additional objects, I wonder how users would retrieve that additional info from the parsed ElasticsearchException.
nit: threadContext is not used
Is this needed? I think the observer takes care of this? If not, I can work on folding this into the observer as a follow up.
I'm about to change this in #22586 so that DeleteResponse/IndexResponse/UpdateResponse don't have to repeat all these checks. There will be a assertDocWriteResponse() method, and here we only have to check for UpdateResponse specific fields.
This situation feels very fragile to me. It means in the future if we do want a settings instance here because we want to control fine-grained logging settings for a CLI tool, we can not without accidentally reintroducing the problem.
This is missing a `+` between `newUsed` and `"/"`, so it's not compiling currently
we can remove the unused imports here (namely: `ActionRequest`, `DocWriteRequest.OpType`, `ReplicationRequest`, `ShardId` and `RestStatus`)
of course the problem here is anyways that TaskInfo is not a ToXContentObject ...
I missed this, I wonder if the XContentBuilder#field method should be rather made XContentObject aware by checking if the input `isFragment` and accordingly adding start and end object only when needed
no objections :)
shall we rather use assertToXContentEquivalent ? I am always cautious on comparing json strings (keys ordering etc.) but maybe in this case it's ok
I think negatively named settings are somewhat difficult to reason about. For example, I think it's much easier to understand that `atomicMove = true` is than `nonAtomicMove = false`.
Yes, I changed it in #22675
I think it's OK here because the token is a value, not an array. But later in the loop we need to take care of arrays. I think we can already do it for values and just ignore objects for now
nit: IOException never thrown
I'm OK with this, but it could have been fully randomized using XContentBuilder/XContentType I think
index -> delete
yay for stack traces
just a nit: maybe pass forceExecution before actualHandler so that the order is similar to the `registerRequestHandler` method
I think it should be: TransportAddress address = discoveryNode.getAddress(); TransportAddress expected = poorMansDNS.get(discoveryNode.getHostName()); assertEquals(address, expected);
 much better readable
should this be "not mounting...consistently" or "mounting...inconsistently"? But I would think not the current double negative.
This can just be `System.getProperty`? I thought the point of `BootstrapInfo.getSystemProperties()` was when you wanted to call `System.getProperties()` (which we have in forbidden apis)
@jasontedor Very minor typo, but should be `/proc/self/cgroup`
Not needed anymore.
Not needed anymore.
@rjernst Ping on that cosmetic thing. Not sure it is worth changing though. Up to you :)
Should we add a `default:` clause just in case here? It looks like the original code had one
I think I would parse headers more precisely into a `Map<String, List<String>>`, it shouldn't be a lot of code.
can we test what happens when one calls `setResources` passing in multiple ids? That's the one place where we need lists to work properly.
shall we check what the current token is? if it is a field_name we could use a while instead of a do while? that would be a bit more readable I think.
can we specify that only complex metadata (object and arrays) are not parsed? This will have to be updated though if we move subclasses to use addMetadata whenever possible, later. Also, I wonder if we can do better. We could parse this unknown info into a map, but then there wouldn't be a place for it in the ElasticsearchException, unless we make metadata `Map<String, Object>`. Let's think about it, it may make sense for the future, not now maybe.
Can we compare messages from a normal ElasticsearchException with and without cause, with ones obtained from a parsed one? I wonder specifically if the `Elasticsearch exception [` prefix is needed etc.
super minor nit: in general, when creating maps and lists and knowing the size you could pass it in.
the es. prefix here is a bit weird, but I don't know how to remove it without breaking bw comp. We have to take into account though that even if the info is available, no users will know how to retrieve it :)
you are still casting here and eventually calling toString on Objects. What I meant is manually parse these headers instead and make the parsing code a little more accurate. That also won't require to go through the map once again once parsed.
same as before: shall we check what the current token is? if it is a field_name we could use a while instead of a do while? that would be a bit more readable I think.
yes I know, I value consistency here, let's not support things only in specific cases. we will parse those back when/if we will support proper numbers.
is this needed here? I think it does something only when the current token is start array or start object.
you can probably cast it here and reuse it later
nit: here I would maybe use do while just to make absolutely sure that the first round always runs.
oh I see the trouble that I caused you, because not all of them are actually ElasticsearchException. Good test though, much more coverage now!
right they can iterate over them, that is acceptable.
Maybe we should at least parse List<String> given that we have that in some subclass? I wouldn't extend support for objects etc. here, but just for arrays of strings. that is what the addMetadata supports at the moment. I am not talking about supporting anything that may get printed when overriding metadataToXContent.
I was wondering what the difference is between an exception that is created not going through parsing, and an exception that gets parsed. Wondering about the message specifically.
can we maybe try to trigger this differently? I mean can we for instance try to call `#available()` or can we maybe read the first byte on open and wrap in a `BufferedInputStream` and then do this: ```Java InputStream stream = is.isMarkSupported() ? is : new BufferedInputStream(is); // do the following in doPrivileged? stream.mark(1); stream.skip(1); stream.reset(); return stream; ```
if we run into an exception here we have to close the stream. we usually do this: ```Java boolean success = false; try { // do something with the stream success = true; return stream; } finally { if (success == false) { IOUtils.closeWhileHandlingException(stream); } }
it's actually sen*t*
+1 too - I never noticed these tests...
I actually think these aren't needed any more, I think Boaz fixed the issue with these, they were removed from the `master` branch
yeah - this is the fix: https://github.com/elastic/elasticsearch/pull/22631/ which is already backported to 5.x
how about the root cause? I remember we discussed that it is a problem when we get more than one, but can we add a TODO for it somewhere or do at least something when we have a single one? I don't see where we parse them but I may be missing something. Maybe they end up skipped where we parse metadata arrays in `innerFromXContent`
++ for a checkedfunction
Same here... we don't really need `String[] addresses`
there is a method called `anyMatch(...)`. This will also make it return true on the first occurrence found
no need for if/else, just write ``` return getNodeDecisions() != null && getNodeDecisions().stream().anyMatch(...) ```
this needs to be fixed before merging (as it should go to 5.2.1).
I think that's pretty much illegal it should not throw any and it should be handled internally
I think you can move this to the inner catch then you don't need to remove it from the connectedNodes....
I feel like we might get a working solution by adding something like `XContentHelper.convertToJsonFragment(source, maxFragmentSize);` that would construct a XContentBuilder by passing it a modified `BytesStreamOutput` that would through an exception when it reaches a certain size, then we can intercept this exception add "..." at the end and return it as a string.
I like including the original size here. Maybe instead if the source is chopped it should read like `first 2048 characters out of 10122123: _slice_of_the_source_`.
nit: move this to the line above.
@bleskes was talking below about using only the index, source, and id for the task description. The toString change would still be nice, but less important then. That seems like a good choice to me.
> talking below about using only the index, source, and id probably a typo, but just in case - index, type and id (not source)
@sabi0 I was thinking that if you override writeBytes instead of writeByte and ignore entire content of the method it should work, but it's a moot point now because I think @bleskes's idea of not including source at all is much better.
> And only print the message like "Source is too big - 5kb" +1 to that. Keep it simple
what about `"Source is too big, max length [2048b] but was [XXX]"`
yeah I used `...` so explicitly `source[n/a actual source size: [12345b], max source length: 2048b]`
I'd be more comfortable if this generated random unicode instead.
just keep the change simple that extra allocation
we should be careful here and check for sources that are binary (SMILE etc..)
then do something like this `source[n/a max source size: ...`
I don't think we need the exact number of bytes and if bytes is what we have we should use it. No reason to work hard to get characters.
Nit: it would be great if the loop structure could be changed somehow so these "continue" statements wouldn't be necessary. Not sure if this complicates stuff though. Also I think explicitely consuming the status field value here would improve readability a little bit.
Nit: in other places advancing the parser is directly done in the "ensureExpectedToken" call. Saves one line and I think it is equally readable. There's several places in this method where this might be possible, not sure how much lines this saves. Also, if you don't like it, just a matter of tase I think
Nit: I think you can remove the itemSupplier and call builder.build() instead later.
Is this is necessary given we loop over OpType.values()? If other values are added in the future we should fail because expectedBulkItemResponse/originalBytes is not set anyway.
What was the reason again why we cannot simply compare the parsed objects xContent representation to the original? I forgot, seems to make this test a bit more complicated than I thought.
i don't think you need to save the currentName to a variable here, this can be a single `if (token == FIELD_NAME && STATUS.equals(parser.currentName()) == false)`
ignoring the status here makes sense to me, but I wonder if we are 100% sure that the status returned by `ElasticsearchException#status` (which looks at cause etc.) will always be the same as what we got back (either as status code or within response body).
I think you can just initialize to null
Since this is effects all response that inherit from BroadcastResponse - can you check that all of them do the right thing and filter shard not available failures? might be good to add an assertion in the constructors, looking at all the exceptions this get.
this logic belongs in transportWriteAction
why do we need two of these? can't we have just markShardCopyAsStaleIfNeeded? we can rename `markUnavailableShardsAsStale` to `markUnavailableShardsAsStaleIfNeeded`
this is an inner class anyway, why pass these along? If you want to make it a static inner class, I'm good with it, although I'm not sure if it's worth the extra clutter.
we plan to use this for background operations like the global check point sync, where it's not a big deal if this happen. I don't want the logs to have warning in them. Instead, implementations (i.e., TransportWriteAction) of `failShardIfNeeded` can log a warning if they are going to fail the shard
I would go with a T suffix - i.e., RequestT
I think 5xx (server error) is worst than 4xx, but let's just go with the first.
This seems to work fine for me? ``` diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java index db037ad..2cd1d8e 100644 --- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java +++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java @@ -1040,7 +1040,7 @@ public abstract class TransportReplicationAction< * shards. It also encapsulates the logic required for failing the replica * if deemed necessary as well as marking it as stale when needed. */ - final class ReplicasProxy implements ReplicationOperation.Replicas<ReplicaRequest> { + class ReplicasProxy implements ReplicationOperation.Replicas<ReplicaRequest> { @Override public void performOn(ShardRouting replica, ReplicaRequest request, ActionListener<ReplicationOperation.ReplicaResponse> listener) { diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java index ac79a8c..8a168ca 100644 --- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java +++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java @@ -74,7 +74,7 @@ public abstract class TransportWriteAction< @Override protected ReplicationOperation.Replicas newReplicasProxy() { - return new WriteActionReplicasProxy(shardStateAction); + return new WriteActionReplicasProxy(); } /** @@ -335,26 +335,7 @@ public abstract class TransportWriteAction< * replicas, where a failure to execute the operation should fail * the replica shard and/or mark the replica as stale. */ - final class WriteActionReplicasProxy implements ReplicationOperation.Replicas<ReplicaRequest> { - - private final ShardStateAction shardStateAction; - - WriteActionReplicasProxy(ShardStateAction shardStateAction) { - this.shardStateAction = shardStateAction; - } - - @Override - public void performOn(ShardRouting replica, ReplicaRequest request, ActionListener<ReplicationOperation.ReplicaResponse> listener) { - String nodeId = replica.currentNodeId(); - final DiscoveryNode node = clusterService.state().nodes().get(nodeId); - if (node == null) { - listener.onFailure(new NoNodeAvailableException("unknown node [" + nodeId + "]")); - return; - } - final ConcreteShardRequest<ReplicaRequest> concreteShardRequest = - new ConcreteShardRequest<>(request, replica.allocationId().getId()); - sendReplicaRequest(concreteShardRequest, node, listener); - } + class WriteActionReplicasProxy extends ReplicasProxy { @Override public void failShardIfNeeded(ShardRouting replica, long primaryTerm, String message, Exception exception, ```
I'd rather not introduce more code paths. I would vote to always have the callback pattern. If people have nothing to do they can call onSuccess on the same thread.
I'm curious why we have getValues if all of these classes already extend list? Nothing to do with this PR, just something I noticed: since we are looking at breaking changes in the future, maybe this could be cleaned up too (either extend list, or have getValues())
Can we just keep it simple here and eagerly allocate the array to full size (and fill it completely) above? These should be small sizes (single digits). I don't think we need to optimize as if this array could be hundreds of elements.
what about splitting first and then checking whether the array contains an empty string? I think that would also cover two other cases we are interested in validating: leading dots and trailing dots? (just thinking out loud)
this should be an immutable set...
I'm fine with it, given the explanation. I only saw that it seemed to be a variable name change that was unnecessary.
that's odd, the root cause is really useful only with SearchPhaseExecutionException. But I don't think we can do better than this ATM.
Please revert the name change. It's fine as is, and the name change here isn't thorough enough (there are methods invoked here that still have the parameter named `pluginId`).
No, it should stay "id" in the message because plugins are installed by id (with the exception of some special plugins that can be installed by name only). Yet "name" is fine for removal because plugins are removed by name.
It is called pluginId in install because it is an identifier, which _may_ be a plugin name, but it also may be maven coordinates or a url.
No need to squash, we can do it on merge.
does using object parser cause us to be strict with potentially newly added fields to the response? Maybe it is time to investigate what we can do to make our response parsers more understanding (still having a setting that allows to enable a strict mode). I am asking because I remember I added `cluster_uuid` to this response with 5.0. In general shall we start not expecting fields that may not be there, and being more flexible with fields that are there although unexpected.
good, can you set that flag and test that it does the job? We could work as a followup on making this behaviour configurable. What happens with fields that we expect but are not found (Imagining using a client in 5.4 and e.g. es in 5.1 that may have less fields in its response).
Since we use the key string in RestMultiSearchAction and RestSearchAction and its use is somewhat related to the search API, maybe we can add it with some explanation in one of those (e.g. RestSearchAction)? Just my 2 cents though.
Also it would be good if the separator was a constant somewhere so we can refer to it when splitting the type/name pair while parsing.
I would rather define a protected method that only returns the prefix (`getType` ?), whose default return value is `getWriteableName()`. Otherwise subclasses can potentially override the separator used (or even whether they include the name of the agg or not).
I think adding a constant somewhere for # makes sense. It may make sense to also add a constant for typed_keys, but I don't have a strong opinion on that.
Ahh. I completely missed the parenthesis had been added.
I think it might be a useful to repeat the same message you added to `_all` field here. Something like `"As a replacement, you can use an [copy_to] on mapping fields to create your own catch all field."`
This test should assert that the headers are correct.
I am ok with what you propose Nik!
I would leave this like how it was before without this extra internal class. It's not actually used in any new places outside of Painless from what I can tell (maybe I missed something), so these things are literally just part of the PainlessScript still.
The `routingAllocationWithOnePrimaryNoReplicas` method no longer needs to take a `Version` parameter, would be nice to get rid of it.
Nothing wrong, just the collateMatch default value (`null`) when it's missing is defined at 2 places instead of one
Can you revert this? I think it's fine to have ctor->ctor->ctor
java doc this pls, most notably the return value..
given that the suffix is also known before we go and provide index, type, and id... (even in case of e.g. _search) I wonder if we can get rid of the list, and just provide parts and suffix as constructor argument, then convert it into string straightaway. Not even sure we need a builder for this.
Maybe we can replace calls of this helper with the new Endpoint class? Might get a tad bit longer in the end, so I'm fine either way.
Nit: maybe move this up next to the other methods (ping/exists/get) that output new Requests.
Scratch that, I see this is private and only used internally, and the request only has one of each parameters, so duplicate calls are unlikely. Just me being paranoid.
I don't believe its necessary, ++ for taking it out
Good call @jpountz.
I don't think that would work since Map.get always takes an object, regardless of the type of the keys.
Another reason why I think we should remove the Void context argument if we can, passing this null value is confusing
It would be nice to have some doc that indicates that it's going to be parsed and stored in a temporary map before being created using createFromMap() method, and the reasoning around why we do this. Also, maybe we could rename to MAP_PARSER? Just an idea.
Can you at least change it to: ``` @SuppressWarnings("unchecked") private static <T> T get(String key, Map<String, Object> map, T defaultValue) { return (T) map.getOrDefault(key, defaultValue); } ``` I don't want us to reimplement core stuff just to avoid an explicit cast
+1 on removing the `Void context` from all methods. The `declareInnerHitsParseFields` is already complex to read I think, that won't add much.
Right... but I'd be happy if we could unit test this, and if we do then we need to ensure the object start.
Sorry, I just saw that you remove them already, thanks!
We don't need to use this local ref
You could use `map.computeIfAbsent()`
Just a thought: maybe instead of referring to the Delimiter in Aggregations we can have an own constant in Suggest and either declare the same symbol there or delegate to InternalAggregation.TYPED_KEYS_DELIMITER? The delimiter doesn't need to be the same in aggs and suggestions, only the parsing part needs to refer to it (although its good to have the same like we do now I think)
I think we should only make the change for total until we see evidence that adjustments need to be made for the others.
Nit: `}` is in a funny place.
nit: I am wondering if using "true" and "false" would be more readable, simply because I always forget if it's lowercase or uppercase.
Maybe throw an exception here if partition criteria supplied? Partitioning not supported when using regex-based filtering.
`randomValueOtherThanMany` might make this easier to read.
Let's maybe reorganize this code a bit for clarity? Something like below: ```java if (fieldType.tokenized()) { ... } else if (is keyword) { ... } else { throw IllegalArgumentException } ```
It should fit on 1 line
I think some of them could be private
I was just wondering if it's IDE configuration. does everything compile and build without the suppression? Asking because I am not sure we generally add those type of suppressions, but I may be wrong.
Could we put the check here so that it is run for both the transport and rest APIs? When put here we should make it an `IllegalArgumentException` and we should update the other Range Aggregation Builders too.
Since this PR can go to 5.x can you add a check on the version, like this: ````` if (in.getVersion().onOrAfter(Version.V_5_4_0_UNRELEASED)) { boundaryScannerType(in.readOptionalWriteable(BoundaryScannerType::readFromStream)); } ````` This will ensure that a node in 5.4 can safely send request to a 5.3 node.
and the last one ;)
this makes our code checker unhappy since enum are always static in this context.
this fails the build, the `toString` is not correctly applied
The merge of the boundaryScannerLocale is missing
I don't mind as long as we use `writeString/readString` and `writeOptionalString/readOptionalString` consistently. So you can maybe just change the `readFrom` to explicitly use readBoolean.
I think it's too lenient, we should fail if the type is unknown.
if randomInt() returns -2^31 then docCount will be negative (since the absolute value cannot be represented as an int in that case)
the Test annotation is banned in our project :) Something should fail when running the build from gradle. if the test method starts with test we are good, no need for the annotation
we should escape expectedWarningHeaders, or alternatively unescape when we read.
You can use `ESTestCase#assertSettingDeprecations` here.
Can you call this something other than `total`? It's really hard to read things like `total.total, total.total, total.total + total.total`.
It becomes part of the public API.
Sadly yes... We need to use `BaseCompositeReader` in the method signatures of our field data code instead of `DirectoryReader`.
I opened: #23338
I think you can use `RestClient.SyncResponseListener` here instead
add a short method that checks the content type and get rid of the list? If this class becomes too big I am good with getting the bulk part out. some of this methods are going to be useful for msearch too at some point.
it also supports smile. should we validate that the format of all the index/update requests is the same and use that one? I wouldn't want to make things too complicated though given that most likely nobody uses smile here. People may also theoretically be using cbor or yaml in the single index requests now using the index api, as they don't have to go through REST.
this code block is repeated and always deals with index requests I think. We can probably factor it out to a method.
maybe we should throw illegal argument rather than illegal state.
FYI we don't throw version conflicts on replicas any more... (not related to your change)
I wonder (though not sure) if we can check for the existence of a seqNo and that's it.
this is typically called `prepareX`
You could just make this construct the InstanceProfileCredentialsProvider here, and have no ctor args.
Can we make this 1 hour? If it times out it's nice to get thread dump
No, it's OK, thanks
Change this to `// tag:example[]`
Add a `.` to the end of this line.
Okay, I was only suggesting that if we don't support it on the parsing side, we might as well remove it everywhere (it's use in CustomSuggester, making Suggestion class abstract, removing from transport layer etc...) and then it would probably be better to do this in a separate PR. I'm okay with not supporting the parser.
Should we keep support for this "generic" suggestion type as long as it can (in theory) still be instantiated? I'd be in favour of making it abstract and removing it from the transport layer (including support for its use in Custom Suggester) on master in a follow up PR after re-checking with @areek that we realy don't need it.
With my limited knowledge of suggesters, I tend to think that we can remove the "generic" parsing case: we're not sure if custom suggesters really need this and even if it's the case a custom parser will have to be registered in the named xcontent registry anyway.
This method looks much simpler now 
probably `new ThreadPool(Settings.EMPTY)` :) I wonder if the helper could hide the creation of the thread pool even, maybe not, but we will see that later. As you pointed out it may be that the helper is not needed at all.
I may have forgotten, but what has changed here compared to the startFlush method? We don't call daemonThreadFactory as we dropped the name and settings requirement for logging right? I wonder if we should still call that and just provide the standard "bulk_processor" prefix.
should it not be required? If so I would move it to the constructor of the Builder and remove the setter for it.
for the record, I think it is ok to remove setName here. the name was only used to have slightly different logging for the flush daemon thread. We can obtain the same now that we use the thread pool in the same way we do everywhere in this class, through node.name in the settings.
can we have two static helpers that allow to create the processor either providing the Client or the RestHighLevelClient ? I am thinking of users, there are many existing usages of BulkProcessor out there. I may be ok to change the way it gets created, but as a user I would be surprised to have to pass in a method reference. That should be more of an implementation detail.
nice, effectively this change will be backwards compatible for users using this method. Not for users calling the public Builder constructor, but I think that is a good reason to make them switch to this static method.
I think I would consider taking settings out. We try to extract node.name from it as far as I understand, which I don't think is ever going to be set on a transport client. Maybe we can just live without that part of the thread name and remove some complexity. This may very well come from the times where node client was widely used, hence node.name made sense in the past, but it doesn't anymore.
got it thanks.
got it thank you.
you can replace these two lines with a call to `ThreadPool.terminate`
A `Collections.newSetFromMap(new ConcurrentHashMap<>())` would be more efficient IMO.
I haven't thought much about it but this is not an easy problem indeed...
By keeping track of contexts in 2 different data-structures, I think you are potentially introducing race conditions, eg. if a clear scroll action is issued concurrently with an automatic release of the context due to the fact there are no more hits to process.
docCount is a a long so it's totally fine, sorry.
I wonder if this new method/accessing the char[] is something we should just deprecate immediately? Long term I don't think eg we should not be using the keystore to store a password for another keystore; the contents of the other keystore should be moved into the elasticsearch keystore.
I also don't think a copy should be made for this example. The secure string should be closed after using it to decrypt the keystore.
> think we should skip this change altogether yeah. Let's just drop this. Thanks for trying..
I know we talked about it but reflecting on this, I think it's too implicit to wrap things here. Better be explicit and put this as a parameter type and let the caller be aware and decide.
I think this block and the following one could be reordered to get the same effect? ``` if (exists(secondary)) { return get(secondary); } if (fallbackSetting == null) { return get(primary); }
You can use `ESTestCase#assertSettingDeprecations` here (and throughout).
I don't think we should add new settings like this which are unsecured. This and `account` above should be secure settings.
That plan concerns me, as it means there is the (however distant) possibility these settings get released before being converted to secure settings.
I think the message is a little deceptive, because it is not account, but the client name? And in the backcompat case, I think it may be confusing, because the client name is fake (ie primary or secondary, but a client configuration by that name does not actually exist).
Can we deprecate and remove this one too? I don't think we should have global settings like this.
please do `Long.compare(second.docCount, first.docCount)` instead of multiplying by `-1`
nit: please cap this at 72 columns.
There is a tab (`\t`) here, should be converted to spaces 
Ah, you have enough in CustomUnifiedHighlighterTests, I think.
nit: extra space
Yeah, I'd make these a hard check and throw a `new IllegalArgumentException("Usage doesn't look like UnifiedHighlighter")`.
I *really* don't like `BreakIterator` for this. It is a much wider interface than the highlighter needs. But it is what we have to work with. So we shall!
what to do here otherwise? not really likely to happen that it's not a number I guess... maybe leniency is good here
The reason should be more explicit about why this needed.
Nit: `findHostName` -> `getHostName`
Can this whole block be replaced by: ```diff diff --git a/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java index 95619b1d40..d65686e356 100644 --- a/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java @@ -122,11 +122,7 @@ public final class HdfsRepository extends BlobStoreRepository { UserGroupInformation.setConfiguration(cfg); // Debugging - if (UserGroupInformation.isSecurityEnabled()) { - LOGGER.info("Hadoop Security Is [ENABLED]"); - } else { - LOGGER.info("Hadoop Security is [DISABLED]"); - } + LOGGER.info("Hadoop security enabled: [{}]", UserGroupInformation.isSecurityEnabled()); UserGroupInformation.AuthenticationMethod method = SecurityUtil.getAuthenticationMethod(cfg); LOGGER.info("Using Hadoop authentication method : [" + method + "]"); ```
Let's make this an `UncheckedIOException` too.
Remove and create again is not needed I think
May be randomize the container name as we do in `AzureSnapshotRestoreTests`? ```java private static String getContainerName() { String testName = "snapshot-itest-".concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT)); return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName; } ```
I think this is not needed. It should use the default account available.
I think this is not needed. It should use the default account available.
You can use `expectThrows(MapperParsingException.class, () -> { /* code that throws */});`, then check the message on the returned exception.
use a try-with resources for the parser value
use a different variable
space between `if` and `(`
I think it would be good to use `XContentParserUtils#ensureExpectedToken` or `XContentParserUtils#ensureFieldName`
Please use a space between the `if` and `(`. Also, I think it would be good to use `XContentParserUtils#ensureExpectedToken` or `XContentParserUtils#ensureFieldName`
Change "param required" to "parameters are required"
space between `if` and `(`
can you add a space between the `if` and `(`
same here with removing retry logic
same here with removing retry logic
same here with removing retry logic
same here with removing retry logic
number of retries should be removed as a constructor arg
we should get rid of the retry logic here entirely, there is no need for the while loop when we aren't retrying.
same here, all retry logic should be removed
same here with removing retry logic
thanks for adding this
We have other analysis components that also use Java regexps, I don't think we need to block this one.
Do we want to be exposing more java.util.regex stuff? We've been trying to be careful with that package. Is there a version of this that take Lucene's regexes? You can limit the time and space complexity of those....
this threadppol name makes not much sense
s/Defaults to/Implementers should default to/
If we are going to reformat, it should be within the 100-column limit.
If the line fits within a 100 columns, there is no need to reformat it.
The indentation is off here.
We can drop everything after and including the comma.
I think that the method signature should change to not throw a generic checked exception, but instead be explicit about the only two that are present here.
There's a typo here, it's `IOException`. This typo would fail the build. Please run compilation before submitting.
We can remove the period here, this is a sentence fragment (as these usually are).
This is not the right condition, a plugin bin directory is not required to exist.
We can drop everything after and including the period.
Let's remove the period from the end of these, they are typically not sentences.
In fact, it should probably say something like `Remove the plugin specified by {@code pluginName}.`
These are not the only exceptions that can be thrown. There a bunch of situations in which an `IOException` can be thrown. It's probably worth changing the signature to reflect this too.
While you are here it is probably worth mentioning that this will print the the command's usage info. The other might not, I didn't scroll down and check because I'm on mobile.
Name of the plugin to remove.
I'd just say "Remove the plugin named {@code plugin named}".
`pluginName` should still be wrapped in a code block (`{@code}`).
This should be `pluginName`.
Let's drop the uppercase on `Terminal`.
I think we can state this more generally as it's thrown if any I/O exception occurs while performing a file operation (of which there are a few).
This `,` is optional in English. I don't tend to include it most of the time so I wouldn't change this.
I think this should be: "we have an empty index is only a segments_1 file *so* we can't tell"
operation can be `final`
`originalLocation` param is redundant
nit: extra line
Just as a note, LuceneTestCase (which we inherit from) offers a nice helper to shorten these try/catch constructs, it goes something like this: ``` IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ...do Something...); assertThat(... some assertions about e...); ``` I see that the rest of this test also still uses try/catch for most exception tests. This is okay as it is, I just wanted to mention it.
the enum values make no sense in the context of the name. I wonder if we should call it `OperationAge` or something like this.
the semantics of this method are weird. I pass in an operation and it returns OLDER if the given operation has a higher version? I would have expected the opposite. This semantics also make the rest of this method hard to read since it has to negate the return values in 2/3 of the cases. I think you should flip it.
Could be `contentType = scriptMetadata.getOptions().getOrDefault(Script.CONTENT_TYPE_OPTION, DEFAULT_CONTENT_TYPE);` And then you can remove the null check below
Same feeling here about how I'd much prefer to have this extracted to a class, where you pass in the parts that are needed, if possible.
Please add a string message about the context registry being null
With all these nested `null` checks it makes me with we used `Optional` more places in our code :-/
Would be nice to see this parsing code pulled into a function or helper on the parent class so it doesn't need to be the same in both implementations
I wonder if we should wire this to `org.elasticsearch.index.translog.TranslogWriter#getHeaderLength(int)` with a static constant somewhere.
Right but the only reason I suggested to test it here is that if/when it gets implemented this test will fail and will serve as a reminder to whoever implemented it that the test needs updating. I don't feel particularly strongly about this so its up to you but I think it might be useful.
I tend to prefer letting the test throw the ClassCastException rather than asserting this first.
you can do `indexWriter.getReader().close()`, which will create a segment too but not do a fsync
I think that should be s/i/(bucket << p)+i/
Above this, we don't look at whether we need to do the mapping update but let it to `updateMappingsIfNeeded` to decide. I think it will be clearer if we capture this as a boolean and use it to call `updateMappingsIfNeeded` (now renamed to `updateMappings`).
shall we test POSITIVE_INFINITY too? seems like we return null for that too but the default value once parsed is only one...
ah here it is... I guess it's just a matter of picking positive or negative and getting over the fact that we can't have both. null would be better here but we can't have it.
I think that these log parameters are backwards.
I don't understand wrapping the field name in backticks like this is markdown? (And a many other places.)
I think all of these can be made final.
I'm not convinced in th soundness of the bookkeeping here. I think that `totalNanos` can include the total task time for more than `tasksPerWindow` tasks.
can we detect this rather than catching this exception? I'd feel better about it if we could
As @s1monw would say, "you have 140 characters, use them". 
You can use the diamond operator here.
Okay, I see later that we catch the overflow exception and that we skip adjusting; I need to think about this.
You can use the diamond operator here.
Nit: `automattic` -> `automatic`
This is really unfortunate, it establishes another volatile read on every offer on the queue.
Yet it's reverted now. 
This can all fit on one line.
I think this can all fit on one line more cleanly if you break after the equal sign.
I think that rather than copying this code from FixedExecutorBuilder, this logic should be pushed up to a package-private static method in the super class.
I'm not convinced we should ignore failures. These tasks still occupy queue capacity, and there is no guarantee they failed quickly, a thread can be executing for awhile before failing.
You could use SetOnce if you want protection from overwriting.
I see, it looks like the log message is written as if the size should come after "queue size" and the thread pool name should come after "threadpool: ".
It looks like this could fit in 140 columns.
These always read clearer to me as `<= 0`.
Maybe we could make `capacity` private again and have size-blocking queue rely on `#capacity()` which can be protected and overridable in resizable-blocking queue where the backing `capacity` field can be volatile. You might think it's an odd change, removing a volatile read for a method invocation, but this will be a biomorphic call site so the JIT can inline it.
I wonder if a wikipedia link or something would be useful.
make this a hard check please
please assign the `System.nanoTime()` to a local var that way you only need to read it once and you have consistent values.
this file can go back to 140 chars as well...
I think we should add custom validators / parser to make sure that `min <= max` at the settings parsing level. I know they have a cyclic dep. So I think you need to create two instance of each for the validation and for the actual registration, I thinks worth it.
I think this is right, we don't need it. It might be that we introduced the key sorting later, I remember that the main concern was reproducibility with the test seed, but that should be taken care of by sorting the keys before shuffling.
I think this block would be simpler as: ```java if (accessKey.length() == 0) { throw new IllegalArgumentException("Missing access key for s3 client [" + clientName + "]"); } if (secretKey.length() == 0) { throw new IllegalArgumentException("Missing secret key for s3 client [" + clientName + "]"); } final BasicAWSCredentials credentials = new BasicAWSCredentials(accessKey.toString(), secretKey.toString()); return new S3ClientSettings(...); ``` Instead of the nested if statements and double-negative-ish `if` statements
connec to to -> connect to
It's too bad our settings don't mimic the `Map` `getOrDefault` to make this one operation
There should be an extra space between the `if` and the `(`, and the `))` and the `{`.
1. There is a minor typo/grammatical mishap here - text should read "[cluster.name] must not _contain_ ':' character" 2. Id consider putting this exception text into a final static variable somewhere it would make sense to put it. This text is currently used in two places in the code - once here, and once in a unit test - and the way things are now, if you want to change the contents of this text, you need to change two strings in two different places in the code. If you had this text in a final String variable, and you referenced that variable here and in the test, you would only ever need to change the string in one place.
Duplicating the string is fine, the maintenance here is low as this string is not going to be changing, and the lack of indirection keeps it simple.
Also, I think that the exception message should include the offending cluster the name. The pattern that we usually use is ``` [cluster.name] must not contain ':' but was [<offending cluster name>] ``` This is another argument against introducing indirection here.
There should be extra spaces here, between the `if` and the `(`, and the `))` and the `{`.
Ah - I see the other side of it. Up on line 925 I thought you were building a LinkedHashMap from a HashMap rather than casting. Up where you call `parser.mapOrdered`. Anyway, `mapOrdered` should make this reproduceable like the `Collections.sort` was trying to do.
I think we should keep the `Collections.sort(keys)` part to keep the reproducibility between different jvms if we can.
can you fix the indentation here? all parameters do not seem to start on the same column
nit: hasFields ? score docs are always sorted ;)
I suspect that some aggregations could be grouped under the same parsed aggregation implementation, so we won't really have a 1-1 relationship between the internal agg and the parsed agg. Like an aggregation of type "sum" (ie InternalSum) and "min" (ie InternalMin) can be parsed back using a same `LongSingleValueParsedAggregation`. In definitive I'm not sure we should add the getType() here or also in the Aggregation interface.
Do we really need to also duplicate the typed_keys logic here? Can't we just print out the name of the parsed aggregation (it can be initialized with type#name)
I understand. But this requires to grab back the type and delimiter where initializing the parsed aggregation directly with the name "type#name" would allow to parse back the result too. Also, in a client side point of view, the name is "type#name". But I'm nitpicking, we can change this later if we want.
I also like "ExternalAggregation" but I'm OK with ParsedAggregation too.
If we are going to load the class directly (i.e., via the class itself) there is no need to use `Class.forName`, you can just say `final Class<?> class = HttpAsyncResponseConsumerFactory.class;`.
This is weird as this value is not returned with the response. But we can hardcode the formatter because this agg doesn't support format? I am a bit confused. Ideally we wouldn't have to implement this at all.
as I said above, that makes sense, I adjusted it on our branch that way.
I pushed a very similar change to our branch. There was a discrepancy between empty and null handling in this class compared to InternalAggregation, I aligned it to InternalAggregation. this should not be needed anymore here.
nit: reactivate randomization
Instead of providing the instance parser by overwriting this method in the subtests, can subtest simply provide their own xContentRegistry with the appropriate parser by overwriting `getNamedWriteableRegistry()` from AbstractWireSerializingTestCase? I might be missing something though.
I believe this can be provided by overwriting EsTestCase#xContentRegistry().
I think I saw this in Christoph's PR too. Hopefully you don't need it.
Sorry, my bad. What I meant was overwriting `xContentRegistry()` from ESTestCase, I mixed that up. That way you can e.g. provide the parser that you need in the test without the need of instanceParser(). I don't mind either way, feel free to use it or not.
not sure why this is still mark unreleased, we have already released alpha1, I think you should use alpha2.
yea I think it would be good to have a test that makes sure that the index expression specified in remove_index resolves against indices only, rather than aliases and indices. I don't think the current test does that.
nit: can you use assertEquals rather than `assertThat(..., equalTo())` ? I know we do that in quite some places in our codebase but we seem to be preferring the former over the latter nowadays.
can you expand IndicesOptionsTests#testSerialization to pass in also the ignoreAliases flag when creating the random indices options and check that they get written/read correctly. You will need an if based on the version there as well.
sorry I was almost sure I saw a change in GetAliasesRequest too :) I was wrong.
add a whitespace after the if and before the parentheses
oh good point, maybe leave that as the last change then so it doesn't mess up the diffs for review :)
please give us messages for the assertions
same here please add a nice constant that is human readable
can we get a constant here instead of the plain number
would be great to assert on the number of docs recovered. It's hard but I think it's good to keep a tight control of this one - it's complicated code and we need to know it keeps on doing what we expect it to.
I like the idea of the predicate. It could for instance be something like `registry.supports(name, Aggregation.class)`. This way we could also support ignoreUnknownFields for forward compatibility
but we do have to do both right? the logic should be: check whether the agg is a registered one by type, where the type is part of the name (type#name). If not, ignore the field (for forward compatibility).
this example is not that close to reality. we cannot really parse such an agg as we need its type in the name (type#total_number_of_ratings). That is what makes the field not unknown, which is why I am not convinced that the mechanism should be a match all.
do we need == true ? :)
This should also require that `this.matchFieldPredicate` is not already set. For now, let's limit ourselves to only supporting one of these on each parser.
I think it'd be more right to say that this tests support for parsing unknown fields.
I am trying to think if these can be a single field rather than two that are so tightly coupled. I couldn't come up with a good idea but maybe you will...
BiPredicate<String, NamedXContentRegistry> ? is that ugly? :)
I'm not super comfortable making this. I think maybe instead we should add the match skipping at the `FieldParser` level. Maybe some kind of subclass that skips or something. Not sure.
I'm bad at naming things but I'd call this `fallbackFieldParser`
I find it confusing to have a fallback/unknown field parser and still be able to ignore unknown fields. I'd make these two mutually exclusive, ie not possible to define an unknown field parser if `ignoreUnknownFields` is set to true.
I think we should not set the the dynamic flag here, because this setting isn't actually updated at runtime in `PipelineStore` (it is an immutable property). Or instead we should make it a setting that can be setting that can be changed dynamically at runtime. (by using `ClusterSettings#addSettingsUpdateConsumer(...)` )
Double negative. 
Oh, I misread it (mobile phone, sorry). The only thing that needs to change then is file:// -> file:/.
Also I think it is one slash only, annoyingly.
So it's starts with and (ends with or ends with).
This isn't quite right. Wrap the ends with checks in parentheses.
that's just a suggestion, I tend to do thsi this way: ```Java Runnable toRelease = () -> {}; //... semaphore.acquire(); toRelease = semaphore:release ``` that way you don't need to check any boolean logic and can just call the runnable
does this need to be public and also does this class need to be subclassable
How do you feel about reversing the negative? I feel like `environment.defaultPathData() == null ? 0 : 1;` is easier to read since it's less like a double-negative
my personal option is that we should deprecate staring multiple nodes on the same datadir altogether. that will let us drop this and it will be much simpler? That is the future PR that I am seeing here. if somebody wants to start another node they need to give it a dedicated data directory. I know we will have all the crazyness about out of the box experience... we will deal with it.
Nit: `based` -> `based on`
It's so nice to finally see this arriving.
I know it was like that before, but we are here now. 
I know it was like that before, but we are here now. 
oh please give us a message here what field it is... it will save some CPU cycles if shit hits the fan
Can you add more randomization here. You can build a random list of field names for the includes and the excludes for instance.
Maybe name it `UNCONFIGURED_LOG_CATCHER` or something.
please wrap in {}
please wrap in {} and use `== false`
please wrap {}
please wrap with `{}`
can we call this `getNodeLockId()` pls
you have 140 chars use them :)
I'm doubting if we should introduce generics for the BatchingTask and batchingKey. I'm fine with keeping as is if you don't like the generic version.
Nit: +1 to Jason's suggestion
oh yeah oh well :) fair enough...
I wondered if there was something better than iterating too but there's not since `IndexWriter#getLiveCommitData` only returns an `Iterable`.
hmm not sure, it does seem a bit weird to be there so I'm leaning towards "remove it". It doesn't feel like a great idea to be running numeric aggregations on a boolean field and I don't know if its something that works by design or just happens to work right now and might break in the future. /cc @jpountz who might have thoughts on that
I think it just happens to work today and might break in the future indeed.
In which case I would say remove the BOOLEAN format here since its not actually a numeric format
count > 0? just being paranoid :)
leave the //norelease ? we will have to get rid of it at some point.
I saw it, thanks!
Also, we could put this check in a protected static method and reuse it in ParsedDerivative
This is similar to the internal implementation, nice
I added a `randomNumericDocValueFormat()` method in core which seems appropriate for this.
++ on hasNormalizationFactor . I would probably associate the flag to the parsing of normalized_value only. the other field is only optionally printed out.
it's a delta, mandatory argument in comparisons between double values.
I am slightly more in favour of having precise checks where needed rather than sharing code with unnecessary checks.
`min` can be named `simple` or `aggregation`
This would read cleaner as: ``` if (shouldHaveScriptStack && hasEmptyScripStack) { ... } else if (shouldHaveScriptStack == false && hasEmptyScriptStack == false) { ... }
If we don't count token with 0-increment this should be equal to 2
I think the naming is fine. This feature as described in the issue is about not counting tokens filtered from the token stream. This is what `enable_position_increments=false` does and I think it's all we need to do here. If your analyzer adds alternative tokens to each position they should not alter the final count since we're looking for the number of tokens in the original text.
Is there a reason these three fields need to be test instance members be randomized in the init() method? Otherwise I would prfer making them local in createTestIntstance().
I'd prefer percents/keyed/docValueFormat to be declared and instantiated here if there is no other reason I am missing atm.
I'd also merge it with the testPercentilesIterators() method if this is the only place where it is used.
I was trying to understand why this step got necessary, can you briefly explain? I'm just curious what happens here since it didn't happen in the test before.
Thanks, that makes sense. I didn't think about the reducing tests.
Doesn't hurt, I guess, but it might obfuscate the fact that all the parsed aggregations don't implement equals/hashCode. At a quick glance people might think all subclasses properly implement equals()...
I think you can initialize the capacity.
Nit: could this by simplified even more by randomizing the xContentType, humanReadable and shuffle flag inside of `parsedAggregation`? Not sure about other callers of this method...
I think these ranges are also too small, we should probably test a large range of values since this aggregation will likely be used for large ranges. Mostly here I'm thinking this might catch bugs in the reduce logic rather than the serialisation logic.
I actually meant to keep this with a range of random value for the frequently branch. Something like ``` long count = frequently() ? randomPositiveLong() : 0; ```
Aggregations is not abstract anymore, you can remove the curly brackets. also if we had Aggregations as a member we wouldn't have to create it on the fly here
I think if we had Aggregations as a member we could call Aggregations#toXContentInternal instead
shall this extend ParsedAggregation? then it implements ToXContent and we can drop a cast below I think.
Do we need the generic type here? The interface just defines `Object getKey()` but I guess it safes us some casting somewhere else. Just asking.
I just wanted to understand why its there. At the moment it doesn't complicate things a lot, and if if helps avoiding casts thats fine.
maybe extract `settings.fieldsAndWeights()` to a variable to avoid this weird indentation
I guess it is "these" marked consumers now.
typo in the method name here too
oh I see thanks for explaining. leave it as-is then, thanks! I will merge this in the coming days.
there's a typo in the method name ;)
if you want to save some characters you can replace all these `execute().actionGet()` with just `get()`
Incides -> Indices ? ;)
typo: filers -> filters
I think you are missing a `\n` here.
:+1: I think this could be useful in a few situations. I think we do some nasty stuff when parsing aggregations right now, for example.
Now that we separated MasterService and ClusterApplier, wouldn't it make sense to make MasterService the custodian of the last committed cluster state instead of keeping it in Discovery. I feel like it might simplify a few things in wiring dependencies here.
Origin should REPLICA
I think I've seen this somewhere else today ;-)
ok I am totally fine with removing the check for now and reintroducing it later once we have two different code-paths.
Since the check is being removed, maybe it makes sense to continue to store this with the content-type set to JSON? This only works now because of the default in the mustache engine which is likely fine because I don't see the default changing.
I think here we can go with an ordinary `BytesReference` and use it's efficient iterator `BytesRefIterator iterator()` the returned `BytesRef` is just a pointer into the underlying `byte[]` that you can wrap in with `ByteBuffer#wrap` and do your accounting what is left for writing in this method. so there is no need to use a `NetworkBytesReference`
here I'd do the same as above an pass in an some kind of BytesReference factory that can produce new BytesReferences and return a `CompositeBytesReference` instead the `int` to signal how much has been read. We can figure out how to do SSL and stuff afterwards this is too hard to do in one step
any chance we can remove the interface and just name this class NioChannel
there is only one impl
I'd feel better if we had a dedicated method for this called something like `closeAsync` or something like this, I think the logic would be simpler...
Ah, okay. Then I'm good with it as-is; we can consider redirects separately (if ever).
Nit: space between the cast operator and the target.
The assert message should be `https` not `http`!
s/toa /to a /
implement ToXContentObject instead? I wish it would be possible to make default methods final :) I would fix this on master too. Not a huge deal but I think it's more correct this way.
maybe move the loop in a method then we can return early and don't need a label / boolean var in the outer loop
Rather than a spin I think you can say: ```diff diff --git a/core/src/test/java/org/elasticsearch/cluster/SnapshotsInProgressTests.java b/core/src/test/java/org/elasticsearch/cluster/SnapshotsInProgressTests.java index 75ac8993fd..4d1a1a6e58 100644 --- a/core/src/test/java/org/elasticsearch/cluster/SnapshotsInProgressTests.java +++ b/core/src/test/java/org/elasticsearch/cluster/SnapshotsInProgressTests.java @@ -31,6 +31,7 @@ import org.elasticsearch.test.ESTestCase; import java.util.Arrays; import java.util.List; +import java.util.stream.Collectors; /** * Unit tests for the {@link SnapshotsInProgress} class and its inner classes. @@ -72,10 +73,6 @@ public class SnapshotsInProgressTests extends ESTestCase { } private State randomNonWaitingState() { - State state; - do { - state = randomFrom(State.values()); - } while (state == State.WAITING); - return state; + return randomFrom(Arrays.stream(State.values()).filter(s -> s != State.WAITING).collect(Collectors.toSet())); } } ```
I think we can drop this flag and just explode if the stream is broken.
I think we can drop the benchmark.
Oh boy, I hope nothing is broken here.
I think we should test cases with score separately from those without scores and retain both types of test.
Could you have the score scripts as separate scripts rather than repurposing the existing ones? I think its important that we continue testing scripts that don't use score as well as scripts that do.
I think the existing version is more readable and I doubt that the performance difference matters at all.
no need to make this public; package-visible is good enough.
just use the public one with one less parameter...
I'm generally on the fence about adding getters for stuff that *might* be used but isn't at the moment. It looks like this wasn't needed so far (at least there is nothing using it in this PR, might be the clients need it), so I'm +0 on adding stuff we don't really need, but maybe I'm missing the reason. Same for the other method here and the "getFilter()".
thanks, if there is a reason thats fine ;-)
++ I can't see anything good happening if they aren't the same length. The Iterator for example will throw an out-of-range exception if they don't match
I wonder if it'd be save is we defaulted to safe = false and only set it to true at the bottom of the try block. That way if we throw anything in the try block we'd resume the thread.
I don't think this indenting is intended.
Can we pull this initialisation code into a method. I imagine most derived classes will implement `updateRemoteCluster` by storing the cluster names/addresses into some form of collection. But if that collection is a field, then it won't have been initialised at this point (in the parent constructor). The typical constructor is going to need to look like: ``` { super(settings, false); clusterNames = new ArrayList<>(); initialize(); } ```
out of curiosity: do we have similar shared methods for toXContent in some other place? not a huge deal but I am a bit on the fence on adding these. Ideally the two impls wouldn't depend on each other and one day will be split apart. On the other hand we can ignore this and fix it later when needed.
Good point about the parse fields. Let's not worry too much for now. We can't be consistent if we don't enforce these constraints. Let's share these simple things for now and one day we will change that when we need to.
+1, removing this from the `Plugin` extension
this class is not needed anymore as Aggregations is no longer abstract and also implements ToXContent
Does it buy us much to use `final` here? (I am not against it, I am really just curious).
Maybe worth explaining that this does nothing if called more than once. I presume you do that because the plugin is loaded multiple times during testing. Is it worth logging something if we skip it? If we're outside of testing then it'd be fairly nice to know that it isn't working.
I'd either go with a hard exception or an assert and warn level log depending on how bad this is to get wrong.
A blank line above and below. 
> Clear as mud? Indeed. 
the TODO can be removed.
I wonder if this should be null unless we've asked for deduplication.
I see now. I think it is worth mentioning that this is controlled by the circuit breaker. Maybe right after the big about English not having that many variations.
This is going to be very funny for term vectors because `fieldText` is empty.
It looks like you have proper ram usage stuff. Maybe it'd be simpler to refuse to expand the tree if it'd put the `bytesAllocated` above a certain size.
I think this might be important to have up front but I'm paranoid about filling up memory because I've been paged too many times for things like this. Some kind of numeric limit is good enough for me though. Like "only 10,000 `ByteSequenceLeafNode`s are allowed in the entire tree". Or something.
I think we usually prefer a space after `if` and before `(`.
I'm fine with the answer being "no, you are crazy Nik" or "not right now".
We've gone back to 140 character limits now so you don't need the newline if you don't want it.
I wonder if it is worth compressing the hitCount into byteSequence? That way you only have to store an array of ints? You'd have to cap `hitCount` at 255 but maybe that is ok? Or you could use an array of long and have tons of precision on your `hitCount`.
I *think* `0xFF & b` would amount to the same thing and is a bit easier to read, at least it is for me.
Usually we put these on the same lines as above.
Same here with line breaks. Usually I see this as `} else {` in our code.
Where is this done? I think to make things cleaner we in `doBuild()` we should check if sourceFieldNames is null and if so set it to `fieldName` in the factory. This means that everything past that point can deal with fieldName and sourceFieldNames as if they are different field even if they are the same.
What about something like ``` p = nextBufferPos(p); int sequence = 0xFF & sequenceBuffer[p]; p = nextBufferPos(p); sequence = sequence << 8 | (0xFF & sequenceBuffer[p]); p = nextBufferPos(p); sequence = sequence << 8 | (0xFF & sequenceBuffer[p]); node.add(sequence << 8); ``` No byte array required.
I'm not sure if the cast is worth it here. It is usually simpler to just work in integers even if we know if can't be more than 255.
Could you add a short description here? Something like `Add a byte to the sequence.` would make it so you could skip the `@param` bit and still keep the `@return` which is so nicely descriptive.
Can we unit test this instead of creating a integ test? (by extending from `AggregatorTestCase`) ~~We can have a simple rest test (eiher yaml test or simply extending directly from `ESRestTestCase`) that verifies the aggregation has been integrate properly, but it feels like most of these tests can also be tested with unit tests.~~
I see that there is already a rest test, great!
can you use `InetAddresses.forString` intead, which guarantees it won't do a dns lookup
can you indent so that the throws is not indented at the same level as the content of the block? (similar issues on the below lines)
should we use a long just to be on the paranoia side of things
maybe add an explicit `continue;` here to indicate that it's being skipped
Maybe add an explicit `continue;` here to indicate that it is being skipped
or maybe give the return value a variable and name it accordingly
I think this should happen first to make this PR less complex
I think we should fix our datastrucuture first and don't make Path trie super complicated and flexible. This should be fixed first before we make this change here.
I think we use the empty string somewhere yes, not sure if that was a wise choice. I don't mind leaving null, no biggie
But yeah, keep it now.
This also seems like the kind of leniency that we'd want to remove in the future.
I don't think there is any need to keep `META_FIELDS`, and this is incorrect do both because `META_FIELDS` is static right now (so should not be initialized in the ctor) and because it leaves the incorrect values there now. `META_FIELDS` should be removed completely, and the methods that use it should instead pull the keyset from `mapperRegistry`.
@StefanSchmidtOz We use a randomization framework that is reproducible (each test execution is associated with a seed).
Similar to above, I would suggest to refactor so if a test failure occurs it is reproducible.
Oops, sorry @imotov
> I assume it runs each test method several times @StefanSchmidtOz We rely on CI for that.
I would prefer to iterate over the enum. Random selection is not directly reproducible if the test fails.
Thank you @jasontedor, I'll have a closer look at how the tests are run. I assume it runs each test method several times, otherwise I would still suggest to restructure `#testInvalidEnum` so it always asserts the invalid enum case.
Is this generating a random number between approximately -2 billion and +2 billion (i.e. the full range of `int`)? If so, the proportion of tests of valid enum values (in the range 0-2) is going to be so vanishingly small that the CI might not do a test of the valid path for thousands of years.
Idea: ``` if (toResolve.remove(snapshotInfo.snapshotId())) { snapshotInfos.add(snapshotInfo.basic()); } ```
Since `getSnapshotInfoInternal` (just below) is only used by `getSnapshotInfo`, we can move the code in `getSnapshotInfoInternal` directly into `getSnapshotInfo` and get rid of `getSnaphotInfoInternal`
remove extra new line
Can you use StreamInput#readList ? You need to check for the version here since this code can receive requests from nodes in previous version. Something like ````if (in.getVersion.onOrAfter(Version.V_6....)````
We'll keep this format around not only for backward compatibility, the array form is just another format.
Same here, you'll need to deserialize differently depending on StreamOutput#getVersion
I think you still need to use the low level client for that, using `client().performRequest()`. That said, why not change the order of the test blocks around and move the `// Testing deletion` block to be first. It will create the index.
missing whitespace after the parentheses :)
I don't understand why it is @After and not @Before anymore... This is going to break the test.
space after <`
maxDepth can be final
Please add the word "of" back.
This can have a better name.
I'm not sure if this is practical in all cases, but IMO a lambda would work pretty well here: ``` nrReplicasChanged.forEach((fNumberOfReplicas, indices) -> { ```
Can we test more than one version at a time ? At least one of v2 and one of v5.
now I see what you meant yesterday saying that we have to parse meta here
nit: "Set up" -> "Create"
typo: no -> not
We should keep logging as the last component to shutdown, maybe we should do this: ```diff diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java index 3bfc2ac3b5..1e53faa9ef 100644 --- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java +++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java @@ -180,8 +180,8 @@ final class Bootstrap { Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { - try (Spawner spawnerToClose = spawner) { - IOUtils.close(node); + try { + IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); } catch (IOException ex) { @@ -258,8 +258,8 @@ final class Bootstrap { } static void stop() throws IOException { - try (Spawner spawnerToClose = INSTANCE.spawner) { - IOUtils.close(INSTANCE.node); + try { + IOUtils.close(INSTANCE.node, INSTANCE.spawner); } finally { INSTANCE.keepAliveLatch.countDown(); } ``` Also, we are probably missing a logging shutdown in the `stop` method, but let's worry about that separately.
do you have indentation at 2 chars only for this method? We use 4 chars in our codebase. I'd appreciate if you could change that.
if it can happen only due to a bug, I would not handle it then. Rather make sure that it is always set through tests or assertions? Or even leave the NPE.
Might be nice to add a check for existence of these parameters for completeness.
I think this should be pre-fixed with `es.` like we try to do with other custom system properties that we use. I'm not sure if we're 100% consistent, but we should aim to be when adding new ones.
I still think we should rewrite the query all the time. The percolator in 5.x does rewrite at index time but we may have new rewrite rules that should be applied in minor releases ? My point here is that it should be safe to rewrite the query even when the query has been rewritten already.
testing will be tricky but doable. I have some ideas here similar to what I did on `RemoteClusterConnectionTests` where we basically mock the calls to clusterstate and return a pre-build state but we can also put some sleeps into it.
maybe we should unify the `latch.countDown()` and `closeConnection()` into a single method called "onDone" on the AbstractRunnable that everyone calls? this it's less trappy and people wouldn't forget to do one but not the other.
can we call the latch in a finally block just to be absolutely sure
we also need to close the connection in `public void onFailure(Exception e) {` since we might get rejected or something like this.
maybe remove `settings` fields? because it is unused.
mention here too that this is what we do also in the corresponding builder
this can only be a runtime exception you can just bubble it up no need to use `convertToRuntime`
you can use `exception = ExceptionsHelper#useOrSuppress(exception, e)` here instead
we might use `ExceptionsHelper.reThrowIfNotNull(exception);` here instead
Maybe also use a constant here
nit: maybe the field name could be a constant under `Names`, the same as in e.g. GeoPointFieldMapper (or reuse that one)
coerce -> ignore_malformed
Maybe also check the explicit flag.
extra empty line that was probably already there.
nit: can you use assertThat or expose the actual values in the message.
@markharwood remember to take hash % copies# collisions into account. So if you see two shards being search on the same node, they should have the same hash modulo number of shard copies.
can we check that the distribution is balanced in a multi index / multi shard case? like that thing differ at most with 1.
Thanks for moving this to `InnerHitContextBuilder` and its subclasses!
If it is relevant for String impls then I don't see why it should not also apply in the long impl.
I'm okay with the UnsupportedOperationException for now if we can track this question (whether we can reach consistency between the functionality the transport client provides via the SignificantTerms.Bucket interface with the rest response) in a separate issue
I was wondering if `getSuperset/SubsetSize` is part of the Bucket interface but not rendered via the Rest response, should we either add rendering of these values to the bucket response or remove it from the interface to get equivalent behaviour of functionality of the transport client with the high level rest client here? I think this can be done in a separate issue though, maybe its not needed at all.
We render a subsetSize as the DOC_COUNT field in the surrounding aggregation, I'm not sure if this is equivalent with the bucket subset size but maybe it could be used here? Probably would need some checking with somebody who knows the aggregation better though.
Should we lazily compute a bucketMap like in InternalMappedSignificantTerms and then be able reuse it when this method gets called many times? I have no strong opinions on this though.
Could we reuse the constants used in InternalSignificantTerms by making them public? I think we did that elsewhere and also do this with the CommonFields.
remove one of these empty lines? ;)
do we need ordered things? does order help anywhere? If not I would just use HashMap
Hahaha! I missed that one :)
Nit: `get's` -> `gets`
nit: could be one line
Luca mentioned in another PR that we might not need the need @Bfore here since the parent method already has it.
good point I am curious too now :) I hadn't noticed this at first
Just a suggestion, I don't mind if we remove it here entirely. I used to like those tests because they show how a typical xContent output of those classes looks like. But with these large ones its kind of debatable whether it is useful.
Although I liked it at first, I'm starting to doubt the usefulness of these tests. And formatting makes it unreadable. If we keep it, can you change this to something like: ``` StringBuilder expectedString = new StringBuilder(); expectedString.append("{"); { expectedString.append("\"took\":0,"); expectedString.append("\"timed_out\":false,"); expectedString.append("\"_shards\":"); { expectedString.append("{\"total\":0,"); expectedString.append("\"successful\":0,"); expectedString.append("\"failed\":0},"); } expectedString.append("\"hits\":"); { expectedString.append("{\"total\":100,"); expectedString.append("\"max_score\":1.5,"); expectedString.append("\"hits\":[{\"_type\":\"type\",\"_id\":\"id1\",\"_score\":2.0}]}"); } } expectedString.append("}"); assertEquals(expectedString.toString(), Strings.toString(response)); ``` That should be more robust against auto-formatting.
nit: extra new line
nit: extra newline
nit: extra newline
Under what circumstances would the mappings for an index be null (as opposed to an empty map)? It seems the default for `GetIndexResponse` is to always have an empty map for mappings (and aliases and settings) and it would only get assigned to a non-null map.
I did some research and I think I get why this assertion was put in there. This assertion should check for V_6_0_0. That said, I'm not even sure we should have it and the accompanying TODO. I suggest changing the assertion to V_6_0_0 in this PR and I will work with @dakrone to clarify this in a follow up.
Do you want to remove the stuff in the TODO below like the note asks? I don't think we're likely remember to remove it unless you do it now or open an issue to do it as a followup to this PR.
Or do it as a direct followup, I suppose.
it is also very specialized given that before dance that creates the suppliers, maybe wise to leave it here for now.
Good for me, I didn't have a strong feeling about it.
I wonder if it's worth having this in RandomObjects instead. But given that it is only used here I don't have a strong opinion on that.
You can save a line and go `Object value = valueTypes[level].get();` directly
space after comma
I don't think you need @Before here, the parent method already has it.
I'm not sure if this is enough, this wouldn't reproduce the class cast exception that revealed the issue. Ideally, we would want a test that did *not* have to be changed (other than making it compile) as part of inlining global checkpoints to catch the issue.
Based on the discussion above (whether we need to account for the "has no results" case at all after "reduce", I would opt for at least testing it less frequent, and thest the "result" case mostly.
Good catch ;-)
This can maybe go inside the following `else` branch.
A question out of curiosity: the analyzer we get here doesn't have to be closed (via closeAnalyzer) because its not a new instance? I don't know enough about the lifecycle of these objects yet I'm afraid.
Can this be split into the two cases `request.normalizer() != null` and `(request.tokenFilters() != null && request.tokenFilters().size() > 0) || (request.charFilters() != null && request.charFilters().size() > 0)` in two separate `else if` blocks instead of separating these cases later? I'm not entirely sure if this works, but I think it would make this part easier to read.
nit: maybe use VandomUtils#randomVersionBetween()
++ thanks for adding these checks
Thanks, I think its better than nothing
can we make this a proper setting, so the default is in the object and it gets validated and such? I think this an oversight when the settings were migrated.
right it's a double not Double, then no NPE :) makes sense
I wonder how this one happened to work in AggregationsTests :)
I think we could check that successful == total shards and that total shards is greater than zero
I don't think this needs "WithAlphas", because the unstable branch always has alphas (when we switch to beta, we cut a new stable branch).
maybe just catch (Exception)
Calling this just `primaryTerm` is confusing (in light of a future PR that uses this code during resync). Here it means the term of the primary that was sending this operation, not necessarily the primary term of the log entry that is being replicated.
I think this should be kept as is exactly for the reason that @bleskes mentions regarding the consequences in production code of changing this to a hard failure and the possible loss of visibility when tests are running. This really should never happen.
I am not sure that this test is that useful. I think that these two bytes reference objects are even equal when compared directly so they don't need to be compared using assertToXContentEquivalent. We should rather test that order doesn't matter by shuffling keys and/or verify the behaviour of assertObjectEquals .
here too, maybe go straight to what was expected and what was found. I don't understand which of the two is what in this error message, and the last part confuses me. Maybe it's me.
actually, you may not even need the array here.
Thinking about this more, I wonder if we should have this run-even-if-there-is-nothing-to-do complexity. I'll reach out to discuss.
another upside of using permits is that it guarantees the sync runs after history gaps have been filled in
as we discussed - corruptions can't lead to mapping parsing failures. They fail way earlier when reading from the translogs
I missed that IllegalShardStateException is also handled as a delay in PeerRecoveryTargetService. All good.
I think this is a tricky place to put this - it doesn't really know what the retry semantics are (that we always retry a full batch). This is why we had the BatchOperationException. If we want to remove it from the "official exception list" (+1 on that), we can still make BatchOperationException dedicated non ElasticsearchException by always rethrowing it's cause.
Maybe change "also known as" to "ordinarily". It is possible to use a `background_filter` to redefine the scope of the background set you want to "diff" against.
Why do we need this? The TokenizerFactory.name() was never used here before.
Since this is now a suffix, can you change the constant name to reflect that, eg `CITY_DB_SUFFIX`? And same with country as well.
@abeyad I think in this case 500 is probably fine since we are dealing with a fault condition anyway (old master didn't leave the cluster cleanly). I think it should be an error, because we don't really know if this was the result of a rogue master or a bug in our code, but we might want to improve the error message
I am not quite sure why this needs to be treated differently and it seems a bit dangerous to me. In both case we should remove the snapshot record form the cluster state as the next step.
Sorry, I wasn't explicit, but you're missing "term" everywhere except for the ones that you just fixed.
nit - an extra d? release**d**Delayed..
nit - I like the blockOperations naming... syncX is just one letter away from asyncX..
better yet, how about the following? This way it's clear how a `delayOperations` call is matched by the undelay logic. ``` diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java index 7fc56a1..f1c8b0d 100644 --- a/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java +++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java @@ -27,6 +27,7 @@ import org.elasticsearch.action.support.ThreadedActionListener; import org.elasticsearch.common.CheckedRunnable; import org.elasticsearch.common.Nullable; import org.elasticsearch.common.lease.Releasable; +import org.elasticsearch.common.util.concurrent.AbstractRunnable; import org.elasticsearch.common.util.concurrent.ThreadContext.StoredContext; import org.elasticsearch.threadpool.ThreadPool; @@ -95,7 +96,11 @@ final class IndexShardOperationPermits implements Closeable { throw new IndexShardClosedException(shardId); } delayOperations(); - doBlockOperations(timeout, timeUnit, onBlocked); + try { + doBlockOperations(timeout, timeUnit, onBlocked); + } finally { + releasedDelayedOperations(); + } } /** @@ -110,12 +115,21 @@ final class IndexShardOperationPermits implements Closeable { */ <E extends Exception> void asyncBlockOperations(final long timeout, final TimeUnit timeUnit, final CheckedRunnable<E> onBlocked) { delayOperations(); - threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> { - try { - doBlockOperations(timeout, timeUnit, onBlocked); - } catch (final Exception e) { + threadPool.executor(ThreadPool.Names.GENERIC).execute(new AbstractRunnable() { + @Override + public void onFailure(Exception e) { throw new RuntimeException(e); } + + @Override + protected void doRun() throws Exception { + doBlockOperations(timeout, timeUnit, onBlocked); + } + + @Override + public void onAfter() { + releasedDelayedOperations(); + } }); } @@ -150,25 +164,30 @@ final class IndexShardOperationPermits implements Closeable { throw new TimeoutException("timed out during blockOperations"); } } finally { - final List<ActionListener<Releasable>> queuedActions; - synchronized (this) { - queuedActions = delayedOperations; - delayedOperations = null; - delayed = false; - } - if (queuedActions != null) { - // Try acquiring permits on fresh thread (for two reasons): - // - blockOperations can be called on recovery thread which can be expected to be interrupted when recovery is cancelled. - // Interruptions are bad here as permit acquisition will throw an InterruptedException which will be swallowed by - // ThreadedActionListener if the queue of the thread pool on which it submits is full. - // - if permit is acquired and queue of the thread pool which the ThreadedActionListener uses is full, the onFailure - // handler is executed on the calling thread. This should not be the recovery thread as it would delay the recovery. - threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> { - for (ActionListener<Releasable> queuedAction : queuedActions) { - acquire(queuedAction, null, false); - } - }); - } + releasedDelayedOperations(); + } + } + + private void releasedDelayedOperations() { + final List<ActionListener<Releasable>> queuedActions; + synchronized (this) { + assert delayed; + queuedActions = delayedOperations; + delayedOperations = null; + delayed = false; + } + if (queuedActions != null) { + // Try acquiring permits on fresh thread (for two reasons): + // - blockOperations can be called on recovery thread which can be expected to be interrupted when recovery is cancelled. + // Interruptions are bad here as permit acquisition will throw an InterruptedException which will be swallowed by + // ThreadedActionListener if the queue of the thread pool on which it submits is full. + // - if permit is acquired and queue of the thread pool which the ThreadedActionListener uses is full, the onFailure + // handler is executed on the calling thread. This should not be the recovery thread as it would delay the recovery. + threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> { + for (ActionListener<Releasable> queuedAction : queuedActions) { + acquire(queuedAction, null, false); + } + }); } } ```
can we used delay to control the flow here? I think it be easier to read that to make `tryAcquire` check on delay. WDYT? ``` diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java index 7fc56a1..ebab08d 100644 --- a/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java +++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShardOperationPermits.java @@ -190,10 +190,7 @@ final class IndexShardOperationPermits implements Closeable { final Releasable releasable; try { synchronized (this) { - releasable = tryAcquire(); - if (releasable == null) { - assert delayed; - // operations are delayed, this operation will be retried by doBlockOperations once the delay is remoked + if (delayed) { if (delayedOperations == null) { delayedOperations = new ArrayList<>(); } @@ -205,6 +202,13 @@ final class IndexShardOperationPermits implements Closeable { } else { delayedOperations.add(new ContextPreservingActionListener<>(contextSupplier, onAcquired)); } + releasable = null; + } else { + releasable = tryAcquire(); + assert releasable != null; + } + if (releasable == null) { + assert delayed; return; } } @@ -212,7 +216,10 @@ final class IndexShardOperationPermits implements Closeable { onAcquired.onFailure(e); return; } - onAcquired.onResponse(releasable); + if (releasable != null) { + // if it's null operations are delayed, this operation will be retried by doBlockOperations once the delay is remoked + onAcquired.onResponse(releasable); + } } @Nullable private Releasable tryAcquire() throws InterruptedException { ```
Fair enough. We can also maybe just have a something we can drain and let it be a final field.
Personally I don't like mixing `this.stream` with `stream`. It makes me think you are shadowing and need `this.` sometimes. No big deal, but it slows down reading to have to follow that little mental dead end.
I wonder if it'd actually be clearer *not* to have `shouldCompress` and instead check for reference equality here.
maybe use `shouldCompress` here to make it a little clearer? ```java if (shouldCompress) { IOUtils.closeWhileHandlingException(stream, bytesStreamOutput); } else { assert stream == bytesStreamOutput : "the stream variable is not the same instance as bytesStreamOutput"; IOUtils.closeWhileHandlingException(stream); } ```
lets keep this class here and make it pkg private and final. it's very special
maybe just call this `public BytesReference materializeAndClose() throws IOException` and don't even return the stream.
Cool. The pre-configured version of `lowercase` is in core so this'll work!
`translogs` -> `translog's`
Remove comma to start line, place at end of previous line.
`it's` -> `its`
`bytes and` -> `bytes, and`
there is no exception possibility here? I think this is overparanoia
`of the it's last` -> `of its last`
`file reopened` -> `file is reopened`
wouldn't it be simpler if you remove the `viewGenToClean` and just do this `return new View(deletionPolicy.acquireTranslogGenForView());`
I don't think you need the `Integer.toString` bit.
We can allow flush here, I think.
Maybe it should just be `long` instead of `long_sort`.
The number of buckets should be limited to 10 (4x4 matrix, but we only represent the upper triangular matrix). I would like it better if the maxNumberOfBuckets() was somehow tied to the number of filters we generate in this particular test. There are only three possibilities: 3 (for a 2x2 matrix), 6 (3x3 matric) and 10. Is allowing for max of 10 here possible (the AggregationsTests setup overwrite is okay I think)
I think we can share this line and the return new BulkItemRequest line? these two clauses will need to set a final `updateResponse` field.
can we call this primaryItemRequest? It's the one that's sent to the primary. Also, if we pass it as a `BulkItemRequest` parameter, we can avoid sending `requestIndex` and `BulkShardRequest` (from which need the concreteIndex, which I think we can from the shard). Last can we assert that the `BulkItemRequest` has as a request object the `updateRequest` we got? this is all super trappy but we can take one step at a time :)
+1 then we shouldn't forget about it :)
nit: s/final ArrayList<ParentIDFieldMapper>/final List<ParentIDFieldMapper>
`ParentFieldMapper` sets this to `IndexOptions.NONE`. I wonder if we should that too here? Upside of adding an indexed field is that somone doesn't need to use the `parent_id` query, but on the other hand it does increase the index size and I'm not sure how often one would search by this field. With `_parent` field the field was made a non indexed field with the idea in mind that only a few would ever use _parent field for plain querying.
yes, lets do this in a follow up change.
In fact, I bet a few places should have `randomBoolean()` instead of `false`.
I'd prefer to add the argument and pass `false` all the places that call this unless there are far more callers that I can see.
:+1: (I added the same method here locally (differently implemented) )
I'm okay with this.
Extra space is extra.
`. Delete it if there` -> `so we delete it if they exist`
`them` -> `them;`
`crush` -> `crash`
I think this needs to be `Version.V_6_0_0_alpha3` now.
Extra space is extra.
I think it should just be called `commit` or `commitWriter`.
And I'm doing what I can to influence abandonment of it.
instead of making this method protected I think we should use `MockDirectoryWrapper#failOn(Failure)` and pass some failure to it that fails if we commit the indexwriter like this: ```Java Failure fail = new Failure() { @Override public void eval(MockDirectoryWrapper dir) throws IOException { for (StackTraceElement e : Thread.currentThread().getStackTrace()) { if (doFail && "commit".equals(e.getMethodName())) { throw new FakeIOException(); } } } }; ```
can we use an iterator here instead? it would be more clear to me if we'd do that..
Let's change the conditional so we can avoid the negative: ```java if (readers.isEmpty()) { return current.getGeneration() } else { return readers.get(0).getGeneration(); } ```
I see, that is hideous. 
I see, that is hideous. 
Ahh okay, I missed that usage then, thanks!
Can you add the `oldMemSize` and new estimate to the assertion message? (In case it fails)
Can we add an assertion that this is never negative? I don't think it ever will be, but just to be sure...
This seems quite sneaky that you modify the lists returned by `getGetMethods()` and `getGetReturns()`. Additionally, I'm not sure how it actually works since you increment `get`, but then but remove, so the size is changing. Can you instead create a copy of the lists in SSource ctor and use those local versions? I think you could then just create the lists on SSource only containing variables contained in `reserved.getUsedVariables()`? Then just iterate the member lists here to add to the `mainMethod`.
ah right, I think it's ok since it's the main mapper. We should just hide the other mapper builders since they are only used internally
oups no, that just me not reading the change correctly
Same here, we should check the index version created.
I think it's better to use the index version created to test whether the old or the new parent join should be used. This way you can make sure that the correct explanation is returned in the exception if the parent field is not filled.
Nit: Should still work as protected as far as I can see.
Nit: curious about what this does, it shows up as "Unsupported @SuppressWarnings" in Eclipse
Nice, I just figured out how you check the response by preparing it here.
This shows up as unused for me.
could be `status.getStatus()` in the event we change the local variable above.
Personally I think we should return `RestStatus.PARTIAL_CONTENT` (206) in the event that requested aliases were requested and *some* were found while some were not, but it sounds like this ship has already sailed as far as what to return.
You can probably use `aliasMap.values()` since you don't need the key for anything right? I don't think it's necessarily any better though, so either way is fine.
We can remove the `!` if we reverse this if statement, so ```java if (difference.isEmpty()) { status = RestStatus.OK; } else { ... the error stuff ... }
I think we should have a consistent message regardless of the number of aliases, I know it's bad form for people to write tests against error messages, but that doesn't mean people don't. So, I think we should stick with just `aliases [foo] missing`. What do you think? (I'm only +0 on the change)
You can use `UncheckedIOException`
You don't need to create an explicit default ctor since the super class has a default ctor.
No need for this ctor.
Looks like the wrong copyright header...
No need to override readFrom or writeTo
This cast should not be necessary. You can use `in.readMap(StreamInput::readString, StreamInput::readString)`
No need for an empty default ctor when the super is also a default ctor.
Not sure why the order matters. Since we know this is a string -> string map, let's write it for that: `out.writeMap(grokPatters, StreamOutput::writeString, StreamOutput::writeString)`
do we want to add another one of these alpha(N) onOrAfter alpha(N-1) checks? not sure if that is necessary
To be clear, I don't think we need anything extra here.
It is to make sure that the version comparison logic orders alphas, betas, and RCs correctly.
> If set it to `MAX_VALUE` or something and it goes wrong the stats will be extremely off and hard to recover. Well, it would only be when assertions are enabled. > Felt like an overkill. Okay.
I think that `node);` fits in the previous line
I think this line fits in the previous one
I think that what confuses me here is that we call performRequest and performRequestAsync here, why do we mock the rest client then? Wouldn't it be better to test that RestHighLevelClient subclasses can use performRequestAndParseEntity and performRequestAsyncAndParseEntity (which are private at the moment)
right, I had totally misunderstood how the test works
nit: we can check the expected token and then create the searchProfileResults map
They shouldn't diverge as these will never change other than eventually being removed when we no longer have to be backwards compatible with 5.6.0 (say, 7.0.0). This pattern occurs often enough that the field is not needed to clarify that this was added in 5.6.0 instead the code already speaks for itself in that regard. Finally, the git blame will tell us the commit that added this which will have linkage back to this PR if someone *really* needs to go on a cave expedition through that. 
Also, no need for the field, you can inline it in the places it is used.
You can set it to 5.6.0.
I think it makes sense too
Yes, that is much preferred, thank you.
I think that we can save the instanceof checks, builder.value(Object) does it already
oh boy, thanks for catching and fixing that
Throwing a RetryOnPrimaryException feels ugly. I see why you did it and I can't come up with something better. On top of that, this made me realize that RetryOnPrimaryException has serious problems. I'll reach out to discuss. to be clear - this shouldn't stop this PR as it is an existing situation.
Nit: I personally like the replica.markSeqNoAsNoop name better. applyNoOp is a strange notion (why do you have to apply it if there's nothing to do). Also, It conflicts with the NOOP execution mode one case above it.
nit: just call this performOpOnReplica? the normal is for the execution mode choice
So if I had one concern with this PR, it is here. Netty (and other nio options) will not throw on `sendMessage`. So just because that method returned without exception, does not mean the data was sent. The write could be cancelled due to a `ChannelClosedException` (or any other type of `IOException`). If you only want to increment that counter in case of a successful send, you will need to put it in the `ActionListener`.
This thread can leak and fail the test, I think that you need to clean it up (join on it in tear down).
That's a good point @tbrooks8, we should not do this.
that should not be `doesNotRunTemplateUpgrade` I assume
nit^2: `assertThat(putTemplateListeners, hasSize(additionsCount));`
nit: extra newline
`EMPTY` could have a more descriptive name
you could just use a `EsExecutors.newDirectExecutorService()`
oh... didnt see the `updateInvocation` counter... nvm
nit: extra newline
I think it is ok to use the Version from Version.java.
Log the **node** versions? Can also be done directly in the loop where you are adding the nodes :-)
no need for iteration here, you can get the node directly by calling `state.getNodes().get(shardRouting.currentNodeId())` (which will return `null` if no node found)
we don't need to determine `replicaToBePromoted`. Candidates also does not need to be filtered with ` !s.equals(replicaToBePromoted)`. It's ok to just check candidates.size() > 0 here to see whether there is going to be a new primary. In that case, we fail the primary + `random(0, candidates.size() - 1)` replicas and check afterwards that the new primary is on a node that is at least as high as all replicas.
this is not needed. createIndex automatically reroutes.
it also checks for other properties through the super.equals call
it is currently a bit confusing since eg. `ignore_malformed` can be updated
nit: can we test the whole positive/negative int range here? I think it shouldn't complicate things a lot and since its testing the parser it would provide better coverage.
this is unneeded see above
this should go away
also make this setting `Setting.Property.Final`
maybe reverse this check? (`expected.equals(map) == false`)
These make sense. I haven't checked the system call numbers but I trust that they are right. I will double check before final review.
And yes, I misread the assertion at first but the puzzle why a change is needed only here remains! 
The matrixStats PR is here - https://github.com/elastic/elasticsearch/pull/18300 - with code from @nknize et al.
Yeah, it's puzzling!
Yeah, I checked https://github.com/torvalds/linux/blob/9705596d08ac87c18aee32cc97f2783b7d14624e/include/uapi/asm-generic/unistd.h and they all look right to me.
Nit - there is a stream::toArray(Directory[]::new))
This one asserts far fewer things about test2 then it does about test1 so it'll be annoying to untangle when we no longer support multiple types.
Can you rename these indices? I know I'll get `test` and `test1` mixed up. Even `test1` and `test2` would be better.
`0-9A-Za-z` being consecutive in ASCII would have made things much easier... ;)
please give it some text here it's always better to have a message :)
For posterity: this is handled by the gradle logic that scrapes the version file and a test in `VersionUtilsTests` that verifies that the gradle logic and VersionUtils line up.
I don't think we need the constant.
Weird that this wasn't used!
I know this setting name was (and argument now is) `path.conf`, but can we call this getConfPath? It doesn't really make sense without the dot (might even call it getConfigPath).
maybe also fail if INDEX_MAP_UNMAPPED_FIELDS_AS_TEXT_SETTING is set too
yes this looks good to me
you can also remove the dependency by passing a `BiFunction<Connection, SearchActionListener, ActionListener>` instead of ResponseCollectorService and require it to be non-null then it's easier to test? and we never need to do nullchecks
> I played with this, however, it requires making SearchExecutionStatsCollector public instead of package private, since we need to construct it within the BiFunction passed in to SearchTransportService in Node. I think that is ok!
I don't like to have 2 data structures that we have to keep in sync. if you really wanna optimize for removal / insertion we can just use an array or an array list and sort it? I doubt we should to be honest. Maybe we just go with a list and linear scan or we use a set only and the same iterator approach and with the synchronization in place we can just set the iterator to null if we changed the set? that might be the easiest
With the change I suggested above we can leave out the == PEER condition and instead add ` && currentRouting.isRelocationTarget == false` to `currentState == IndexShardState.POST_RECOVERY && state == IndexShardState.STARTED`
At this point I don't know that `@param` adds anything either.
This is not longer correct - more like "Creates a new QueryBuilder from the query represented by the xcontent in the parser." or something.
I don't think is necessary. `request` is passed as`params`. So, "verbose" is already in `params` if it was specified, you just need to be careful resolving defaults. You would also miss other parameters here such as human and pretty.
would be nice to have an assert after this line that we consumed all bytes
Could we instead have RestHandler have optional overridable methods for each verb? That would seem less error prone (and greatly reduce the number of classes we define from what we currently have).
how would you feel about naming this method (and it's counterpart) `activatePrimaryMode` ? I was confused a couple of times as initialize and primary terms already used in the IndexShard context (a primary relocation target is a primary shard and is already initializing long before the method is called) .
This is entirely too harsh, this will fail the node if we get this wrong. We should fail the shard for sure though.
+1. Good catch. I missed it. It would still be good to kill the node when testing - so we should have some assertions here too.
Same question as for the adjacency matrix one.
Fair enough. I wouldn't change the capitalization though.
I had a quick look and opened #25519 with what I imagine the strategy is. It certainly looks big enough to be worth doing in its own PR.
I wouldn't name it in capital case because it isn't a constant. Otherwise I'm fine with whatever rename you like.
nit: I think remove "next", since this is no longer a second step in the method
If you want to test the multi-write behavior you could make a testing aggregation here that needs to be rewritten twice. I'm not sure how important that is to you, but it ought to be possible.
I think it is more idiomatic to statically import the `instanceOf`.
And it looks like you cover the response below. So you can ignore this.
I believe this text is out of date now that we have a macro.
Nit: space after `,`.
I wonder if it'd be nice to have the assertion in the docs with a note about how this is the response. And maybe a note that you don't have to assert anything if you don't care whether or not it was created up updated because non-200s throw exceptions.
Actually, is that true about the non-200s? It is with the low level client.
this is much better!! 
We discussed this via another channel, @bleskes will work on removing this guard and obviating the need for even considering adding a 6.0.0 version constant.
minor nit: "for" -> "on"
It feels weird to place this assert here, rather than where `ignoreOnReplica` is actually set, it also would make debugging more difficult, because instead of a stacktrace letting you know where it was incorrectly set, it only manifests once serialized. How would you feel about combining `setPrimaryResponse` and `setIgnoreOnReplica` so they are entangled and an exception is thrown if an invalid state is configured when the primary response is set? Perhaps something like: ```java void setPrimaryResponse(BulkItemResponse primaryResponse, boolean ignoreOnReplica) { assert ignoreOnReplica == (primaryResponse != null && (primaryResponse.isFailed() || primaryResponse.getResponse().getResult() == DocWriteResponse.Result.NOOP) ) : "unexpected ignoreOnReplica value. primaryResponse [" + primaryResponse + "], primaryResponse [" + (primaryResponse == null ? "null" : XContentHelper.toString(primaryResponse)) + "]"; this.primaryResponse = primaryResponse; this.ignoreOnReplica = ignoreOnReplica; } ``` It could also be configured as a regular exception there also, instead of an assert (`IllegalArgumentException`). It would also have the benefit of not leaving the object in an illegal state when an exception was thrown (nice to be clean)
I think you should pass the Result here. Maybe also add an assertion in the ctor that the result is either `Result.DELETED` or `Result.NOT_FOUND`.
Same deal here as on Delete. I think you should pass the `result` into the ctor and `assert` that it is sane.
it's a matter of taste so I will leave it to you my suggestion is to call it `blocking` instead. but again up to you!
I think when this is rewritten, I would prefer to have an `.isConcreteIndex()` method on `AliasOrIndex` to avoid the double negatives in comparisons, I had to stare at this for a while to validate it is working correctly.
can you split this line in two? otherwise the <1> ends up in a new line in the rendered docs
For a better readability, could we have something like: ``` String[] includes = ... String[] excludes = ... FetchSourceContext fetchSourceContext = new FetchSourceContext(true, includes, excludes) ... ```
can add a sentence or two about what is currently known to be potentially missing? (in sync markers for shards on new nodes that should be accounted for GP calculations). I think it will help (at least it would me) to understand what this is about.
nit: this needs to be adapted to the last change
yeah, I think it's good to say why it can suddenly go out of sync. That's not normal.
This would have deserved a `// <1>`
See RandomScoreFunction (it does not depend on the order of visiting docs, and just substitute using the uid field with either docid or a user supplied field).
not really wrong since we do not require things to be reproducible in that case, but I'd rather like to use context.reader().maxDoc() instead of context.docBase so that matches only depend on the current segment
instance variables should use camelcase
we should use actual exceptions instead of assertions since we are validating some user input
I'm wondering why you decided to override this optional API. Is this impl expected to be faster than pulling the iterator and calling next in a loop? (this is what the default impl does)
I think it is worth adding something to this and the term suggester one.
It looks like you have a typo here. It's very helpful it you can get gradle to pass before raising a PR, so that these sorts of things get picked up by tooling rather than people needing to look at them.
- do we need to `new`-up a distinct `SSEAwsKeyManagementParams` each time `S3BlobStore#getSSEAwsKey()` is sent? Can we instead `new`-up one in the constructor (e.g., is `SSEAwsKeyManagementParams` externally mutable?) - from what I can tell, this and `S3BlobStore#serverSideEncryptionKey()` can be _package-private_; exposing public accessors on encryption keys is unnecessarily risky.
again, picking around unrelated changes with `git add --patch` is hugely helpful in reducing noise.
don't include unrelated changes
even though this is just `debug`, logging an encryption key is worrisome
don't include unrelated changes
unconventional newline between `new` and the class that's being constructed makes this hard to read; should probably just use `S3BlobStore#getSSEAwsKey()`.
our `blobStore` already has a method that will return us an instance of `SSEAwsKeyManagementParams`; we should probably use it instead of `new`ing up our own here.
the unconventional newline between a parameter's type and name makes this pretty hard to read.
import appears unrelated to the whitespace-only changes in this file
on second thought, after seeing how our `blobStore` appears to be the true keeper of the encryption key (because it has the most distinct things asking for it, maybe we shouldn't change the constructor signature of this method and _consistently_ reach into `S3BlobStore` for our key.
I think it should be a separate if statement, not an `else if`
you can use Arrays.copyOfRange
also you probably want to check that `info.getPointDimensionCount() == 1` instead in order to skip range fields.
do I get it correctly that you had a byte[8] with longs and a byte[4] with ints? If yes, then you would need to add padding bytes in the beginning in order to always have a byte[16]
points are allowed to reuse the byte[] to I would make a copy of it before adding it to encodedPointValues
I think it would be better to do something like `return "[" + new BytesRef(ranges, 0, BYTES) + " TO " + new BytesRef(ranges, BYTES, BYTES) + "]";`
+1 to that as a first step
I think I would not check the instances' classes but instead compute how many values the interval has using NumericUtils.subtract (it returns a byte[] that is comparable).
Actually I was thinking of doing something like this so that we use as many bytes of the hash as possible. For instance in the case of ints/floats, we'd still put 12 bytes of the hash in the values. And if we ever come up with types that need 12 bytes, then we'd automatically put 4 bytes of the hash in values. ``` byte[] bytes = new byte[BinaryRange.BYTES * 2]; BytesRef fieldAsBytesRef = new BytesRef(rangeFieldName); MurmurHash3.Hash128 hash = new MurmurHash3.Hash128(); MurmurHash3.hash128(fieldAsBytesRef.bytes, fieldAsBytesRef.offset, fieldAsBytesRef.length, 0, hash); ByteBuffer bb = ByteBuffer.wrap(bytes); bb.put(hash.h1).put(hash.h2).put(hash.h1).put(hash.h2); assert bb.position() == bb.limit(); int offset = BinaryRange.BYTES - minEncoded.length; System.arraycopy(minEncoded, 0, bytes, offset, minEncoded.length); System.arraycopy(maxEncoded, 0, bytes, BinaryRange.BYTES + offset, minEncoded.length); ```
I didn't know `HttpSmokeTestCase` existed! Woah!
+1 (and we probably want the same thing for the global checkpoint - although that's a different change). Can we also not generated this on request but rather when it changes? the it being null means that it should not be relevant.
I don't think these should be removed, but if they should, then please explain why.
typo: an -> a, concat -> concatenated
typo: hypens -> hyphens
This can simply be `randomLong()`. However, I don't think this will work because the cleanup in `wipeAzureRepositories()` would then not know about the generated name. If these need to be unique, then this method needs to stash the generated name so that the repository can be cleaned up after the test.
I'd name the shrunken index `index + "_shrunk"` or something. I like that the indices are all named after the test that uses them.
I might do `assertThat(totalShards, greaterThan(1));`.
I think `"*".equals(defaultField)` should use `Regex.isMatchAllPattern`
Wouldn't it be more in sync with the majority of ES msgs if `indices` is replaced with `index` in `specify the corresponding concrete indices instead.`
I have no idea too...
master is the future 7.0, so I would do the following: ```java if (INDEX_MAPPER_DYNAMIC_SETTING.exists(indexSettings.getSettings())) { if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_7_0_0)) { // 7.x index throw new IllegalArgumentException("Setting " + INDEX_MAPPER_DYNAMIC_SETTING.getKey() + " was removed after version 6.0.0"); } else { // 6.x index DEPRECATION_LOGGER.deprecated("Setting " + INDEX_MAPPER_DYNAMIC_SETTING.getKey() + " is deprecated since indices may not have more than one type anymore."); } } ``` Then when backporting I'll just remove the 7.x branch and make sure that we only emit a deprecation warning on 6.x indices (you don't need to worry about it).
no need for the alt variable? (but +1 to make vals[2] go through Double.parseDouble to make sure it is a valid double)
beware it might not do what you think: https://stackoverflow.com/questions/8819738/why-does-double-nan-double-nan-return-false
true by default? shouldn't it be false like the docs say? (we might need some logic to make it true for pre-6.3 indices in case we used to accept it in the past)
should be 6.3
this needs to be version 6.3 now
no need for this `alt` variable
@jpountz @nknize Wouldn't it be better to be more lenient and accept the first 3 (or 2) dimensions, rather than completely fail? Adding failure now might break compatibility with previous behavior.
please fail if vals.length > 3
can you remove this empty line? :P
To be clear, one thing is testing the toString output, and another thing is to make sure that the class implements the right interface, so that its output can be generated and is valid. I would do both from separate methods.
this way you don't really test the toString :) I think it's good to leave testQuery.toString(). We will later change the toString to not depend on ToXContentToBytes.
I don't think this should be randomized. If we test toString, we ought to test that one all the time. If we also want to test something else, I think we should make that part a separate test method.
we will have to be careful though. If a very short-running method with < 256 calls is timed using this approach, we will have significant overhead from `System.nanoTime()` calls.
nit: "so we assume"...
If we call `start()` and `stop()` in a time frame that is below the resolution of `System.nanoTime()` (at best ~ 30 ns) then we will have also significant skew here (assuming 1 ns between consecutive calls).
I'd prefer ``` else { return false; } ``` Just a bit more balanced looking. No big deal either way I guess.
I think I was trying to keep them in alphabetical order.... No big deal.
Good one @bleskes.
something I noticed while reviewing the bwc code - we can pass the indexMetadata (or primary term) as an argument. We look it up on in the calling method. Save on double lookups.
good call on making it package private.
Instead of this being a generic class with a generic name can we call this `OutOfRangeSpec`, remove the generic arguments and instead use `K -> String`, `V -> Number`, `M -> String`. Also I think it would be ok for you to declare this class here and then reuse it in the `NumberFieldTypeTests` below instead of re-defining it
Or just make it `abstract`
No. Floats that are between 65504 and 65520 will be rounded to 65504 however floats that are equal or greater than 65520 will be converted to +Infinity.
should probably be `Math.abs(value) >= 65520` rather than 65504. 65504 is indeed the maximum value but values up to 65520 excluded would be rounded to 65504
please make it a double rather than Double and use Double.isFinite(value) == false below
please make it a `float` rather than `Float` and use `Float.isFinite(value) == false` below
let's make it take a `float` so that we do not have to worry about null values
Those last checks are redundant. We should do only one of them.
can you add an assertion on the origin here? i.e. PEER_RECOVERY or REPLICA
nit: can we use the suffix "source" instead of "string" for these scripts? That matches what we call it now in scripting code.
We should be using a new script context here. Then this can be SimilarityScript.Factory. The new context can return `float` directly, and take Stats as an arg.
I mean in the code but just noticed there was one already
nevermind I was confused... all is good
should we move the `configureSocketChannel` call into the try block as well here? seems cleaner
++ . nit: add the state to the message please.
nit: extra line
removal removal removal
You probably want to remove `true ||` before pushing ;-)
space missing between closing and opening bracket: `false){`
`newMasterId` (= `getEphemeralId()`) cannot be null, `getMasterNode()` can be, however.
maybe it's nicer to just capture the `DiscoveryNode` object here and use `equals` on that (internally it will use the ephemeral id for comparison anyhow).
should we release the releasable just in case the exception comes from the listener? this would allow us to only implement on failure.
maybe extend the message to `"aborted by snapshot deletion"`
Should there be a clearer error if the property wasn't set? While that shouldn't happen (since all our scripts explicitly set it), if it's missing then you get a not-very-helpful NPE.
Can you rewrite this block using `expectThrows`? Also, instead of `assertTrue`, it's more effective to use the built-in matchers like `assertThat(e.getMessage(), hasToString(containsString("invalid wait_for_active_shards"));`. The reason for this is because if the assertion fails with `assertTrue`, the failure message only says something like "expected true but was false" where as with the matchers we get something like "expected \"invalid wait_for_active_shards\" but was ..." so we already have immediately from the failure message more information about what is happening.
You can put these matchers in `test-framework` under `org.elasticsearch.test.hamcrest`.
Let's replace the `assertTrue` and `assertFalse` by more effective matchers.
This is another case where I would use a more effective matcher.
Can you rewrite these to use `assertThat(..., equalTo(...))`. I prefer this form because it's clearer which is the expectation and which is the value under test whereas with `assertEquals` it often gets confused.
Let's use `assertThat(..., equalTo(...))`.
Let's use `assertThat(..., equalTo(...))`.
Let's replace the `assertTrue` by more effective matchers, and replace the `assertEquals` by `assertThat(..., equalTo(...))`.
Let's replace the `assertTrue` by more effective matchers, and replace the `assertEquals` by `assertThat(..., equalTo(...))`.
Let's use `assertThat(..., equalTo(...))`.
make it also final just to make it clearer? Although the private constructor says it all.
You are right on this, good point. using equals to do the assume was kind of a silly idea :)
I'd move this to line above, but I like the thought behind the change.
I might make something like ``` private void expectMissingBodyError(Matcher<String> responseMatcher, ThrowingRunnable exec) { ResponseException responseException = expectThrows(ResponseException.class, exec); assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode()); assertThat(responseException.getMessage(), responseMatcher); }
+1 to hardcode UTF8
right, other encodings have no use here. Sorry for the holdup...
same as in the other PR, I would rather throw UnsupportedOperationException in the default case
Did you run `gradle precommit`? This will fail. We do not use wildcard imports.
You should use `expectThrows(...)` instead of try/catch.
looks like it can be final
maybe add propNode to the error message so that it is easier to understand what the issue is if a user ever complains of getting this message
could be made more concise/efficient by using Stream#anyMatch or Stream#allMatch
oh sorry, I had missed that you used the filtered collection below
should we really return 1 if nothing was extracted? Let's return Integer.MIN_VALUE (or NEGATIVE_INFINITY once we are on floats or doubles) to make sure it won't be selected if another query could be extracted
we usually take `floats` for doubles. Even though it is not necessary here, I'd parse the value as a float or double to be more consistent with other parts of our code base
nit: when the method is complex (there are 5 different arguments here), I find that explicitly implementing the interface is easier to read than lambdas
Ok fair enough, I'm happy leaving this as is then
maybe use `isEmpty() == false`
I'm actually wondering if it would be better to commit with the `onOrAfter` line and just accept the errors for a build or two. The last good commit stuff should mean that only the intake build fails on this. You could also set up the backport for 6.x branch before pushing the change on master so you can push both at the same time and minimise the chances of builds failing.
nit: spaces around `else`
I think consensus is to avoid build failures entirely whenever possible
extra new lines
you can maybe use `StreamInput#readList()`? like `in.readList(in::readString);`
Actually I'd still prefer to go with Colin's idea to use empty sets. We can still optimize later by making sure to use a Collections.emptySet (which is a singleton) if the size is 0.
this should be a `boolean`, not `Boolean`
this seems to be a very expensive operation I wonder if we should special case this here rather than adding a generic way of doing this.
This is why I said moving to compute instead of computeIfPresent so that we could assert that we do have a mapping for nodeId in that map at that point. To be clear I think that what you did is correct, I'd just like to add assertions to it to make sure the invariant is respected.
Looks better to me but I think I'd rather like a Map<String, AtomicLong>, ideally pre-filled with every possible action name so that the map is effectively immutable afterwards and concurrency is only handled at the AtomicLong level? It would also create fewer boxed longs.
Sorry I got confused about what keys were about.
maybe mention that it is important to remove entries that have a value of zero to avoid memory leaks
> In the latter case it would fail the query early because useAllFields freeze the fieldsAndWeights map. I think it's better than the current behavior which accepts this query but fails when the query is actually built (fields and useAllFields are mutually exclusive) ? Sure, that sounds fine, as long as we're okay with this failing (esoteric) use-case. > For the first example I don't know if it's really a problem. Maybe we should just disallow useAllFields(false) ? I don't see a good reason to unset it especially now that the index default field defaults to * ? Yeah, I don't either, the only argument I can think of keeping `useAllFields(boolean)` is that then it's apparent it's a "setter" and not a "getter"
It works for strings by using illegal characters. However here, ranges use a binary encoding and all values are allowed. So we should probably use `null` as a sentinel value.
let's split on both chars at once? `split("[>\\.]")`
please add e3 as a cause of this exception
explicit call to default ctor is not necessary
a single, explicit empty ctor should not be necessary (that is what the default ctor is)
Ah sorry, I could not see all the setters in the review without expanding.
Strings.toString already catches IOException. there is a variant of it where you can pass in pretty and himanReadable flags.
this applies to all the classes that don't extend from ToXCOntentToBytes anymore and don't declare their own toString .
it should be a TextFieldMapper: keywords do not support term vectors
let's just rename originalContext to context in the argument list
Well that's the problem, we don't know what's important for a custom rescorer. `SearchContext` is mutable which is why I think it's too sensitive but the same applies to `SearchSourceBuilder` so you're probably right. We can find ways to pass more information in the `RescoreContext` anyways so +1 to keep this simplification.
Is this really needed ? Why not keeping the original `build(QueryShardContext context)` ? Custom QueryBuilder have the same signature for the build function and the `QueryShardContext` does not expose too many things like the `SearchContext` so it should not be a problem to expose this in a plugin.
Maybe replace `SearchContext` with `IndexSearcher` if we don't want to expose `SearchContext` in plugins.
Hmm, I see that the field `ignoreCase` is deprecated but this change looks unrelated to this PR? Looks like you removed this field together with this PR (I think a separate commit would have been better but it's fine with me).
I don't think reusing Lucene doc ids as slots will work in case of nested docs.
maybe it should say `multiple/nested` to be clear it can happen for both reasons
FYI the two above lines could be merged into `DirectoryReader directoryReader = DirectoryReader.open(indexWriter);`
The path is a required option and this is unreleased code so :+1: to make field and write mandatory.
If you add `// nocommit` any time that you add a debug line like this, the build will help you remember to remove it (a pre-commit check will fail).
This looks like a leftover.
++ to do that
Not related to tests but this function should be deprecated.
Shouldn't we test only three cases: no_sort, new_sort, old_sort ? Mixing the old and the new format should not be allowed.
"have to eagerly evict or ..." -> "have to eagerly evict expired values or ..."
This assumes a version format that while fairly standard is not guaranteed.
Good point, I forgot about the pretty printing. `equalToIgnoringWhiteSpace` sounds good.
I'd prefer to have a simple `assertEquals` and to a String comparison here. No need for all the constants either, makes it harder to read IMHO.
For this simple test, maybe you can inline the builder creation.
I don't think you need to explicitely test toXContent, this is done by toString implicitely and this is the change that we want to check.
We should fail if the put replaced an existing entry? Or rather, not replace if the mapping already exists (ie where a 2 different whiteslists are whitelisting methods on the same class). But surely they can't be allowed to have different names in painless for the same java class (one would overwrite the other here).
Same here, original exception is dropped.
It means "ordinals" indeed, which we commonly refer to as "ords".
I think 0 is a good minimum value.
Extra `" + "` I think.
nit - since we call it both on the master and the joining node, I think we should say "if the node and the cluster state are incompatible"
Maybe do sth. similar as this: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java#L962
It doesn't have to be JSON, we support Yaml, Cbor and Smile, so better to say we expected the beginning of an object or something along those lines.
If the error message is different, maybe you can differentiate between the case where we detect a missing START_OBJECT and the case where the underlying Jackson parser throws.
> we'll still see this infinite loop for "{" for example. I'd expect the parser to throw an exception on this? > Is it really a good idea to have the behavior of org.elasticsearch.rest.action.RestActions#parseTopLevelQueryBuilder be to loop forever on part of a valid JSON request? :) Of course not, and this is not what @cbuescher said.
Thats a String token though...
That won't loop forever, as far as I can see this will run into a JsonEOFException because if the unmatched brackets eventually.
I'd rather like to have a check outside the parsing loop that asserts that the first token the parser emmits is XContentParser.Token.START_OBJECT. I think its save to assume we are expecting full json objects. I'd also just throw a ParsingException in that case.
I think this is a sign that `getActionFilters` maybe should take `ThreadPool` as an argument.
The purpose of createComponents is to create services. This creates an unenforcible ordering dependency between creating these services and calling `getActionFilters()`. `getActionFilters` should take whatever is necessary to creation action filters.
Can you change the line wrapping on this somehow? Like stick `new InputStreamStreamInput` on a new line and indent it? I think as is it'd break how I visually scan try-with-resources.
I think we want this test to be more evil. Right now its inserting the sequence numbers into the translog in order. This is divorced from reality; lets shuffle the sequence numbers. This means youll have to change the assertions that you make below.
This can be final. This makes it easier to immediately see what is and is not immutable.
8096? Maybe 8192? 
We can leave `API` out of the test name.
Should probably also be explicit that you need to replace it with a custom analyzer using `standard` tokenizer and `html_strip` char_filter, plus `lowercase` filter and any other filters you have been using.
I don't think we need this part? Even if you've created an index with 6.4, you still want to be warned that things are going away if you upgrade to 6.5
These are cosmetic changes, let's revert them here.
Its fine @javanna, pull it in in this one. I would prefer we keep changes like this out of PRs, they can go in separately, I want less to think about when I review.
If you're just going to use another builder internally, why not return Settings from this and the other fromXContent and remove the builder arg? Seems a little odd to take a builder, and also return a builder, when the advantage of taking the builder (not creating another builder) is not used.
You can use `XContentParserUtils.throwUnknownToken()` (that would throw a ParsingException instead but I think it's appropriate here)
Nit: Typo in validateValue()
Looks like the toXContent() and fromXContent() are not completely mirrored; the former does not render any root object while the latter expects it.
This can be replaced by ` ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser::getTokenLocation);` from `XContentParserUtils`
I don't think this method is used anymore. So, maybe you should clean it up as well while you are at it.
can this be `searchReqeust.source()::size`
Nit: `[] {` -> `[]{`.
removed? It does not seem to be used.
I prefer to keep this package protected. I think in `CompositeAggregatorTests#setUp()` we should do this instead: ```java DateFieldMapper.Builder builder = new DateFieldMapper.Builder("date"); builder.docValues(true); DateFieldMapper fieldMapper = builder.build(new Mapper.BuilderContext(createIndexSettings().getSettings(), new ContentPath(0))); FIELD_TYPES[3] = fieldMapper.fieldType(); ```
I think this should be `Strings.collectionToCommaDelimitedString` and then you can use our Strings.java instead of JOpt's version
Minor nit, this can be `new HashSet<>(indices.size())`
Perhaps add the duplicate size to the assert message here
This can just be named `allAliasNames` since it's just a set of the alias names, the "duplicate" part was throwing me off
exiting -> exists
whoops I read it backwards, so yeah, not really necessary
I think this would be cleaner as ```java aliasAndIndexLookup.compute(aliasMetaData.getAlias(), (aliasName, alias) -> { if (alias == null) { return new AliasOrIndex.Alias(aliasMetaData, indexMetaData); } else { ((AliasOrIndex.Alias) alias).addIndex(indexMetaData); return alias; } }); ```
I think we should discuss such ideas in follow-up PRs. It's not clear to me that replacing duplicate code with more abstractions would be a win.
Sure, I was just wondering as this patterns appears now at least 3 times.
There's utility method that could replace all of this in `org.elasticsearch.common.String`. I think `toString(ToXContent toXContent, boolean pretty, boolean human)` can replace all of this methods code.
This is just a log message. I think we want all the details.
NIT - I tend not to like that level of code reuse. It just makes it hard to see how the end result looks like. I also think we can just use string concat and be done with it.
I missed the logging of the exception. All good then.
I think we can just use `this.stopped()` instead of having a separate boolean? we are setting the state value before we execute the `doStop` method
Okay, I understand this now. Makes sense, although it seems nice to hide the call to splitAndValidatePath in the getMapper method.
nit: seems this can this be made private.
Can you explain what moving out the call to splitAndValidatePath() from getMapper() saves here? As far as I understand it it seems better to centralize this in getMapper, but I might miss something here.
I think we should still wrap this in a doPriviledged block same asn the entire thread creation and set* below. It will allow us and other users to give fine granular security permissions
I think we should do this in a `AccessController.doPrivileged` block since we need a special permission to modify threadgroup. It's good to respect that since we use this in reindex etc. as well
nit: "not all shards" -> "required shards of index" (now that we do splitting it may not be all)
sourceIndexService could be null? (you're doing the null check in the next line)
nit: mergeSourceIndex -> resizeSourceIndex
How can the compiler know which method this is overriding? It can not, so this can not compile.
There is no way this compiles.
This can not possibly compile. There's no way the JVM can infer that the lambda here is intended override `BootstrapChecks.OsXFileDescriptorCheck`. Even if there were a way to infer it, I would prefer this as-is because as-is it's clear what method is being overridden, losing that behind lambda makes it less clear.
Nit - strictly speaking these are publishing stats, can we open the object with just published cluster stats (drop received). You can maybe received back in the keys, which can be shorted by dropping the cluster states from the key names - its implied from the object theyre in.
This method can be package-private.
Here you can do something like this: ```diff diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/PublishClusterStateStats.java b/core/src/main/java/org/elasticsearch/discovery/zen/PublishClusterStateStats.java index 356b9a29dc..36794e880f 100644 --- a/core/src/main/java/org/elasticsearch/discovery/zen/PublishClusterStateStats.java +++ b/core/src/main/java/org/elasticsearch/discovery/zen/PublishClusterStateStats.java @@ -65,9 +65,11 @@ public class PublishClusterStateStats implements Writeable, ToXContentObject { @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("published_cluster_states"); - builder.field("full_states", fullClusterStateReceivedCount); - builder.field("incompatible_diffs", incompatibleClusterStateDiffReceivedCount); - builder.field("compatible_diffs", compatibleClusterStateDiffReceivedCount); + { + builder.field("full_states", fullClusterStateReceivedCount); + builder.field("incompatible_diffs", incompatibleClusterStateDiffReceivedCount); + builder.field("compatible_diffs", compatibleClusterStateDiffReceivedCount); + } builder.endObject(); return builder; } ``` which makes the JSON-structure clearer in the code.
We've been moving away from these inner fields classes, we don't need them to encapsulate some strings.
This method can be package-private.
This method can be package-private.
We normally name these `in`, it leaves for shorter code and it would be consistent with other places in the codebase.
I think you want ToXContentObject here.
It looks to me like it duplicates the logic of creating a XContentBuilder in a given type and then write the filtered source as map. Could it be something like this? ``` ... Object value = source.filter(fetchSourceContext); try { if (nestedHit) { value = getNestedSource((Map<String, Object>) value, hitContext); } final int initialCapacity = Math.min(1024, source.internalSourceRef().length()); // deal with null here try (BytesStreamOutput streamOutput = new BytesStreamOutput(initialCapacity)) { XContentBuilder builder = new XContentBuilder(source.sourceContentType().xContent(), streamOutput); builder.value(value); hitContext.hit().sourceRef(builder.bytes()); } ... ```
I'd call this `nestedHit` and make it final
EPERM static constant can be removed if it is not use here anymore
Is it possible to not implement BytesReference just to throw the exception but to actually use a "source" that will trigger some JsonParseException when creating the parser from source? I think that would be more realistic and a better test.
Looking at the issue this is trying to fix (#21946), I'm not sure if we need to include the "reformat" and the "x_content_type" in the slow log output.
Can you change this assertion to match the whole error string? It might be a bit fragile to future changes in tests, but since we don't randomize anything here the error should stay stable for a bit and this makes it easiert to understand the whole error structure from reading the test.
Sorry, what I meant by the previous request was to do an assertion on the whole error string (e.g. wie assertEquals), unless there are any reasons preventing this.
Ah! More like: ``` private static final ObjectParser<Parameter, Void> PARAMETER_PARSER = new ObjectParser<>("parameter", true, Parameter:true); static { PARAMETER_PARSER.declareBoolean(Parameter::setRequired, new ParseField("required"); } ``` And they you can call it like `PARAMETER_PARSER.parse(parser, null).isRequired()` like you have. That way we reuse the parser. It probably isn't a big deal, but it is what we do everywhere else so we may as well do it here.
We tend to make these static.
Maybe `Set<String>` instead? The list should be short so it likely doesn't matter but it feels better to me if it is a set.
the closeable contract says it's closeable multple times so we are good I guess.
I don't think it is correct: `included` should also `true` if `nested.isIncludeInRoot` is true? Otherwise we might recurse on a wrong value.
Nit: maybe fix the indentation to allign everything. Then again, code formatting will probably destroy this again soon, so no big deal...
This should be an assumeTrue for windows. Otherwise the test will show up as "passing" for other distributions, even though it hasn't run anything since all the checks below are within a windows block.
I think the less-specific types are better when you can get away with them so that you do not come to rely on implementation details of the concrete type.
Let's type the left-hand side as `Set<String>`.
Just wondering: Is it possible to move `seqNoService` field to `Engine` make the `seqNoService()` method there final? The `SeqNoServiceSupplier` (or BiFunction) should then also be moved as an constructor parameter to `Engine`.
I see, that makes it trickier. Thanks for explaining that.
This empty `if` followed by this line looks off.
Is this clearer? ```diff diff --git a/core/src/main/java/org/elasticsearch/node/AdaptiveSelectionStats.java b/core/src/main/java/org/elasticsearch/node/AdaptiveSelectionStats.java index bed282b899..787fa1e9d1 100644 --- a/core/src/main/java/org/elasticsearch/node/AdaptiveSelectionStats.java +++ b/core/src/main/java/org/elasticsearch/node/AdaptiveSelectionStats.java @@ -33,6 +33,7 @@ import java.util.Locale; import java.util.Map; import java.util.Set; import java.util.concurrent.TimeUnit; +import java.util.stream.Collectors; /** * Class representing statistics about adaptive replica selection. This includes @@ -102,10 +103,12 @@ public class AdaptiveSelectionStats implements Writeable, ToXContentFragment { * Returns a map of node id to the ranking of the nodes based on the adaptive replica formula */ public Map<String, Double> getRanks() { - Map<String, Double> ranks = new HashMap<>(nodeComputedStats.size()); - nodeComputedStats.forEach((k, v) -> { - ranks.put(k, v.rank(clientOutgoingConnections.getOrDefault(k, 0L))); - }); - return ranks; + return + nodeComputedStats + .entrySet() + .stream() + .collect(Collectors.toMap( + Map.Entry::getKey, + e -> e.getValue().rank(clientOutgoingConnections.getOrDefault(e.getKey(), 0L)))); } } ```
the important part is having multiple open readers on this as well.
What I am missing here is a real unittest for this class. Something on the lucene level that does som indexing with random refreshes etc. and that make sure it holds multiple readers etc.
nit: maybe the null check isn't necessary
nit: maybe the null check isn't necessary
nit: looks like searchType cannot be null
at some point we need to get rid of the generics here. I wonder why we still need it but this is a followup...
It feels like this is the wrong place to do this. I think we should do this in `Bootstrap` and then just pass a `Path tmpDir` to `InternalSettingsPreparer#prepareEnvironment` and make sure it's mandatory for all instances of environment that are not the single arg ctor. Btw. it feels like there are some sleeping bugs if this ctor is used in prod code.
I am not sure we should try to clean up stuff in here. We didn't do this for the `java.nio.tmpdir` so I am not sure we should do this here.
maybe just default it to `true` then you don't need to override it in the existing ones
Why remove the internal constant? Seems like this is just asking for a typo in the future in one setting to cause a storm of problems :/
can we maybe cache `(1 + bitArrayKey) * bitArraysSize - 1` to something like lastSeqNoInArray ? I think it will be easier to read.
since this is a `FixedBitSet` I wonder why renamed everything to use bitArray? I don't mind. Just curious.
maybe use `indexOf(...)` and then `indexInsert(...)` and `indexGet(...)` respectively to avoid determining what the slot is for a key several times? ```java final int slot = processedSeqNo.indexOf(bitArrayKey); if (processedSeqNo.indexExists(slot) == false) { processedSeqNo.indexInsert(slot, bitArrayKey, new FixedBitSet(bitArraysSize)); } return processedSeqNo.indexGet(slot); ```
ah, I didn't realize that. Maybe we should do this to make optimal use of the `slot`? ```java final int slot = processedSeqNo.indexOf(bitArrayKey); if (processedSeqNo.indexExists(slot) == false) { FixedBitSet bitSet = new FixedBitSet(bitArraysSize)); processedSeqNo.indexInsert(slot, bitArrayKey, bitSet); return bitSet; } else { return processedSeqNo.indexGet(slot); } ```
Can you test something that is not byte-aligned, like /15 or /17? We used to have bugs in those cases.
I expanded your assertions a bit if you are ok with them: ``` public void testRoundsUpperBoundCorrectly() { ScaledFloatFieldMapper.ScaledFloatFieldType ft = new ScaledFloatFieldMapper.ScaledFloatFieldType(); ft.setName("scaled_float"); ft.setScalingFactor(100.0); Query scaledFloatQ = ft.rangeQuery(null, 0.1, true, false, null); assertEquals("scaled_float:[-9223372036854775808 TO 9]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(null, 0.1, true, true, null); assertEquals("scaled_float:[-9223372036854775808 TO 10]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(null, 0.095, true, false, null); assertEquals("scaled_float:[-9223372036854775808 TO 9]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(null, 0.095, true, true, null); assertEquals("scaled_float:[-9223372036854775808 TO 9]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(null, 0.105, true, false, null); assertEquals("scaled_float:[-9223372036854775808 TO 10]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(null, 0.105, true, true, null); assertEquals("scaled_float:[-9223372036854775808 TO 10]", scaledFloatQ.toString()); } public void testRoundsLowerBoundCorrectly() { ScaledFloatFieldMapper.ScaledFloatFieldType ft = new ScaledFloatFieldMapper.ScaledFloatFieldType(); ft.setName("scaled_float"); ft.setScalingFactor(100.0); Query scaledFloatQ = ft.rangeQuery(-0.1, null, false, true, null); assertEquals("scaled_float:[-9 TO 9223372036854775807]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(-0.1, null, true, true, null); assertEquals("scaled_float:[-10 TO 9223372036854775807]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(-0.095, null, false, true, null); assertEquals("scaled_float:[-9 TO 9223372036854775807]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(-0.095, null, true, true, null); assertEquals("scaled_float:[-9 TO 9223372036854775807]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(-0.105, null, false, true, null); assertEquals("scaled_float:[-10 TO 9223372036854775807]", scaledFloatQ.toString()); scaledFloatQ = ft.rangeQuery(-0.105, null, true, true, null); assertEquals("scaled_float:[-10 TO 9223372036854775807]", scaledFloatQ.toString()); } ```
It works fine with a runtime exception, you don't need to change the constructor: ``` IllegalArgumentException exc = expectThrows(IllegalArgumentException.class, () -> new NGramTokenizerFactory(indexProperties, null, name, settings).create()); ``` .. and then you can check the message of the exception.
Can you add output_unigrams in the formula: maxGram - minGram + (outputUnigram ? 1 : 0) ? This way the default diff is 1 since output_unigrams defaults to true.
I just realized that this would make the upgrade experience really difficult for indices that break this limit already. They would have to close their index before the upgrade, change the limit in the new version and open the index again. That's a bit too much I believe so to be safe we should check this setting only for indices created before it existed: ``` if (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_7_0_0_alpha1)) ``` ... and only log a deprecation warning if the filter breaks the limit on an index that was created an a previous version (see DeprecationLogger and its usage to see how to do that). This way we could also backport this new setting in 6.x and makes the upgrade easier.
Maybe we need to keep ShapeBuilder.parse() and forward to the new method in 6.1 (and deprecate).
I think we need to keep these methods at least in 6.1
I don't think we can remove the ShapeBuilders helper class in 6.1., that might break some peoples code and I think this is user-facing enough. Not sure if that was the plan, I think its okay to break in 7.0 but then it needs to be deprecated.
nit: these could probably even be package private
nit: maybe package private, see above
I don't see any implementations extending this at the moment, are there any plans to add some later? If this is just going to be a collection of static methods and ParseFields I'd suggest making this an interface.
Interesting syntax, any reason not to cast using "(MultiLineStringBuilder)" here? I'm just curious ;-)
Sound good then, no idea about the compiler optimizations tbh, I was just curious.
Thanks, I get the general idea now.
Okay, can you briefly explain the (maybe future) relationship of the ShapeParser and the above GeoJsonParser class? Currently they both seem to mostly consist of a "static ShapeBuilder parse(XContentParser parser, GeoShapeFieldMapper shapeMapper)" method, don't have any state themselves but ShapeParser calls GeoJsonParser. It would be useful to understand where this is going.
we decided to live on the edge and have fun. The concern was around non ascii codes breaking tooling but CI seems happy. Let's see how far we get.
If we assume that all surrogate pairs need encoding, I think we can make this simpler? ``` int startIndex = i; for (i++;i< s.length() && doesNotNeedEncoding.get(s.charAt(i)) == false; i++) { assert Character.isSurrogate(s.charAt(i)) == false || doesNotNeedEncoding.get(s.charAt(i)) == false; } final byte[] bytes = s.substring(startIndex, i).getBytes(UTF_8); ```
Lol - I spent some cycles trying to figure out how the hell we know this won't throw an index out of bounds exception, only to end up learning something about the BitSet api - it's funky ;)
We could use `BytesStreamOutput`
nit: space before RESPONSES
sounds good to me Martijn thanks.
> Also getting the raw byte[] which is what gets used, requires extra steps (BytesReference -> BytesRef -> copy BytesRef's byte buffer) Good point
nit: could be `new MultiSearchRequest().indicesOptions()` instead
nit: could we use `index` rather than `indices`? That is what we have in our docs, indices is just a synonym but I don't think that other clients use it either.
it should be permissive....but we are not testing forward comp properly because of the structure of these objects. This is quite a bummer, we should probably look into testing this, otherwise permissive or not doesn't make a difference as we don't test it.
yes but we discussed as part of this review issues around testing this, because the response contains an array of objects which can be either exceptions or an ordinary search response. Exceptions still can hold metadata fields, hence injecting random fields in there would cause issues and needs to be skipped. It is complicated to distinguish between exceptions and search responses in the array, hence we don't currently insert random fields.
Right, thanks @javanna, I've forgot this discussion. As long as it is permissive, even not tested, it's OK to be merged.
I think it could be more permissive and just ignore the field? We used to be permissive when parsing responses in order to have a better forward compatibility...
I wonder if this should rather be made part of Request#multiSearch like we did for bulk. I see that it may be nice to have read and write methods close to each other, on the other hand the only place where we need to write this format is in our client.
yea I see that also bulk depends on BytesRef which is not great. If it's too much work we can do it as a follow-up.
gotcha. shall we have an innerFromXContent that does not require start_object then, and accepts that we are already at the field_name token? That could be called from msearch? Exceptions should be parseable already with this technique.
nit: I think that we can drop this loop here and in other tests, and instead just rely on multiple runs of the same tests in our CI.
we have a test util method that we use to shuffle fields in the response so we make sure that our parsers don't rely on specific keys ordering.
this is odd especially because it seems that once you set a value for this field, you can never reset it to its original default value.
I see, yea we can't avoid this then. Maybe share the default value through a constant so at least we don't duplicate it. Odd! :)
I fell like a functional interface here doesn't really buy us anything comparing to a simple if statement with two calls.
Looks good. Thanks :)
Hmm...I wonder if we should have this here at all. There are so many cases where this is not correct and I think that could be very frustrating for users: - node does not bind to localhost - node does not accept HTTP (only HTTPS) - node requires basic authentication - user does not have `curl` (Windows?) - node does not bind to port 9200
make it `final`
I feel we should still have some form of warning if you claim to be using a charset that isn't UTF-{8,16,32}, as per #22769. This might not be the time or the place to ask for that, however.
can we add to "not strictly needed here" , "not strictly needed here (we will never have collisions between sequences numbers in the translog files in a new primary as it takes the last know seq# as a starting point)"
while we're at it , can you check more than just the routing table and the index meta data? we have blocks, nodes and customs (out of the top of my head).
I think that inserting random fields here would reveal problems on the parsing side with the current code.
Is this a typo? Not sure how this compiles...
It's a non-static inner class, that's the syntax for constructing an instance of an inner class that is tied to an instance of the outer class.
Ahhh, it's been a long time since I have seen this syntax...
nit: can be package private (maybe I didn't see that in the master PR)
nit: can be package private (maybe I didn't see that in the master PR)
This is not thread-safe. The problem is the underlying cache implementation that backs `IndexFieldCache` [says @this](https://github.com/elastic/elasticsearch/blob/1e99195743a9a4d0cb637f4f7ceec92ab7867518/core/src/main/java/org/elasticsearch/common/cache/Cache.java#L553-L559): ``` /** * An LRU sequencing of the keys in the cache that supports removal. This sequence is not protected from mutations * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is * undefined. * * @return an LRU-ordered {@link Iterable} over the keys in the cache */ ``` and this method is invoked behind the scenes to invalidate all the keys.
It might be cleaner and create less new-Function objects if you extract this compute block as a new method, say "newIndexFieldDataCache(fieldName)", then just do `fieldDataCaches.computeIfAbsent(fieldName, this::newIndexFieldDataCache)` here.
we also support a parameter called `updateAllTypes` here.
I would flip this logic to remove negations. ie `mergeWith.meta == null ? meta : mergeWith.meta`. Positive logic is easier to reason about. It might also be better to just move it out to a local instead of trying to inline it.
To be clear, I was referring to both `globalCheckpoint` and `docId`.
Do these really have to be test instance fields? Can they be passed around? They are easy enough to construct.
Minor nit, we probably don't need to pull this into a variable if we're just immediately passing it into the `parseFieldsAndWeights`
I was trying to understand why this works, because of the forward iteration here (with nested, we usually seek backwards (`BitSet.prevSetBit(...)`)). So this works because all live doc ids (root docs and nested docs) are evaluated in order.
Ok I missed the recovery-private thing. Thanks
nit: some whitespaces around the operators etc... would be nice
just as feedback, nothing to change really, but I liked the previous variable name better ;-)
nit: `long`, otherwise it gets outboxed later anyway
I see, and you are right, camel case is preferred. I probably misread the "NoNestedDocs" part of the name as "no nested docs" and that confused me for a second, but either way is fine.
Thanks, I probably read the test to quickly and missed the startArray() part. You are absolutely right, this is how I expected the test. Sorry for the noise.
Yes, I saw the explicit test and I was hoping for a randomized polygon/hole thing, but I just realized that also GeoJsonShapeParserTests doesn't do this fully randomized. So if it is difficult I'm also fine if there is only the explicit test.
Is this extra method needed? I would combine it with the previous one that seems to be its only caller atm.
I get a warning for a missing GEOMETRYCOLLECTION case label here. Even if that shouldn't be possible, maybe add this to the switch (and/or a default case) that throws.
nit: weird indentation, could this throws clause move to the previous line
nit: might throw in a DEFAULT case instead.
I think we need that test.
I would prefer at least one good and one malformed case per test run. Otherwise we only catch bigger errors on multiple repetions and not e.g. on PR tests.
Ok, I see. Nevermind, since its a private method I leave it up to you to change or not, I was confused a bit but the method is short enough to understand what its doing.
Would be nice to add this as part of this PR unless something big blocks it.
same problem as above I think, only that unmatched closing brackets will create holes here.
This will also accept something like "BBOX (-10) 10) -10) 10)", I think this should throw an error. Its a bit constructed but I think the parser shouldn't be that lenient.
The shapeFieldMapper seems unused here.
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
nit: formatting, add some whitespaces
nit: formatting, add some whitespaces
nit: doesn't really matter but mostly I see this as // TODO. Might be better to keep it similar to be able to grep similar things or for other tooling.
Might be slightly better to return a StringBuilder here as well to not create an additional object? Maybe this could also be done in several other places in this PR where partial WKT strings are built (e.g. all the contentToWKT calls)
nit: formatting, add some whitespaces
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: Invalid number of points in LineString (found 1 - must be 0 or >= 2) at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:3A8577712E4A2AD2]:0) at com.vividsolutions.jts.geom.LineString.init(LineString.java:102) at com.vividsolutions.jts.geom.LineString.<init>(LineString.java:93) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:539) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:531) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseLineString(GeoWKTShapeParserTests.java:99) ... ```
nit: formatting, add some whitespaces
you can do some streaming java8 magic here.
do we protect this from double invocation? I think we should just make sure we only invoke once. can you wrap it in here and ensure that? maybe just use `NotifyOnceListener`
it's find in this case! LGTM
It's that, or we can replace replace FsRepository with this one, but we need to beef it up.
I think we can just use an FsRepository for this. All our other shard-level tests do the same, so no need to optimize this. If we want to change that in the future, I think it's easier to switch to jimfs and continue using FsRepository.
Is there a need to substract the identical ones? The filesToRecover, which we iterate over, won't contain those anyhow.
you can move this method (and the one below it) up to IndexShardTestCase (in test:framework). It could be useful for other people.
please use the root locale
also please use `== false` for comparison
I would use the following message: "ignored as shard is not being recovered from a snapshot" and not have an explicit check for `shardRouting.primary() == false`. That case is automatically handled by this case too as replica shards are never recovered from snapshot (their recovery source is always PEER).
"close or delete the index" I would also lowercase the "reroute API"
this assertion is not correct I think. If a restore for a shard fails 5 times, it's marked as completed only in one of the next cluster state updates (see cleanupRestoreState)
I would write this check as ``` if (shardRestoreStatus.state().completed() == false) { ``` and then add an assertion that `shardRestoreStatus.state() != SUCCESS` (as the shard should have been moved to started and the recovery source cleaned up at that point).
just wondering if it's possible for `shardRestoreStatus` to be null. I think it can be if you restore from a snapshot, then the restore fails, and you retry another restore with a different subset of indices from that same snapshot.
can you also add ``` @Override public Decision canForceAllocatePrimary(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { assert shardRouting.primary() : "must not call canForceAllocatePrimary on a non-primary shard " + shardRouting; return canAllocate(shardRouting, node, allocation); } ``` as this is a hard constraint with no exceptions
Randomized runner should not need these @Test annotations.
This package statement is in the wrong place (it should be after the copyright header).
can we move this part to the setRefreshPending method? this will come at the expense of a dedicated listener but all the code that changes pendingRefreshLocation will be in one place making it easier figure out.
I spent some cycles reading this and convincing my self that we can do this regardless of the searcher scope - i.e., that if we end up here with an internal searcher it will be OK (it is!). I would personally think it will be simpler to follow if we mark access in `awaitPendingRefresh`. That would mean all flow /state management is around the engine and you don't have to go through engine code to see how things work. This is of course subjective, so just a suggestion.
nit: maybe call this `awaitSearchActive` (or `markSearchActive` if my other suggestion is accepted to move setting the timer here) ? pending refresh is an internal implementation detail..
I'm on the fence as to whether we should only do this on non-realtime get. Real time gets don't really relate to refresh cycles (they force a refresh if needed). They are already "efficient" in the sense that they only refresh if they need to (i.e., there's a pending doc change in the version map).
thanks for unwrapping
+1 for deprecation of `jarowinkler`. Not sure about that score creep tho :)
This sucks.. I want to see how big of a deal it is to keep things as they were. Indeed snapshotting a commit will keep it's translog around but I'm not sure anymore it's worth this kind of wrapping layers. Maybe we should invest in faster clean up on those snapshotted commits. I'll reach out to discuss.
nit: please use lowercase start for variable names
nit: please use lowercase start for variable names
nit: please use lowercase start for variable names
nit: please use lowercase start for variable names
I just see that this is almost the same as in TokenListCreator. One way to maybe avoid this would be to create a small inner TokenCounter class here that encapsulates the current count, the maxTokenCount and would have e.g. a "count()" method (naming might be different) that takes care of incrementing, checking the maximum and potentially throwing the exception. That counter could be passed around instead of the `int maxTokenCount`.
Maybe add something like "produced by calling _analyze" and maybe the index name (not sure if this is easily available here, if not its fine) to make it even clearer what the offending call was.
nit: since the tokenCount is not really an argument of the analysis request, maybe use IllegalStateException? I just checked and I also used IllegalArgumentException in a similar case recently, so I'm fine either way.
I think it would be good to be consistent and always throw
This can (and should) be final.
This empty line can go.
I don't think "Not implemented yet" adds anything other the exception type (and could be misleading if we never intend to implement).
I don't think we need an implementation for this method however I think this could be `RamUsageEstimator.shallowSizeOfInstance(CountedBitSet.class) + (bitset == null ? 0 : bitset.ramBytesUsed());`. You could even fold `RamUsageEstimator.shallowSizeOfInstance(CountedBitSet.class)` into a static final constant.
I think this should be a hard illegal argument exception.
similarly here I would like it better with a regular for loop and by making fillSegmentInfo take a single segment at a time
@cbuescher thanks very much for your detailed explanation. I have not noticed that there was another berlin query. It makes total sense to average precision across queries.
we need these only in IndicesClient, so that users can do client.indices().openIndex() . No need to add them here too.
thank you, seems like I left this out :)
hard -> shard
the fact that we do not look at the value of msm for matchAllDocs looks like a bug
Shouldn't this only be consuming the `help` key? Any random other params should still cause an error.
As @rjernst says, this is incredibly broken behavior. It invalidates all the work we did to reject unrecognized URL parameters, now it accepts any parameters including garbage parameters.
can we assert that if we need to generate a new history uuid we always use `*_CREATE_TRANSLOG` as an open mode? that's why we rely on the translog uuid only for trimming purposes (and avoid thinking about what it means to generate a new history uuid)
lol. over fanaticism. can we just have -1, 100ns and 60m? (ie., none , very likely to happen and never)
nit: add the current open mode to the message please
Yes, I am fine with that.
Please rename to `ineligibleTargetNodes`. Also, can you use a set of `ModelNode` here, similar to `throttledNodes`? Finally, can you make it a local variable to the `moveShards` method, so that it's clear that this is only used when moving shards. You can also add it as an additional parameter to the `decideMove` method and pass in an empty set when called from `decideShardAllocation`.
add space between `if` and `(` (in other places as well)
this method should be renamed to `canMoveAnyShardToNode`, as we assume that the shard that's being allocated here is a relocation target, otherwise we would not know whether to check `cluster.routing.allocation.node_initial_primaries_recoveries` or `cluster.routing.allocation.node_concurrent_incoming_recoveries`.
OK I was lacking context actually. The use of `string` potentially hid a bug, switching to `text` fixes it. +1 to change it.
This doesn't look good. This test must be broken is some way.
while we're looking at this: Does it make sense to put this into a conditional along the lines of ``` if (indexShard.getTranslog().getLastSyncedGlobalCheckpoint() < globalCheckPoint) { indexShard.getTranslog().sync(); }
nit: extra space between `=` and `setting`
I think this can be done easier with an Iterator ```java final StringBuilder spaceDelimitedJvmOptionsBuilder = new StringBuilder(); Iterator<String> it = jvmOptions.iterator(); while (it.hasNext()) { spaceDelimitedJvmOptionsBuilder.append(it.next()); if (it.hasNext()) { spaceDelimitedJvmOptionsBuilder.append(" "); } } return spaceDelimitedJvmOptionsBuilder.toString(); ```
I think this should have a `continue;` after it for the sake of completeness, otherwise it's possible that it will be added to both the valid and invalid lists (it'll still fail the parsing, but to prevent future bugs)
Whoops yeah, I only looked at the version for `Files` rather than `Files.newBufferedReader`, sorry
Same here about parsing end ranges that are out of the integer range
I think this needs to be wrapped in try/catch so that this doesn't cause a missed invalid line like: ``` 2147483648-:-XX:+TurnOnCatLasers ```
This could be: ```java try (BufferedReader br = Files.newBufferedReader(Paths.get(args[0]))) { ... } ```
Why did you opt for the consumer route rather than having `parse` return a `List<String>` and still have an invalid line consumer? My personal preference would be for a purely (or closer to purely) functional interface rather than a callback-ish one (I'm not saying you have to change it, I'm just curious what the reasoning is)
Do we gain anything by passing a reader into `parse`? It seems like it'd be easier for testing if you pre-read the file into a String or a list of lines and then passed it into parse. Are we concerned about the memory of reading `jvm.options` up front? (It would also let us separate errors reading the file from errors parsing the file from a feedback perspective)
I do think it'd be nice if we could accumulate errors and print them at the end, rather than bailing on the first error, forcing the user to rewrite/rerun, then bailing on the next error, over and over for each error.
Also we should wrap the `-` in `{@code -}`
These should all be wrapped in `<pre>` or `{@code ...}`
It would also allow us not to have to make `parse` throw a checked `IOException`
Perhaps mention that the argument that is being expected is a filename of the jvm.options file
I hope we never have to implement this for minor or patch versions of the JVM :X
nit: maybe use expectThrows() like we do in many other places
can we remove this default ctor and pass the non-recycling closure in the test directly. I think it's just one place.
you can use `ExceptionsHelper#rethrowAndSuppress` here
I think we can just access the member directly.
+1 looks redundant to me
+1 from my side as well
+1, this is not needed.
this should be "white spaces" -> "whitespace"
"white spaces" -> "whitespace"
I am leaning towards 2. it's much cleaner
if we fail to start do we close the started ones
if we can prevent this kind of state handling we should. My experience with stuff like this is consistently negative so lets try to move it to the ctor and make it clear once the group is created it's ready to use. I still think we should have an `ensureOpen()` method that checks a boolean if we are still open.
Alternatively we can move this logic to the `beforeRefresh` method as this is the only place it's used at.
I think this is a left over.
is it true that it's not needed or is it more that we don't want to change refresh semantics in an assert method.
just an idea - shall we fold this into the IndexingStrategy? this means that we can set this flag under the static methods such as `IndexingStrategy.overrideExistingAsIfNotThere`. I think it will be easier to follow the logic.
this breaks backwards compatibility, you will need if based on version in both `readFrom` and `writeTo`
Not sure about this. We're making a string of the form: ```threadName > ... > threadName > lockOwnerName``` This change removes the last `>` between the final `threadName` and the `lockOwnerName`. I think the original _behaviour_ looks right, although the condition was always true so the `if` could be simply unwrapped.
was this another problem? I wonder why this assertBusy is needed.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
I know that I told you to add this TODO, but I looked deeper and I don't think we will address this anytime soon. Let's remove it, I think what we do now is fine.
why not calling `RestActions#buildBroadcastShardsHeader` instead ? Aren't we losing support for the `group_shard_failures` flag? It is not relevant for the high-level REST client as there is no way to set it but I think it's important given that the parsing code is in ES core. Which reminds me, we should probably test this as well in `RefreshResponseTests`. This param can be passed in as part of the `ToXContent.Params` when calling `toXContent`
I don't think this boolean does what you want? if start commit is null it will always pass.
I think you want to say that no commit point was found which could be recovered from the translog.
> We fillSeqNoGaps until the max_seqno -> we need to use max_seqno from the last commit to have full history. We do so after we replay the translog. at which point we'll know what the max seq is (and it may be higher than the one in the last commit, but it can't be lower)
i like the final approach better this gives a uniform return value (easier to understand) and it makes sure these things are set.
since we open the translog before the writer now - can we use the open translog for all this information rather than static reading it? this will make sure that we use something that went through all the right validations.
strictly speaking I think we need to read this from disk after the flush - i.e., make sure that what's on disk is OK.
I see - we reuse the same engine variable. I think it's cleaner to have it contained.
if (this.totalSizeInBytes == -1) { this.totalSizeInBytes = other.totalSizeInBytes; } else if (other.totalSizeInBytes != -1) { this.totalSizeInBytes += other.totalSizeInBytes; }
nice, sorry I missed it.
you can make one of them public and call it from both tests, I don't mind
move the double parsing below the float parsing? So that it is consistent with integer/long auto parsing.
Can we use CountDownLatch instead of assertBusy
I think we can clean this method up - it doesn't add much but craft and at least in one place we go and do the same conversion later on - might as well capture it as a value on the caller ``` try (Releasable ignore = acquireLock(get.uid())) { // we need to lock here to access the version map to do this truly in RT + versionValue = getVersionFromMap(get.uid().bytes()); + } ```
nit: I think the code style is to avoid negating using `!X` and prefer `X == false` so its harder to miss the negation
I think we should make this a `boolean`
I think we should make this accept a `boolean`
yes I meant just the setters
NO unfortunately there is no `Property.ClusterScope` so I think it makes it more important to have the setting resolved on the coordinating node so that all shards use the same value.
this is interesting - we explicitly check it's not there? https://github.com/elastic/elasticsearch/blob/master/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java#L586
I suspect these will be too small and we'll have time outs.
In case of LOCAL_SHARDS/RESTORE, we could again call activatePrimary here.
Note that in case of peer recovery with a retry, we could end up with a higher gcp in the globalcheckpointtracker than what we're setting here.
I think a nicer approach (can be a follow-up done by me) would be not to call `updateGlobalCheckpointOnReplica` here, but instead call ``` globalCheckpointTracker.activatePrimaryMode(SequenceNumbers.NO_OPS_PERFORMED); ``` either here or in the IndexShard constructor (where we create the GlobalCheckpointTracker) when the recovery source is EMPTY_STORE.
I would prefer not to call `updateGlobalCheckpointOnReplica` on the `GlobalCheckpointTracker` if the shard is a blessed primary. A shard that's created from snapshot / local_store / local_shards is by definition blessed from the master. It should just activate the tracker. The activation logic for a replica can be different than for a primary.
I think there's an extra 'if so' here.
I have a couple questions about this assertion: - I think the determinant could be 0 if the three points lie in a straight line. Is my understanding correct that this won't happen because of the way we choose `top`? It might be nice to add a quick note to explain. - Are we concerned about numeric underflow here? I'm curious as to how we handle numeric issues in other parts of the geo code (in particular, whether we should throw a runtime exception when an issue is detected).
This condition is equivalent to simply `seenMount`. Effectively, `seenDevice` is unused. I think this means something is missing/wrong in the test cases.
This seems to test the case where the same path has a number of distinct mount points. Can this happen? I can't think how.
Nit: please add spaces around the `=` sign.
The sizes are `long`s so I think `randomLongBetween()` would be better.
The existing code doesn't filter out paths whose `mount` property is `null`, so I don't think we should start doing so as per this test case.
This doesn't look right to me. If `subPath.path` is null, or if `seenDevices.add()` returned false then we don't call `seenMounts.add()` despite having seen the mount point in question.
I think this adds a `timeZone.previousTransition()` computation even for fixed time zone cases. I think to save computation we could return even earlier in unproblematic cases when `timeZone.isFixed()` like before.
The idea behind `sniffAfterFailureDelay` is that once a node fails, you want to re-sniff, but you may also want to check if the node has come back through a new sniffing round earlier than you'd usually do. So in general `sniffAfterFailureDelay` would be lower than `sniffInterval`. I see that here you are using now a different interval for cases where sniffing itself has failed, but that was not the purpose of the `sniffAfterFailureDelay`. It is not a bad idea to actually re-sniff earlier if something goes wrong, but it should be a different concept.
I suspect that having to pass the next interval which differs from the usual interval is the reason why this was not async in the first place :) we need to find a way to do that though, we currently lose that behaviour with this change. Another idea, maybe overkill, could be to change the hosts sniffer API to be async and accept a listener as an argument.
There's no need for reflection here - writing out all the fields, in a sensible order, is much preferred.
Generally multi-line output from `toString()` is a bit of a pain to work with. +1 for using the system's line separator, and I think `System.lineSeparator()` works too as we're in Java 8 here, but a one-line output would be better.
why change this to an AtomicReference? Just because of stylistic reasons or is there more to it? Looking through the PR, I could not find a reason for this change. Every write access to it is guarded by a mutex, and read access is ok with volatile. Let's keep it a volatile variable.
instead of changing the state first and then checking whether the previous state was the right one, let's only change the state if the current state matches (note that we're under the mutex here already, so it's safe to do this).
Oh, that error handling!
Yannick and I discussed this option first, but this needs extra care, for instance to not copy the write lock. It's also a bit more involved if we want to track statistics as well for the first source. In the end it's not clear to me which option is better.
I was just opening the issue but I'll wait to see the conclusion here first in case we decide copying the first directory manually is still a better trade-off.
It was not really updated. That's just set here so that endSnapshot is called below. Maybe instead of updatedSnapshot and accepted variables we should have an endSnapshot variable that captures the snapshot to end.
`state == State.STARTED`? Otherwise no need to define the local variable `state` above
add `assert entry.state() == State.ABORTED` here. You can directly write the message as "snapshot was aborted during initialization" which makes it clearer which situation is handled here.
For now, let's move the construction of Definition out of the loop, so we still only create one, until we can figure out if we need an internal cache to ensure too many copies aren't loaded of common classes.
I think `iff` is intended here, meaning "if and only if" here (https://en.wikipedia.org/wiki/If_and_only_if), I'd leave this as is.
Nit: space after `))` so `)) }`
Nit: space after `))` so `)) }`
I don't think this will be needed if you take the suggestion below, but in general `assertFalse` and `assertTrue` should be avoided because they do not give good error messages (a failing expectation would only say `AssertionError`). Here, we could use `assertThat(value, not(isEmptyOrNullString())` as then the expectation would say ``` Expected: not (null or an empty string) but: was "" ``` which is a lot more helpful!
Nit: spacing between `!` and `value`.
Nit: spacing between `while` and `(`.
same thing here. We could just call `abortIfNotCompleted`, and then take a snapshot and then act based on FINALIZE, DONE, FAILURE
this looks a bit odd of a toString() implementation as it's very much targeted towards that one logging call site. Maybe change it to be more generic.
no need to be volatile anymore
this does not need to be volatile anymore.
I would prefer to have the previous boolean `isAborted` method. Call sites currently catch this exception and then throw another exception.
I wonder if we can also use `abortIfNotCompleted` here. Finally, looking at `BlobStoreRepository.snapshot`, we should also do the `store.decRef()` before we move to finalization. The store is unnecessarily kept open (possibly causing ShardLockObtainFailedException) and the snapshot process does not check abortion while it is doing its finalization.
much more elegant ++
this smells like it should be a setting validation thing. Testing for this so deep, on every request without throwing exceptions feels wrong.
This should use ephimeral ids, not object identity
All of this is equivalent to the simpler ``` return Objects.equals(newMasterNode, previousMasterNode) == false; ``` (see equals implementation of DiscoveryNode)
`getMasterNode()` already returns null if `masterNodeId` is null. Maybe cleaner to make that explicit in the `getMasterNode()` method (and not rely on the map implementation to do that for us) and also use `@Nullable` on the return type. The effect is that we won't need these conditions here and can just write `return new Delta(other.getMasterNode(), getMasterNode(), ...`
as an alternative you can mark it as final abstract then you don't need the private ctor
I'm all for 256 bit, but the Oracle JDK 8 ships with a limited JCE policy that restricts key length for AES to 128 bits. To prevent friction with using our secret settings, we need to use 128 bit keys until we have Java 9 as the minimum supported version, http://www.oracle.com/technetwork/java/javase/terms/readme/jdk9-readme-3852447.html#jce
cool lets move on with this.
nit: than -> then (or just leave it out)
nit: extra space
Nit: maybe if..else would be more readable
Maybe it would be good (for ease of removal in the future) to flip around the joda impl below, so that it is the wrapper method, and the core parse method (which handles no time zone) is the same that handles ZoneId? This could be a follow up.
I don't think this is correct? Do tests pass? This should fail on unmapped fields.
we should probably throw an UnsupportedOperationException here, as we should not use it.
we should probably throw an UnsupportedOperationException here, as we should not use it.
Minor typo, "Non" -> "No"
I think it's always a single node cluster, but I'm good to keep it like this.
Thanks for reformatting ;)
`maxWrnHeaderSize` -> `maxWarningHeaderSize`
My concern here is if a user sets the budget to `H` headers and `B` bytes because they can not handle more than that (e.g., the common case being a proxy) then we have to subtract a header (or possibly many) to stay under the `(H, B)` budget after we include the missed warnings warning.
I think it'd be useful to get one more warning when the limit is hit, saying that there were more warnings but we dropped them because `http.max_warning_header_count` is set to `<n>`, and similarly for the size limit.
I think it's more useful if we spend the last of the budget saying the list is truncated, in anticipation of a time when someone forgets that the list they're looking at might be truncated and makes a poor decision on the assumption that it's complete. These excessive warnings are often repetitive, so the specifics of the 62nd warning seem less valuable to me. (Not that it shouldn't be in the server logs too.)
Nit: spacing between the `)` and `{`: `){` -> `) {`
`newMaxWrnHeaderCount` -> maxWarningHeaderCount` (and reference the member field by dereferencing `this` then).
`){` -> `) {`
Make the method parameter `final` too; this is a safety guard against accidentally assigning to the method parameter instead of the member field.
`newMaxWarningHeaderSize` -> `maxWarningHeaderSize`, and reference the member field by dereferencing `this`.
Take care, the warning headers are de-duplicated below. The accounting should happen after de-duplication, otherwise we truncate when a warning header would not have been added because it is a duplicate of an existing warning header.
This is a great suggestion, I think it should be a warning in the server logs as opposed to an additional warning header that eats into our budgets.
Make the method parameter `final` as a guard against accidentally assigning to it instead of the to the member field.
Let's use traditional Java variable style names here (e.g., `maxWarningHeaderCount`).
`wrnHeaderSize` -> `warningHeaderSize`
The user already expects if they have set this setting that warning headers will be truncated. Therefore, I don't think we should do this, losing a warning header (or more) at the expense of a warning header saying that there are more warnings that can be found in the logs. Instead I think that we should log (in the main Elasticsearch log) when we truncate.
I think that we want to log *each* time that we drop a warning header, not only the first time for a given request. Also we can be more precise than the current implementation which says one or the other condition is met, but we always know exactly which condition it is so we can help the user more by letting them know.
This is a code smell, setting static fields from an instance constructor. I think that we want to do something different here, so that these fields are instance fields. I can offer a suggestion on how to do this, but I prefer to let you spend some time coming up with a solution on your own (I don't want to take the joy of problem solving away from you).
I think there is a problem here. Imagine that `maxWarningHeaderCount` is set (not equal to `-1`) and then a user dynamically sets it to unbounded before the `if` statement on the following line (i.e., after we have passed the set condition). Then we will see the updated value and `warningHeaderCount` will always be greater than `maxWarningHeaderCount`. We want to avoid these sorts of race conditions.
`curWrnHeaderSize` -> `currentWarningHeaderSize`
Higher up at the caller we log in trace. If we want debug info then I think we should have a nice summary of the recovery starting info. I would recommend just making this trace for now. Also nit- maybe say something like "calculated starting seq# based on..."
yea this sounds good to me.
yes I would adapt the docs, let me know if you prefer me to take this over given that it's becoming a bigger task.
I don't mind either way
can we check the commit id in the stats and make sure it's the same? alternatively, can we maybe find the segments info for the right commit and create a stats object there? You're right about the synced flush not caring, but I'm worried that we get false scary warn message. I want to avoid that.
it's important to log the shard routing of the out of sync shards
I believe this should be ``` + if (MULTIVALUE_MODE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { ```
These might be better if they just fail rather than go to the DepercationLogger. This is fine though.
That was not my intention. I meant it the way @jaymode suggested: we should fail when trailing garbage is present, and have a test for that.
Adding the `is` verb is good enough on my end, thanks.
The score of this query depends on the number of shards, the default similarity, ... To make sure that we have consistent scoring you can use a `function_score` query like the following: ```` QueryBuilder query = functionScoreQuery( termQuery("name", "one"), ScoreFunctionBuilders.fieldValueFactorFunction("my_static_doc_score") ).boostMode(CombineFunction.REPLACE); ```` ... and add the `my_static_doc_score` at indexing time.
You can use the same for the rescore with another field for instance
More indentation that is hard for me to read.
Same deal as last time with indentation. Also, why not `&& TYPE.match` like you usually do? I like that.
And when we go to actually solve the problem is gives us a potential way forward.
Bleh. These are problematic because `CreateIndexRequest` really feels like it shouldn't need stuff "from the server". At some point we'd like to extract it for the high level rest client. Can you make a `source(Map<String, ?> source, DeprecationHandler deprecationHandler)` version of this method and make this one delegate to it? That doesn't solve the problem but I think it makes it more obvious.
I think it'd be cleaner to take the `DeprecationHandler` as an argument here. I really want to minimize the places where we use the static instance of `LoggingDeprecationHandler`. My ulterior motive is that we *might* be able to remove it entirely at some point which would be super neat.
If you think we shouldn't I'm ok with it but I find it so much more readable that way.
I wonder if it's easier just to isolate some of the replicas among each other, i.e., take a subset of replicas and isolate them from the remaining replicas. This means that all replicas stay connected to the master.
maybe, to be more precise, it would be good to check the partition that included the new primary.
start the data node first? There's no need to wait on yellow then (index creation waits for primaries to be allocated by default).
Can you indent this one extra time? When the body of the block below it and a part of the `if` statement line up I have trouble separating them properly when reading quickly.
Looking at this I was wondering if the latch was needed with the introduction of the barrier, it can probably be removed.
+1 for doing it this way. I think that this'll make extracting this class from core into an xcontent jar more difficult but I think we should cross that bridge in another PR.
Maybe it doesn't have to come at all.... I think only `copyCurrentStructure` is part of the xcontent implementation. The rest is just "stuff that ES uses". I think.
++ for doing it this way for now.
Funky! It'll do though.
why do we need to read the checkpoint again? I think we need to be consistent here - either with throw the underlying exception (and thus not protect about FileNotFoundException) or catch everything and rethrow a corruption exception. I tend toward the later..
I think we can just read the uuid of the generation associated with the checkpoint? I think this is overly fanatic, no? (I want to make a more complete validation at one place in a different PR - complete in the sense, check multiple lucene commits and multiple generations.
I think it'd be nice if we had a nice error message for folks that install a plugin with the directory.
I opened #28603.
I think that we should have left an assertion here that the Java version is not JDK 11 (I think we will be able to remove this for JDK 11). I also think that this code should have been guarded by an if block checking that we are on JDK 10 and otherwise not add this permission.
Out of curiosity, we cannot do a check with the similar intention in the policy file itself, no? To me it looks like this part of the code is only used for testing.
No, it cannot be private, since it's used by `AttachmentProcessor` in the same package. That was the part that I was missing. I think its misleading.
I can push the change if you don't have it ready yet.
To be clear - I think we want to know how the oldest file is, regardless of the generations. It will always be the oldest generation and the first in the reader list, but I don't think we want to rely on it. Part of the role of the stats is to validate things are correct.
oh I see. I missed it.
it will be good to use randomized numbers here. It will be easier to reproduce failures and it will also test time go back.
I would rather not have the uncheckedGetLastModifiedTime method. Instead just do old school iteration on the reader list, keeping the min of the last modified time. Then do a one off extra min with the writer's value. Re the test - I would prefer if you make an array of random positive long values, map readers to them and make sure the method returns the right value when compared to a calculation that is made directly on the array.
Can we add a Math.max(0, currentTime - Math.min()) ? we rely on this being non negative, but time may go back and the FS may have other quirks.
maybe make this method static, give it a list/stream of readers and the current time as parameters? this would allow for proper testing with mocks and fixed time.
> Sure but we can't use BaseTranslogReader:: getLastModifiedTime as the method throws a checked exception. Fair enough. No streams for us - we need to do it the old fashion way :D > Does Stream.concat(readers.stream(), Stream.of(current)) not include the writer? Yes. Current is a TranslogWriter.
these are eclipse specific tags right? I don't think we'd want them in our code.
inserting random fields is not a good idea for requests. we should parse them strictly. In fact if this test succeeds it means that we have a bug :) we should rather test that we are strict when parsing requests.
yes please I haven't seen them used before in our codebase. At some point we will automate formatting and these classes will have to somehow be ignored I think.
I was thinking something like: ```diff diff --git a/server/src/main/java/org/elasticsearch/Version.java b/server/src/main/java/org/elasticsearch/Version.java index 1e1993e6d6..c0181d4f1e 100644 --- a/server/src/main/java/org/elasticsearch/Version.java +++ b/server/src/main/java/org/elasticsearch/Version.java @@ -372,7 +372,9 @@ public class Version implements Comparable<Version> { public final byte build; public final org.apache.lucene.util.Version luceneVersion; - private List<Version> declaredVersionsCache; + private static final class DeclaredVersionsHolder { + static final List<Version> declaredVersions = Collections.unmodifiableList(getDeclaredVersions(Version.class)); + } Version(int id, org.apache.lucene.util.Version luceneVersion) { this.id = id; @@ -414,17 +416,10 @@ public class Version implements Comparable<Version> { public Version minimumCompatibilityVersion() { if (major >= 6) { // all major versions from 6 onwards are compatible with last minor series of the previous major - List<Version> declaredVersions = declaredVersionsCache; - - if (declaredVersions == null) { - declaredVersions = Collections.unmodifiableList(getDeclaredVersions(getClass())); - declaredVersionsCache = declaredVersions; - } - Version bwcVersion = null; - for (int i = declaredVersions.size() - 1; i >= 0; i--) { - final Version candidateVersion = declaredVersions.get(i); + for (int i = DeclaredVersionsHolder.declaredVersions.size() - 1; i >= 0; i--) { + final Version candidateVersion = DeclaredVersionsHolder.declaredVersions.get(i); if (candidateVersion.major == major - 1 && candidateVersion.isRelease() && after(candidateVersion)) { if (bwcVersion != null && candidateVersion.minor < bwcVersion.minor) { break; ```
I see, thanks for investigating.
I think this should be `p.getDeprecationHandler()`.
I think an explanation why it's ok to throw an exception here might be helpful for future us.
Probably it would be worth implementing AbstractStreamableTestCase#getMutateFunction in this case, so that the equals/hashCode tests are more complete. I think without it a part of them is skipped.
+1, I'm in favour of changing the indentation. Unfortunately we don't seem to have a way to catch accidental reformatting of these sections in the future. Just a remark, nothing to change here I think.
Nit, and I know it was there, but there's an extra space between `URLClassLoader` and `)`.
Wondering if this could be abstract, so subclasses don't forget to implement it. Looks like many would have an empty implementation though, so just a thought.
s/it/in , also not sure what you mean with "the BytesReference will be removed in a future commit", maybe the sentence needs to be clarified.
Yes please, the change has nothing to do with the title (it does with the goal, but not the title).
Shouldn't this test still be calling getSelectedClient (which in turn calls generateOperationContext)? Otherwise the name should be updated.
Could you assert the entire text of the message as you had done previously? It helps to clarify what this bit of code is testing.
Nit: please add spaces after the `if` and before the `{`.
Nit: please add spaces after the `for` and before the `{`.
This line is longer than 120 characters so CI will reject it.
`ThrottlingAllocationDecider` is not imported, so this is a compile error.
Nit: please add a space before the `,` separating the function arguments.
this needs a message
Apparently @s1monw prefered the reverse. I'm fine with leaving as is.
Looks like the max is still here for time stamps? maybe assert it's -1 (for both seq# and timestamp)
can we introduce a method similar to mayHaveBeenIndexedBefore that does the check and also updates the `maxSeqNoOfNonAppendOnlyOperations`? I think it's good to have both marker handling consistent.
I think you should inline this into `planIndexingAsNonPrimary` then we don't need all the asserts
maybe just inline this into the `planIndexingAsNonPrimary` method? I think that would be cleaner.
`Matchers` is the more idiomatic way to do this.
is there any difference? I personally never know which one to pick :)
no let's not wait, I would get this one in, for now let's never throw exception, users can read errors if they wish.
`fielddata` is the preferred name as of my merging #28943 today.
Can you update the `\rest-api-spec\src\main\resources\rest-api-spec\api\indices.clear_cache.json` as well? ( do we need to specify all supported params, or only the preferred ones. There is also a `recycler` flag in the rest specs which I do not see in the code )
Could you wrap these lines to <120 characters? We're trying to cut down on overlong lines.
Nit: I think it'd be better for the message to read: ```[move_allocation] can't move abc123 from node1 to node2: node1 is not a data node``` (NB less punctuation, and no need to say `since its not allowed`)
can we move this validation up to the start of the method, next to the validation of the fromNode? then all of this is together and we can fail early.
Please could you assert that the content of the message is correct? (`expectThrows` returns the exception thrown, so you just need to assign it to a variable and check its message.)
what I had missed is that we cannot set it to false unless we get rid of the leniency caused by parsing the settings object etc.
I would set it to false too. I think that what you propose is a good idea, we have been using that flag only to test that we are lenient with responses, but we could use it also to test that we are strict.
I wonder if we should test this, in general requests should always be strict, unless they accept key-value pairs like in this case with settings. I think that I would always return `false` here
the nice thing of it is that you wouldn't need to write the usual code to test parsing etc. indeed in this case you would have to rewrite assertEqualInstances to not rely on equals/hashcode so that we only check stuff that is serialized over xcontent. I would give this a try if you don't mind, unless there are complications that I don't see :)
do you think that it would be possible to have two different test classes, one for the xcontent bits, that doesn't rely on equals/hashcode , and the other one like this on that we have that tests serialization and ordinary equals/hashcode ? Let me know if I am missing something.
I see now better what I made you do! I am sorry, I think that I don't like it :) Our requests don't fit well with this way of testing. While responses are usually entirely serialized both on the transport layer and on the REST layer, with requests the toXContent is only the request body, but there's also the query_string params that are serialized at transport but not as XContent. This makes for impossible equals/hashcode and complicated testing. To do things right, here we should probably have two different tests, one for the Xcontent part and one for the serialization. I am wondering though if this is becoming besides the scope of this PR. I am totally fine with going back to testing only XContent here. Thanks for trying, and sorry for the back and forth.
Tbh, I was not sure whether a system property or a setting would be a better choice but using a setting for this is consistent with what we did previously (see e.g. #20511).
Adding this mutable field (and thus making `RestRequest` not immutable) is not attractive.
This change still bothers me a lot. Again, I appreciate the motivation for it but the implementation that we are being forced into here leaves a lot to be desired (I am not faulting you for it, we are somewhat painted into a corner here). Another problem here is that now the object is a garbage state, we have no idea what the state of `params` is here yet we are allowing construction of this object to complete as if nothing happened. This is dangerous and I think that we should not do it.
A problem for another PR, I assume.
This is needed until we have a better understanding of the implications on nodes with small amounts of native memory.
Thanks for removing unused exception.
 to this escape hatch
As long as you're using Optional here, I think this could be ```java ExceptionsHelper.maybeError(maybeFatal, logger).ifPresent(e -> { try { logger.error(maybeMessage, e); } finally { throw e; }}); ``` Up to you if you want though
Extremely insignificant, but just for clarity: I think you mean "does" (indeed contain a fatal error) on this line.
Please revert to `== false`. This is used throughout the Elasticsearch codebase as a more visually abrupt form of negation to not be easily overlooked.
Again, this doesn't seem to actually use the `entry.getValue()`.
That's fine; throw an `AssertionError` then. Also, I think that you can keep the existing structure: `if / else if / else`.
Lucene has a FilterFileSystemProvider which allows mocking of various file system calls (for example the VirusCheckingFS class in the lucene test-framework which throws AccessDeniedExceptions on delete)
This seems like as super critical thing not to have a stronger assertion about. I think it is worth brainstorming a better test for a followup.
Recently we've been doing something more like this: ``` clearIndicesCacheRequest.queryCache(request.paramAsBoolean("query", clearIndicesCacheRequest.queryCache())); clearIndicesCacheRequest.requestCache(request.paramAsBoolean("request", clearIndicesCacheRequest.requestCache())); ... ``` Rather than the loop. The whole loop thing is more appropriate for by-hand xcontent parsing then url parsing.
super minor nit: I wonder if we even need the string constants at this point :)
I don't think we either but I know some folks like them so I certainly don't object to them.
can we extract the uncommitted gen from the `lastCommittedSegmentInfos`? Also uncommitted gen is confusing because the gen's id is in the commit point.
please get this from the current commit. I don't think it makes sense for the engine to get this from the translog. I can know on it's own!
I'm not happy with the extra boolean flag to include / exclude the current generation as a fall back. It's too subtle an error prone. How about doing the following (I think you had it in the past and we moved away from it towards the uncommittedX api - sorry for that): 1) If the min gen for the local checkpoint + 1 is > current committed gen , return true. 2) If the min gen is equal to the *current* translog gen, the current gen is not empty (using `totalOperationsByMinGen`) and the local checkpoint is equal to the max seq#, return true.
nit - we flush also it will reduce the size of uncommitted gens but strictly speaking it doesn't mean it will be below the threshold
did you double check the implications of a `ShardLockObtainFailedException`
Instead of acquiring the shard lock for a second time, I would prefer if we would do it once, and move this call under that lock and just rename `tryOpenIndex` to `tryOpenIndexUnderLock`, removing the locking mechanism from it. Same thing for `TransportNodesListShardStoreMetaData`. You can then also remove the `ShardLocker` interface, which irked me for a while.
`luser` -> `user`
Does this rest path potentially limit api additions/changes moving forward given that there is no description of what action is being taken as part of the path? Something like admin/scripts/lang/painless/action/execute may be significantly more flexible.
Maybe we could name the context (and the class) something more descriptive for it's purpose? While the rest api is for executing a script, I think this context is a generic test context? Perhaps it could be "painless_test" (and PainlessTestScript) or something like that? I like having "test" in there because it is clear this is not for production uses, but to test painless code. It would also be more clear for when we do support other contexts in the execute api.
The factory is what holds onto the params, so that they can be passed to the constructor. Think of the factory as the signature for the constructor. It's not boilerplate; it is actually needed based on current uses of scripts throughout the system. Also note that the factory signature is what allows the script instance to have arbitrary objects passed in. If `ScriptService.compile` were to return an instance directly, instead of a factory, we would need some way to pass in this information in a generic way, which would probably mean duck typing through a String->Object map and then require casts. With the factory, we get static type checking of the arguments a script needs to be constructed.
Let us leave adding 6.1.5 out of this PR, that should be picked up separately. The strategy would be: - add 6.1.5 to 6.1 (plus a version bump to 6.1.5), 6.2, 6.x, master - add 6.2.4 to 6.2 (plus a version bump to 6.2.4), 6.x, master
we can't change the format of the response at this time. We can at some point, but for now we just have to parse what we have, and figure out what we should do to make things better for the future, meaning potentially breaking changes etc.
as odd as this sounds, could you rename the methods to flushSynced as that's how this API is referred to in our [SPEC](https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/indices.flush_synced.json) ? request and response can and should stay the same.
we try not to throw exception when parsing responses as they may become a problem when it comes to forward compatibility. The client should be able to speak to future versions that have added fields, arrays or objects, by just reading what it knows and ignoring the rest.
Compared to our other parsing code this is a little weird because it doesn't know what field it is parsing up front. I get why you do this, but it is weird. Also it is weird because we don't serialize all that much information. You get almost nothing if there isn't an error.
we usually have a switch based on the token found. For instance here we would have one for start_array, and we would do something if the current field name is `failures` otherwise ignore. And another `if token.isValue()` for total, successful and failed. You can find something like this for instance in `SearchResponse`. This makes it easier to reason about what we are parsing I think.
Maybe as a followup (at sometime, not that important) the callers of this could be made to use the one line impl here. Just seems like a silly method to maintain.
I think "from" should be "from2" here. Also I would make the calculation simpler here as well, I think otherwise min might be smaller than max in the calls to "randomIntBetween"
if you throw above, maybe throw inside, only in case the selector returns empty here too, that might make the code a bit more readable.
And how about activating ordinary sniffing instead , maybe with a very high resniff interval or something like that? I do see why replacing the hosts is not ideal, but I think that not doing it complicates things in our production code, which is even worse.
s/y ou/you Also I think upfront is one word.
But even if we don't rename, users who implement the interface will have to adapt their code due to the method signature change? So I think we could break this too? I don't expect many users, if any, to implement this interface at the moment.
oh right sorry I keep missing that.
as far as I can see there are now two reasons why we need more rounds: 1) concurrent modifications to the blacklist (like before this change) 2) the selector rejects all hosts (introduced with this change) . Instead of trying max 10 times, can we figure out whether the reason for having no hosts is either the former or the latter and act based on that? In the first case we can just retry (indefinitely) while in the second case I am not sure. Do we fail the request or do we try a node that the selector doesn't want us to go to? Probably the former but just double checking.
nit: shall we rather initialize to empty and replace this with an empty check? I see that empty is currently not an option anyways.
why we don't we fail early in case hosts is empty? I see that the length check has been moved below
maybe it would be nice to also print out how many nodes were provided as well (I know such info is not part of this class but it could help figuring out why we can't send the request)
remove this part which doesn't apply anymore
something is wrong in this sentence :)
Maybe at this point the revival process should also make up a sorted list with all of the dead nodes and pass that one to the selector, although we will try to revive only the first one in the list? Not too sure though, just a wild idea.
I think you are right, I will try to find out what other language clients do in this situation just to make sure.
I see that we need it from another package, I think it's ok.
actually I think would simplify things as we don't even have to wonder whether we need to retry or throw exception. If the selector doesn't return any host we just throw, it is responsibility of the selector to return at least one host given one or more hosts. Knowing that, a selector can fall back to some backup plan in case e.g. it doesn't find any non master only node.
If we went for one suggestion that I left above, on filtering the list rather than the current per host predicate, the selector could decide what to do directly. Either go for another node, or return an empty list, and we would always throw whenever we get an empty list (after also trying to resurrect nodes). The current extension point seems a bit limited in that it doesn't give context on the set of nodes that are to be tried.
I was hesitant in the beginning due to the need for the additional class, but this looks great, it simplifies also concurrency as we read only once per node from the blacklist, so from then on we don't even have to retry as we kind of act based on the previous view of the blacklist. Not only does it allow us to remove the max_attempt, but also to remove retries due to the concurrent changes as a whole!
hopefully the `System.nanoTime` goes away once you merge master in.
This is no longer used, could be removed
It *feels* to me like this should be part of `testDeadHostStateFromPrevious`. If you really want to have a version of the test that use `System.nanoTime` I'd name it with `DefaultTimeProvder` in the method name and name the method that uses the `ConfigurableTimeSupplier` more like `testDeadHostStateFromPrevious`.
If you use a provider for the time then you can be quite explicit here. I think that is worth doing.
In that case I'd move the `System.nanoTime` call to before building the `DeadHostState`. That way if there is a super long pause due to the CI machine swapping or something terrible we still won't mind.
I'd keep away from using the default one for all the unit tests just in case of funny pauses and stuff. Maybe one, very small, very paranoid check that use nanoTime is col though.
maybe 7 indices with 50 docs is a bit too much (= slow test), let's reduce randomness to 3 indices, each max 2 shards, and 10 docs.
nit: this is not needed
nit: please keep the formatting
please remove this or use the logger instead.
calling `ConcurrentHashMap#size()` can be quite expensive IMO. I think we should keep track of the open ctx in a counter instead of using the map. I don't think being a little off here makes a difference. I think we don't need to add any sychronization changes here.
This can be added in the SearchService.
Yes it can @mayya-sharipova but if we restrict the counting to `scroll` queries let's use an AtomicInteger. The overhead should be limited since we create the search context for a scroll query only on the initial request.
nit: not needed
nit: this change is not needed
I think this should be `NamedObejectNotFoundException` instead, right? It is more specific.
All of these should be `NamedObjectNotFoundException` I think.
> What if instead we were to encapsulate the cache and move to an argument to compile, which each script engine may choose to use? Inserting into the cache would then increment the counter and trigger the limit check. 
I think this terminology and workflow is confusing, since the name would imply compile won't be called, but that is exactly what we still do. What if instead we were to encapsulate the cache and move to an argument to compile, which each script engine may choose to use? Inserting into the cache would then increment the counter and trigger the limit check.
 because we know query will never match
I feel like `pattern bank` is an internal naming convention, and externally we just call it `patterns`. not sure it matters here though, since one can only assume we are talking about the same thing.
if this is an attempt to catch more things... maybe add an example with type coercion as well? ``` bank.put("NAME", "!!!%{NAME:name:int}!!!"); ```
nice! I like this. super helpful for keeping track
spaces missing after `if` and before `{` 
don't drink and code  (same line twice)
this should be reset after each test (and in particular cleaned up at the end as well). I hate statics but don't see another way to achieve this right now.
I can't unsee this, it is like  in my .
derives -> derived
No "unsupported HTTP method" message? :)
I see. for the record we'll probably need to change this anyway one day for #18518
Ok, my confusion stemmed from the fact the first result when searching down the diff was ScriptProcessorFactoryTests.java, which this PR changes from testing against `getMessage()`, to using `getDetailedMessage()`. I see now that is the only case. Can it be switched back? Changing the tests to use `ExceptionsHelper.detailedMessage` seems ok given they all already use it.
This seems to only be used for tests. Maybe it should be a helper method in the test framework instead of part of the public api? I would be afraid of something accidentally using this in ES code.
This is why I do not like `assertEquals`; this is backwards from expected and actual. Instead: `assertThat(t1.v1(), equalTo(2L))`.
i will need to think about this for a bit...
I really wonder if we need to separate this out and make object creation and complexity necessary in all cases. This new abstraction confuses me a lot. I wonder if we should think more in the way of composing messages. ie. ```Java class Writeable { //this may be called with subsequent messages until we have all and then we write it back // optimizations can apply in here and depend on the context. public Writeable compose(Writeable writeable); } ``` This will all for only adding the complexity when it's really needed in this pipelining edgecase. WDYT
we have a method for this I think, `setRandomIndicesOptions`
Here, I think you should adopt the pattern we use throughout the rest of the class, i.e., ``` try { closeWithTragicEvent(e); } catch (Exception inner) { ex.addSuppressed(inner); } throw e; ``` so that we get the original cause.
nit: I find equals impls easier to read when they are symetric, eg. something like this should be correct? ``` Objects.equals( timeZone == null ? null : timeZone.getID(), other.timeZone == null ? null : otherTimeZone.getID())`
is there always at least one element in this list? (I haven't checked whether we assert it somewhere else)
maybe `clauses` can directly store Integers.
@adityasrini Rather than doing the two String.toLowerCase(Locale.ENGLISH) which requires 2 changes, you should replace the new HashMap() with a new TreeMap(String.CASE_INSENSITIVE_ORDER).
@javanna Sorry my suggestion should have been to replace the `new HashSet()` with `new TreeSet(String.CASE_INSENSITIVE_ORDER)` at line 433 , and remove the 2x `toLowerCase(Locale.ENGLISH)` additions, obviously mentioning TreeMap was nonsense.
Like it or not, our style is to use ` == false` instead of `!`.
@javanna > we would be introducing a sorted data structure where we don't need ordering, but only case-insensitive lookups. The internal structure of treeset doesnt matter, any more than the internals of hashset matter. Hashsets also sort/arrange their keys, it might not be alphabetically, but the entire buckets thing also has its own system sorting system when it allocates keys into a chain of buckets, but who cares. All that matters is the *set allows us to determine if some key already exists in it.
+1 to keeping a hash set. Moving to a tree set would make operations perform in `O(log(size))` rather than constant time. Even if this set would usually be small so that it wouldn't matter, we have seen in the past that users are sometimes very creative when it comes to pushing the system to its boundaries.
I think that jumbo frames are common enough that we should try to go the extra mile here. If it's not possible to do cleanly, I would at least like to see a system property that can be set to set the MTU to 9000.
Nit: it does not really matter here but this boxes and this should instead be `Long.parseLong`.
I think we can cache this in here in a constant so we don't need to re-create a new one every time.
we don't need this here since it's match all. Lets just omit it until we use it
while at it, shall we mention that it throws if cluster alias is not found (rather than for instance returning null)
oh one thing s/optinoal/optional :P
I am not sure I follow. As far as I can see ARC was indeed 2, and we are reading it as 2 above. Could you explain? Did it change more than once? Should we add another version check? I also think we should replace this `write` with `writeVInt` while we are at it. I mean it works for small values since at the end it's just writeByte, but it's just a bit confusing that we read one way and write another.
I think it would be easier to reason about it it was expressed as something like ``` if (clientVersion.before(Version.V_5_3_0)) { ... } else if (clientVersion.before(Version.V_5_3_3)) { ... } else { ... } ``` There is also no `else` after this statement, which means we write ord twice... And if tests don't catch this, we might need to figure out how to write a better test that would.
Yeah, I think the problem with the test here is that we don't make sure that nothing is left in the stream after we read it. That's why we didn't catch it here.
As we never expect a null value to be passed as parameter to this internal method, I'm not a fan of sprinkling this check here. This is defensive programming at its worst.
I like the level of detail in these tests, the degree of splitting them in into so many test methods and helpers is maybe a little bit hard to read. The following are just some suggestions of how to maybe group some of the tests. e.g the null, true and false test could be grouped into one case that checks allowed fields fort the allowMalformed field.
Although the code is clear, the level of indirection here makes it hard for the reader to figure out that this (and similar other) test does. As a suggestion, what about combining test_object/test_object_IgnoreMalformed, then the code from `sourceWithObject` can be inlined in the test case. Also I would make the assertion on field1 and field2 explicit, even if that means a few more lines of code. In this case I would trade repetition for readability.
Could you explain why you log a deprecation her? Might be missing some context, but I thought this PR wasn't about deprecation but about adding some option to the field mapper.
we could as well make this public, I don't see harm in that. After all it's what the user set.
Can this lead to user code change? ( as you changed the tests above )
As far as I understand in the REST layer, we don't print out any index for which there are no aliases to return, but only in case the alias (name) parameter was provided.(https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/rest/action/admin/indices/RestGetAliasesAction.java#L139). I believe this change tries to mimic the same behaviour for the transport client, but in the REST layer we don't look at what the name parameter matches, only at whether it's provided or not: ``` curl localhost:9200/_alias?pretty { "index2" : { "aliases" : { } }, "index" : { "aliases" : { "alias" : { } } } } curl localhost:9200/_alias/_all?pretty { "index" : { "aliases" : { "alias" : { } } } } ``` That may be right or wrong, but I think we should try to return the same results if we make this change, so I'd say without changing the behaviour at REST (although we may want to discuss what the right thing to do is) the best we can do at transport is to only look at whether any specific alias was requested rather than whether the expression matched all or not. Furthermore, I'd expect that once these changes are made to `MetaData`, the REST action should be updated as some logic can be removed? By the way, related but it should not affect this PR, there's also #28799 under review that is moving the REST logic to the transport layer, yet the REST logic remains in `GetAliasesResponse#toXContent` which doesn't change what the transport client returns. I would probably consider renaming the `AliasesRequest#aliases` method (to `replaceAliases`?) and make sure that it's only used internally (although it needs to be public), and have users call the current setter. We clearly can't have both call the same method or we lose information about what was set in the first place. That would make it possible to keep track of whether specific aliases were requested or not through a flag, similarly to what you do now.
Similarly - this feels like it could move onto `Maps` or even `Tombstones` - AFAICT the only other thing it needs is `maps.getMinDeleteTimestamp()`.
You'll need to handle backward compatibility here. You can check https://github.com/elastic/elasticsearch/blob/5263b8cc7ee7b1a52880ab98587b88d6c00557b8/server/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java#L132 for an example (the version should be v7_0...).
Same here, write only the `out.getVersion` is before 7.
I think it might be worth asserting that `Result` is one of `DELETED or NOT_FOUND`.
I think we should just call `wrapAllDocsLive(DirectoryReader in)` it's really unrelated to soft deletes
please don't load stuff lazily. go and load it all in the ctor. they are in memory anyways.
I will make it in a follow-up.
We have dedup in this PR already (line 161-163). The `lastSeenSeqNo` is used for dedup and range check. I am fine to remove the primary sort and dedup mechanism.
I see. I missed it. I think it's surprising to put it in `readDocAsOp` and shortcut. I'd prefer to do it in `next` where do all our state updates and then everything together. it's rare anyway and doesn't require optimization imo. That said, it's all nits. If you prefer it otherwise I'm good. Thanks for clarifying.
@s1monw I tried but realized that `NumericDocValues#advanceExact` method requires increasing docID values but it's not the case here. Do you have any suggestion for this? ``` /** Advance the iterator to exactly {@code target} and return whether * {@code target} has a value. * {@code target} must be greater than or equal to the current * {@link #docID() doc ID} and must be a valid doc ID, ie. &ge; 0 and * &lt; {@code maxDoc}. * After this method returns, {@link #docID()} retuns {@code target}. */ public abstract boolean advanceExact(int target) throws IOException; ```
As discussed - this should be needed in the future. Maybe we should remove it and instead assert that we never have duplicate seq#
Oh, I think I see why, it's for closing. I think it's still to pass in a search and close it on exception as you did now.
@bleskes I moved this to `next` but we also need to dudup for nested docs then I moved this to `readDocAsOp` again. I think we should optimize for nested docs. I am open to suggestions here.
I think I need to reset the DV :)
I also wonder if we want to pull the `tombstoneDV` in the ctor next to `List<LeafReaderContext> leaves` and a `List<NumericDocValues>` for seqIds... I think this would be nice and prevent getting stuff from the reader over and over again.
this is unnecessary.
The caller should continue consuming the snapshot until the `next` method returns null. In the last call, lastSeenSeqNo equals to toSeqNo and op is null. This guard is added to avoid checking in this case. I am +1 on the assertion.
I wonder if we can pull all these in the constructor into an array that we can access by index of the leaf reader. this is how we do things in lucene for stuff we access frequently.
I think I miss something here because I think we need it for now but not in the future after we have a Lucene rollback. I will reach out to discuss this.
+1. I passed an engine searcher directly.
Bah, daft scoping rules, you're right it can't just be a `long`.
I mean that there's a block of lines that does something to doc1 followed by an essentially identical block of lines that does the same thing to doc2 - both here and below in the blocks containing `assertThat(collector.getTotalHits(), equalTo(1));`. Also the parallel variables `opsDoc{1,2}`, `lastOpDoc{1,2}`, `lastFieldValueDoc{1,2}`. The nice thing about combining this stuff together is that it lets the reader see that there's no differences between the two treatments without needing to check the parallels line-by-line.
This duplication isn't to my taste - I think I'd try and pull the notion of "doc" out into a class of its own and have this kind of thing be methods there.
I don't think it needs to be an `AtomicLong` - it's only updated on this thread.
Nit: extra blank line
I wonder if `profile` and `explain` should be forbidden too? Both have non-negligible impact on performance, and seem irrelevant to ranking as well.
update: We spoke offline and decided it is better to tackle this in a separate discussion/PR
does it make sense to remove the setters? I imagine it feels more ergonomic to use the `IndicesStatsRequestBuilder` for building up a modified `IndicesStatsRequest`
as in the previous test - you need way more evilness - more roll generations, more terms, move variance.
I don't think you need to fail always but rather fail randomly. This is important to test failures that can come in any step, not just the first.
can we not wrap a translog but rather just keep this test translog next to the normal one? keep it simple and readable :)
Note that you want to make sure that you test the difference between the terms in the ops and the terms in the files. These are not the same.
Shouldn't we combine them? it looks so similar
I'm sorry. got confused. nevermind
please don't use two letter variables names for acronyms.
can you add more rolling while adding? Also *sometimes* increment the primary term
this is the only place we expect exceptions to maybe be thrown. Can you just wrap this line with try/catch (and remove the expectThrows - we don't always expect it.
This is too subtle - if someone reduces the number of iterations it is very easy to miss the point that it has implications for this. How about making a max attempt constant (set it to 5) and then here do something like fail.failRate(attempt == MAX_ATTEMPT ? 100 : 30)
can you please inline this while [adding docs](https://github.com/elastic/elasticsearch/pull/30176/files#diff-ed6e20d0c4d03d97ae9b7a9a33190c4bR1532)? We need to have more roll overs randomly
can you check that you can reopen the translog and that you can at least read all operations that weren't trimmed? this should not result in a translog corruption.
you can use expectThrows
please make sure all files closed and no file is leaked.
If that's the case, we don't need - that's making sure :D - where do you see it's done in ESTestCase? (I didn't check myself)
just fix a number, I don't think randomizing this adds much.
This enforce that the exception is thrown, but the current code doesn't *always* throw one.
Will categorisation jobs use `AutoDetectResultProcessor`? If so it should be renamed.
Make it `public abstract class`
you should call here `if (maybeFailEngine("delete", ex) { throw ex; }`
This needs to use safe commit. This is the commit the engine will use (probably an existing bug) .
same here just use `getExactNumDocs`
To be pedantic it should be `N`th.
`n + 1`
I am ok with it. stuff like this is always tricky
this looks pretty much like the code in the delete part below. Can we maybe break it out in a shared routine and pass a closure to it to actually process the operation.
I think you should have all options listed here and don't use default. Be explicit here please
style wise, I think `getV3Key` reads easier
nit: unneeded extra line
can you add `for FIPS 140-2 compliance`
Maybe ``` Builds a "pyramid" out of all clauses by combining them pairwise. So {@code combine(List(a, b, c, d, e), AND)` becomes <pre><code> AND |---/ \---| AND AND / \ / \ e AND b c / \ a b </pre></code> ``` The picture would make me feel better.
Can you name `equ` and `eq` a little more differently? They sort of blur together to me when I read this.
I see. This one `converts {@code SELECT * FROM a < 10 AND a = 11} into {@SELECT * FROM FALSE}`.
I talked with @costin earlier about this - he wants to keep the order the same and my proposal doesn't. What about this? ``` while (result.size() > 1) { ListIterator<Expression> itr = result.iterator(); while (itr.hasNext()) { itr.add(combiner.apply(itr.remove(), itr.remove())); } } ``` Your version works but `for (int i = 0; i < result.size() - 1; i++) {` make me think it'll be a normal loop and then you remove and add and I'm confused. Using the `ListIterator` forces the reader to think.
I wonder if this should be `Queue<Expression> work = new ArrayDeque<>(exps);` and then you do `work.addLast(combiner.apply(work.removeFirst(), work.removeFirst()));` The `for` loop and `remove` together scare me.
I'm fine with leaving it, yeah. I did want a prettier one but if this is what we can do, it'll do.
Nit: `aId:` -> `aId : `
As I mentioned above I think we should set the type in the constructor so the precision can be set directly
I just think its weird to declare the field expects the value to be an int when actually we are also expecting string values that are not the exact string representation of an int (i.e. cannot be parsed using `Integer.valueOf(String)`). It took me a little while to work out how this worked when I reviewed it so personally I think its worth making the change for code readability.
We need to read the version form the stream here and only write this to the stream if the version is on or after the version we add this feature.
We need to read the version form the stream here and only read this from the stream if the version is on or after the version we add this feature and otherwise set the default value
Given what we are doing below I think we should declare the `ValueType` as `ValueType.VALUE` here because otherwise its confusing when reading the code to see INT here and then the fact that we might expect a String that isn't just an int value below. We should then add an else to the below method to throw an exception if anything other than int or String is supplied.
The precision stuff here looks messy to me. I think an alternative would be to pass the type into the constructor of the builder and make the type `final`. Then change the PARSER to be a `ConstructingObjectParser` so we can still parse the type but have the constructor use the result. Lastly we can then have the parsing of the precision work directly because it will always know the type by the time its called.
Sorry, I am not sure why but my mind inserted a `static` into that field definition. That's clearly not there so of course what you have is fine.
this does not change anything here? We are already catching the `NoSuchFileException` in the line below, which is an `IOException`.
this does not change anything here? We are already catching the NoSuchFileException in the line below, which is an IOException.
I preferred the exception handling how it was before, i.e. local to each action. This allows better debug messages.
before, we would ignore any IOException, now we only ignore NoSuchFileException. I think, for optional clean-up, the previous one is better.
I think it's still nice to keep the debug logging as we had before.
`Writes a new index file for the shard and removes`...
oh, woops. thought I counted right. sry
ah right, oops. anyways, a positive outcome!
I know this is out of the scope for this PR, so this can be ignored... should we just get `nowSupplier.getAsLong()` once in this method and reuse its value? I'd imagine seeing three separate values here for a single step change can be confusing
do we want to check the phase/action times since those are meant to change
depending on the answer to my previous question about ErrorStep being flexible with its name attribute, may make sense to fix the name to `ErrorStep.NAME`
I understand with `wrap`. I think it'd be a little more clear if you just caught the `IOException` and sent it to `onFailure` but what you have will work as well.
In general chaining listeners is fine. I don't tend to use the `return` early style in listeners because I feel like they should *look* symmetric. They just *feel* better to me when they do. I use the early `return` style everywhere else though. The only concern with chaining listeners like this is that some APIs are synchronous *sometimes* and *if* you chain on a synchronous API you'll get a stack overflow. The initial phase of search has this problem. I don't know if you have that problem here but I'd dig through the code to make super sure you don't.
You *shouldn't* need to `catch (Exception` here because the caller already does and will forward any exceptions you throw to `onFailure`.
How about moving the static method to `HighlightUtils`? That seems to me to be a better place for it anyway, if it's being called from multiple highlighters.
Can we merge wrap() into getAnalyzer()? It feels wrong to have both methods
Right, I mean change it so that it isn't static. Or have a different, non-static getAnalyzer() method that calls out to the static version in the base class.
If the usage of forbidden APIs is in a few places, I would consider it better to suppress only at the lowest level (sometimes I like wrapping those in a private method I suppress). The reason is that if an unintentional forbidden call creeps in it will be caught.
nit: space before brackets
I think this should be `org.elasticsearch.core.internal.io.IOUtils` now. Since this work started there has been another PR that switched from Lucene's IO utils to an equivalent Elastic class.
It might be worth catching `Exception` here as a runtime exception creeping in in the future might prevent the node from starting and it's probably not a good reason to do so.
The name of the method seems to explain where it's expected to be called from. I wonder if that is necessary.
nit: space after `[{}]` plus typo on temporay -> temporary
typo: optain -> obtain
I think we could pass the requested size as a `ByteSizeValue` and only convert at the lower level. This will make the API less error prone.
I would make this class extend `AbstractXContentTestCase`, then your randomPutIndexTemplateRequest would become `createTestInstance`, and ``` @Override protected PutIndexTemplateRequest doParseInstance(XContentParser parser) throws IOException { return new PutIndexTemplateRequest().source(parser.map()); } @Override protected boolean supportsUnknownFields() { return false; } @Override protected void assertEqualInstances(PutIndexTemplateRequest expected, PutIndexTemplateRequest parsed) { assertNotSame(expected, parsed); assertThat(parsed.version(), equalTo(expected.version())); assertThat(parsed.order(), equalTo(expected.order())); assertThat(parsed.patterns(), equalTo(expected.patterns())); assertThat(parsed.aliases(), equalTo(expected.aliases())); assertThat(parsed.mappings(), equalTo(expected.mappings())); assertThat(parsed.settings(), equalTo(expected.settings())); } ```
Also, do we use the `minStringLength` and `maxStringLength`? Just wondering if instead we could do `Arrays.asList(generateRandomStringArray(...))`, unless we need the unicode-ness and the min/max string size
do we want to do something with is error? (not related to this change)
can we call the callback onStarted ? it is not called on error
We usually directly import `AwaitsFix`.
Nit: `UnFollow` -> `Unfollow`
redundant `public` as mentioned by the gradle checkStyle task
you could use `scriptRequest.setJsonEntity`
given that the request goes through validate first, I think we could remove this assertion, this is already checked in as part of validate which will throw an error otherwise.
Can you please put these constants into something like `GeoUtils` so `DataTypes`? I went with a similar approach in #30418 for readability and maintenance.
I assumed that there is no problem setting values and checking that the output of the conversion from high-level request to low-level request is the expected one. We don't validate etc. I would do only what is straight-forward.
space between `try` and `(`
I do not think we should log here. This is on the reload of a file and not an update to the ciphers settings
I'd be happy with an `isNotBasic` method.
do you mind switching this and other usages to `isBasic(currentMode) == false`? The elasticsearch team has a preference for this over `!`
This is a little misleading, which is due to how Logstash was already phrased so it may be moot. Technically Beats will continue to poll, right, but they'll see a basic license and not short circuit, then repeat later? Perhaps this should say > Beats will no longer be able to use centrally-managed configuration
Technically Beats will continue polling, but won't get new configs until license is back on track. @pickypg's message may be more accurate
Earlier in the SecurityLifeCycleService the cluster changed event was handled in order - first, to handle the event in SecurityIndexManager later it would start the index audit trail. Is there any order defined which does not change the behavior of handling cluster event? Not sure if this would be of any concern.
this is a nit and was there before this change, but maybe we should call this `securityIndexManager` everywhere instead of `securityIndex`
And the first one feels fairly special around copy and ngram together? But I it can be a yaml one too without too much trouble.
maybe we should expand testing a bit, just to make sure? the byte representation is quite convenient as it doesn't require the usual base64 based testing for these bw comp scenario. I think that we should test more of the previous values, if not all.
I think it would be nice then to test equals/hashcode separately. We can probably use EqualsHashcodeTestUtils
nit: I think deprecation messages usually begin with a capital letter? Looks that way in most every other place I looked at a quick glance.
I think I would prefer to catch the exception in createTestInstance instead. We should not need this in most of the cases, and it would be nice to keep this aligned with `AbstractWireTestCase#createTestInstance`
I really dislike this style of variable reuse in our tests. If I use my IDE to navigate to the definition of this variable I end up on a line assigning a value to this variable that is removed from its current value. This hinders readability, especially in longer tests. Lets avoid introducing it here, we should be moving away from it.
I see what you are talking about. Weird. I'm fine with it then. I mean, I don't like it, but I don't really have to like it. It makes sense to make it look like the test above even if the test above looks funny to me!
I mean, we still need to create the index, but i don't think we need to do it inside a new block.
Can you remove the extra block? I don't think it buys anything.
++, the only thing is I would even go with a Map<String,String> if that works. not sure what you and Nik think about that.
Would it be possible to return what the default JVM heap is here instead of null? Seems like we still may want to make changes to JVM options for an unset heap. Either that, or since we explicitly set it in our `jvm.options`, emit a warning or error that no heap has been specified (someone must have removed the option)
> The JVM chooses the heap size ergonomically when the heap size is not specified. Do we have the ability to see what it would choose? If running on a small machine, for instance, we'd still want to disable Netty's pooled allocator if the JVM is going to automatically choose a 400mb heap
nit: s/read blob's/read the blob's
This is where a safeClient() would be helpful, so that you have less chance that the underlying storage instance changed between the copy and delete calls
There is history around the azure plugin so I won't take this as an example. > Unless you have other thoughts I will go ahead and have a simple private method inside the BlobStore that returns the Storage instance. Code inside the Blobstore is responsible for not caching the instance. :+1: Let's do that and not block this PR. This is something we can still revisit later on.
I understand why you did that but I find that the two storageAccess() just adds extra unnecessary noise. I think we could instead have a simple private `safeClient()` method that returns the `Storage` client to use, and later do things like: ` SocketAccess.doPrivilegedIOException(() -> safeClient().get(bucketName));` I find this easier to read and to understand where the stack is cut for access control.
Maybe this can just be "terms_set" too
Can we call this just `score`? I've been trying to give the script context identifiers names that don't include "script" since that is implicit.
Nit: this could just be `logger.debug("using endpoint [{}]", endpoint);`.
I presume, by the way, that passing `null` for the region here is ok. I haven't tried it.
These methods should be gone now, they have been moved to IngestClient upstream
can you add a punctuation mark at the end of the sentence? It will make the generated html slightly better
now that #30490 is in, could you replace the header argument with the RequestOptions one? In the method that accept a listener we add RequestOptions between the request and the listener though.
the changes should not be here, these tests have been moved to IngestClientIT upstream
I just realized that these API should belong to the tasks namespace rather than cluster, according to our API spec, see #30906 .
Same here, can you add the punctuation mark
I'd probably do something like "testreason" so no one gets confused.
I'd supply a lambda, yeah.
Do you think you can make something like: ``` protected static <T> ConstructingObjectParser<T, Void> setupParser(String name, TriFunction<List<TaskInfo>, List<TaskOperationFailure>, List<ElasticsearchException>> ctor) ``` in ListTasksResponse? That way you wouldn't need to copy this stuff.
Lets leave off the third sentence
s/The upgrade/The template upgrade
how about changing this to `if (upgradesInProgress.decrementAndGet() == 1) {` so we can remove the return statement. I find this easier to read as the action only happens when the in progress value is 1.
I wonder if it would be better to use `ClusterServiceUtils#createClusterService` and other utility methods here. It reduces the mocks that we use
I'm not a big fan of this field. It feels like it could just get pushed down to be an `AtomicBoolean` inside `upgradeTemplates`, or if we need it to be a field, I think it works better if its meaning is reversed e.g. `detectedUpgradeErrors` As it stands we reset it to `true` even if we know something failed, which just feels wrong.
we can just call `terminate(threadPool)` here
this is a personal preference, I like to avoid overriding the setup and teardown methods of estestcase and use separate one
I think we should do this as one of the first steps in the if block. The reasoning is that if an exception occurs before this, we will never get the chance to continue operating as nothing will trigger this method again
+1, we could multiply `nodeCount` with a small constant to favor latency over throughput but the reasoning for the default value is to make sure that we hit all shards in a default index even when there is a single node so this change is consistent with the new default number of shards.
Can we provide more details in this message (e.g. the name of the index/alias)
ideally, you would leave this to true and exclude some of the paths where injecting random fields is not supported, I can imagine that the top-level is problematic here and the _source part as well. You can override `getRandomFieldsExcludeFilter`.
what made you return false here? checking whether this is expected or not. may have to do with exceptions which are parsed differently compared to their original representation. In that case, we could have an ordinary test without exceptions and a variation of it with exceptions where we disabled this check. We do this already in `ListTasksResponseTests`
MultiSearchResponseTests was written before the test base classes were made more flexible, we should probably migrate that too if it works well for your case
you can override assertEqualInstances
that tolerance is applied, it's a way to test forward compatibility of the client.
obviously there are places where we injecting random fields makes no sense, like _source etc
with the recent changes, I think that you only need the filter above when testing with failures. here and in the other test.
same - wdyt about a condition suffix/
rename method? it maps to sendUpdateStateRequest..
For backporting to 6.3, I think this needs to be changed to 7.
nit: extra line
I'm not sure we want `match` to be `contains` in some cases. If we want this we should make a new construct.
These look like leftovers.
nit: missing space
I don't think this test is needed. `testSpanMultiTermQuery` does the same thing.
nit: add an import for Settings
Why does it need to be public ? I'd prefer a private static class since it's not used outside of this class.
I think it's better to throw an exception, at least in master. I don't think we need a `max_expansion` option though, we can just honor `BooleanQuery#getMaxClauseCount()` and fails the query if there are more terms. The span scoring rewrite does not allow to limit the number of expansions, it is all or nothing and I think it's a good property since it will not confuse the users by returning partial responses on prefix queries that match more than 1024 terms. This is the main reason why I opened the issue in the first place, we should never build a disjunction on more than `BooleanQuery.getMaxClauseCount()`.
My original intent was to add a `max_expansion` to force the rewrite of all multi terms to use a top terms rewrite when used inside a span query but in this case the `max_expansion` param can be interpreted as the number of top terms to keep during the rewrite. However I prefer your approach which throws an exception on `multi_term` queries that don't use top terms rewrite and matches more than 1024 terms. In this case we can fail the query with a nice message explaining that a top term rewrite should be used on the `multi_term` query if applicable. `max_expansion` is confusing in this scenario and doesn't bring much so I think we should simply honor the BooleanQuery max clause limit.
thanks @nirmalc !
> Dynamically changing is very useful in case of cluster with multiple indices/data shapes. Which `multi_term` query are you using ? Is it to increase the default value of 1024 ? Any multi term query that don't use top terms rewriting is not a good fit for the span query in general. If you need to perform prefix queries within span queries it is preferable to index the prefix using the `edge_ngram` filter. This way you can transform any prefix query into a span term query on a single term.
lets revert this. We do not need it here
can you just leave the constant in this class? There isn't a need to put it in realm imo
In general this would be better as a unmodifiable map. I am not a fan of doing it this way. We should be able to challenge with more than a single scheme
Can we leave the default as null and only create if necessary? This is only being used for kerberos
I think so too!
I believe this is about memory allocation - I think we can discuss this but in a different issue.
You can use CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey() to get the above string constant already today.
This one has the same problem with java 8.
This one also uses a Java 9 method.
This one also has the same problem with java 8.
Same here, this it the field type tokenized flag, no? Not sure if this matters.
params are not nullable any more - please make sure this doesn't revive it.
we never use xcontent for cross node communication. I think we can use state and be tolerant of reading status when parsing? (until 7.0)
I think we need the status too for bwc? can you see what it takes to have a BWC test which shuts down a cluster with a persistent task and then does a full restart and check it started again with the right state? This can be a follow up.
nit: `missing` value -> `missing` weight
add a return method here just in case? I don't like this construct but can't think of how to improve it (all other options I see suck too)
I know it's not needed now but it's a bug waiting to happen when someone adds a line below.
FYI - for future work - they should be mergeable (i.e. leader into follower) not necessarily identical.
I'm not really sure this needs a param tag but it doesn't hurt anything.
I don't think the`@return` tag adds anything here. I think it is fairly obvious from the type of the response.
Sorry, i don't think i was very clear. I meant the hashing algorithm here is an HMAC (a keyed hash). For the anonymization use case we could use a non keyed Hash to the same effect. The key doesn't provide much value unless you are verifying the hash's integrity. To avoid swapping implementations based on a key's existence, a hard coded key can be used, which effectively (but not technically) changes from a keyed hash to non keyed hash.
nit: extra line
nit: extra line
Do we want to _require_ a user provided key ? If I understand correctly, it really only protects against rainbow table attack when used in this context, but with the salt and potentially hard coded default key it seems we should be covered.
I think it would be valid to fallback to a generated salt if none is set (for other use cases), but it doesn't necessarily need to be part of this PR.
@jkakavas - thanks for the thoughtful reply. I don't want to derail this PR and will open an issue for further discussion. EDIT: Issue logged: https://github.com/elastic/elasticsearch/issues/31692
Do we want to default to random salts here ? If I understand the primary use case correctly it is to anonymize fields for analysis which would prefer the same input values to result to the same hash. (e.g. hash(jakelandis) always results in "adslkjv983d")
When you use it with an instance of `PreBuiltAnalyzers`.
It *looks* to me like this will hit a `NullPointerException`.
It'd be super nice to explain that we only need this because of `PreBuiltAnalyzers`.
I think it'd be nice to remove this second ctor so we're explicit every time.
I think the old indentation was better here.
`retentionPolicySupplier` is confusing. It's a prune query supplier.
If `recoverySourceToKeep` was a bitset, we could do a leap frog, which would be faster if `recoverySourceToKeep` is sparse. ```java final ConjunctionDISI intersection = ConjunctionDISI.intersectIterators(Arrays.asList(numeric, new BitSetIterator(recoverySourceToKeep))); return new FilterNumericDocValues(numeric) { @Override public int nextDoc() throws IOException { return intersection.nextDoc(); } }; ```
should this be `BAD_REQUEST` instead? If we cannot handle the request, it's not that there's an internal server error, but that the request is unexpected.
I think this is dangerous - it's correct now since the way we stream guarantee that the bytes ref is offseted with 0 and has a length equal to the bytes array but this isn't guaranteed by the API. I think we should honor the API or, if this turns out difficult assert at the very least.
I meant [this](https://github.com/elastic/elasticsearch/pull/31163/files#diff-2ec2c2b070f96bf66b888db94648ffe0R321) . The standard engine ends up returning a `SnapshotIndexCommit` which is why I used the term lucene snapshot. I hope this is clearer.
We can pass the global checkpoint as a LongSupplier to an engine. ```java final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED); try (Store store = createStore(); InternalEngine engine = createEngine(store, createTempDir(), globalCheckpoint::get)) { engine.syncTranslog(); // Make sure that the global checkpoint is persisted to the translog's checkpoint } ```
To avoid this getting out-of-sync in the future, it would be nice to use `PARENT_TYPE_FIELD.getPreferredName()` instead of hard-coding `parent_type`. The same idea applies to `query` below.
This can probably be on one line
I think the `&` needs to be `&amp;`
nit: space between `if` and `(`
I think you can change this to a `Supplier<Analyzer>` now.
Ah! I get it now. LGTM
No longer needed
No longer needed
can u also add a set to the withWaitForCompletion
You don't need to store this in a field, it's available as `super.serviceProvider.getReqAuthnCtxClassRef()`
Nit: I think this import is unused now.
how about `onGet` as a name instead of primer
how about `onReset` instead of `finalizer`? `finalizer` makes me think of garbage collection
Ok - I reverted the 4.8 change locally but kept the other changes and everything *looks* to work. I don't think we can upgrade to 4.8 until we get to the bottom of this.
And some more: this is not caused by the build compare plugin. Maybe by gradle 4.8 or maybe by one of our hacks to make 4.8 work.
Oh, wait, I just got it! Neat.
When I pulled this locally and reverted the changes to this file I didn't have any trouble. We've traditionally been very weary of making changes to this file so I'd really like to make sure we need this before we do it, even if it is temporary.
This looks like it is caused by gradle not passing *any* system properties to the test sometimes. We use the absence of a system property set by gradle to engage "IDE mode" when running tests which mucks with the security features a bit. If you run tests from gradle in IDE mode they won't work right. Security, at least, won't work right. If these were to happen for other tests then it'd explode in other ways.
I've dug some more. This is caused by us running the tests with the built in gradle test runner rather than the randomized runner. We configure the randomized runner to run with the system properties but we don't ever configure the standard runner.
Same thing, let's output the type of `msg` here; the `FullHttpRequest` part can be read from the assertion line (at least my first step when an assertion trips is to find the line in the codebase, so we only need what can only be known at runtime).
This is causing a compile error for me, pls use `assertNull(object)` so it compiles clean
this will not get executed as a test if the method does not begin with `test`
I wonder if we want to rename this one to avoid confusion.
Same question about tracing the cleanupOldFiles method run.
minor - spaces between `if` and `(Strings`, and space between `){` at the end of line
don't be sorry, I can explain! at REST, we also accept the `context` query_string parameter. It is a string and PutStoredScriptRequest already supports it, you just have to read it from the request and set the corresponding parameter so that we pass it through to the REST layer.
pls refer to `current` instead of `6.2`: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html
same here - pls refer to `current`
no those have to stay as they are as it's already existing classes that people have been using with the transport client.
Same thought as @tvernum here.
I think this'd be more clear if you said something like "invokeStatic assumes that it is not invoking a method on an interface."
This'd only come up if the target augmentation method is defined on an interface, right? Maybe we should not allow that at all.
if it prints out null it may be ok, if it gives NPE we need a null check, that's what I meant.
I thought that you wanted to set the id in both cases and that the else block should become `new GetStoredScriptResponse(id, null)`
this should not be needed anymore
thanks for checking, that is fine then
we may have a small problem here, when toXContent is called on an object deserialized from a previous version that didn't send the _id .
Can this be a `AbstractStreamableXContentTestCase`? The toXContent method calls `filter.toXContent()` so you can use the filter parser
Rename `calendar` -> `filter`
And you'd have to change the parser declaration to: ``` private static final ConstructingObjectParser<ExplainResponse, Boolean> PARSER = new ConstructingObjectParser<>("explain", true, (args, exists) -> new ExplainResponse((String) args[0], (String) args[1], (String) args[2], exists, (Explanation) args[3], + (GetResult) arg[4])); ``` Or something like that. I'm not 100% sure on that bit to be honest, but *somewhere* in the building function you'd use `exists`. And then you can
Correct, we have removed support for parent in master. I will have to remember this when backporting to 6.x where we still support parent.
scratch that, I think this will be fine as-is in 6.x as well.
Ah! I see, you want the boolean as the context so you don't need to add `setExists`. You could do that like this: ``` response -> { CheckedFunction<XContentParser, Resp, IOException> entityParser = parser -> ExplainResponse.fromXContent(convertExistsResponse(response), parser); return parseEntity(response.getEntity(), entityParser); } ``` You make ExplainResponse.fromXContent like this instead of how you have it. ``` public static ExplainResponse fromXContent(boolean exists, XContentParser parser) { return PARSER.apply(parser, exists); } ``` I think that isn't *too* bad.
It relies on the assumption that we store millis internally, but I think that's fine. :+1:
the reason we don't use timeouts is that if they happen you don't get any info other than the time out. This way, the suite times out and you get a thread dump which helps (sometimes) to see deadlocks and where things are stuck.
> There are people in the team that prefer it this way rather than having a random timeout on the latch Then I guess I'm fine with it, it just made failing tests hang locally for me for quiet a while (I guess there are some hard timeouts after all).
here you may be able to use copyCurrentStructure
maybe a regex would be more readable here :)
is it necessary to parse everything into a map in the first place? At some point later on we do need a map, but I wonder if we can postpone calling parser.map till then.
Why this change? It seems unrelated to the feature being implemented.
`String description = request.getDescription() == null ? filter.getDescription() : request.getDescription()` for readability
Is this method not deprecated? I think it should be `XContentParser parser = XContentFactory.xContent(XContentType.JSON) .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream))`
If it's not used afterwards, maybe we don't need a ref to the location. We could simply use `.put("location", randomRepoPath())`
Can you make this final? Also, I think you should use `getDataOrMasterNodeInstances` as the repository is only registered on master eligible and data nodes. The method `getInstance()` retrieves the instance from a random node and if it's not a data or master one it will not find the repository.
Can you use more explicit name? (Also for getRepositoriesResponse1/2).
I don't think we need to check the implementation class, instance should be enough.
You can use `assertAcked()`
You can use assertAcked(client.admin().cluster().preparePutRepository(repositoryName) .setType(FsRepository.TYPE) .setSettings(repoSettings))
Thinking about it twice, I don't think that this is necessary. I think you can remove it, run the test multiple times andsee if it fails or not.
Nit: typo in originalRetRepositoriesResponse
You don't need to pass the default cluster `settings` here but `location` is enough for a FS repository
`updated` and `updatedRepositoryType` can be final too
Usually we'd stick this on the end of the last line.
I think it might be easier to read the code without this method to be honest. It saves a line every time you call it makes me go "what is going on here?" every time I see it. Not sure.
It'd be nice if the map were `unmodifiable`.
I feel like it'd be cleaner to have this be the result of falling off the end of an if statement chain.
I'd prefer iterating over the map entries here.
There should be a way to do the conversion without doing the raw math in there (not to mention the whole /1000, * 1000 losses approximation). Why not use the `ZoneId` from the calendar to compute the offset: ``` ZonedDateTime zdt = Instant.ofEpochMillis(value).atZone(cal.getTimeZone().toZoneId()); ``` or potentially use `getOffset()` instead of `getRawOffset()`
The method name seems incorrect since it's not a check but a set. Something like `setIfNull` or `handleNull` is more appropriate.
CCE is never a good idea to throw out - especially since it forces the caller to handle it. It should be handled inside convert directly.
This should call `setObject` directly not `setDate` (with 2 params) which effectively is `setDate(paramIndex,x,NULL)`. The 2 arg method should invoke the 3 arg one not vice-versa since the former is a subset of the latter.
maybe fold this if-else into the next one.
remove extra line
Nit: addresses -> address
If you wanted, this could be tested with `EqualsHashCodeTestUtils.checkEqualsAndHashCode` for testing the methods and mutation
maybe add what we computed as the scope of the alias and the path to the error message
maybe some docs would help on the two above methods, for instance to say that new aliases have not been added yet when validateField is called and that new fields have already been added when validateAlias is called
can we use different ids for the different indices? I find this super confusing to reason about. Maybe also add the routing value you expect to be used to the id.
I think that we need a different mechanism here. Either we need to invent the notion of a private setting, or as I prefer, these should be considered custom index metadata. While we have partially deprecated and neutered custom index metadata, I think that we should bring it back. We also have a similar need in index lifecycle management. As @colings86, @talevy, and me are all out next week, I will pick up a thread when I return from vacation to resuscitate custom index metadata and then we can use it here and in index lifecycle management.
I guess I get that its our way of saying `!isUpdate` since the `!` operator is frowned upon for readability, but I would just be fine seeing `docAsUpsert(false)`
Technically, you don't have to start the job if you just want to make sure the task is running. We start the persistent task when the job is created in the CreateRollupJob API. Although I wonder if there might be a potentially rare timing issue here? The CreateRollupJob API returns when the persistent task framework acknowledges that task was created. But I believe there might be a lag between that and when the allocated task is actually created on the target node? So it might be possible for the StartJob API (or `assertRollupJob()` if starting is skipped) to fail because the allocated task hasn't started yet? Maybe we need an `assertBusy` checking for the job, like below? I may be wrong about that though, you're much more familiar with how the persistent tasks work :)
Ah yeah, the docs are a little confusing with regards to how it works internally. The persistent/allocated Task is always running, start and stop just toggles internal state. If the task doesn't exist, the job doesn't exist basically.
Weird. When in Rome, I guess.
Its cuz when i did it, i had no idea that other thing existed. Would be a nice fixup PR after all these Snapshots related PRs got finished.
maybe just simplify these by setting it to the value of the `randomBoolean()` and using `Boolean.valueOf`? Not super necessary, just a nit.
I don't know that `== false` is worth it here.
nit: it is slightly better to set a value only randomly, that way you also test that we do the right when the value is not set (this can be applied also to ignoreUnavailable above: ``` if (randomBoolean()) { boolean verbose = randomBoolean(); getSnapshotsRequest.verbose(verbose); expectedParams.put("verbose", Boolean.toString(verbose)); } ```
I would recommend using `RealmSettings.getRealmSettings` instead. It slightly reduces your reliance on the underlying realm config model.
maybe we should also decrease the chance that we add another child object compared to leaf fields.
I don't think that we need such big docs, 100kb seems a lot still even if it's the upper bound.
Up to you but I find it more intuitive to collect all the explicit write indices (i.e. flag is true) regardless of size if that is empty we check if `referenceIndexMetaDatas` is of size 1 and if the first value has ` null` for the flag, if so we set `writeIndices` to referenceIndexMetaDatas and be happy.
It looks ok to me
Instead of passing the clients in the constructor, I would like to make this class abstract where all the methods that require a client are abstract. Then the PersistentTaskExecutor can instantiate a method that delegates async requests via clients but tests can do something else (synchronously return something, throw exceptions or what ever)
I think we can set this to the followGlobalCheckpoint and send a pick request. No need to preflight imo
That while loop is too complicated (any `while(true)` should be avoided whenever is possible). Can we put all of these conditions under a `hasReadBudget` and have the while be something like ``` while(hasReadBudget() && lastRequestedSeqNo < leaderGlobalCheckpoint) { numCurrentReads++; long from = lastRequestSeqNo + 1; lastRequestSetNo = Math.min(from + size, leaderGlobalCheckpoint) sendShardChangesRequest(from, size) } ```
I don't think this should be here as future scheduling is a special case of a read response that comes back empty. I thinks this method should always send at least one request (even if locally we think there is nothing to do). The flow goes: 1) If we can, always fetch something or at least send a peek request (as you call it though there isn't anything special about it) 2) When a request comes back empty we don't call this method immediately but rather schedule a future call.
I think all of these need to be trace and we should enable these in tests that are relevant.
if we change this to never be null, I think we can share the logic here and make it simpler. Something like: 1) Add everything we get to the buffer 2) coordinatewrites (always) 3) If max retrieved seq!= requiredMaxSeq, send another request 4) else if ops.length == 0 schedule a future coordinate read 5) else if coordinate read now.
in this re-write, we have a lot more things we probably want to report in our status.
same request - please have a method called `haveWriteBudget` and do `while(haveWriteBudget() && buffer.isEmpty() == false) { `
the current `to` parameter represents a hard upper bound this request is responsible for. Can we name it something that reflects this `requireOperationsUpTo` and also never set it to null? if need be we can set it to `from` or the leader global checkpoint (and it shouldn't be used as the size limit of the request).
I think it would have been worth it but now that you mention it - other requests may have changed this in the mean time too, so let's leave this assertion.
size should always be maxReadSize and the last parameter should be Math.min(globalCheckpoint, from + size)
you can use `ActionListener.wrap(r -> handler.accept(r.getLocalCheckpoint()), errorHandler::accept)` instead
please make sure they are positive or don't use vint to serialize them
can we open an issue/track it somewhere that these should be evaluated
last parameter can be set to from.
also, please make a note to make this configurable.
if it was always local it should all be taken care of by the put mapping api (retries etc). That said, this is also remote and I was mistaken. Sorry for the noise.
I think that it's worth logging the index metadata version here too (and the mapping metadata version in the future).
I think you can write this more simply as: ```java LOGGER.debug( () -> new ParameterizedMessage("{} error during follow shard task, retrying...", params.getFollowShardId()), e); ```
This duplicates the `hasWriteBudget` check and can never fail by virtue of being inside the `while` loop.
I still wonder if we should use a priority queue here (ordered by sequence number) so that operations are applied closer to in order than they otherwise would be? Something like: ```java private final Queue<Translog.Operation> buffer = new PriorityQueue<>(Comparator.comparing(Translog.Operation::seqNo).reversed()); ```
+1 to the statement and other problems previously mentioned (a network hick up will cause a failure for each outstanding request incrementing the counter by more than it can take). I asked Martijn to think about the retry counter as a follow up in order to reduce the scope of this PR.
this retry counter is tricky as we need to have a budget that allows all current read/writers to fail on a network hiccup. There's also the question on how people know what happen when the task is failed (where we might need support from persistent tasks). I think we can leave this for now but have to deal with it in a follow up.
can we make this configurable? also 500 millis is way too small and will busy spin. I guess 10s ? (the real solutions will be to have long polling, but that will come later).
these are covered by isShardNotAvailableException
IMO lets drop them all. IF you have to make them trace you can also just add them back if you need it.
I think it can be even less in tests. No one is worried about sending multiple requests there.
do we have a todo to extend this to the new setup (in a follow up)? There's much more that should go here.
if we didn't get any op (i.e., lastOp.isPresent == false) we should look at the maxRequiredSeqNo - if it's not equal to from somethings have gone terribly wrong and we need to break with a hard error (please double think if the retry logic catch all errors that may case 0 ops to be returned so we catch these before we get here)
you can use `ActionListener.wrap(handler::accept, errorHandler::accept)` instead
at that point you want have a read budget, which I mentioned above.
> while in this case we only make a copy if maxBatchSizeInBytes limit has been reached. he - I'm pretty I saw we did in some previous iterations. And yes - it must allow reads because we block reads when we don't consume the write buffer. In a sense the right moment is when you coordinate writes and you consumed some buffer elements. Both are fine with me
> In that case we always make a copy of the underlying array in ArrayList, Not if you change the request and such to use lists. > while in this case we only make a copy if maxBatchSizeInBytes limit has been reached. which may be frequent (i.e., every request), which is why I was considering making a change.
I'nm still missing the buffer size, the max requested seq no, leader global checkpoint , follower global checkpoint etc. I'm fine with a follow up for those, but that's what I meant.
What do you think of: ``` private void handleReadResponse(long from, int maxOperationCount, long maxRequiredSeqNo, ShardChangesAction.Response response) { maybeUpdateMapping(response.getIndexMetadataVersion(), () -> { synchronized (ShardFollowNodeTask.this) { globalCheckpoint = Math.max(globalCheckpoint, response.getGlobalCheckpoint()); final long newMinRequiredSeqNo; if (response.getOperations().length == 0) { newMinRequiredSeqNo = from; } else { assert response.getOperations()[0].seqNo() == from : "first operation is not what we asked for. From is [" + from + "], got " + response.getOperations()[0]; buffer.addAll(Arrays.asList(response.getOperations())); final long maxSeqNo = response.getOperations()[response.getOperations().length - 1].seqNo(); assert maxSeqNo== Arrays.stream(response.getOperations()).mapToLong(Translog.Operation::seqNo).max().getAsLong(); newMinRequiredSeqNo = maxSeqNo + 1; // update last requested seq no as we may have gotten more than we asked for and we don't want to ask it again. lastRequestedSeqno = Math.max(lastRequestedSeqno, maxSeqNo); assert lastRequestedSeqno <= globalCheckpoint: "lastRequestedSeqno [" + lastRequestedSeqno + "] is larger than the global checkpoint [" + globalCheckpoint + "]"; coordinateWrites(); } if (newMinRequiredSeqNo < maxRequiredSeqNo) { int newSize = (int) (maxRequiredSeqNo - newMinRequiredSeqNo) + 1; LOGGER.trace("{} received [{}] ops, still missing [{}/{}], continuing to read...", params.getFollowShardId(), response.getOperations().length, newMinRequiredSeqNo, maxRequiredSeqNo); sendShardChangesRequest(newMinRequiredSeqNo, newSize, maxRequiredSeqNo); } else { // read is completed, decrement numConcurrentReads--; if (response.getOperations().length == 0 && globalCheckpoint == lastRequestedSeqno) { // we got nothing and we have no reason to believe asking again well get us more, treat shard as idle and delay // future requests LOGGER.trace("{} received no ops and no known ops to fetch, scheduling to coordinate reads", params.getFollowShardId()); scheduler.accept(idleShardChangesRequestDelay, this::coordinateReads); } else { coordinateReads(); } } } }); } ``` PS - note the difference in handling of `lastRequestedSeqno` - I think the way you had it had a bug.
I wonder if we should use an ArrayList with initial capacity. We can then change the request etc to use List<> instead of array
is this correct? don't we want the maxrequired to be global checkpoint (i.e., from - 1 )? now it seem you have to bring at least on seq# back.
I think we want to assert here that lastRequestedSeqno is the global checkpoint
typo: tracker -> tracked
Sorry I wasn't explicit, I meant create a GH issue and link here.
Use `Collections.singletonMap` here and `Collections.singletonList`
I think it makes sense to let the realms handle this. I don't know what other headers we could expect - but that's kind of the point, we can just support a relatively generic method and let custom realms do whatever they need. My guess is that anything other than `WWW-Authenticate` would be due to weird proprietary protocols, but if they exist then we can support them.
> Did you consider a getAuthenticationFailureHeaders() method instead? +1. I feel like this is the way to go. Ultimately this might allow us to remove the authentication failure handler (way off low priority future idea)
this is not concurrency-safe. Closing can be happening concurrently to this.
let's check this on every access, not only creation.
ok, then assert that it's either snapshot or generic threadpool
Lifecycle component is not concurrency-aware, and not all methods that create the blobstore etc. are guarded by "if-not-closed" checks. We need to guard against creating a blobstore / container after the closing has started
this only verifies the repo on the master, not the other nodes...
let's add an assertion that this method is only called from the snapshot thread.
Do we really need a generic here? It looks to me that it's useful for innerBlobStore() / blobContainer() methods used in tests but otherwise it's not really required.
let's add an assertion that this method is only called from the snapshot thread.
Same here, make this a `SetOnce< BlobStore>`
Let's make this a `SetOnce<BlobContainer>`
undo formatting changes in this class
I'd expect this to be in a synchronized block
If you need this in test, you can still call it getBlobStore()
should it be `final` ? seems to be a const semantics
the name is not too descriptive but I dont have a good alternative at the moment either
For this case I was thinking about initializing the value and multi-threading (does the value need to be volatile), but now I realize that's not really an issue so the old way is fine. Please feel free to go back to that
The PKI realm should clear its own cache for both of the role mapping changes as of #31510. For the explicit LDAP clear cache, it is the cache clearing of a different realm and this user is technically coming from the PKI realm and the cache should be cleared for that user in the PKI realm; now if the PKI realm has authorizing realms that are caching realms, then it should delegate the call to clear the cache for the user to those other realms as well.
++ to talking this through but to put it out there, what I am thinking is that we re-build the user after the lookup. For this case we have PkiUser and LookedUpUser. The final user will be the combination of the PkiUser's metadata, the LookedUpUser's metadata, and the LookedUpUser's roles. The looked up user's metadata would trump the PkiUser's metadata in case of a conflict. This does get trickier when you do this in an AD/LDAP realm since some of the metadata comes from the group resolution. In that case, I would only include the metadata that does not involve group resolution from the authenticating realm.
do we want to cache when we are using delegated realms for resolving user? because we are doing lookups in any case
nit: space between `if` and `(`
maybe we should just use a `SetOnce` which would enforce the only initialized once
Hmm I don't like that we do not get the metadata from the pki realm when we use a delegating realm and we do not even attempt to map roles. There may be cases where a PKI cert doesn't map to an AD/LDAP user but role mapping is desired, so we now need two realms.
Not a specific concern, but just more configuration options for the end user when it is not being that effective. The code is trivial and not of maintenance concern so I am fine with we being consistent in all cases.
Why is this put into a local var? It's only used once.
just as a sanity check that declares we do not support arbitrary unicode. I don't think we have that around
> the behaviour there kind of sucks in that we're just quietly returning null for fieldnames that can't even work to begin This definitely sounds like something that should be fixed in a followup.
can you add spaces? `new KeyManager[] { km }, new TrustManager[] { tm }`
indentation is off after other changes
remove line wrap
remove line wrap
remove line wrap
remove line wrap
for the line wrap can you split it right before `List<String`? I find this easier to read
Instead of combining these and creating our own unique "region" iso code (which I don't think is actually a thing?) can we expose the subdivision by itself? The rest of the geoip data is exposed directly from the database without alteration or manipulation, and it would be nice to keep that simplicity.
Please put it into an assert if you keep it. Id remove it.
I wonder if we need this at all. The blocking call to the client executes it anyway. The issue was that there was no testing. I think this entire transport action can use a ml threadpool instead
preferably put this into the `next()` method instead so it will also cover the other blocking calls in this class. Could you also write this as `assert Transports.assertNotTransportThread(...)`, this will save from extra CPU in non-debug mode.
same heere, randomIntBetween(0, 5) would be more life-like
Nit: extra line
I think with this removal the `requestId` is now no longer used and can be removed.
I would call `indexedValueForSearch`.
Conversion to bytesref is done elsewhere with `indexedValueForSearch`. I'm unsure of the impact of rejecting anything but bytesrefs.
BytesRef implements `Comparable`, so you should be able to do something like: ``` int comp = lowerTerm.compareTo(new BytesRef(type)); ```
we shouldn't be lenient in case `lowerTerm` doesnt't implement BytesRef
we don't need this `if` block, do we? All 6.x and 7.x indices have a single type.
we shouldn't be lenient in case `upperTerm` doesnt't implement BytesRef
Why use a static block to initialise this? `Sets.newHashSet` can turn this into a 1 liner, and then you can wrap it in `unmodifiableSet`
Oh, and _nit_ on the unnecessary brackets :)
I'd just remove this and change standard types to explicitly list the types. `Collections.unmodifiableSet(NativeRealmSettings.TYPE, FileRealmSettings.TYPE, LdapRealmSettings.AD_TYPE, LdapRealmSettings.LDAP_TYPE, PkiRealmSettings.TYPE);`
We typically use `Locale.ROOT` rather than `ENGLISH` for case conversion.
I didn't know that, thanks for checking!
this one ends up sending parameters that are getting ignored, see `IndicesOptions.fromMap`. we should remove the last three parameters. This should be cleaned up, the problem is that some indices options are settable at REST, while some other are internal properties that define each API (the default argument in `fromMap`) which cannot be changed, so they should never be printed out nor parsed back.
`Mockito.whatever` is more idiomatic usage for mockito.
Worth putting the different mapping as first in the List as a second test? `(EsIndex("diff"...), EsIndex("same"..), EsIndex("one"))`
You can use `jobState.isAnyOf(JobState.CLOSED, JobState.FAILED) == false`
s/should match/should not match
It would be nice to have test coverage of this. There is a `JobProviderIT` class could you index some forecast stats in that and assert this method.
instead of ruling out the states which don't allow this to be called, I think it's easier to understand if we put the states where we allow this to be called.
I was confused because `request.index()` does not exist here. There is `getCurrentItem().index()` though.
I think this should just be `assert currentItemState == INITIAL`
I think this is only called in INITIAL state, so let's `assert currentItemState == INITIAL`
please move this method up next to `findNextNonAborted`
this whole method can be abbreviated to ``` return new BulkShardResponse(request.shardId(), Arrays.stream(request.items()).map(BulkItemRequest::getPrimaryResponse).toArray(BulkItemResponse[]::new)); ```
I wonder if we can add this (and similar ones) as invariant to the class (similar as was done for ReplicationTracker) we then call `assert invariant()` on each of these methods. For example, one invariant might state that if we are in TRANSLATED state, the executionResult is null.
*its (it's wrong in some other places too)
`ActiveDirectorySessionFactory#ldapPort` is no longer a class member.
This name seems deceptive because if you pass any parsers, then this is *not* used as a parser, right? If it should also always be a parser, then I think it would be better to just have a `formatters` member, and use `formatters[0]` for printing.
Let's stop this on shutdown.
are we considering all of the description etc a contract that we need to validate does not change? The reason i ask is cuz maybe it does not make sense to test this much detail as to what the output of the strings are.. I get that we can easily check available/enabled, but id hate to see a test fail here cuz we changed the description (unless we view it as a contract)
you can delete this test. It doesn't do anything meaningful.
@jaymode no. the opposite. I prefer not outputing fields with empty values. This is the norm now, and outputing empty field values is only useful in a tabular log format (column names at the top).
I think we can change this to `return ", opaque_id=[" + opaqueId "]";` and in the log message `, opaque_id=[{}]` will just be `{}` since we always add this at the end and we keep consistency with the way we handle indices.
Generally we don't output empty brackets `<field>=[]`, eg. `indices`, so `opaque_id=[]` is better to not being displayed at all.
`escapeCtx == null ? null : string(escapeCtx.escape);` feels a little cleaner to me. No need to pitch `null` through `string` and make the reader wonder what `string` does with null.
you should protect from double closing here.
++ can't hurt :)
"err" doesn't sound like "error"? Have you though of using a diff name here and in code? It is fine to keep this name as well if you think so.
iirc we add `Asynchronously ...` to this sentence in the other APIs. But its a minor nit...
I think you don't need to create cache keys and could directly use LeafReader instances as cache keys.
Should this be cached somehow? /cc @jpountz
this shouldn't be done here - it's part of the indexing logic.
I think this should be done on the top level try no? before we call acquireSearcher.
this can be private / package private. It can also be an InternalEngine thing. It's an implementation detail IMO.
These names would be a lot easier to read without the redundant info. Eg `testDifferentMapData()`
We discussed this on Slack and concluded that this is an unimportant special case in which it's painful to check the authorization correctly but, moreover, we can just ignore the auth checks on this API without losing anything significant. Arguably this could just use a `nonAuthPath`. I think get this special case out of the way first and then neaten up the rest and move it into `Bucket`.
... and this doesn't need to know it either.
... so that this doesn't need the `{credentials}` parameter in the URL ...
Why `ec2Key` here? This should be the instance profile name, and can reasonably be a fixed value...
Could this remain as `<working directory>`? Trailing `>` was lost too.
On deeper thought, this seems unduly lenient: it should only return credentials for the role that `GET /latest/meta-data/iam/security-credentials/` returned, and should return 404 otherwise. Also I think `credentialResponseFunction` can be inlined, it's only used in one place. Also also we could prevent cheating slightly more by inventing random credentials when the service starts up, rather than synthesising them from the role name.
Could we handle the `authorisation == null` case first to avoid these null checks proliferating? It's also helpful to distinguish no authorisation header vs a bad one, which this would achieve.
I think it'd be clearer to simply inline these overloads and spell out all the parameters at the call sites. We only make three of these objects, so four overloads of the constructor seems excessive :) Also if `Bucket` were not `static` then you wouldn't need to pass `buckets` in.
It should now be possible to make up the `RoleArn` and `SecretAccessKey` using the seeded RNG rather than by concatenating strings like this. They can reasonably be different each call.
I suspect it's now neater to turn this logic around - find the appropriate `Bucket` using `request.getParam("bucket")` and let the `Bucket` check the authorisation, look up the handler, and do the necessary.
The first two of these fields are unused. I think that's right, and we should remove them and also `ec2Bucket`, by generating the key and token and then passing them into the bucket's constructor.
This _nearly_ feels worthy of abstraction over the various sets of credentials, and I think that'll definitely be worth doing when the ECS-style credentials are added. Optional now, but worth thinking about.
I really think this should a hard-coded value and not passed in from the environment. I don't think we gain much by accepting it from outside, and I envisage it being the sort of thing I have to look up each time I come across it. The `BUCKET_NAME`/`KEY`/`TOKEN` inputs are clearer (despite that the `KEY` and `TOKEN` used here could be generated internally if we could do so deterministically).
I think we cannot (yet) do this with auth tokens received externally because they need to be passed to the test suite as well as this fixture, so we've had to settle on just using the same (long) string in both places. It would indeed be nicer if there were no magic strings in the fixture at all. For deterministic testing, maybe you can pass a seed in from Gradle somehow? Or just hard-code them, it's not that important. I'd rather they weren't synthesised as they are now.
Sure, good plan.
I'm going to be pedantic about the inconsistent indenting of this multi-line string.
I sometimes wish we had something like `Settings.singleton("url", allowedUrl)`...
simpler to write `(origin == Origin.PRIMARY) == (versionType != null)`
I think this is used.
oh, I missed this. thanks
I suppose that would make things super clear, but you're right, it would be a bug... so tossing a stacktrace vs. an illegalstateexception is almost the same thing . all good
since `getActionFromPolicy` can, theoretically, return `null`, would it be safer to switch these around to ``` newAction.equals(currentAction) == false ```
`false == Strings` is our style and I've grown to rather like it! I find it a little more obvious.
*usually* we add a space on both sides of the `+`.
I would make the class `final`, and these members `public final`
Why do you have `get` and `is`? If we don't make these public final members, then at least there should only be one of these methods.
make it final
add a private constructor so no one instantiates this class
make it final
@martijnvg After the last write operation, we might need to sync the global checkpoint manually in the test `leaderGroup.syncGlobalCheckpoint();`. This is fired async in the production code.
I think we should not execute these writes directly here but extend ESIndexLevelReplicationTestCase#ReplicationAction then run them via the infra of the new action (see ESIndexLevelReplicationTestCase#IndexingAction).
I think we can put both leaderGroup and followerGroup into a single try-clause.
There is an `empty()` matcher - `org.hamcrest.Matchers.empty`
let's keep as is, with the assertion message I think it's ok. I wonder if we should have an assertion at the end of this method to say something like "if we have an active primary shard that's not relocating, then the replication tracker is in primary mode".
this is not needed anymore now (i.e. the `resyncListener` does not need to exist outside the try-catch block
why not protect against double closing in the snapshot it self? this is a common problem
Minor thing: method called `testSelectLeft()` but the test itself is about the `RIGHT` function.
You are right, I was confused because of what I saw in ScriptImpl for painless (naming implying it was in variables for the script, but that is actually the params). I still think this needs to be its own context. We can eventually move these to direct arguments of the execute method (again, so params can be read-only in the future).
They can be added as direct arguments in a separate PR. There should also be deprecation messages along with that so we can remove inserting them into params.
I think this should be its own context. Putting these into params would be a breaking change, and also not utilize the intent of having contexts (different variables for different uses).
Missed a term<->version swap.
You could look at `GradleUnitTestCase` it does the same by pulling int the randomized runner only. What I was wondering about w.r.t order is that if it really makes sense to have it fixed. If all we are doing is going trough methods sequentially what advantage does it bring to have them in separate methods ? Maybe better error reporting ? Should we keep the randomized method order and make sure it actually works like that? I'm not saying we need to change it just looking to understand the implications.
Is this really necessary? Seems like it will produce a lot of noise.
Do we really need a before and after? These are run completely sequentially, so the "before" of one test is the "after" of the previous. I'm just thinking of what the old output used to look like (a single line per test in most cases with "OK") compared to what we are moving to here (many lines per test, if I understand correctly).
I think the indentation was better before, indicating these lines are a continuation of the try with resources.
got it. Thanks.
nit exta space between `TRIAL` and `)`
nit: remove space before `parser`
I am wondering if we should add `buffer` (size or operations) to the Status object? We can do it in a follow up if you are okay.
We can setup other scenarios to test the cancellation if we remove `updateLeaderGlobalCheckpoint`. For example, make the read limits reached, then cancel, then verify that we won't issue any read request.
isn't this exposed in the status object? can't we use that maybe? just an idea
Can we avoid using camel_case here? I think `testMaxConcurrentReads` should be good.
Can we fold all these assertions into a single one? I think this should cover enough. ``` assertThat(shardChangesRequests, contains(new long[][]{ {0L, 8L}, {9L, 8L}, {18L, 8L}, {27L, 8L}, {36L, 8L}, {45L, 8L}, {54L, 8L}, {63L, 8L} })); ``` Moreover, the leader should not return more than the requesting batch size. Here, we request 8 operations, but it returns 9 operations.
nit: can we use the "without" terminology? I think this better matches other code like the builders for java time stuff having eg`withTimeZone`. Drop implies mutating the current object.
Why do we need this leniency? The semantics should be either no qualifier is null, or empty string, but not either.
> we still want to use an beta over an alpha There shouldn't be anything needing to choose a beta over an alpha? There should be nothing using any qualified build to check bwc.
An enum won't work since these are numbered (or it would require separate variable to keep track of the number). But I don't see why the leniency needs to exist here to add the dash if it doesn't exist. I think the qualifier should always have no dash internally, just like we don't have dot prefixes on the numeric parts of the version.
> The version class is now used more broadly, including in figuring out dependencies ( as it's string value is set as project.version which is then used when considering dependencies ). There shouldn't be any dependencies on a qualified version, so there should be no need to serialize it into a string value. > I don't think we should remove the qualifier from the version right now, we should eventually better express requirements for a version used in bwc Why would we keep something around that is unused? I think it only adds to confusion in a class that is already difficult to under (our gradle's Version class).
I don't know what you mean by "consider qualifier in the build". There should be no attempt to test bwc of any qualified version, so I don't believe the build needs to know about it in our build Version, since that class is all about which versions we bwc test against.
why did this become public? Did you remove the support package in the test directory structure
Failing on less stuff is fine, but we should indeed ignore unknown fields. I think `ConstructingObjectParser` would work fine for this if you declare `shapshot` as an `optionalConstructorArg` and you declare `accepted` as boolean `optionalConstructorArg` and throw it on the floor when building the actual object. It isn't *super* efficient but it will properly handle unknown fields and it isn't *too* bad.
And it means we don't need to think too hard about the parsing. Which I don't think is worth it for an API as low traffic as this.
This definitely does not ignore unknown fields. It will fail if snapshot or accepted is not in the body. @nik9000 how should we handle this? We can go back to the older for loop style of these, or just fix it here using object parser. Id prefer the latter given @danielmitterdorfer has infinite time (which i dont know).
I think it is weird to just drop this on the floor. But we really don't have anywhere to put it.....
we need to take care of 5.x indices indeed, but the condition in the if statement should be `context.getIndexSettings().isSingleType() == false`
It may return no types if no mapping has been put, but indeed never more than 1 type. FYI the right way to check is `context.getIndexSettings().isSingleType()` rather than `indexCreatedVersion >= 6.0`
Nit: `casted` should read `cast`.
I would simplify this to just "importedClassName". Any name used in code always has to be canonical (like in java).
The distinction of what "class" means (why it does not include def or arrays) should probably be explained before this list.
Ok sounds fine then.
I think think `type` (not the naming scheme, but the concept) should be explained outside of this list? It applies to eg javaType above as well.
should this method be public? The scheduling methods are public as well as the advanceTime. I think ` hasRunnableTasks`, `hasDeferredTasks`, `getCurrentTimeMillis` and `runNextTask` should be as well.
We could add a default value (.i.e. `< 6.1 `) in the parameterized message because > Automatically enabling security for older trial license (null) might be slightly obscure.
this should happen after we update `isSecurityEnabledByTrialVersion`
Could you explain why this still needs to throw an exception rather than just returning an unmodified cluster state? Also I know we threw an IllegalArgumentException before but it feels wrong to me since the user won't have supplied anything invalid
I think the assertions are fine because ILM is in full control of the phase, action and step settings (they are INTERNAL settings so can't be touched by a user and will soon be moved to a custom index metadata object further locking down access to them. Therefore, I don't think its necessary to check that if one is set all three are set in production code but its useful to have the check in testing to make sure we don't do something silly so assertions feel right to me.
+1 to remove these assertions, the `query_string` query can build all sort of queries so I don't think we should track the full list here. This will also allow more flexibility in the random query generator.
This should say `storeStats`, not `storeState`.
One more thing, looking at the implementation of `failShard`, we don't need to append `e.getMessage()` here.
Think it'd be good to keep the `translog stream is corrupted` string here unless there's a good reason to change it. It's useful to be able to search for exception messages when working on support cases, and this sort of change makes that technique less useful.
The flow makes sense but I don't follow why this should happen via a `TranslogCorruptedException`.
Ok. Changing messages like this can cause a hassle when trying to diagnose issues, because it breaks the "grep the code for the error message in approximately the right version of the code" workflow, but in this case I think the new message is a sufficiently large improvement.
Those arguments confued me at first because a constant named `DEFAULT_...` is being used to set the minimum (3rd arg), and the numeric literal is used for the `defaultValue` (2nd arg). It's not a big deal, but I suspec it will confuse others down the track.
nit: the more I see of this, the more I think that it's worth extending the `onDPKG` type of API to be able to produce values so reading trough the code is more consistent.
I wonder if using fromSeq and toSeqNo (instead of size) will result in this being less confusing.
We could inline `releaseAndNullLocks` here, and moreover we don't need to set each lock to `null` since we don't reuse the `locks` array.
This could just be `close()`
`new AtomicReference<>();` with the extra `<>`.
nit: Extra whitespace here
How about building a set of invalid keys and adding them all to the exception? This would be a little friendlier to a user with multiple secure settings
Since we're moving that, we could inline this using turnary.
Can this be more specific so that we at least check the expected exception type? By checking the exception message I think we at least used to expect a ClassCastException or something alike. Would be great if we can to that here now that the assertion on the exact message text needs to be relaxed a bit.
assert for verification whether it is created
does it ever kick in? I would assume you get a `FixedBits` instance for live docs instead of a mutable FixedBitSet.
I tried to think this through and we might be subject to concurrent deletes if we do it this way. Can't we instead do something like this: ```Java DocIdSetIterator soft_deletes = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator("soft_deletes", sr); Bits liveDocs = sr.getLiveDocs(); FixedBitSet hardLiveDocs = new FixedBitSet(numDeletes); hardLiveDocs.set(0, numDeletes); for (int i = 0; i < liveDocs.length(); i++) { if (liveDocs.get(i) == false) { if (soft_deletes.docID() < i) { int doc = soft_deletes.docID() == DocIdSetIterator.NO_MORE_DOCS ? DocIdSetIterator.NO_MORE_DOCS : soft_deletes.advance(i); if (doc != i) { hardLiveDocs.clear(i); } } } } ``` note I didn't try this out.. just to provide an idea
I don't like the locking semantics - read/ write for stats updates and fully synced for class execution. I think we should stick with one. I understand you did it to make sure stats reading is consistent. My vote goes to the simplicity of synchronized access and making the status creation method synchronized as well. It's super light.
I think these don't need to be volatile any more, now that we read under lock.
I don't think "abstracts away" is quite right here.
Ah, I see why this is a function ref - so that the `toString` generates the right method to invoke. That feels a little brittle but I understand what is up.
I don't think `operation` belongs in equals or hashCode.
This does not do anything. The listener takes `Void`. `v` will always be null.
No need for this as the listener does not release anything.
I don't know that we care about closing the handler. It probably does not matter too much, but there should not be any resources hanging around if we properly consume all the requests.
nit: drop this extra line
No need for an else, it can just be outside the if
since you are returning the script, I think this can just be called `extractConditional`
The proposed fix is ok, but I wonder if the following wouldn't be better: ``` @Before public void beforeTest() throws Exception { if (serviceHolder == null) { assert serviceHolderWithNoType == null; long masterSeed = SeedUtils.parseSeed(RandomizedTest.getContext().getRunnerSeedAsString()); RandomizedTest.getContext().runWithPrivateRandomness(masterSeed, (Callable<Void>) () -> { serviceHolder = new ServiceHolder(nodeSettings, indexSettings(), getPlugins(), AbstractBuilderTestCase.this, true); serviceHolderWithNoType = new ServiceHolder(nodeSettings, indexSettings(), getPlugins(), AbstractBuilderTestCase.this, false); return null; }); } serviceHolder.clientInvocationHandler.delegate = this; serviceHolderWithNoType.clientInvocationHandler.delegate = this; } ``` The idea is to execute this conditional initialization step under the suite randomness rather than the method randomness. If we don't want to parse the suite seed back from a string we could also generate our own suite seed, the important bit is that we generate it in the beforeClass which is executed under the suite randomness (master seed). I find that this way what we are doing is clearer. The proposed fix is good but can easily be reverted by mistake by just moving lines around.
Can you add some randomization ? We run this method multiple times and then perform some checks on the generated query (serialization, correctness, ...).
You don't need this anymore, this method always run with a single registered type.
I am not sure we should do that, can we restrict the geo context to `geo` fields and validate this assumption from the mapping (`context.mapperService`) directly ? If the field has the correct type in the mapping we don't have to check why it's missing.
I know the "dots in field names" discussion has been a long running one. Do we not yet have a more general/graceful way of throwing these exceptions? This is more of a question out of my own curiosity and not intended to hold up the PR.
This can be `true` since the check is itself guarded by `isInFipsMode`.
if this can be null maybe do a rarely() and assign to null.
Pls be sure this is not null. Other converters do a null check and return and give this `addCommaSeparatedPathParts` a empty array if need be. check `forceMerge` for an example
When lookup-realms is merged, we'll have the `initialize` method available which would allow us to move this into the realm itself which would be a bit neater.
as a followup PR to this change, lets combine the logic with the internalTypes logic and we can just have the same message that we're using for kerberos.
The not-yet-merged `initialize` method gets passed a list of all the configured realms (which includes access to their type). A Kerberos realm can use that metohd to check that it is the only realm with `type == "kerberos"` and throw an exception is there are multiple such realms. That would allow the _kerberos has special requirements_ logic to be contained within the Kerberos realm.
nit: can you use `this.` consistently when setting members in ctors here? It is difficult to tell what might be locals declared earlier in the method and what are members.
I think this should still be fully-qualified.
Just a note: this is a no-op for Terms, but the other groups (date, histo) use it to store the interval. So we'll need to preserve that functionality somehow/somewhere for the other groups
go for it, thanks @tlrx !
The other configs should also implement ToXContentObject and change their `toXContent` but let's do that in a follow up.
We should also remove the other builders to be consistent. WDYT @polyfractal ? I can do that in a follow up.
This line probably needs shortening. See CI error: ``` 21:07:59 [ant:checkstyle] [ERROR] /var/lib/jenkins/workspace/elastic+elasticsearch+pull-request/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/esnative/tool/SetupPasswordTool.java:67: Line is longer than 140 characters (found 160). [LineLength] ```
This can't be a method on `LifecycleType` because the Lifecycle type needs to be in the API but the `ActionStepCreator`s cannot be. I think we should have `ActionStepCreator` registered globally and not tied to a `LifecycleType` and then each `LifecycleType` can choose whether it supports each Action
I think it might be cleaner to maintain this constructor and have the StepsFactory passed into it. Then it can call `this(new TreeMap<>(), new HashMap<>(), new HashMap<>(), stepsFactory);` which means the caller doesn't have to worry about what kinds of maps are best for the way it uses them internally
I think this needs to be `randomLongBetween(random, 1L, Long.MAX_VALUE);`, since the validation check makes sure the interval is a value greater than 0.
Is there any advantage in randomizing these? I assumed that since unit tests are fast to execute, we should go ahead and test the obvious paths all the time so that failures aren't flaky.
Since timezone is now a string, we should probably check for `Strings.isNullOrEmpty()` instead of just null now. (or null/empty check, I'll leave that up to personal preference :) )
Why not have the canonicalClassNamesToClasses above be a TreeMap? Then their iteration order is sorted by the keys. But since this seems to only be used by the doc generator, I would keep the sorting to only occur in the doc generator, rather than making PainlessLookup always incur that cost.
Not 100% sure of the rules here, but `private` seems too restricted for this.
Does this question have an answer? I _think_ any failures are passed to the response handler.
We could get rid of this `== false` by inverting this statement.
Can we also assert something about the error, and also assert that the successful acks are as expected.
Can we assert something about the errors themselves, and that there were no successful acks? (also throughout the other tests)
We should assert that this is called (and I guess that we can say something about the response that is passed in).
not sure it makes sense to talk about `applying` the state here
We could get rid of this `== false` by inverting this statement.
Could we also have a demonstration of the happy path on a three-node configuration, with the assertions adjusted accordingly? In the three-node case it's possible that publication responses interleave with commits, and this should be covered.
Could this be: assertThat(ackListener.await(0L, TimeUnit.SECONDS), containsInAnyOrder(n1, n2, n3));
Could this make finer assertions about the state that the publication is in? More precise requests follow...
This is `INTEGER` in other mappings.
I think at this level `NESTED` is definitely overkill and should be `OBJECT`.
This is an interesting question. At the moment this is what other APIs use to determine that a job is in the process of being deleted. Maybe storing that flag in an index won't be sufficiently atomic in the future. An alternative might be to make the job deletion process a persistent task, and use the existence of that persistent task to determine whether a job is being deleted. This field is probably just one of several places where indices won't give the same ordering guarantee that cluster state gave us.
Request -> ExplainLifecycleRequest
Indentation is off here and the line below (super minor)
Same here about indentation now
Can you remove this class entirely in favor of just returning AcknowledgedResponse? (similar to #32722)
Another to remove
Nit: space after the `:`
This is where I was a bit uncomfortable with requiring the underlying stream to have an unbound support for mark. If I read things correctly we end up with a stream that just wraps a byte reference and we therefore don't care. If you're comfortable relying on that behavior, then I'm good but then I also think we don't need to allocate a growing size of `firstBytes` but rather read byte by byte until we find the first non-white space.
use `terminate(threadPool);` (this method is in ESTestCase)
`min_version` is the earliest version of the product that can read the model snapshot. This is for the autodetect process to protect an older version trying to read model state with new features it is isn't aware of. For informational purposes only and shouldn't be set by the client. We don't have any APIs that accept a `ModelSnapshot` doc - update and revert use the ID- so I think we should leave this in.
Remove it if not used
This isn't needed client-side.
I don't think we should have this constructor on the client side, as there's no way to sensibly add the missing values (and no sensible reason why the user would need to).
Yes, end users would only ever read this object, hence the lack of a friendly `Builder`. I think it's OK to leave as-is.
I've noticed this field isn't in 6.x so please remove from the backport
This test won't be needed when `getId()` is removed
It could be useful for debugging too. In the future it's conceivable that the support diag tool might use the HLRC, and we wouldn't want to be dropping this value.
It's needed somewhere because a `model_snapshot` embeds a `model_size_stats`. If you prefer you could remove it here and put this in `ModelSnapshot` instead: ``` public static final ParseField MODEL_SIZE_STATS = new ParseField(ModelSizeStats.RESULT_TYPE_VALUE); ```
This isn't needed client-side.
simpler as it decouples these two things. And no need for having this method return a boolean.
this case got lost
add space after `//`
why change the semantics here to only call close when setting the tragedy the first time? Let's keep the existing semantics and make `setTragicException` return void
make this final
space missing between `)` and `{`
remove extra newline
Yes, I would ditch DOS (as you have done). I don't think we run any CI on non-ACL aware platforms. If it turns out we do, then we should just do an `assumeFalse` as the test is not possible on that platform.
The case statement is `keytabPathWithNoReadPermissions` but read-only false is not no-read, it's can-write. So the setup for DOS isn't consistent with the test scenario.
I think this message should be a bit more clear. Can you include: - the path - the supportedAttributes - some explanation about what attributes we're looking for It can just be `"Don't know how to make file {} non-readable on a filesystem with attributes {}"`
This was testing the complex validation in the builder that's been removed, so it's not worth having in the client.
Yes, I think we should make `Job.Builder::setJobType` `public`. The user could change it if they used the low level REST client.
This was testing the complex validation in the builder that's been removed, so it's not worth having in the client.
It would be worth requiring that `jobId` and `jobType` are not `null`.
Should be `this.detectors = Collections.unmodifiableList(detectors)`. (This is probably a bug in X-Pack core too.)
Since this is more likely to be read by external developers I don't think it should mention "Autodetect", as that's an internal details. Maybe something like this: > Analysis configuration options that describe which fields and analyzed and which functions are used to detect anomalies.
We probably shouldn't allow `detectors` to be `null` as other code makes the assumption it's not set. Probably on the server side the `build()` method will check this, but on the client side we might as well `requireNonNull()` here.
Wow, you are totally right, I see that now :)
the XContent here does not match what you removed in the REST API. There was a bit about early termination, as well as count that you need to include. You likely need to also include the begin/end calls, or else this will fail tests. You can check the tests with `./gradlew :server:check` from the base of the checkout. If tests dont fail then we need some better tests around the response hehe
spaces between commas
Remove these empty lines pls
+1 to this, there is always the low-level rest client for this, and we can revisit adding it at a later time if we change our minds.
Similar to above, `[job_id] message`.
Consider moving the common options to `ProcessBuilderUtils`
I know you've only moved this message between files but it might be worth making the format more consistent with others, i.e. `[job_id] message`.
To make jobs built with this method reusable when we come to send data to them in other tests, I think it would be better to fix the time format to `EPOCH_MS` and time field name to a specific value, e.g. "time". However, since it takes hours to get the PR CI build to complete and we'll be changing this same file in future PRs that implement the other endpoints I'm happy to leave this as-is for now.
We should check all levels in the path, for instance in `foo.bar.baz`, `foo` can be a simple object field and `foo.bar` a nested field.
We can use the concrete class`RollupIndexJobStats`.
This should be fatal.
I'm not following, do you want it to be a regular exception instead of an assert? We still need to do the `readVInt` regardless of `-ea` or without, it's just the 0 size comparison that is an assert for tests
This should be fatal.
This should be fatal.
I think this should be lowercase (suuuuper minor)
Are we returning here a mutable map? I don't think we should do that here. We should at least wrap it in `Collections#unmodifiableMap()`
I think that we need to do a hard exception indeed, we won't be able to read the rest of the message at this point because we can't read the customs (no `IndexMetaData#lookupPrototypeSafe anymore`) when there are unexpected ones. That is, `customSize > 0` is fatal for us.
Yep good idea, I'll do that
hmm do we need to skip the size if we are in production? I mean that assert will not trip if we run without -ea
This check is unnecessary as if a job is being opened we are certain there is ML Metadata already installed.
nit: space after `if`
nit: it seems that in most places we use hyphens to name the cluster state update tasks. It would be more consistent to rename this to `clear-finished-time-for-job-{job_id}`.
We tend to start the messages with the job_id. So, this could become: `logger.error("[" + jobId + "] Failed to clear finished_time; source [" + source + "]", e);`
I think we should just log the error. As @dimitris-athanasiou said, if it fails here the job will have a status of `open`. So anyone who reacted to the failure to open by trying again would be bemused to find out that they couldn't open the job the second time because it was already open.
Usually, the static variable initialization is done outside of the static block. Though this is not a strict pattern, I just don't see why you chose to initialize the value here instead of at the declaration.
This does not necessarily need to be within a static initialization block.
Why did not remove this from being a `ParseField`? This seems to go against the prevailing pattern.
Should this else clause be returned to what it was before the original refactoring PR? https://github.com/elastic/elasticsearch/pull/32068/files#diff-c94184ea4ef180f10817aa2bbd41a8edL119
I think we _do_ need to consider BWC for these lists. If you look at the implementation of `readList()` and `writeList()` they start by reading/writing the list length. So we need to write an empty list to versions before 6.5, and read a list of something. We can replace `PartitionScore::new` with a function in `Bucket` that reads the same stuff that `PartitionScore::new` read but just discards it.
That is true for when there is a transport client which I didn't think of at the first place. So, yes, we'll need to do the trick of reading the scores. There is another place where I'm doing this: https://github.com/elastic/elasticsearch/blob/6.x/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/job/config/Detector.java#L253. You can take a look and follow a similar approach. Note we only need that code in the `6.x` branch.
I believe we can get away without doing anything for BWC for the buckets because they are not being transferred between nodes. But I would like @droberts195 to confirm as well.
Because this method provides a general way to validate context references, I wonder if it makes sense to future-proof a bit and instead reference `ContextMapping`, and also rename `validateGeoContextPaths` to `validateContextPaths`.
do not work.
make these variables protected please and access them directly in IndexShard. I don't like accessing fields on self through getters. Also, this makes the PR harder to review, as it adds much noise.
I would prefer for IndexShard to just override `changeState` for now, call super, and then do the listener thing. This means one less abstract thing in this class.
You mentioned the overflow that could happen here. Can you please add a safeguard? Otherwise I might not be able to sleep anymore 
sure, nothing bad happens, but it might still be a user configuration error (user configured initalTimeout, but forgot to set maxTimeout). I think it's nicer to just throw an exception and report it.
if you put it like that, you can undo c08cd8b
Missing a space between the `,` and `T` here
nit: I think this should be worded differently given its public: ``` Marker interface for index lifecycle management actions ``` Reasons: * This is a marker interface not a parent class * The fact that its a client side copy is an implementation detail that client users should not need to know about * The fact that we use it to have a common category class for the NamedXContentRegistry is an implementation detail I don't think clients should need to consider
I was planning to do that as soon as we merge the standing PRs. We'll have to hold on more PRs for a bit, but I think it's worth it :-)
I'd remove the word `post` from the beginning of this method name. We don't usually specify the request type when it's POST.
If I see this correctly, you're doing `recoverFromTranslog` under the mutex here? This can potentially block the cluster state update thread for minutes.
AFAICS (correct me if I'm wrong) you had to it this way because we don't know on what node version the primary is (i.e. if it is going to send maxSeqNo or not), and the shard is reset when we acquire the replica operation permit (i.e. possibly before we receive the first resync request). It's a shame because it means we can't ensure consistency for older indices. The only other solution I can think of right now would be to always send the maximum sequence number with the replication request (same as we do for the global checkpoint). We could then pass this to acquireReplicaOperationPermit (same as the global checkpoint).
should this be synchronized so we get a consistent snapshot of the two engines? Also, again, please do the closing outside the lock.
do we really need to do this? I mean if we hit the exception our search layer will retry for us? I don't think we should do this at all.
VersionUtils has a method for this
> However, I am not sure if we should do it. why is that? We're building all this machinery to have search availability during the transition, except for this very short moment? I had the same idea about retrying. An alternative would be to do refcounting for closing the engine, to ensure that we only actually close once all in-flight `acquireSearcher` calls have been completed.
same here. I would prefer to have a separate `runTranslogRecovery` method that we use for the local replay after reset. This method could then use a different stats object as well as a different origin.
this should indeed be extremely unlikely to cause a user-visible issue (as at least the new primary should not run into this). Still, it's not great if an active shard cannot serve searches at all times. Let's take this out of this PR for now and rethink this later.
having this `maxSeqNoOfResettingEngine` variable is messy. Can't we get the max sequence number information from the translog? We essentially want to check if Lucene needs to catch up with what's in the translog.
same here, this might be called by a user-invoked force-merge
this might also be called I think
I am not a super fan of this. I wonder if we can afterwards rethink this.
`performPreSyncedFlush` might call this. Let's make this noop
afaics this might be called by `maybeFSyncTranslogs` because engine might be swapped between `isTranslogSyncNeeded` and this call
this might be called by `scheduledRefresh`, which can happen at any time
I think we need to have dedicated stats. I assume we should do it based on if we are resetting or not. We can do this in a followup.
this could possibly called I think
worrying *about* the ...
something like this: ```Java public SearchOnlyEngine(EngineConfig config) { super(config); try { Store store = config.getStore(); store.incRef(); DirectoryReader reader = null; boolean success = false; try { this.lastCommittedSegmentInfos = Lucene.readSegmentInfos(store.directory()); this.translogStats = new TranslogStats(0, 0, 0, 0, 0); final SequenceNumbers.CommitInfo seqNoStats = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(lastCommittedSegmentInfos.userData.entrySet()); long maxSeqNo = seqNoStats.maxSeqNo; long localCheckpoint = seqNoStats.localCheckpoint; this.seqNoStats = new SeqNoStats(maxSeqNo, localCheckpoint, localCheckpoint); reader = SeqIdGeneratingDirectoryReader.wrap(ElasticsearchDirectoryReader.wrap(DirectoryReader .open(store.directory()), config.getShardId()), config.getPrimaryTermSupplier().getAsLong()); this.indexCommit = reader.getIndexCommit(); this.searcherManager = new SearcherManager(reader, new SearcherFactory()); success = true; } finally { if (success == false) { IOUtils.close(reader, store::decRef); } } } catch (IOException e) { throw new UncheckedIOException(e); // this is stupid } } ``` I did something similar a while back so I had it ready... I am not sure it safe to use 
can you try to exercise this method to make sure we open a new searcher and close / release everything
nit extra newline
make this synchronized too. it's safer since you modify both references
I think you should protect this against double counting down the `closeLatch` by wrapping this entire try block in ```Java if (isClosed.compareAndSet(false, true)) { } ```
How do we ensure that searches are not accessing acquireSearcher on the closed engine and switching to the new engine? Also, is there a test that checks that searches (with preference set to this node) continue to work during this transition.
this log message does not contain the right "before" local checkpoint as you moved it to after the local checkpoint reset
The extra qualifier pushes this over 140 characters wide.
super nit: I tend to like validation to be first
I had not looked closely now I see why you were using mockito before :) I don't mind if you prefer to go back to it, I am happy either way
is it necessary to mock the rescorer builder, seems like an easy object to create manually, unless I am missing something. In general, I tend to use mockito only as a last-resort, when things are really hard to reconstruct, that's why I'm asking.
As what we're actually storing is the list, I would make this constructor the leaf one. Then the varargs one can just call `this(Arrays.asList(jobIds))`.
use `Objects.equals` for all once changed to potentially null references.
I wonder if this really belongs in the request converter. It is the request logic that requires this bit of processing so we could simply call this utility method in the request conversion.
We should be testing serialization here by extending `AbstractXContentTestCase`. Unfortunately, that means we need to also write a parser for the request but it's worth it.
make `Boolean` and only serialize when not null
Actually, even better, we can have a single constructor that takes `(String... jobIds)`.
make `Boolean` and only serialize when not null. Also remove setting the default. The idea here is that by doing so we inherit the defaults of the backend without having to duplicate them in the client.
Shall we add a constructor that takes a `List<String>` and make `jobIds` final? As the job IDs is the required part of the request, I think replacing the setter with a constructor guides the user better towards making a valid request.
There could be a number indices on a single remote cluster or several remote clusters `remoteIndices.size()` is not a measure of the number of remote clusters `RemoteClusterLicenseChecker.remoteClusterAliases(remoteIndices).size()` should be used instead.
nit: we prefer to use `fieldType().hasDocValues() == false` for clarity.
We can restore those assertions about the state of the join helper here - i.e. no accumulated joins when leader or follower.
This definitely feels like overkill now the `JoinHelper` is mode-aware and its mode is in sync with the coordinator.
The `Coordinator` becomes leader in `joinHandler.test()` not in `handleJoinRequest`, and that's outside this mutex, so it's technically possible that it could become a candidate again before this synchronised block.
This is only used in the constructor, doesn't need to be a field.
No, that's fine.
I'm undecided about whether this is too much machinery, and a simple `Mode` variable would be enough.
Should really ask for `toString()`s on these handlers too, although this adds noise.
I suggested a slightly different split in https://github.com/elastic/elasticsearch/pull/33013#discussion_r211957615
Great. I now wonder if this whole block can move into the `JoinHelper`. If the code above returned its `mode` and called `becomeLeader()` then the `JoinHelper` should be able to work out the right things to do with the join.
Static is about whether there is state needed from the outer class. This is still package private, so I don't see it as a problem for someone trying to construct this (especially since it is all within the geoid plugin project anyways.
can you add that it's an inclusive bound? (i.e. up to XYZ included)
I think we should have a proper (top-level) class for this, supporting both min and max. min would be useful for `newSnapshotFromMinSeqNo` (see e.g. PrimaryReplicaResyncer, which still has to filter based on min = startingSeqNo, all of which could be accomplished through the Snapshot), and max would be useful for this one here (where it might also have a `min`). We might even make the interface of this `newSnapshot` method purely sequence-number-based, where you can specify the range of operations to recover instead of the translog generation. That last part is not something I would change right away, but maybe something to look into later.
I think it's good though. less likely to misuse.
why did you change this to take a `TranslogGeneration` with the uuid instead of just the `long minGeneration`? It's not using that uuid anywhere here AFAICS.
maybe we could randomize the names of the 2 settings we have in this test
nit: extra newline
from skips the specified number of **buckets** This class will be reused won't it so not necessarily buckets
Misspelled in the \@param tag also
excude_interim - missing 'l' -> exclude_interim
nit: extra plus in `+ 5 * + 3600000L`
Nit: extract 1533081600000L to a constant FIRST_BUCKET_TIME as the literal is used several times
Can we not extend and override `StubbableTransport` like this? Ideally maybe the class should be final. It provides methods to attach lambdas for "stubbing" the behavior (although I think the method will need to be made public). The method is either `addConnectBehavior` or you can add a `setDefaultConnectbehavior`. Similar to `setDefaultSendBehavior`.
The theoretical idea here is to try to move away from overriding methods like crazy at the transport level. So if refactorings need to happen, we can (hopefully) just move the stubs to different locations, opposed to dealing with a billion different tightly couple to the `Transport` interface tests.
I think this is expected to be a sorted list on the `job_id`.
This seems like it should be handled in the `JobConfigProvider` so that any callers don't have to deal with the same error handling.
I think this was already expected to work in an async fashion. It was just queueing the update but the callers are not really expecting the update to complete before they respond. Unless I'm missing something, we probably can remove the listener here and simple have an internal one for doing the callback after we expand the job ids.
We prefer using `== false` instead of `!` as it the first is more obvious when scanning code.
I think we can do this more simply by looking at `endsWith(".jar")` of the uri string. We don't really need to convert the uri to a path, since we don't need to load the file. Then, the original if statement can simply be wrapped with like: ``` URL location = clazz.getProtectionDomain().getCodeSource().getLocation(); if (location.toString().endsWith(".jar") == false) { // original if and exception here } ``` Basically, if the file is in a jar, we don't need to worry about it here, as those would have already been added to the codebases map by `Security.getCodebaseJarMap`. This method is about adding classes that are on the classpath, but not via a jar (ie built by the IDE).
I'd prefer to only do the replace on Windows.
So, this could be simplified to `assertFalse` Or could be something like the following (which admittedly, is probably less simple) ``` import static org.hamcrest.CoreMatchers.everyItem; import static org.hamcrest.Matchers.greaterThanOrEqualTo; import static org.hamcrest.beans.HasPropertyWithValue.hasProperty; ... assertThat(response.records(), everyItem(hasProperty("recordScore", greaterThanOrEqualTo(50)))); ``` Man, that is frustrating that hamcrest does not support just passing a lambda as a type of matcher :(
Fine with me, just a thought :)
I would prefer not to use `ExtraPropertiesExtension`. Let's add our own extension instead. We would need to go trough build scripts now, but having that extra namespace would make it clear when reading the build script where these are coming from, and there are fewer chances to clash with something in the build script ( thinking about users of build-tools here).
should this check be in the same place where we make the actual transition? then it doesn't look a `LIFECYCLE_FORCED_PHASE` would have to be set as a signal back to this execution.
is it the intention to have `getCurrentStepKey` return the "NEXT_*_SETTING", while there exists a "CURRENT_*_SETTING" that can be misunderstood to be just that, the current setting? seems like it is more a "previous" setting
For the future, you can use ```java expectThrows(IllegalArgumentException.class, () -> new DeleteLifecyclePolicyRequest(randomFrom("", null))); ``` Instead of having to do the try/fail/catch check
++ plural is good
I don't think it should be 0, I think it should be -1 so that we can distinguish a fetch just happened less than 1ms ago from a fetch just happened.
Add `Millis` to the name.
This can be `final`.
I think we can use the `relativeTimeSupplier` and merely report the time (elapsed) since the last fetch.
This means a serialization change too, to support -1.
I think that we should avoid re-computing this value every time we get status (think of a monitoring system polling the stats every second). We are creating unnecessary garbage on every poll for every shard.
Additionally, that's a calculation done under a lock, a lock that synchronizes a lot of other methods in this class.
Thank you for keeping these in the same order as the constructor parameters, etc. 
I think we are missing quick overview how about changing this to: 'Flushes internally buffered data for the given Machine Learning Job ensuring all data sent to the <link to POST API> has been processed. This may cause new results to be calculated depending on the contents of the buffer'
nit: put the second in a new line (it'll make it more readable)
I'd rather have ISO timestamps in the example as it's what I'd prefer users to use. Same for start and end.
actually it shouldn't be 0 but the same value as the wrapped query since timings are supposed to include timings of children
Exactly, or in that particular case, from their child since constant-score queries can only have one child.
Wasn't this one for serialization? I would have changed the other call from the TransportAction, this is ok as-is I think, unless we want to completely get rid of the default constructor.
Missing return statement.
Just an idea that I think makes this more readable, but it's only my opinion. I would refactor the method to return a boolean `areDefinedInClusterState`, then call `if areDefinedInClusterState(...) { listener.onFailure(...); return }`. This way we don't have to store the error in a local variable, etc.
This method seems like it could be in `JobConfigProvider`? Then we don't have to duplicate it between this action and the put-datafeed action.
This is again one of those tricky points. I think it might now be possible to get 2 datafeeds referencing the same job as it's possible 2 concurrent PUTs end up passing the validation. We need to think this through.
`;;` at end of line
We mostly use `id` (all lower case). There's a few `Id`s around, but we should probably favour `id`.
We use `Locale.ROOT` for as many things as we can, rather than the English locale here
I think this should take an `OperationMode` to push the conversion into the caller, rather than doing it in the constructor (it seems cleaner to me that way). This is just my personal preference though, so up to you if you want to change it.
Can you put the `ParseField` into a class private var and then use it in the parser (so we don't accidentally typo/change it in the future)
`aliasAdded == false`
I think `aliasAdded = false;` is clearer.
`aliasAdded = true;`
Forget the question, I assume that class is still used in places where there is passed in a string directly.
this is breaking, we need to take care of backward compat
Do you need this? I dont see it being used anywhere.
I dont have a good answer for this yet. While we can totally test this, the value in the server -> client.fromXContent is greater than just testing the client.toXContent -> client.fromXContent. I think these kinds of tests are not really providing much value, and we also test the former in the IT tests.
this logging statement is unnecessary IMO.
Is it intentional that this is looking only at `isAuthAllowed` and not `isSecurityEnabled` ? The implementation here will disable the cache for trial licenses even if they have not enabled security.
This should check `isSecurityEnabled`. The other methods like isAuthAllowed do not take into account the default distribution behavior where security is disabled with a new trial license unless it is explicitly enabled.
instead of passing `null` here, can you move the implementation to this method and then call call this method from `canRebalance(ShardRouting shardRouting, RoutingAllocation allocation)`, similar as was done for `ClusterRebalanceAllocationDecider`.
are we doing this just for cosmetic reasons? because if we can't inc the store we will fail here with `IllegalStateException` if we don't want to do this we should maybe use `if (store.tryIncRef())` and if we can't throw an already closed Exception? I jsut wanna make sure this is known that this is best effort.
 much cleaner. There was no need to stop a node inside the callback.
Set capacity to `2`
nit: we should rename `tempDir` to `translogPath`
We should debug log here the number of times that we have retried up to here.
We can use `Randomness#get` here.
I think that we need to guard against overflow here!
This is not quite what I think of as exponential backoff. The problem that I see with an implementation like this is that we can have a thundering herd problem. If there is some failure that causes a bunch of tasks to simultaneously fail (e.g., say that we have a bunch of outstanding fetch tasks waiting for a response, and the network connection breaks, failing all of them), all of the retries will keep waking up at the same time, itself potentially causing issues due to a herd. Typically it would be that there is a random component in exponential backoff, to avoid this herding. As a first approach, what I suggest here is: choose a random value k between 0 and 2^number of retries - 1. Then retry after k * delay seconds. We can cap this at max retry delay.
at first I thought this format would make it difficult to know how many policies didn't use a certain field, but after a few test queries, it looks like this is sufficient
after is now minimum_age
Thanks for improving this test!
Knowing the supported time formats would be helpful for the user. (this goes for all the time fields in this object)
Not necessary with `ConstructingObjectParser`
This should be a `ConstructingObjectParser` so that the private empty ctr can be removed.
should be `final`
Ah right, that makes sense -> sorry for the noise :)
I think we only have general IT tests ci runs. This class is not wel tested :(
I *think* it should be enough to just call the API without any assertion. We already should already throw an exception if the response isn't a 200.
minor: you may also want a second (mock) processor here, and assert that the second processor never executes.
@dimitris-athanasiou may have a different opinion around this ^, So I will defer to him.
You're right @benwtrent, we've been dropping the `@throws` clause in some of the methods in the client. We'll need to revisit and add them. I'll make a note to do that.
I know some methods add the `@throws` and others don't. I think we should still add the `@throws` entry even if it is a checked exception. I have not been consistent about this myself either :(.
I looked at the implications of exposing an engine that isn't fully recovered yet and it's OK, with the exception of syncFlush(). Depending how this ends up being, we may need to make sure that runs under a permit.
it feels weird to do these things here - this method now only creates an engine but doesn't change the IndexShard fields - imo it shouldn't touch the store (because it doesn't know anything about any current engine running it)
it will be good to have some kind of progress logs here (like log ever 10k ops or something) under debug.
if we do this, why did we need to change how createNewEngine behaved (i.e., update currentEngineReference etc.)
can this be trace? we already have a info log message before.
I don't see the benefit to making the `MonitoringService` mutable in this way. Disabling the scheduler in the same way that `setElasticsearchCollectionEnabled` seems like it's enough and then the `Collectors` can be immutable and never shuffled around. I would change `scheduleExecution` and `doRun` to ```diff + /** + * Determine if the Monitoring Service should schedule the collection of Elasticsearch stats. + */ + public boolean shouldScheduleExecution() { + return isElasticsearchCollectionEnabled() && isMonitoringActive(); + } void scheduleExecution() { if (scheduler != null) { cancelExecution(); } + if (shouldScheduleExecution()) { - if (isMonitoringActive()) { scheduler = threadPool.scheduleWithFixedDelay(monitor, interval, threadPoolName()); } } // ... // ... @Override public void doRun() { + if (shouldScheduleExecution() == false) { - if (isMonitoringActive() == false) { // ... + if (shouldScheduleExecution()) { - if (isMonitoringActive()) { exporters.export(results, ActionListener.wrap(r -> semaphore.release(), this::onFailure)); } else { semaphore.release(); } ``` and then drop all of the other changes from this PR not related to setting / getting `elasticsearchCollectionEnabled`. This also keeps the collectors a little simpler by allowing them to run even if this is set (as it currently behaves).
I'd be okay with either way. I even though about package protected after I initially wrote it (so it can be tested but not reused). But you're right it doesn't need to be `public`.
This one is used in the Rest endpoint to silently block incoming traffic.
For human readable-ness, an additional field should be added, we shouldn't replace the field with a human readable version. You should be able to do ```java builder.timeField("modified_millis", "modified", modifiedDate); ``` (replacing the field names with the fields we want to use) and then you don't have to check the human readable flag yourself
Same here about multi-line toString methods
Here again I think we should use `builder.timeField` to handle this
Can you make this non-pretty, it's always weird when you log things and then end up being multi-line
Can we switch between the string and the millis representation fo the modified date using the `human` flag like the explain API already does? That way we can just have one `modified_date` field in the output? Also the parser will not need to worry about the string version in this case since the client it will never set the human flag
Its implementation though can be `final` and be an instance of check against `ListSetting`.
I think that will fail compilation? 
Nit: `if(` -> `if (` (add space)
Oh, never mind, I misread. Sorry for that. 
nit: extra line
I also need to go back and do this for the PutUserRequest
Can we also clear the temp `charBytes` array, something on the lines of: ``` final byte[] charBytes = CharArrays.toUtf8Bytes(password); try { return builder.startObject() .field("password").utf8Value(charBytes, 0, charBytes.length) .endObject(); } finally { Arrays.fill(charBytes, '\u0000'); } ```
I think @albertzaharovits's line of thinking makes sense and let's not implement closeable. In the PutUserRequest I did not claim ownership but the array is cleared by the close method. I'll open a PR to resolve the issue there.
nit: remove extra new line
no need to implement/override this method if no validation is required
++ to a dedicated class for these Rollup converters
This other approach looks fine to me as well. Thanks for the clarification!
@jtibshirani is correct
I need to sit down to make a approach, ill finally have time early next week. Id prefer the manual parsing in order to test the fromXContents, for now.
I think you can avoid that by overriding `AbstractXContentTestCase#assertToXContentEquivalence`? I think it's worth using `AbstractXContentTestCase` here, it's going to be much more thorough than hand-rolling parsing tests.
I think using `ConstructingObjectParser` would make all these parse methods much clearer
Yes! you are right 
I don't think we need this branch anymore.
Given that signaling this failure up the stack would be a mess, I wonder if partial results really bring us any advantage. I suggest we fail (call the listener's failure handle). When we'll move the security searches over a different threadpool, partial results would be even scarier.
Need a `return` after this, or we'll respond twice (`failure` and then `success`)
I believe `ScrollHelper.fetchAllByEntity` already wraps the listener in the client's `threadContext`.
Is this an oversight? `DEFAULT_KEEPALIVE_SETTING.get(settings)`
The global checkpoint in the stats object and directly from the index shard are sourced from the same place, the replication tracker. The problem here, as I understand it, is that the global checkpoint could have advanced after capturing the stats. Here is what can happen then: - suppose that `fromSeqNo` is 17 - suppose that the global checkpoint in the stats instance is 16 - suppose that the global checkpoint advances to 17 after the stats object is captured - the `fromSeqNo > indexShard.getGlobalCheckpoint()` check will fail (because of the advance), meaning that we skip returning an empty operations response - we then calculate `toSeqNo = Math.min(globalCheckpoint, (fromSeqNo + maxOperationCount) - 1)` where `globalCheckpoint` is from the stats instance; this would give `toSeqNo == 16` - now we have `[fromSeqNo, toSeqNo] == [17, 16]` which produces the invalid range error message This all happened because we allowed the global checkpoint advancing to become visible to this logic. Had we reused `globalCheckpoint` from the stats object then `fromSeqNo > globalCheckpoint` would have succeeded and we would have returned an empty operations response.
@jasontedor Thanks for the explanation!
Question: so `indexShard.getGlobalCheckpoint()` may return a lower seqno than acquired from `indexShard.seqNoStats().getGlobalCheckpoint()`? I always assumed that the seqno acquired from `IndexShard` could not go backwards.
I would prefer to just move this next to the `rebuildUnicastHostFiles` method as it's only used there.
newline, no need for `throws EngineException`
newline, also no need to write `throws EngineException`
I think this should be an `assert false;` + throw new UnsupportedOperationException
newline also no need to write `throws EngineException`
no need for `throws EngineException`
new lines missing. Also we should assert this is never called. Also no need for `throws EngineException`
add a note that this be opened concurrently with another engine? It think that's not obvious.
java docs on obtainLock please.
hmm... I really wonder if we are better off throwing in an exception here...
we should assert this is never called (same for the other places here where `UnsupportedOperationException` is thrown), as this indicates a bug.
`clien` -> `client`
"sent to the clienT"
Space missing between `}` and `is`.
maybe also here `"foo"` -> `{@code foo}`
typo: `i` missing. Also, maybe rephrase to something like: `materialized and represent the result columns sent to the client`.
`foo`-> `{@code foo}`
I think it should extend `AbstractStreamableTestCase` instead of `AbstractXContentTestCase`. As it stands at the moment this test is not testing that the wire streaming read/write methods are consistent.
If the intention is for this to be immutable then you should wrap the `new TreeMap` with `Collections.unmodifiableSortedMap()`, because otherwise a caller can modify it via the getter.
Similar to above, `new TreeMap` should be wrapped with `Collections.unmodifiableSortedMap()`.
The indenting is out here
As above regarding complex operations in `equals()` and `hashCode()`
afaics, all accesses to listeners are under lock, but I may miss something.
nit: Excluding the first word the error message is duplicated and could be extracted.
I think that makes sense. Otherwise we may throw an index not found exception if the index patterns the user is interested in does not yet exist.
btw the build fails because of the removal of String.EMPTY_ARRAY its import is now unused.
nit: "an started" -> "a started"
I am wondering if we can use another data-structure (instead of AtomicArray) to avoid passing the slot-index around. It may be less error-prone because we now have two slot-indexes (clusterAliasSlot and slot) in `handleClusterAlias` method.
nit: a space before `else`.
maybe just push the `maxUnsafeAutoIdTimestamp` up to engine and make the methods final
nit - updateMaxUnsafeAutoIdTimestamp
_value is only for agg scripts, we shouldn't have it for anything else
Usually we also make a few API calls to the server, e.g. https://github.com/elastic/elasticsearch/blob/2aba52de8f9315b0e384e1c657d7b0401d26a1b0/qa/vagrant/src/main/java/org/elasticsearch/packaging/test/PackageTestCase.java#L121-L122 I'm not completely sold on the value of those though
Nit: can you put this and `runWithoutJava` next to each other
I think these should get taken care of by `uninstallService`? Or is the point here that we want to assert exit code 0 when uninstalling it in these tests
Thanks for adding this warning. Since ` metadata.name() ` refers to the repository name, we could maybe change this to: "ignoring use of named client for repository ["
You can do this directly on the member variable: ``` private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class) ```
Can you open a lucene issue for this? Meanwhile, I think it's a good idea to copy the implementation of checkResetException() into the test code here, as it checks the contract pretty strictly.
This should only be done in close()
This should be done in reset()
I think this integration test is better  .
nit: extra lines.
Nice catch. Yes this message was changed due to the cleanup of missing class reporting.
It seems like there are unnecessarily many levels where `null` is allowed. You're allowing `aggregatorFactoryBuilder` to be `null` here, but also in `FeatureIndexBuilderJobConfig` `aggregationConfig` is allowed to be `null`. I think at most one of these possibilities should be allowed.
typo: randon -> random
It's unusual to override the base class `@Before` method and then call the base class version first. JUnit will by default call all the base class `@Before` methods before the derived class `@Before` methods, so it would be more usual to just give the derived class `@Before` method a different name and let JUnit call the two methods in the standard order, avoiding the need to call `super.setUp()` here.
For immutability: ``` this.sources = Collections.unmodifiableList(new ArrayList<>(sources)); ```
You could `import org.elasticsearch.ExceptionsHelper` and then you wouldn't need the `org.elasticsearch.` here or 2 lines below.
It would be nice if this class was immutable, and this line shows why it isn't currently. This is how to make it immutable: ``` List<CompositeValuesSourceBuilder<?>> sources = new ArrayList<>(num); for (int i = 0; i < num; i++) { CompositeValuesSourceBuilder<?> builder = CompositeValuesSourceParserHelper.readFrom(in); sources.add(builder); } this.sources = Collections.unmodifableList(sources); ```
At minimum this should be typed (we should have separate script classes for each underlying doc values type), but I would much rather this be built into the script itself, so instead of setting the value, an iterate type call is made, similar to the doc values api.
Indent these a bit more please!
thanks looks good :)
NIT: Noisy reformatting
NIT: noisy reformat :)
NIT: noisy reformat :)
I think this is not correct because it would also throw for repeated invocation of the same pipeline right? (Since `PipelineProcessor` doesn't implement `equals` you will probably only see this in some crazier scenarios (A calls B calls C and then another step of just B which would call C again making it throw on seeing C twice even though it's not a recursive invocation)) See #33419 for more here. I think the easiest fix to get out of that problem is to do the same thing that the pipeline processor does and simply track the current stack of pipelines in `pipelinesSeen` instead of all the pipelines ever seen (in this case this means just removing a pipeline proc from the `pipelinesSeen` after it has been unwrapped)
I know "vars" is the terminology used in the code originally, but irrc this is actually `params`? I think that naming would be better.
Rather than use `0` in the case of "unknown" in a mixed version cluster it would be nicer to have a specific "unknown" value, say `-1`, and then not include the count in the JSON output if it's unknown. For example: ``` if (in.getVersion().onOrAfter(Version.V_6_5_0)) { this.nodeCount = in.readInt(); } else { this.nodeCount = -1; } ```
This can be `final` too
Going with the approach of not printing "unknown", this can be: ``` if (nodeCount >= 0) { builder.field(NODE_COUNT, nodeCount); } ```
One nit: `TimeoutHandler` -> `timeout handler` so that we're speaking plain English instead of code in these messages
I am okay with you pushing this directly.
I don't think we need to be fully synchronized here (and so can save ourselves from locking for the quick returns). I think that we only need to lock on `this` on access to `processedSeqNo`. I see that we have asserts that we have a lock on `this` in `getBitSetKey` and `seqNoToBitSetOffset` but I think that neither of those are necessary.
you can directly call `getShardOrNull` on `IndicesService`
I do wonder if this class should be in a separate utility file somewhere? It's a third of the entire file length. On the other hand, this is all going away in master, so maybe it doesn't matter.
nit: No need to change but I think this would read more easily if it was: ```if (timestampField != null && (jodaTimestampFormats == null || jodaTimestampFormats.isEmpty())```
Reading through the code, I wonder if it would make things simpler to encapsulate this in a `TimestampFormat` class which will then own the joda/java pairs (or additional types if the future brings them). That would create that sort of 1-1 mapping between the alternatives. Just a thought
I think the local variables approach would definitely make it clear enough.
I see your point. Sounds good!
I think these two methods will fail when you create a test that feeds a random `ZoneId` here and a random `DateTimeZone` to the method below as timezones are not the same. `EST` for example IIRC exists only in joda time.
We could totally do a default constructor that sets it to `_all` and ensure this one does not do that behavior w the null check
The server-side request (and REST endpoint) converts null, empty or `*` into `_all`. So we should probably do the same here, or remove that from the server-side request and add the logic to the transport action. Otherwise an asterisk from the HLRC will go unconverted and the server will try to look for a job literally named `*` :)
ah ok I misunderstood what this was doing, I'm +1 on it working as you describe to match keyword fields
I'm not sure regarding why wildcard is different. I suspect its just because we haven't before needed for change the behaviour of wildcard queries based on the field type. I can't see any reason not to change it so we can control the behaviour here though. If we do make the change it might be best to make it directly on master and then pull the change into this branch to disallow it as the change may become difficult to maintain in the feature branch
nit: I'd remove `currently` here since we don't yet know for sure if we are going to be able to implement `copy_to` or if we'll have to have a different mechanism.
I think this needs to be made volatile
maybe we should call it `cluster.global_safety_factor` and use non-numeric values, e.g., - CARELESS (maps to 1) - HEALTHY (maps to 2) - PARANOID (maps to 3) 
I think it makes sense to define it like that (so that it represents the level of fault-tolerance instead of something that projects majorities). Regarding the name, I want to take the weekend to think more about it
if it's not important, maybe a safer default is nicer :) I'm fine leaving as is for now. We can revisit after the bootstrapping.
I don't think this will satisfy the "equals" contract: !(A, B).constainsAll(A, B, C) but (A, B, C).constainsAll(A, B). I think and additional size check will solve this though.
You indent this differently than the thing above it.
Cool! Only thing is we now could make this method non-static and not have to pass in the settings. There are possibly more methods in there that could be non-static. Not necessary to do this in this PR though, only if you feel like it :-)
I am not sure why you changed this.
why do we need this toString() here? We run it right away and don't log it anywhere.
~~Are you rounding down to the nearest second here? Do you not need an `L` to ensure the result of `System.currentTimeMillis()/1000` is an integer i.e. System.currentTimeMillis()/1000L~~ Sorry ignore that. You do seem to be mixing seconds and millis below in `long oneDayAgo = now - 86400;`
I think both alias and index names can have `:` in the name until 6.last? https://github.com/elastic/elasticsearch/blob/6.x/server/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L170 This validation method is both used for alias and index names. From 6.0 we log a deprecation warning, but not fail it. From 7.0 we fail such index or alias nems
Just for my own education, and it is certainly super minor: when reading this part I was wondering if it would make sense to get the maxClauseCount limit once at the beginning of this method since its unlikely to change to avoid method calls in each iteration). Maybe Java does some clever optimizations to avoid this though and the effect most likely negligible.
> However I can't think of a good way to reliably assert that we don't do this I don't have any good idea here. I'll keep thinking about this. Should not block this PR though.
I'm confused. Does this mean we restart the leader checker on every incoming publication? We call becomeFollower on every incoming publication
maybe put the runnable into a private helper method on Coordinator, so that we keep the constructor smaller
Is there a way we can check this property on startup as well/instead? Maybe we could check it in SearchModule or MapperService? The concern I see here is that restarts can be a pain for users and here they will only get feedback on the setting when they try to use a span query and then they can't use the query until they get the admin to restart the cluster
> I didn't go further and disallow wildcard queries for all non-keyword or text fields, as some other field types like _index explicitly support wildcard queries. I missed this part sorry. I think we should explicitly add the support in the `_index` field type rather than supporting this query on all fields. Currently the support for `prefix` queries is also broken so we don't really use this ability.
The `keyword` field applies the normalizer on `termQuery`. Depending on the normalizer the wildcard and escaped characters could be removed/replaced so I wonder if we should apply the same logic than `QueryParserBase#analyzeWildcard` for `keyword` fields. This is out of scope for this pr but it made me realize that we might have a bug here.
I think the same reasoning can apply for `wildcard`, `prefix` and `regex` queries so the default impl should throw a `QueryShardException` ? Only `StringFieldType` fields should be able to build a `wildcard` query.
This change would only break `wildcard` query on these fields, right ? +1 to make them string fields, `prefix` and `regex` query do not work currently because of this so it would be a bug fix. I am also ok to do that in a follow up, the changes in this pr have a different scope.
Right - RollupIT is the right place
Okay, thats fine then.
You can push a fix for this oversight directly, dismiss my disapproving review, and merge. 
OMG `== false`! 
If we are going to add the plural (+1), then we should add a deprecation warning for the non-plural.
can we fold this into `updateIndices`? There we use the same iteration order and have the same lookup patterns, so I think we can save a lot on the boiler plate and can avoid iterating yet another time over all indices
I'm not sure if it's worth the complexity of this code here just to provide a better message as to why an index service got removed. If you think it's useful, maybe factor the logic of determining the `AllocatedIndices.IndexRemovalReason` based on currentState and newState into a helper method so it can be reused by removeIndices
I'm confused. The cluster state was already applied on the node, no? I don't understand the extra restriction here.
you can also use this method without needing to subclass `ESAllocationTestCase`.
can "printer" be null? I don't think so, but maybe guard agains it in the ctor.
Same as above, reference equality shortcut would probably fit here as well.
Can you use `Strings.splitStringByCommaToArray`, it should be equivalent but that's the function used by the other action when they parse the `index` param.
This function is called a lot in tight loops and it's not a simple conditional as `timeoutExceeded` is volatile so there is a memory barrier for each invocation.
Nit: missing `@Override`
I wonder if it's easier to move the "check if there's only a left-over `MetaDataStateFormat.STATE_DIR_NAME`" logic into the `if (FileSystemUtils.exists(path)) {` condition above. Then you won't have to remove stuff again from `existingPaths`.
I think saying that we can not convert the follower index to a non-follower would be clearer. My concern here is that if `bar` is following `foo` and this message says `cannot unfollow index [bar]` it would be confusing since it is `foo` that will no longer be being followed by `bar`.
maybe this can be static too, and I would rename to assertDeprecationWarnings or expectDeprecationWarnings
I don't know how often this class is created, but maybe it makes sense to store a reference to the logger for later use if this class is instantiated more often.
While you're here, would you make these lowercase please: `ILM` -> `ilm`
`ilm-move-to-error` -> `ilm-move-to-error-step`
make it final
I am confused how this works when created is only within role mapping but we ignore role mapping
@bizybot can you open up a issue that describes this behavior of the object parser and label it with discuss? Then we can move this PR forward.
I think we could leave this called `simpleString`. The convention seems to be not to call out the existence of the Validator explicitly in other calls. I see that this might cause some issues with overloading but I think they're surmountable.
```suggestion * Validate this setting's value in isolation. ```
```suggestion * Validate this setting against its dependencies, specified by {@link #settings()}. ```
We should not to restrict to primary shards because this assertion is executed inside assertBusy and we also need to verify the optimization of the replicas. However, let's improve this assertion later.
Ah, nope, I'm just bad at Java. `[Metric1, Metric2]` is exactly what I was wanting. :)
Now that there are two cases, I wonder if we should push the isAggregatable() check out of the type conditionals (e.g. apply to everything), then have an `if...if else...else` for the types? That way we won't need another conditional inside the first that specializes for the date type. Not sure how that would look, so the current way is fine if that ends up being messier. :) Has a nice side effect that the validation error will include both a message about non-aggregatable field as well as the metric missing at the same time.
I wonder if we should have `SUPPORTED_NUMERIC_METRICS`, `SUPPORTED_DATE_METRICS`, and then a union of those in a third list `SUPPORTED_METRICS`? E.g. in the MetricsConfig constructor (and a few other places), we validate that an agg is one of the supported metrics. We don't yet have the field mapper type, so we can't validate numeric vs date field and their associated metrics, but it's a quick check to make sure the config is at least sane. The current `SUPPORTED_METRICS` is technically a superset of the date metrics too so everything is fine. But I'm worried in the future the two lists may diverge for whatever reason, and we miss the validation because dates support something numerics don't, or vice versa.
Super tiny nit: could we wrap the `SUPPORTED_DATE_METRICS` with brackets (`[...]`)? Makes it easier for the user to see the difference in message and dynamic values. Ditto to below with `unsupportedMetrics`
nit: spacing is off here
A++ new constructor code
This method also does not need to exist, as you can use `this(indices, IndicesOptions.strictExpandOpen())`, and fix the validation in the other constructor.
This is a question, not a change request: What is our philosophy regarding having setters vs. immutable request objects going forward for the HLRC? I've been under the impression we preferred immutable objects, but it doesn't seem to be consistent.
yea Im all for not exetnding that class. And Im also all for putting things that are primitive and easily validatable into the constructors. Optionals, i think im ok with setters but i think this also deserves a wider audience to discuss.
One option might be for this class to hold the already serialised form of the exception, but I'm not sure if that is better or worse than the current solution
I think this should be at debug level
The migration process will only move datafeeds from cluster state to index when the entire cluster has been upgraded to whatever version this goes into (6.6 or 6.7). So if we can find the minimum node version in the cluster in `toXContent()` then we can write the extra fields into the X-Content representation only after the entire cluster has been upgraded. That will make full cluster restarts work in the case where the entire cluster is on 6.6/6.7 (and it is essential this works because some people will run 6.7 for a year after 7.0 is released). So if `job_id` is `null` after a full cluster restart then that implies the cluster was not completely upgraded to 6.6/6.7, and hence the migration will not have started, and hence the information can be obtained from the `MlMetadata` in cluster state.
The `isInternal` data member hasn't been removed - is it correct to remove it here? (If it is for some reason then it should also be removed from `hashCode()`.)
Do we need to still read from the wire using something like this? ``` // TODO change CURRENT to specific version when feature branch is merged if (in.getVersion().onOrAfter(Version.V_6_3_0) && in.getVersion().before(Version.CURRENT)) { in.readBoolean(); // was waitForAck } ```
I think we generilize this and test that any response (empty or not) clears errors.
Think this is just `publicationInProgress()` again now.
I guess we should call this method `updateAppliedStates()` and the field should be `appliedStatesByVersion`.
Nice. I think this is a strong enough link between `currentPublication` and `mode`.
Wasn't me, it was the tests :)
This adds an extra `2 * DEFAULT_DELAY_VARIABILITY` to the maximum time it might take to complete a publication - one delay on a follower and another on the leader, so we need to update `DEFAULT_CLUSTER_STATE_UPDATE_DELAY` accordingly.
I think this is a duplicate of `connect()` above (minus an assertion) although I prefer this name a bit.
I think I'd prefer `PublicationTransportHandler` to have its own logger.
I think this'd be better put into the `Mode.CANDIDATE` branch below. I think we can also assert in the `LEADER` branch that either we're the master or nobody is. Anything could happen to a `FOLLOWER`.
This isn't really necessary, there are later assertions that will fail if this would have.
Clearing the current publication could be a method on the publication, avoiding the `thisPublication` malarky. The only other use is to call `toString()`.
Hmm, this is so we can call `runUntil(getCurrentTimeMillis() + ...)` but that's always how we call `runUntil`. Maybe move the addition inside, call it `runFor(...)`, and adjust the logging suitably (cos searching for absolute times is a useful debugging tool).
This is unused.
On reflection I think this means we don't need `lastCommittedState` any more.
I'd say something along the lines of `Creates a builder initialized with a default {@link ClearRealmCacheRequest} that can be used to configure the request to clear realm caches.` This method is more about the creation of a builder for programmatic building of a request than this method actually populating a request.
Maybe warp the listener using ActionListener#wrap which does the write things and will simplify the code here too.
fillResponse can throw an already closed exception. We should make sure we deal with exceptions here correctly
doc level failure (normal failures are OK from an algorithmic perspective).
a bootstrap API will use networking an have this thing arrive in different times on the node. Should we already simulate a short network delay here? (i.e. schedule as a task)
I think we should we add this as part of stabilisation (as an initial step).
maybe inject failure both before or after executing the actual action.
yep. sorry I missed it.
After discussion in another channel - I understand the release method will be called from other places too.
I think this can leak a reader if `reset(DirectoryReader delegate)` fails (especially when the `this.delegate != null` is true)
can you java doc the semantics of this one? it's kinda funky: "give me something if you have it cached but if you don't give me null".
Thinking about this more and seeing how it's used, maybe we should split this into two methods: `getReader` which will return a reader if one was previously opened and `getOrOpenReader` that calls `getReader` and if it returns null, opens a new one.
s/it's leave/its leaf/
Maybe call this 'totalOpenedReaders'? I was a bit confused reading the tests as it looked as though it was reporting the current open count
I know this is for concurrency reasons. Just wondering: Would it make sense to move the update of the term under the mutex? In that case, this condition would not need to be checked here.
In the other tests that are migrated to use Zen2 we set this to `true` (i.e. we are not testing the Zen1 case any more). I think that's what we want to do here too, but in any case we should be consistent about this.
I wonder if we should consider putting the metadata object on the authentication object? It might be extra overhead that we don't need so I am not asking for it to be done here.
This isn't a big deal, but I wonder if we should just use `contains("before receiving peer's close_notify")`
is this new assert needed? after all the following cast will fail if the request is not a Replaceable...
but doesn't AliasesRequest extend IndicesRequest.Replaceable ? I think I don't follow. Not a big deal though :)
I think this has to happen before you start the cluster, or else the cluster will start with full knowledge.
I think this has to happen before you start the cluster, or else the cluster will start with full knowledge.
I would inline `superSettings` and now there's another `put(Settings.EMPTY)` to nuke.
`.put(Settings.EMPTY)` is a no-op. Also, `public static final Settings DEFAULT_SETTINGS = Settings.EMPTY;` means that `.put(DEFAULT_SETTINGS)` is also a no-op. I think this means that both `nodeSettings` and `transportClientSettings` are just `Settings.EMPTY` (and this means there are more no-ops elsewhere).
This check is not necessary, Gradle doesn't allow for it unless the task is marked `@Optional`
I think using `nextLine` might be easier to read, and since we need all the lines in memory to perform the de-duplication anyhow, I would consider doing it in one step. Using java streams and a set as the Groovy implementation did to take care of the de-duplication would make this concise and easy to understand: ``` files.getFiles().stream() .flatMap(Files.readLines(null, StandardCharsets.UTF_8).stream()) .collect(Collectors.toSet()) .forEach( line -> { // write each line to target }) ``` You will probably have to collect to a different set implementation to preserve order but this is the general idea. Writing a test will make that obvious.
We should use a try-with-resources syntax with this.
 I will work on it.
Should we in a follow up add a Gradle module inside the qa directory that contains these helper methods and let the qa modules depend on this module? We've been repeating these methods many times now.
This TODO indicates that the timeout we calculate below can now be made shorter since we don't have to wait for an in-flight check to timeout. Same applies to the other TODOs like it.
Oh wait, there are now tests of both cases, don't mind me.
I would prefer two tests to test these two paths.
I think this assertion should be in `getAnyNodeExcept()` - it's ok to return an empty list here.
I think I'd prefer two tests to test these two paths.
Should this be 6 instead of 9? When I try `Instant instant = Instant.from(formatter.parse("0.0000001"));` in the tests I get an `java.lang.ArithmeticException: Rounding necessary`
As mentioned above, I think whatever object we return should know how to take a builder and add itself appropriately.
Before calling this, we should probably add a check that the `batchedIterator.hasNext()` and throw if not. It might make our future life easier when trying to debug stuff :-)
We enforce that a date_histo is always configured, so at a minimum there will be buckets of time. We can leave the check for now, it might not be useful to just have time buckets + min/max time. Probably easier to remove later if a user needs it, rather than allow it and regret it later :)
Since we always guarantee at least one metric is included (either explicitly, or through default), I think we can get rid of the `"At least one grouping or metric must be configured"` check a little bit above this line. That was just to make sure users didn't configure a completely empty Rollup. I think that's moot now that we include some defaults, and an empty rollup with those defaults might still be useful (min/max of histos per bucket, etc). ~We can also remove the null checks on `metricsConfig` in the `getAllFields()` and `toXContent()` methods I think?~ On second thought let's just leave those, doesn't hurt and makes me feel better in case we mess up elsewhere :)
Yeah I think you're right, that's an NPE waiting to happen. Let's ignore that for now in this PR to keep the changes minimal... I (or you if you want! ;) ) can followup with a bugfix for that null groupconfig issue.
We prefer adding the `set` prefix. ```suggestion public void setContents(Object contents) { ``` Also in this particular case I think we should also make a semantic change and type `contents` with `String`. This does mean that groovy will resolve GStrings sooner, but looking at the uses of this task that is ok and I would prefer the stronger typing, it's one of the reasons we convert in the first place. We can always _lather_ add a version that takes `Supplier<String>` if we need it to be more dynamic.
```suggestion public void setFile(File file) { this.file = file; } public void setFile(String file) { this.file = getProject().file(file); } ``` Along the same lines as above to avoid the use of Object.
I would prefer we use something like `Files.write` where we can be specific about the encoding. `FileWriter` will rely on the default encoding, something we generally try to avoid.
Is it still used ? @elasticmachine test this please
Please use `getProject().file()` here to be consistent with how Gradle resolves strings to files. For relative paths, `new File()` is relative to the current working directory, while `project.file()` is relative to `project.projectDir`.
I tend to move `writeTo` so it is on the same screen as the reading constructor just so it is easier to eyeball them at the same time.
I see that `ScrollHelper.fetchAllByEntity` already wraps the listener inside a `ContextPreservingActionListener`.
There's no need to specify `this` : `if (isString() && other.isString())`
typo: if return -> is returned
typo: this this
You are already in `ESRestTestCase` so you don't need the to reference the class.
OK because we're racing? Maybe we should add the `ignore` parameter to the request in that case.
This seems a bit broad.
It is how other plugins do it, yeah. There isn't a clear definition of "correct" here.
I think it would be worth renaming this method, otherwise it will cause confusion in the future about whether it's expected to be a bug or not (since the only reference to the exact bug number is in the line you've deleted). Alternatively, the format this method tests could be moved back into the method above (which is where it was originally).
I think you can remove `isSSLPropertyPresent` here, it's always true as it's checked above.
Can you add an assertion post `super` call that the ssl handler is still first? That way a future change does not accidentally mess up necessary ordering on handlers.
you can get rid of the builder and just return an expressionrolemapping by changing the `Void` to a `String` and when you call this use: ``` String name = parser.currentName(); ExpressionRoleMapping mapping = ExpressionRoleMapping.PARSER.parse(parser, name); ``` then the function to build would be: ``` (args, name) -> new ExpressionRoleMapping(name, (List<String>) args[0], (RoleMapperExpression) args[1], (Map<String, Object>) args[2], (boolean) args[3]); ``` Obviously that is missing the checks for values, but those should be in the constructor of ExpressionRoleMapping anyway
What about using a ConstructingObjectParser? You get the arguments and then can just return an ExpressionRoleMapping instead of the builder.
are there concurrency concerns here that closeClients is called while we are initializing? I see that both `hasXPack` and `nodeVersions` are explicitly assigned to non-null values. anyways, I guess assertions do not hurt!
cool. this is sufficient for ILM for now, so that makes sense
I guess this could still throw on us? How about we catch the channel exception here and re-throw it as an I/O exception? Then I think we are covered on all bases.
The fact that we process noops differently than indexing / delete ops (w.r.t localcheckpoint) sounds like a bug (different) PR)
we do this so often. I wonder if it's time for a utility method.
`.addPathPartAsIs("_xpack", "rollup", "job")`
no need for extra space
I think it'd be useful to see the filenames in the exception message.
I think we don't need this line or the following two, since they duplicate docs found elsewhere.
I think it'd be useful to see the path in the exception message.
I think it'd be useful to see the filenames in the exception message.
I see. Could we just open all the directories up-front and then clean them all up at the end? If possible I think it's clearer that there are no leaks if you can see acquisition and release of resources to be paired up in a single method. It's not always possible, but here I think it is.
I think it'd be useful to see the filenames in the exception message.
This is short and only used in one place, so I think I'd inline it.
Nit: this could be on the previous line.
This `{` block `}` fits the pattern we use elsewhere, but feels unnecessary in this context.
Doesn't actually throw `IOException`.
I think it'd be useful to see the filenames in the exception message.
I think that for consistency with `jobExists` the listener should get `Boolean.FALSE` if the group does not exist and it is up to the caller to know if they want to error on `false` or not. Giving the listener an error here does not seem right as no error actually occurred.
I think you should make `.size(0)` on the search as we don't really care about any of the hits, just the total number of them.
I was confused with the StringBuilder here, as you later on `insert(0` for the prefix of the message. You could use an ArrayList for the `type.getKey()` and a StringJoiner to append them to the StringBuilder. Really minor, just a suggestion.
Typo for `expect`, should have been `except`.
is this check mutually exclusive with the above? If yes I would prefer an `else if` to denote that, otherwise the `errorMessage` should be checked and a `, ` should be appended first.
I think if you add this below then that will work too and it is shorter: ``` (actual instanceof NodeNotConnectedException && actual.getMessage().contains("TransportService is closed")) ```
No, recently we made a change that specifically not marks tasks as failed: https://github.com/elastic/elasticsearch/pull/34404 If the task is marked as failed then it is removed and there is no trace of it other than in the log file of the node the task was running. By keeping the task we can read the fatal error from the ccr stats api. If `fatalException` is set then the task will stop any ongoing operations. The user will need to invoke the pause api in order to get the task removed.
This should be the version we are going back to, so 6.5. In this PR, disable bwc tests in the root build.gradle file. Then re-enable in the backport, and do a followup to master to re-enable there as well. This minimizes the changes necessary to followup to master (instead of needing to remember and change all the places that have this version constant).
Nit: `s/soft-deletes enabled/soft deletes to be enabled`
so many spaces :)
from sampling a few of the others, looks like a few of them do already
I don't think this needs to be a method shared between `AutoFollowCoordinator` and `TransportPutFollowAction`; I think that we can inline the check directly into the call sites. Then we can avoid having a debate about the method not throwing versus returning null versus returning an optional. 
If we think there is value in having this method (for example, maybe for future purposes), then let us discuss that.
I think does not have soft deletes enabled was great!
For java, there needs to be setters/getters, as the annotation will not automatically create them as it does in groovy. The annotations then go on the getters/settings.
no need for a constant here, you can use `StandardCharsets.UTF_8`.
Note that this is different than setting a single property as it adds the inputs to the list.
The method was not named as a setter in groovy so this could be DSL-like. ie, usage looks like (notice the lack of equals sign): ``` noticeTask { licensesDir 'foo' } ```
We use `== False` throughout the codebase instead of `!` because the former is easier to spot visually.
s/token role/token. Mention asynchronously
Can we also defer to `super.nodeSettings(nodeOrdinal)` so we don't also need to set `DISCOVERY_HOSTS_PROVIDER_SETTING` and `MAX_LOCAL_STORAGE_NODES_SETTING` here? (This also picks up a correct value for `DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING`).
Is this necessary? I think that the cluster should know it only has one master node and sets this accordingly.
Ahh ok it makes sense that this has gone from the subclasses. However I think I prefer it to remain on the subclasses for the sake of being explicit.
I think this shouldn't be removed, although the `autoMinMasterNodes = false` can go.
Nit: could this move back down to where it was before, since it's not being changed? Would make the diff smaller.
You should just use [Collectors.toSet()](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html#toSet--) ``` Stream.of(AggregationType.values()).map(AggregationType::name).collect(Collectors.toSet()); ```
This is actually an important case. We probably need a whole suite of tests for how things fail when `.ml-config` becomes unavailable. It could happen for example when all the data nodes are shut down before all the ML nodes during a full cluster restart. Also, it would be undesirable for jobs to go into the `failed` state if `.ml-config` has query failures for a fraction of a second when the node holding the primary shard fails and the replica needs to become primary. But such tests can wait for a followup PR.
nit: updates -> update
This has to be ``` .addPathPartAsIs("Strings", "hard", "coded") .addPathPart(indexUpgradeRequest.index()) ``` for sanitization
you can comma separate these instead... i know the likelihood of us not using `/` is low, but its best to not have them in this.
Could you indent this one more time? I'd probably also add the line break before `implements` rather than after the `,` so it is more obvious what is up when you scan file.
Please complete the sentence ;-)
Would you mind moving maxReadRequestSize above maxOutstandingReadRequests so that we have the same order for both read and write.
Kind of unfortunate that we have to do things like this to avoid long lines, but so be it.
I think this the right trade off here.
this can be replaced by just `ActionListener.wrap(runnable)`
I would move this before the call to schedule. This allows you to use a timeout of 0, which will then still reliably give you a response if the node has discovered enough nodes and not race against the scheduled task.
I like the implementation of the algorithm! Consider using CompletableFuture instead of AtomicBoolean+CountDownLatch though because it's only one concurrent primitive instead of 2.
please add the discovery type to the exception message. You can access it with `DISCOVERY_TYPE_SETTING.get(settings)`
maybe say something along the lines of "cluster bootstrapping can only be done against a master-eligible node"
meh. I think we should change the behavior of ListenableFuture to allow this. As this requires careful consideration of the existing callers, we should not do that change here. Let's keep the `listenerNotified` and add a TODO.
use AbstractRunnable? Then you don't need the try catch :)
also, throw an IllegalArgumentException and you will get a 400 response code instead of 500
I wonder if we need to go to `generic` here. Direct executor / same executor should also do.
side note (can be addressed later): all users of getFoundPeers always add the local node. Maybe we should have that node included by default.
The listenerNotified mechanism is not needed, that's taken care of by the future (you can only complete it once)
same here: I'd like to keep the version using `generateParser` since its the same as in master.
nit: I changed this on master to get the parser from AcknowledgedResponse using a new `generateParser` method that I introduced there on request of Baz. Maybe we could use the same here in the backport to make it match the version on master.
Same here about `Exception` catching
`s/step/cluster state step/`
nit: maybe it makes sense to rename the method not that the underlying property is gone. The other two values that are derived seem to be the only used ones.
Sounds good. Also I didn't see this was a private method, so naming doesn't matter that much IMHO, but still nice if it is changed.
you can remove the TODO
But this one is actually pretty huge. I don't think it'd fit!
I tend to try an indent these manually so they *look* a little more like json. Not that this is required, but it does help when they get big like this.
I guess, are any of the other assertions necessary given that we are checking that source has not changed at all in this case (and no metadata was added).
I am not exactly sure what you are doing here. The `toString()` will always give you a String... and `get(proc)` can throw an `IllegalAccessException` if the field is inaccessible (static, since `proc.getField` will check the `public` part). So why the "does not provide a String NAME field"? Sorry if I missed something.
I would replace ` - ` with `: `. s missing: di**s**covered
s/enable user/clear cache
Thanks for the explanation. Let's do it here. Since the error has an inner cause which provides from_seq_no and to_seq_no information, I think we are good.
> I am thinking to have a dedicated exception so that we can access "fromSeqNo" and "toSeqNo" but maybe overkilled. We should not introduce that dedicated exception I think.
+100 to any way that doesn't rely on message strings. Thanks for iterating on this to get there!
Would you break each of the separate test cases into a separate test? `testValidateNameComma`, `testValidateNameLeadingUnderscore`, ... Also, I'm not seeing a test that a name can otherwise contain an underscore as long as it's not the leader character.
I think we only need to write out an unchanged index metadata file if the manifest file was missing. If it's present in a manifest file then we know it was fsynced, and I think rewriting every index's metadata on startup will substantially slow down the startup of a node. (The same is true of the global metadata, but the effects of that on startup time will be less pronounced.)
I think we should be able to assert here that the `metaStateService` is in its "reset" state, with no pending writes (i.e. empty `cleanupActions` and `newIndices` and `globalGeneration` matching that in the `currentMetaState`.
I think we should be able to assert here that the `metaStateService` is in its "reset" state, with no pending writes (i.e. empty `cleanupActions` and `newIndices` and `globalGeneration` matching that in the `currentMetaState`.
For the record I'd rather we didn't have these intermediate objects that are only really there for testing (i.e. in the production code we construct them and then immediately execute them). But now is not the time to rewrite the tests to assert the things we actually want to assert, so the changes here are good for me.
maybe call this `getMetaDataOrDefault()`
can you provide a better exception message here? e.g. "failed to persist cluster state " + event.state().version()
We've been using `deprecationLogger` because the logger does things rather than is a constant.
I think it's good to reuse this threadpool, but it implies that the analytics process is a "job" from the point of view of deciding how many jobs can run on each node. We definitely need to hook these processes into a unified allocation framework. Also, maybe rename the threadpool to `JOB_PROCESS_THREAD_POOL_NAME` or similar.
I think we should have just one pool for all job types, and the maximum number of open jobs per node should also relate to _all_ job types. Since the number of threads in the pool is governed by the number of named pipes we create per job we shouldn't need to adjust the thread pool size. However, since people might now want more jobs this might be a good opportunity to implement #29809.
Since this is only going to be used in tests, I think we can get away with: ```suggestion return Objects.hash(maxSeqNo, localCheckpoint, globalCheckpoint); ```
I think you want (soft)deleted
that' shouldn't be possible. A live doc is always the one with the highest seq# (if exists).
can we call it `isLive` we usually mark docs as live not deleted
I liked the assertion you had there that if already have a result, this one has a higher seq no
You'll also need to change this in [the associated test](https://github.com/elastic/elasticsearch/blob/216c761a5dd81e65ca9826fb0793ddc53cd2ff27/client/rest-high-level/src/test/java/org/elasticsearch/client/IndexLifecycleRequestConvertersTests.java#L102). CI should catch this, but in the interest of not waiting 3 hours to find out...
I don't think we should save the state at the end of the job. In fact the indexer should never save any state, this should be done outside of the task. I have a branch that does that but it's not ready yet. In the mean time I think it's fine to just ignore the exceptions thrown by `doSaveState` and just call `onFailure` with the real one (the one thrown by the search/bulk).
`doSaveState` is async so there is no need to try/catch this call. We ignore exceptions in `doSaveState`, this is ok IMO since saving the state here is just to ensure that we'll start from the last commit point.
I think it makes sense for term based queries (`fuzzy`, `prefix`, `phrase`) because they need to be aware of the internal representation of terms and we can make optimization based on the different options set on the field type (`index_prefixes`, ...). The `commonTermsQuery` is more a free text query with a very specific strategy. We should make sure that it uses `MappedFieldType#termQuery` to build the bag of terms internally but I don't think we should expose it in field types. This is just an optimized `matchQuery` which is why I used the term 'expert'.
Can we test with small numbers, this seems big for a simple ut.
`addPathPartAsIs("_xpack", "security", "user")`
There is currently a PR for adding `Optional<Response>` performRequest methods, so you can, instead of having a `isFound` boolean inside your `Response`, return an `Optional.empty()`. I am not sure it would work for these APIs tho, as the docs did not specify them throwing a 404 if the user is not found. The code reads like the 404 would be thrown if the user is not found, but the API docs specifies it returns found=false. Can you verify which one is correct, and if return=false is correct we should probably remove the `singleton(404)` in the ignores parameter. Ill leave this here to look at, for returning Optional.empty() on a 404 not found, in case you find it useful. #35166
64e5c25 added support for this.
It might also depend on which implementation of `AcknowledgedResponse` you are using, since we have two, yay duplication!!! You should have a look at `StartRollupJobResponse`, which is an example of changing the word from `acknowledged` to `started`. this should get you on the right track.
Hey I saw some updates on this PR and I just wanted to throw a reminder out that we are not going to do a singleton(404) here, because we want a delete that is not found to throw an exception. Also, last time we spoke you were going to change this to AcknowledgedResponse. <3
so then the 404 does not actually happen, right? If so we should remove it. Im also all for using Optional instead of found=false, in general, but you dont have to go fix that all right now.
cool, so we can remove it from the ignores list
heh, duh... Sorry, ive been on vacation and full of turkey since last week.
Let's improve the expression a bit: 1. add location (so the user can tell where the parsing stopped): `new ParsingException(source(ctx), "")` 2. rephrase the message to better convey the message: "SQL statement too large; halt parsing to prevent memory errors (stopped at depth {})" or something along those lines.
Given that there are 3 tests it would be nice to give all of them descriptive names.
nit: indenting should be only 4 extra spaces
This will fail if `(int)((end - start)/bucketSpan)` is greater than 10000. A check should be added elsewhere to ensure that the window is small enough that this won't happen.
Rather than `True` maybe this method could be called `checkCurrentBucketEventCount`? There's nothing to say it couldn't change again after this method does its work.
Strings.hasLength(xyz) can be used with explitict comparison with boolean False
Strings.hasLength(xyz) can be used with explitict comparison with boolean False
nit: maybe use Strings.isEmpty.
I feel like this limitation is ok for now. We can revisit this later if necessary
I think we want to test index level blocks too here
nit: extra line
I think this should be removed based on the value of `index.blocks.write` (i.e., if true add, if false remove). See `MetaDataUpdateSettingsService#updateSettings`.
Maybe use indexSafe() here? Just in case of the resolved index got deleted before the cluster state update task is executed.
nit: wrong doc
I think we still might want a test for this, for the case when the license expires upstream.
please move this to its own test, almost all other of the CRUD operations are separated out to their own tests.
the exception declaration is not necessary.
`addPathPartAsIs("_xpack", "ml", "filters")`
lets turn this into a constructor that takes these 3 values and make them final in the class. Then you can just remove all of the extra validation stuff and the `addValidationError` below.
Here it is more clean, but again I think using `synonym_query_style` would be better
wouldn't it be better if the exception uses `synonym_query_style` instead of `synQueryStyle`. It would be more clear, I think.
Why not use `DEFAULT_SYNONYM_QUERY_STYLE` as a value ? This way if the default is ever changed then one has to change it at just one place.
For easier debugging it would be good to print the (unknown) value of `this.synonymQueryStyle`
fair enough. that is a good reason.
why does it need to be higher? the comparison here https://github.com/elastic/elasticsearch/pull/35441/files#diff-2dc062611a89dbcea48ead749790ac07R205 is `<= 0` so it's fine to be equal? not sure I follow
well it must have some kind of `LongSupplier` that's the point. The threadpool can be one impl of it.
maybe it makes sense to put `markRead` and `markWrite` on `TcpChannel` and make the stats a pure struct? Would feel more natural
I am concerned this will cause a ton of calls to `nanoTime`. We can prevent this by using `ThreadPool#relativeTimeInMillis()` (if we need nanosec resolution we can add it here too). This means we need to pass a `LongSupplier` to the stats somehow but I think it's worth it.
can we call this class `ChannelStats` code is easier to read if you have context in the class name.
personal taste, I always think of a server channel and a channel rather than client. maybe I am biased due to ServerSocket and Socket. If I could pick I'd invert it and name it `isServerChannel`
I'm not sure what you're testing here with this assertion. why not just repeatedly call `stopRandomNode()`, and maybe check that the cluster is alive and healthy before shutting down the last node.
nit: s/These file represents/These files represent/
Do we want to have this as an assertion or as an IllegalStateException or similar? If a user runs this on some weird Linux distribution that has an empty file here they might get odd errors later on, although I suppose you could argue that if they're not running on a supported OS then they're on their own anyway.
Sorry I missed this earlier, I think this should be `ensureExpectedToken(XContentParser.Token.START_OBJECT,token, parser::getTokenLocation)` (expected, actual, getTokenLocation or else might result in wrong error message.
` // <1>` is unnecessary.
I think this needs to be ``` settings.put(INITIAL_MASTER_NODE_COUNT_SETTING.getKey(), numSharedDedicatedMasterNodes + (numSharedDedicatedMasterNodes > 0 ? 0 : numSharedDataNodes)); ``` because of the condition further below where we make the shared data nodes data-only nodes when there are dedicated master nodes.
We discussed this on another channel, and decided to only do the auto-bootstrapping when autoManageMinMaster mode is active. We also decided to have multiple nodes participate in / run the bootstrapping process.
nit: Probably throw more descriptive IllegalStateException here
Oops must have accidentally pull this code over from 6.x. Thanks for removing
It would be nice to return a simple, non-empty structure here so that we test that aspect of the response parsing.
I'm not so comfortable with separating this code from the one in `updatePrimaryTermIfNeeded` - they are tightly connected. Instead of sharing code this way, how about creating a callback that will run: ``` indexShardOperationPermits.acquire(listener, executorOnDelay, true, debugInfo); ``` or ``` indexShardOperationPermits.asyncBlockOperations(listener, timeout.duration(), timeout.timeUnit()); ```
sorry, I missed the ping here. I think that 1 is ok too, but then we should in general change the terminology in `IndexShardOperationPermits` to move from the notion of blocking operations to running exclusive operations (similar to a read/write lock).
I think @bizybot is correct - we probably need some of the failure cases in `ApiKeyService.authenticateWithApiKeyIfPresent` to have a `terminate` status instead of `continue`. If I'm passing an API Key over TLS, then it would be very strange (and hard to debug) if the authentication use the API Key right up until it expired and then suddenly switched to PKI auth.
final and java docs
`application` needs to be null/empty
Nit: extra line
I missed the fact we don't resolve closed indices by default. Fair enough. Sorry for the noise.
I *think* it will have the side effect of the block not being restored after a cluster restart.
I'm wondering whether the fact that this would still return `false` after `close` has been called could be confusing.
I don't like splitting this up if I can avoid it because you lose a lot of context. I prefer something like: ``` public class TransportShardMultiTermsVectorAction extends TransportSingleShardAction<MultiTermVectorsShardRequest, MultiTermVectorsShardResponse> { ``` Or even ``` public class TransportShardMultiTermsVectorAction extends TransportSingleShardAction< MultiTermVectorsShardRequest, MultiTermVectorsShardResponse> { ```
minor: I think it would be a bit more obvious to explicitly call `DateTimeZone.getDefault()` instead of `null`. Since that is what Joda does with the `null` value. http://joda-time.sourceforge.net/apidocs/src-html/org/joda/time/DateTimeZone.html#line.301
Doh! the recommendation is forbidden API ..never mind.
this is a lot of duplication from the setting. If a new settings gets added it needs to be added here as well. Would it make sense to have `EmailService.getSettings()` and `EmailService.getSecureSettings()` for each of the service? This also applies for the next changes
the naming might be a bit confusing: `clusterSettings` and then `getClusterSettings()` as variable names
I would be using a `Set` in this circumstances.
I understand this is the oversight you've mentioned
looks like trolling :)) what's wrong with `&&`
It's good like it is , it's more civilized, otherwise it would indeed indeed violate the scope of this PR. I think I see the privilege code as new code where creative destruction is more loosely permitted. But I still think it would be nice to do this change in a follow-up. I am happy to pick it up if you wish.
I think it's more usual to just pass the class rather than the name, so `LogManager.getLogger(TransportDeleteFeatureIndexBuilderJobAction.class)` in this case.
I second this, other than this, the PR looks good. Will approve when changed.
how come this is set to false? Id prefer if we allowed unknowns since the server side API can change and we want to be as compat as possible w/ server on multiple versions
It's totally insignificant, but the actual AD realm type is `active_directory`.
Since you touched the response, I am doing my duty to recommend moving these tests to `AbstractHlrcStreamableXContentTestCase` .
I think I preferred the `DataOrMasterNodePredicate`.
I wonder if we can avoid to hook this in here, and whether there's a way to do this in Coordinator. We call `ackOnce(null)` here, which in turn calls `AckListener.onNodeAck(DiscoveryNode, @Nullable Exception)`. We already hook into that acklistener in Coordinator, so we could also get those events there. And we also know the version of the cluster state we're publishing in Coordinator.
let's avoid the extra brackets here.
I would think that it is better to sort this once once all the tasks are added instead of for every `task : tasks`.
Maybe in the future we want to add a new topology library, e.g. spatial3d, so I think it is better to name the method with a reference to the underlaying library.
Note that we'll have a similar problem when transitioning from a master that did not keep the routing table in sync to a master that now does. We will probably have to schedule a clean-up task that brings these two in sync again. There's no guarantee that that task will run before any other tasks, so the assertion might not only be violated while `event.previousState().nodes().isLocalNodeElectedMaster() == false`, but also on some follow-up cluster state changes. I'm wondering if we need to introduce some state to SnapshotsService to say whether we've properly cleaned up as a master and then make that assertion dependent on that boolean variable. Something we can look into in a follow-up.
perhaps move the assertion one level up, i.e., ``` } else { assert priamryShardRouting.relocating(); if (currentState == State.INIT || currentStatus.state() == State.ABORTED) { .. } else { .. } } ```
`X != false <==> X`
I'm a little surprised this passes the length check, I thought it was over. Cool if it does though!
Just thinking about diagnosing possible future failures, I think it would be useful to `logger.info()` any non-matching states so we can see if something weird was going on.
This still doesn't look safe. The listener could receive more than one state update, and the one we want could be any of them. Or indeed none of them if there's an irrelevant state update first and the listener is then removed before the one we wanted.
Ok, thanks for this precision.
It's a pure revert of my previous commit, where I did fix the formatting here. I'm okay with undoing the formatting fix in favor of a pure revert.
I *think* this should stay as it was or else transport client users will start to get a default scroll size of `10` again which won't perform well..
pageParams is missing from the equality check
Since `IfNull` always expects 2 arguments, force this into the constructor: ```java public IfNull(Location location, Expression left, Expression right) { super(location, asList(left, right)); } ```
It should - see `IsNull`
If the constructor is modified, this method won't be needed anymore.
Wondering if the class name shouldn't be `IfNull`...
All these methods can have package protected visibility.
maybe add here "because it's an internal action"
I understand the reason of this change but we can't just replace the toString() method with getStringRep() as it would break the xcontent rendering of "human readable" timevalues. This is what is rendered today for a timevalue when the `?human` parameter is set for cluster stats: ``` "max_uptime" : "8.3m", "max_uptime_in_millis" : 501911, ``` with such change it would become: ``` "max_uptime" : "501911ms", "max_uptime_in_millis" : 501911, ``` Maybe we could have two methods, something like `toHumanReadableString()` and the regular `toString()` (which implementation is copied from getStringRep()) and adapt the xcontent date formatters and parsers accordingly? (See XContentElasticsearchExtension)
can you try and run the test with the seed mentioned in the bugurl and see what happens (also possibly run it with a `-Dtests.iters=2000` and see if that passes) - maybe we drive by fixed this.
Should `job` be changed to the plural `jobs`? In ML we were told to use the plurals of `anomaly_detectors` and `datafeeds`. Other APIs that return lists of configs are also plural - `nodes`, `indices`, `aliases`, etc.
I know there's a lot of renaming still outstanding, but it might be best to use `data_frame` instead of `feature_index_builder` here from the start.
This shouldn't be necessary since the object that was serialized should already have had this logic applied. (Also, using `writeString` in the `writeTo` method means it's impossible for the string that's read to be `null`.)
Remove this empty line.
The extra logic here needs corresponding tests in `TransportGetDiscoveredNodesActionTests`.
We should return an exception if any requirement matches >1 node. I also wonder if we should return an exception if we specified a set of nodes and found nodes that didn't match this set. This statement is a note to myself to check this, no action is required yet.
maybe sort them by the list index `8`
It's really minor, I mean `decimalPrecision` could be before `radix` which is list.get(9).
This logic should be optimizer (changing the tree) instead of the parser which should be pretty accurate to the actual query (helps with reporting errors). Further more it would make this apply if one of the expressions becomes `NULL` in folding and such.
I think here the `i` logic can be replaced by a `boolean`? Seems to be a `true/false` scenario.
The `On` is superfluous. - `coalesceInput`, `greatestInput`, etc...
Since Min and Max are the actual functions, `extremum` seems like a better name.
Technically speaking the doc `type` in urls is being deprecated in 7.0. The general pattern we're adopting is it's not flagged as deprecated if you also pass an `include_type_name=false` along with the request. I can pick this up as part of types removal work if you want to leave that out for now.
If it's possible to directly forward to `LoggingDeprecationHandler.INSTANCE` I'd rather do that because it would avoid duplicating the knowledge that the logger for `ParseField.class` goes to the deprecation log.
It would be nice if `COMPATIBLE_FIELD_TYPES` was assigned a `Collections.unmodifiableSet`. So the static block could build up a temporary set then assign an unmodifiable version to `COMPATIBLE_FIELD_TYPES` at the end.
If you want to support all numeric types, I would suggest doing something similar to what Rollups does: ```suggestion private static final Set<String> COMPATIBLE_FIELD_TYPES; static { Set<String> types = Stream.of(NumberFieldMapper.NumberType.values()) .map(NumberFieldMapper.NumberType::typeName) .collect(Collectors.toSet()); types.add("scaled_float"); // have to add manually since scaled_float is in a module COMPATIBLE_FIELD_TYPES = types; } ```
Since this is a full cluster restart bwc test, I think we could go from 6.x to 7.0 right? Do we need to check if we're on 6.x so we can use the old endpoint? BWC test always hurt my brain so maybe not.
Why pick this order? Either is alphabetically or usage, in which case, boolean should be at the bottom along with IP and the rest higher up.
Sorry I had missed that
Gah! This is not easy to understand. For my sanity: Version.CURRENT on new node = 6.5.1 template version = 6.5.0 Using onOrAfter would be: `6.5.0 <= 6.5.1` = `true` Using onOrBefore would be: `6.5.0 >= 6.5.1` = `false`
this test was meant to specifically test Zen2 so maybe just set it to true here. Note that this test can probably go away once state recovery is implemented.
The existing `Automatons` code throws an exception if a pattern starts with `/` but doesn't end with `/`. So we could simplify all this checking down to a single if/else and still be compatible with existing behaviour. ``` if (indexPattern.startsWith("/") || indexPattern.contains("*") || indexPattern.contains("?")) { nonExactMatch.add(indexPattern); } else { exactMatch.add(indexPattern); } ``` I don't care strongly, but shorter and simpler seems nicer.
can you also adapt the logic in `addIndices` in this class so that it uses the indexmetadata created version when shrinking/splitting? We then have it uniformly everywhere
this is not the version with which the index was created, but the version of the master node in the cluster that took the snapshot :) I think you need to access `targetShard.indexSettings().getIndexMetaData().getCreationVersion()` here
I see. We can't bail here because we won't do the disabling that we need. Got it!
Nit: add a space between `if` and `(`
I wonder if you want a CyclicBarrier here.
we typically wait indefinitely so if things get stuck we get a suite timeout + stack dump so we know where things got stuck
If this stays an error or maybe gets converted to a log message, the line number would be a helpful debugging information for the user the file.
What is the reason you decided to not use this check anymore? I cannot find it in the refactored method.
nit: remove empty line. But I don't think its worth changing this if there are no other changes and CI is green, only if anything else needs changing anyway.
This confused me until I realised it was the var args argument. Is `new Object[] {usedName, modernName}` any clearer or confusing in a different way
can we just inline this in the transport action file? I don't see the need for another file.
In 7.0 this should use `_migration/deprecations` as of https://github.com/elastic/elasticsearch/pull/35976, although in 6.x it will have to use `_xpack`.
is there a way to filter out the index metadata here? We just want the global metadata.
the version here matters. As an initial approximation, we can take the maximum version of data nodes in the leader cluster.
I wonder if we should return all indices of the remote cluster here. That would more naturally map to the notion of the state of the remote cluster representing a snapshot.
instead of one snapshot per index, I think it's more natural to have a fixed SnapshotID(latest, latest) and all the indices of the cluster then as indices that are part of the snapshot.
what do you mean here? The important bit here is that we need index version compatibility for recovery. The data nodes on the follower cluster need to be able to read the Lucene index versions of the source clusters, otherwise we can't open the Lucene index.
I find myself having to think a lot about this sentence. Almost like it needs brackets to parse the logic. Maybe I just need another coffee.
Here we lose the option to fail if an expected warning didn't materialise. Is that OK? Its conceivable some tests can be certain in their expectations of failure and we do need a way to fail when we fail to fail (so to speak).
I think `query` and `aggregations` should use `LinkedHashMap` to retain the original order.
Nit: Needs a blank line.
There is now a base class `RestActionTestCase` that helps remove some of the test boilerplate.
Since these tests are so fast + simple, maybe we could just test both methods every time. I don't think this will be much extra code, as you could re-use the same `FakeRestRequest.Builder`.
hm, thx! I missed something there, sorry.
I don't get why do we need a special overload.
Because of the structure of the grammar it's a way to get the `-` symbol from the immediate parent context in the hierarchy that could possibly contain it.
I don't think this code should be embedded directly in the node start method. Can you factor this into a dedicated method (e.g., see how we handled something similar in 8033c576b71d6577c13fc23fac37787333dd14a5). Then it can be that you test this method directly.
I know this is existing, but I think we can lift this up to a singleton so that we do not create a new instance on every publish to every node.
I know this is existing, but I think we can lift this up to a singleton so that we do not create a new instance on every publish to every node.
I think that if we want to change this, we might need to wrap this in version checks? `readOptionalString()` encodes an extra byte for the boolean
nit: this became milliseconds.
Since the time unit is ms, we should remove this conversion.
This also became ms now.
Not sure about this parameter name (`trackingRemoteClusters`).
can't you use writeArray with a writer? I think that would be good instead of iterating manually. same on teh read end
Alternatively, you could also change all the uses of `context.settings` to `context.environment.settings()`
Why do we need getters? These are all final and immutable
Ah, I see the dilemma. If you want to encapsulate for that reason, that seems fine. Then please use the naming convention proposed in #14266? ie `settings()`, `environment()`, etc since they are read only
I noticed that you break lines on '(' . Was that an intention? Reminds me of my early projects in Lisp :) I am worried that most of other refactoring would be breaking chained methods on '.' what do you think of breaking on '.' for instance ``` client().admin().indices() .create(reateIndexRequest("test").settings(settings))) .actionGet(); ``` or even one '.' per each line - although that might have been excessive
sneaky :D, throwing from the `finally` block to supercede the possibly thrown exception from `grok.captures`
I think an exception other than an `AssertionError` will stop the busy loop. So if an `IOException` can be thrown that _shouldn't_ terminate the busy loop then it needs to be caught here. But maybe there isn't.
`HAVING " + "abs(max(int))` - one String only, no need to split it in half.
maybe simpler to just write ``` final boolean peersRemoved = peersByAddress.values().removeIf(Peer::handleWakeUp); ``` and then further below just ``` return peersRemoved; ```
This will need updating once the setting is moved.
This will need updating once the setting is moved.
I can confirm that changing the name of the template (but not the index pattern it matches) will not break the Logstash Management UI in Kibana. As long as the logstash management index name remains `.logstash`, everything in the Kibana Logstash Management UI should continue to work.
Please fix identation.
We discussed this issue in person and came to conclusion that this dead code is indeed dead code, and wouldn't make a difference if we started to use the parameters. Also we should fully remove the deprecated params.ctx support for 7.x (issue not logged yet).
Without much context to this code, it appears that `parameters` was likely meant to be sent as the first parameter to `newInstance`. The code here checks `script.getParams()` for null, but it does not check it downstream and could result in NPE if `script.params()` is ever null.
Same request for setting keys rather than literals here please.
I was thinking something similar to how we use [addValidationError](https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/common/ValidationException.java)
we should still get the operation mode from ` LicenseService.getLicense(state.metaData())`
Should be under Elastic License
It's better to use variable names with context so for example `check1` could be `keystoreCheck`, etc.
we should probably consolidate the error messages from the results so that we don't only present the first (from a seemingly arbitrary check order) error that was encountered to the user
Depending on other changes/suggestions, we should probably conditionally apply the FipsChecks if FIPS_MODE_ENABLED already in `Security.java` so that we don't have to check the settings value each time.
I'm not entirely sure it is required to have the FIPS checks implement an interface, but if we go down this path ,I think that conceptually it would make more sense to have a `FIPScheck` interface that all the relevant checks ( settings, password hash etc. ) would then implement.
I meant that we can have the if clause once in `Security.java` and call `check()` on all the checks (or the one `check()` if we decide to implement `FipsChecks` in a simpler manner ) IF `fips_mode` is set
ok as a follow-up
should we add a new UNASSIGNED_PRIMARY_TERM constant? There are too many of these magic 0 terms sprinkled all over.
nit: space `primaryTerm >=1` => `primaryTerm >= 1`.
We should also add seqno and term to hashCode.
This can be reduced to a single line using `stream().anyMatch` ```suggestion return expandedJobsIds.stream().anyMatch(jobId -> OpenJobAction.JobTaskMatcher.match(task, jobId)); ```
I think this needs to be `true` as well.
This one should be `true`. We want to check the password length any time we're generating a _new_ private key.
Having through about this a bit more, I think the _prompt_ is going to be annoying rather than helpful. I think we'd be better off just printing out a warning message, and continuing on. Sorry for messing things around like this, but sometimes things become clearer during the review cycle.
We don't want to actually check what version of `openssl` they have installed. They might be generating certificates to be used on a different machine that has a different version of openssl. It's OK to always print a warning about the password exceeding the old openssl limit.
The `withPassword` method is called every time we need a password, even if it's being used to _read_ a certificate file. In that case we don't want to print this warning, because that would cause additional output (and an additional prompt) for simple things like reading a CA file that has a long password. We need to only perform this check/warning if the password is being applied to a new file. I'm OK if we want get rid of the `promptYesNo` and just print out a warning, but we only want to do either of them when we know the password is being used to _write_ a file.
It looks like all of this section is whitespace changes (extra blank lines, additional indentation) that aren't needed.
I think this one needs to be `true`
The formatting of thise code (spaces around brackets etc) is different to the way the rest of this source code is formatted. By the time we get to the end of this review process, we'll want to have it formatted in the same style as the surrounding code.
Rather than `long_size`, it would be better to name this parameter something like `forNewKey` so it describes the logical purpose of the parameter.
This method is defined in `MlSingleNodeTestCase`
This field and the next one can be null perhaps use `randomBoolean() ? ` as above
is `timestamp` the partner of `end_timestamp`? Should it be `start_timestamp`
I think this needs to be "unidented" by 4 spaces. Also just for the "beauty" of things you can move the leading whitespace char to the prev line.
same here for the leading whitespace char.
maybe here and below you can move the `+` to the end of the prev line.
Thanks for this and sorry for the delayed response. Holidays kicked in :-) Happy new year! Elasticsearch is handling `null` in ordinary search/sort by putting those values at the end (or the beginning when the order is descending). It also lets the user change that behaviour via the `missing` parameter. Having said that, I would argue it is overkill to support this in the context of the `bucket_sort` aggregation. Looking more into the code, it actually looks like `ComparableBucket` could never really get any `null` values. There are 3 scenarios: - the sort field is `_key`; keys will be non-null I believe - `gap_policy` is `skip`; the bucket won't exist at all - `gap_policy` is `insert_zeros`; the value of the bucket will be `0.0` and will be sorted accordingly. So, it most probably is dead code.
It might be nice to name the two `bucket_sort` aggs differently for readability.
Just a style note, we prefer the more verbose negation (`foo != true` or `foo == false`) over the short form (`!foo`), because the short form is easy to misread or overlook. :)
Oh, I wasn't suggesting removing it in the copy constructor. I was missing the part that `sourceBuilder` here seems not be part of `searchRequest` yet when calling the copy constructor. All good now, thanks for the clarification.
Please can we create an audit message _as well_ as the annotation. One shows up in the config view and the other in the results view.
Maybe `SystemUser.NAME`? I worry that in the future when we do object level security using a username that doesnt correspond to a real user may be a security hole.
`local = false` is the default. No need to specify this here (also remove from one line above). Perhaps only fetch the cluster state once
there's another 0 here
you had one job 
synchronizing on this method is ok, it should be rarely called
I wonder if this test (and the one below) still make sense ? The test is based on the absence of something that used to be there. I would be +1 to simply remove the deprecation tests (instead of changing them to null tests)
Also, it can use `execute()` rather than `submit()` as it's not interested in getting a `Future` to track completion. Since you're off I'll push another commit that changes both things.
Yes, it can throw rejected execution exception if the thread pools queue is full or it is shutting down.
Maybe it's too verbose, but I suggest to rephrase to `apply filtering on the argument of the grouping function instead`, as `grouped field` maybe be confusing.
I think a plain old `String#format` can be used here, and then we can avoid adding yet another use of this class (which I ultimately want to remove).
If we want to avoid sprinkling `null` all over the codebase, we can also have a second constructor here that does not take `routing` as parameter and simply delegates to this constructor here with routing set to `null`.
we need to find another solution then. The only one I see right now is to introduce commit_timeout as an internal setting.
this means that the test will now fail on a slow CI machine that takes longer than "1s" to commit the cluster state update. I think we need to find a different way of writing this test, not relying on the `publish_timeout`. The alternative is that we have to reintroduce commit_timeout to Zen2 as well, just for testing purposes :/
minor nit: for readability, I would rather do ``` if (leader.skipDuplicates == false || seenSurfaceForms.add(current.getText().toString())) { options.add(current); if (options.size() >= size) { break; } } ``` That makes the if with continue not needed as we are at the end of the while block.
would you mind adding `<>` after `new PriorityQueue` ? otherwise this is an unchecked assignment.
this change is not needed either, as this does not matter which format it is within this test
this is a joda time class
this is the joda time class, which means this needs to remain original upper cased
I'd remove the bitmask - it doesn't seem to add much value (see the previous method suggestions for implementing and); shorter and clearer than using bits.
`if (value == FALSE) value = FALSE;` doesn't change anything so it can be removed.
That might make sense though my preference is to handle this corner cases leniently. I was a bit confused by `UNKNOWN`, I would argue an empty list has `FALSE` nullability (it can never be null) but then again maybe it's something that's worth having a check.
Is it intentional that you don't just catch `Exception`? I don't think there's any exception that could be thrown here that we wouldn't want to wrap with a better message.
Nit: the convention in our code base seems to be `static final` instead of `final static`.
Is this required? I'm missing where...
Since this code is repeated in the `visitIntegerLiteral`, maybe it can be encapsulated in the `minusAwareSource` or in a new method that wraps it.
the `if (deterministicTaskQueue.hasRunnableTasks()) {` condition is unnecessary
Can you give an example of what you mean by 2? i.e. expected behavior vs actual behavior.
you can do that please
This potentially overflows. I mean it's ok for our purposes (it overflows deterministically) but a sufficiently powerful linter would complain. Suggest `^` instead of `+`.
I think we no longer want this to be an `assertBusy()` - we expect that the previous reroute didn't do anything, so it should still be in this state from beforehand.
I think we can afford to make this `HashMap` eagerly and avoid the noise in the loop.
Also this isn't available yet, but this might be a good place to make use of the `StepListener` from #37327
As mentioned offline, I think the name `checksum` captures what we want.
How is verifying that the hash of the IDs the same as verifying that data was not mutated? It seems to me that all this is verifying is that the doc Ids are unchanged.
I'd feel better if we would do it this way ` IOUtils.closeWhileHandlingException(resources, () -> wrappedListener.onFailure(e));`
this is very hard to review, can we maybe do each of these steps as a separate PR? I would appreciate this.
this should be `IOUtils.close(phase1Snapshot, () -> resources.remove(phase1Snapshot))'
I think completely removing it is unrealistic, but we may not get a disconnection event for quite some time (up to ~15 minutes by default on Linux). I do not think it should be delayed beyond the safety phase.
I think we should delay these disconnections. Maybe we should rarely delay them by a lot.
I think it'd be good to test both `DISCONNECTED` and `BLACK_HOLE` here, perhaps using mostly the same value for the duration of a test.
It seems that all the implementations of `DisruptableMockTransport` simply have a getter for some constant value for the local node as their implementation. Maybe just move that getter up into `DisruptableMockTransport` and pass it as constructor parameter while we're changing this anyway? (just to save a bit of noise in the concrete tests :))
The list reference is already final in the class so this isnt really needed
This has the same issue where multiple threads could wind up finishing this since done is not rechecked within the synchronized block.
This could technically add a listener after done is set to true and at the same time something else is reading the list which is not safe.
I dont think this buys you anything in terms of concurrency. The list reference is already final.
This has issues as two calls could wind up finishing this listener. I think it would be better to use a AtomicBoolean and compareAndSet.
yes, I thought about failing the recovery in case where the block suddenly appears during the recovery. This has some other adverse consequences though. Needs more thought
we need to improve this, can you add it as an item to the meta issue? I wonder if we can fix this by acquiring an operation permit for each batch of operations that we send as part of phase 2 during peer recovery, and then also check whether there's a read-only block under the permit.
This message probably made sense once but it doesn't anymore. I'd suggest `Cannot update the job config because job...`
zen2 is the default. Why is this change necessary? These tests are probably already running with Zen2 unless explicitly disabled in some place
Unless your node is named "1", this is not a valid configuration. Please have a look at the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-discovery.html) or reach out to @DaveCTurner or me on how to configure this. Note that if your tests are running a single node in development mode, then no configuration should be necessary.
Also `zen2` is only available in 7.0 so `onOrAfter(6.5.0)` looks wrong.
Autoboxing already happens and I wouldn't worry to much about it considering the depth is not that big. Same for `Linked` vs `Array` (in general arrays are faster except for inserting in the middle as that requires resizing/copying at which the linked structure excels). I think the `Tuple` makes the code a bit more compact and safe (the queues cannot get out of sync) and more readable/simple code always trumps optimization (especially micro ones as here).
minor nitpick - use the `Deque` instead of `LinkedList`.
+1. It looks like a small method and while it might be inlined the extra `Tuple/Integer` are boiler-plate. If the method gets unrolled, both the index and the found value will be available without wrapping/boxing.
Again with the formatting...
Since the index and the Map are associated, how about using only one `Deque` which holds a `Tuple` instead of two `Deque`: ``` Deque<Tuple<Map<String, Object> index>>` queue = ... if (node instanceof Map) { queue.add(new Tuple<>(node, Integer.valueOf(i)); } ```
There's definitely some formatting differences here. Anyway, make sure you set your IDE to format only the lines that were modified as that prevents unnecessary changes like the above.
I would execute the `IOUtils.close(resources);` in a finally block after we sent back the response or the other way around.
This seems to fix the drop. With the earlier commit (`4344305`) geopoint default had the following indexing throughput: ``` | All | Min Throughput | index-append | 216469 | docs/s | | All | Median Throughput | index-append | 220417 | docs/s | | All | Max Throughput | index-append | 222044 | docs/s | ``` whereas with this commit: ``` | All | Min Throughput | index-append | 302604 | docs/s | | All | Median Throughput | index-append | 312456 | docs/s | | All | Max Throughput | index-append | 315466 | docs/s | ``` which seems to be roughly the same performance we had before the pending PR/commit i.e. using `04dcb13`: ``` | All | Min Throughput | index-append | 312749 | docs/s | | All | Median Throughput | index-append | 316424 | docs/s | | All | Max Throughput | index-append | 318845 | docs/s | ``` For the record the Rally command in a 1node env with separate load generator: ``` rally --skip-update --configuration-name=adhoc --target-host="192.168.14.13:39200" --track="geopoint" --car="defaults" --challenge="append-no-conflicts" --user-tag="env:bare,name:geopoint-append-defaults-1node" --pipeline="from-sources-complete" --revision="4344305" --include-tasks="delete-index,create-index,check-cluster-health,index-append,refresh-after-index,force-merge,refresh-after-force-merge" --user-tag="geopoint:regression" ``` vs ``` rally --skip-update --configuration-name=adhoc --target-host="192.168.14.13:39200" --track="geopoint" --car="defaults" --challenge="append-no-conflicts" --user-tag="env:bare,name:geopoint-append-defaults-1node" --pipeline="from-sources-skip-build" --revision="72f71a0e" --include-tasks="delete-index,create-index,check-cluster-health,index-append,refresh-after-index,force-merge,refresh-after-force-merge" --user-tag="geopoint:fix" ```
note that this will make the test below fail because it asserts that an IllegalStateException will be thrown. One workaround is to pack the assertion into a method and override that method in the test with a noop
`If we are retrieving all the s`
Why the prefix `[types removal]`? I dont' think we have any prefixes like that in other deprecation messages.
We had to choose a shared prefix in order for there to be a consistent way to detect types deprecation messages in REST tests (and ignore them). I think @jdconrad is just using this prefix here for consistency.
I think we should also check the taskId here
The type check should happen inside SUB not in its parent class...
Here you can do `if (function() == SUB && r.isDateBased() && DataTypes.isInterval(l))` (`r` and `l` are already defined above).
We discussed and concluded there is no good way to do this, but also no real need to support higher node ordinals.
Suggest: > You should run this tool only if you have permanently lost half or more of the master-eligible nodes, and you cannot restore the cluster from a snapshot. This tool can result in arbitrary data loss and should be the last resort. I'm suggesting to avoid saying "lost a majority" because you also need this tool if you have an even number of nodes and you've lost half of them, which isn't strictly a majority. Also could you explain how it can render the cluster "completely non-functional"? I mean I see how it can break indices, but it should allow a cluster to form and any broken indices can be deleted.
I think that we need to be able to deal with higher node ordinals somehow. To be discussed, as it's not immediately clear how to do so.
I think we should increment the term, and log that we've done so.
Hmm, we make a `private static final Logger logger` in `RemoveCorruptedShardDataCommand`, does that not work? Also, does this logger have the same configuration as loggers in Elasticsearch proper, i.e., it writes to the Elasticsearch log by default? If so, I think we should log more information about this tool having been run in that log.
This seems to be used more as a flag indicating whether or not to bootstrap the cluster, and a default of `do bootstrap the cluster` makes more sense to me.
maybe just subclass `ResourceNotFoundException` and we are good
Nitpick: `must be A constant`? Same for `PERCENTILE_RANK`...
Please put a timeout (10s or whatever you think it's appropriate) to prevent this for hanging up.
Nit on the formatting here (whitespace in front of the + please).
I dont' think this is possible? `getFields()` never returns null
Sorry I mispoke. 7.2.0 is good.
We handle this by temporarily disabling bwc tests. I just pushed the disabling to your branch. Tomorrow I will test the bwc behavior locally before pushing.
Yes, 7.2 was branched, but the 7.3 constant hasn't been pushed yet. I will update this branch today once the constant exists.
please open an issue for that, so we do not forget about it
alternatively just `actual.getNano() % 1000 == 0` - not sure if it is more readable to be honest, so feel free to ignore me :-)
oh sure, thanks for clarifying...
I'd consider this optimization optional for now.
see the ctor check for distinct locale and distinct zone
this method is not returning a boolean
Personal opinion: I dislike this method somehow. There are alternatives whenever this method is called (using `zone()` or `locale()` directly, and calling `this.parsers.get(0)` directly for the rest.
Why all these extra levels of indirection? I think this addSupplier, and the other one added here could just be implemented inside `keystore` that takes a FileSupplier, and the File variant calls that method instead of this indirection.
I think it should be `VersionUtils.randomIndexCompatibleVersion(random())`
Would it make sense to move this method to some SpatialUtils utility class? I feel like it's pretty generic and we might find some other ways to use it. I think I would also replace first three doubles with Circle. And we should figure out what to do with the radiusMeters parameter in Circle since it is not meters in case of `shape`, but this is a topic for another PR.
I think this can be simplified into ``` if (obj instanceof Map || obj instanceof String) { valueWrapper = Map.of("shape", obj); } else { ```
minor nit: you could move this into the WatcherState enum and just have a method `isStopState()`
minor nit: s/currently in a/in/
I think in the next iteration I am going to move this into `AbstractGeometryIndexer` and make it obtainable from `QueryShardContext`.
I think it might be safer to throw an Unsupported Operation exception here to make sure we don't call this accidentally.
I think this can be extracted into something like `float[][] lineToFloatArray(Line line)` since it appears 3 times in this file.
I think we lost some spaces before 123 here and in a few other places below.
I think it might makes sense to split this into `initalPosition` and `initialState`.
FYI the relevant circuit breaker is `IN_FLIGHT_REQUESTS`
I think assumeFalse would work better here showing us if the test actually ran or not.
```suggestion * Executed when a shard returns a fetch result. ```
Since we rely on the fact that index, timestampField, eventTypeField, etc cannot be null and fetchSize cannot be negative, we should validate that it's indeed the case. We could also make serialization more forgiving and avoid serializing the same default values on every request.
I am not sure we need this here. I don't see a scenario when we are going to resolve the index name based on the content of the request itself.
In most cases, if the index name is not specified on the URL we search all indices or use index specified inside the request. In this case I don't think we will ever search all indices and I don't see any provisions from specifying the index name inside the request. So, I think we can remove this version.
I would move this file into a separate trivial PR.
There is little value in reusing the histogram if you still create new inputs here. You might want to have a look at `ByteArrayDataInput#reset`.
I think there is `LuceneTests.randomSortValue()` that generates random sort values. I wonder if we could use it here.
If it is not possible, I think there is a [better pattern for this](https://github.com/elastic/elasticsearch/blob/76de93c25808b51b27f5d3cedaee159cf3d74fea/x-pack/plugin/sql/src/test/java/org/elasticsearch/xpack/sql/execution/search/extractor/FieldHitExtractorTests.java#L588) base on `randomFrom(...)` and `Supplier<...>`
was assuming that the data will be in ECS format ```{ "user": { "group": {} }, "host": { "os": { "platform": "windows", "name": "Windows" }, "ip": "127.0.0.1", "hostname": "localhost", "name": "localhost" }, "event": { "module": "endgame", "dataset": "esensor", "action": "already_running", "category": "process", "kind": "event" }, ``` thus the event.category for EVENT_TYPE
I think "Have you added" or "Did you add" might sound a bit better here... We could also change it to something less accusative :smile: like "If this is a newly added aggregation, please open..."
assuming fullFieldName is going to be used when topHitFieldRef impl is complete
nit: empty line, ocd kicks in :-)
maybe all the action names should contain `index_template` instead of `template_v2`? In this it would be `indices:admin/index_template/delete`. When v1 has been removed the v2 name is going to be confusing and changing that isn't fun from a bwc point of view.
 This method makes validating overlapping templates really easy for index templates v2 :)
I think we should overlapping v2 templates with the same priority in the same way (but then throw an error) (in another change)
As a follow up we need to take into account that indices can be removed from a data stream (except the last/most current index). So we should either check here that the backing index really exists or in the delete index api also modify a data stream if the index is part of a data stream
Maybe also check that the response code is `405`? This is what we return in the case a rest action hasn't been implemented.
Perhaps open a separate pr for the change to this method? This adds additional validation outside concrete index to data stream lookup.
Would be great if the decision on whether to include the items can be made earlier. Maybe in `TransportBulkAction` in the `finishHim()` method around line 541? This would then save creating bulk item response array. Would be great if we could make this decision even earlier, so that no shard level response items are collected. Then in the case that there are failures then we could serialize the successful items as null elements? But then I'm afraid that we break the bulk response format.
Maybe rename `metadataTransaction` to `metadataTransformer`? Since if provided the function will change the metadata builder.
If you pass the plugins in instead of `emptyList()` then I think `SearchModule` will do this stuff for you.
I do think this should be `final` then. I guess the `suppresswarnings` means that the bounds on `AbstractNamedWriteableTestCase` are wrong.
I don't think it needs to be `synchronized` any more because we do all plugin building in one thread.
maybe also assert that the index that is supposed to be removed really is removed? I think we now just check that all other indices but the removed index exists. I think that for that, just checking that the number of backing indices is decremented by 1 should suffice.
I think that instead of looping here just checking that the index that is supposed to be removed is removed and checking that the number of indices is decremented by one is sufficient.
nit: use assertThat(...) with isNull() as matcher instead? I think in general that is the preferred way of writing test assertions.
This validation needs to be moved to the `validateCompositeTemplate` method, that way, when a component template is updated we will still validate the mapping is correct. Otherwise a composable template could be added where the mapping is in the component template, and then the component template could be updated to remove the timestamp mapping. Once we move the validation it'll be checked for both index and component templates
When moving the validation to the `validateCompositeTemplate` method we should be able to reuse the mapping generated in that method
```suggestion throw new IllegalStateException("index shard " + shardId + " must be blocked by " + request.clusterBlock() + ```
Minor nit ```suggestion throw new IllegalStateException("index shard " + shardId + " is not blocking all operations during read-only marking"); ```
Minor suggestion to make it clearer that we're not waiting for the write index not to exist: ```suggestion public static final String NAME = "check-not-write-index"; ```
nit: use the constant from the mapper? content type I think it is called
should we rather throw ElasticsearchException also, and why doesn't the context expose a check method instead that accepts for instance a message? :)
maybe one day we will a base class for runtime mapper field types that does this in one place.
does it need to be protected? Also maybe rename to something like collectValue ? I find it weird to call add against the script itself
don't hardcode script :) we are going to rename it and if you use the existing constant renaming will be easier
oh boy :)
oh boy I was hoping we would not need this sort of stuff, but I guess we do? I mean the instanceof as well as the cast to double array
this seems high
I think I forgot a .value possibly? From your message I did not get if you managed to make it work after all, if not ping me and let's make sure that it works on doc_values
nice, I thought I checked out the test data and that field was keyword, but close enough. Ping me if you notice anything when playing with runtime fields ;)
We don't have the frozen phase any more, so this would have to be removed
I think as it currently is, we'll have to rewrite the parsing, because we won't know what the source index name is until much later, and we won't know the destination index name until actual execution also. I think we may want to not use `RollupV2Config`, but instead keep our own configuration option with the options we need. (At least if we want to keep `RollupV2Config` requiring the source and destination names up front
I think I'd prefer to have a single `static` block, since it's not easily apparent which would be executed first
We may also potentially want a way to have ILM delete the source index automatically after the rollup has been successfully completed, in case the user wants to do a destructive rollup
Ah, this is a backport PR -- nevermind me!
Should you use the static `templateName` import here and throughout? It looks like you might be mix 'n matching.
I actually don't think we should persist this one. Putting a template in place with `is_write_index: true` will be confusing because only a single index can be marked as the write index. I think we should instead throw an exception if `alias.writeIndex()` is set to `true` here (fail early rather than fail late)
I think it'd be fine to rebuild the list each time rather than have to think about the thing being mutable, even though it is only mutable in this fairly limited way.
This change is a little out of the scope of this PR, you did discover that we're missing a method below this one that has ```java public void testForceMergeActionWithCompressionCodec() throws Exception { forceMergeActionWithCodec("best_compression"); } ``` So I think we should not apply this change in this PR
This should not be overridden to true, since in a rolling environment a 7.11 node could start logging deprecation messages that are indexed prior to the master node being upgraded (meaning that the templates would not be in place), so the messages could end up not being indexed correctly
Oh this is tricky! Could we use `null` here instead? I'm not a big of making a `Script` that no one can compile. It'll have stuff like `engine=painless` which is isn't.
On second though, I think `null` is difficult here because we dereference the script all over the place. I plan to rework a lot of those dereferences eventually anyway so maybe we can just swear that we'll remove this "very soon" and be ok with it.
What happens if `enabled` isn't set? I *think* we should continue to do nothing if `enabled` is actually true.
Any chance you could group this so `unit.isMillisBased` is called once? I think it might be a bit easier to read if you juggle the "arms" of the `if` statement.
I wonder if it'd be easier to read if there were two methods? I'd kind of feel more comfortable having two just so you don't need auto-boxing. It might not make a huge difference, but in general `Rounding` doesn't its best not to allocate stuff. I think this method doesn't have to be so efficient, but still.
I pulled some of this out into #68793 so that we can get it merged through before 7.12.0, but I'm still fussing with things on the `equals`/`hashCode` front. Thanks for your patience, @gaobinlong.
I'm still looking into this -- it's a bit of a sticky situation. 
would it be possible to make the second part of this method part of the field script object itself? It would also be fantastic to use the same mechanism on both sides, but I know it is a bit tricky for different reasons: 1) that array that we reuse without resizing 2) we try so hard to avoid boxing 3) the usage pattern is slightly different if we compare doc_values, query and index time execution. I do wonder if it is worth investing on this, possibly having our own PrimitiveIterator or something that allows us to expose a clearer script API to access the computed values. For later I guess.
nit: I was wondering if it is necessary to carry around the whole MappingLookup given that what we need it only the field type lookup function and the list of post parse phases...maybe it's ok though
Maybe the post parse phase should become a class of its own: I am thinking this could help readability, clarify what it needs and what it outputs, and possibly even include some logic from IndexTimeScriptContext
I am thinking that clarifying and consolidating the notion of post parse phase, or whatever we want to call it, we could even make it more specific to script execution because that is all we need it for, would help making this more readable too.
can't help but wonder if these two impls could share more of their code. I guess that it all starts with the factories not sharing a base class, and I vaguely remember talking with @nik9000 about this not being straight-forward...
I wish that there were less functions and indirections: it is hard to follow the flow. I guess this has to do with the script service being available only from ParseContext, when mappings are being parsed. And it does need to be on the mapper because the returned type changes for each impl? Is there a chance that we can have an abstraction around a compiled script, with a generic type, that can be used everywhere, and the compiler method does not need to be on every mapper that allows to index calculated values? I see that down the line, a T type is used in CompiledScriptParameter anyways, which throws me off.
if indexValue is called for every field value, don't we end up calling the _field_names code block once per value rather than once per field? That looks like it's how things already work without your change though.
shall we make the error message agnostic "on field []" and reuse the existing parse method? It will need to be moved to a common place I guess.
having thought about this, I think that unifying the two paths is not possible at the moment, but I would consider making AbstractFieldScript typed , and add a method to it e.g. consumeValuesForDoc(int docId, Consumer<T>) so that we standardize on the API used to retrieve values for index time scripts, given that we can do so.
I wish that we did not have to go from MapperScript to IndexTimeScript, and rather reuse the same concept. I still wonder if this could be `void executeScript(SearchLookup, LeafReaderContext, ParseContext)`? We could make the notion of scripted field known to FieldMapper, let MappingLookup collect all mappers that have a script declared, then each one of those has the execute method called. That way you can also ensure the same behaviour once you add this functionality to other mappers? This suggestion goes against another one I made on making OneTimeFieldExecutor implement IndexTimeScript. MAybe with this suggestion IndexTimeScript could go away and we would have to see what to do with the one time executor.
having another look, index time lookup is needed when creating the search lookup, and I actually made another suggestion around possibly not needing IndexTimeScript entirely, so I don't think we will be able to do without adding indexTimeLooup.
I see, I thought it was only some internal error. cool then
yea I figured why MapperScript is there, and maybe that should be the only abstraction of a script for this functionality. I don't see the value of the indirection that IndexTimeScript causes now, as it could be just a method exposed directly by the mapper. Maybe you can give that a try and see what happens.
I am ok with the plain strings here, but I noticed that the constant is also in the conditional to decide the behaviour, hence I wondered if we need some constant.
can you make LeafReader and MemoryIndex links? :)
it still bugs me slightly that we end up exposing this. Thinking of alternatives, we could move a portion of ParseContext#executeIndexTimeScripts to MappingLookup and rather expose a method that returns a Map<String, Consumer<LeafReaderContext>> but I am not convinced this is a good idea either ;) maybe you have other ideas
I would remove indexTime from the name of these two methods. There is only one script in a mapper anyways.
I can see how having two methods is not fantastic, and why you had done it differently before. I had envisioned script as a member of FieldMapper directly, but we are going to see if that is possible once we add support for script to other mappers. The type of the consumer will make it possibly harder to share the impl but we'll see. I am happy though with the execute method, I find it much clearer than returning an executor like we had before, because it is evident what it does and easier to trace.
I asked before if the bit from runForDoc till the end can be a new method exposed by the Script class, in the effort of consolidating how field script classes expose their values. Do you have thoughts on this? We could also do it later
should we have some docs around what this is for and why? I know that I will look at it in a few months and wonder...but maybe just reading "this resolves all of the fields besides runtime fields etc." would help me
++ on the new name ;)
I find this approach clearer than before, thanks! One thing I still notice is that a Script becomes a MapperScript, which becomes an IndexTimeScript which becomes a Consumer<LeafReaderContext>. Conceptually the same thing is represented in three different ways. I think I see the distinction between MapperScript and IndexTimeScript, as the former is executed against a real index to fetch values as well (could we do something to highlight that?). I am thinking that the OneTimeFieldExecutor could implement IndexTimeScript too, so that at least one step can be avoided. We do wrap the script but it is still something of the same class, which I find easier to reason about.
sorry, I was referring to the AbstractScriptFieldType#parseScript which does exactly the same
ok it's unrelated, let's look at this again in a follow-up.
ok, I was more worried about the handling to make sure that we don't support stored scripts, but indeed, the rest is just calling Script.parse so it is not a lot of code.
nit: I think that you can revert this change now, the root object should be enough as an argument.
Also, I would make the method not abstract and have the default impl throw exception.
I spent some time thinking about whether we can consolidate this, like we do for runtime fields (e.g. the compilation could be done in a single place for all types). We can't really do the same that we do for runtime fields as NumberType is an enum and can't have a generic type, while the different Factory and LeafFactory don't have anything in common throughout the different types hence require a generic type somewhere. Whatever we do ends up being complicated for little gain e.g. saving a few lines of code). One thing is I find it a bit tricky to follow that thanks to this we compile the script at mapper creation time and not at execution time. We could potentially make MapperScript an abstract class with a generic type (the factory) and compile the script in its constructor. That is not perfect but maybe clarifies when the script gets compiled.
Huh, okay, looked into it -- it'd be a runtime exception if `o` is a `Long`. TIL!
could these three methods somehow be in the base test class, at least partially? what I am looking for is avoiding copy pasting when writing new tests, and possibly not forgetting to cover important scenarios.
We should also add this to the HuggingFace upload script. To grab the input size from the config file (if present).
++ The default should be 512 for BERT models.
Maybe either name this constant NAME or CAN_FORM_QUORUM.
I feel like the summary here should be a bit more descriptive. Perhaps "There are enough master-eligible instances to form a quorum."
Does this mean the module version is only the "major" version within a semver denoted jar filename? It might be useful to note that, or even link to any Java spec docs that describe this version extraction for automatic modules.
I don't think we need to use Nullable. In production this should not be null, we just have tests that abuse this contract, knowing how it is used.
I think this can be combined into LoadedPlugin, adding the ClassLoader there.
Instead of adding these plugins as we go, we can get the `values()` from `loaded` at the end of this method.
We have the pluginInfo here as well, in bundle, so we can create a LoadedPlugin
You could avoid this one off method by using a `List<String> lines` instead of a StringBuilder, then returning `String.join(System.lineSeparator(), lines);`
Code snippets should be in a `<pre>` block
Code snippets should be in a `<pre>` block
scoping -> scope
nit: space before instanceof
nit: space before instanceof
I think this isn't just about alias, it can shadow existing class names right? Maybe tweak the wording to make clear it's either an existing alias or classname
`<pre>` is for blocks, I think you want `{@code ...}` here.
nit: the map can be `PluginBundle::plugin`
This turns looking up a feature by name into a linear operation. Is that ok? Seems like we should still perhaps have the map internally but not expose it? It would seem trivial to build the structure and keep it inside SystemIndices as a private member, but still expose the list of features elsewhere? Alternatively, we could keep the list, but sort it, and then use Collections.binarySearch. It won't be as fast as a hash lookup, but at least it would not be linear, and would probably be good enough for the limited use of looking up a specific feature.
