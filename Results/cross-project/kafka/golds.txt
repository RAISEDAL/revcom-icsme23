nit: newline after if condition, also space before and after `!=`, and space after `if`.
nit: space before `{`.
Ah got it, that makes sense.
super nit: the string is slightly malformatted. The script displays: Enter reviewers in the format of "name1 <email1>", "name2 <email2>: Also, Its not clear if I should actually type the quotes when entering reviewers.
nit: you could just as well use a ternary operator here: ``` return (parts == null) ? Collections.<List<PartitionInfo>>emptyList() : parts; ```
@hachikuji I think the overhead should be minimal such that even with hundreds of partitions in the map, looping over them in memory should still be very fast compared with socket IO / ser-de / de-compression, etc.
Just curious, was this functionally needed? I am fine with the change though.
Could we move the fetchable Set in SubscriptionState to TopicPartitionState? More specifically: 1) change !paused to fetchable. 2) set fetchable to true in SubscriptionState.seek(). 3) set fetchable to in TopicPartitionState.awaitReset(). 4) add a fetchablePartitions() in SubscriptionState that loop over the assignedPartition map and add to the returned set if fetchable is true.
Can we rename it to something like "getState", assertAssignment is kind of misleading.
NIT: I think we should keep the check consistent between subscribe(topic) and subscribe(TopicPartition). I am fine with either way of checking.
Sure, that works.
I thought that retaining the fetches meant we wouldn't need this check anymore and we could merge the two loops.
Could we turn this block into a method? For example, throwIfOutofRange() or something like that.
I might be mistaken, but this doesn't seem sufficient. It seems like we should compare the out of range offset with the current consumed position and only throw the exception if they are equal. Presumably if the user has seeked to a new position, then they wouldn't be interested in an out of range error from a previous fetch.
It's definitely not a strong preference and the performance aspect doesn't really concern me either way. I am just trying to think through the full implications. One implication of discarding fetches is that it would be impossible to make progress unless the user handles the exception (either by propagating it or seeking to another offset). It's not clear to me if this is desirable or not. I also wonder whether some of these seek cases you mention are handled currently. Wouldn't it be possible even with the current patch to do a seek while a fetch is pending? In that case, the out of range error would apply to the previous offset and not the current one. That suggests to me that we ought to be checking the consumed position in SubscriptionState before throwing the exception.
There were two things I didn't like about the discard approach. First was somewhat stylistic. Having two passes over responses to look at error conditions looks a little smelly. Second, the decision to throw away the data seems arbitrary. Why should we discard good data just because it happened to come in on a request with an unrelated error? And note that we only discard the data from the request that had an out of range error: fetches on other brokers can still succeed, which means there could still be data available in the record set at the same time that an out of range exception is waiting to be thrown. Now that I think about it again, you would probably need the additional call to fetchedRecords for that approach also, so I'm not sure discarding the data really buys you anything.
Ah, I think I was wrong on one point. Your initial patch discarded ALL data until the exception was thrown. That would resolve the issue I mentioned above. That approach is easier to reason about if we could find a way to collapse the loops to remove some of its ugliness. We might also want to skip sending new fetches until the exception was handled to avoid fetching data we know will be discarded anyway. That would have implications for pollOnce (similar as above). Haha, not sure either approach is safe from complexity.
For 1), consider this sequence: consumer.poll(100); // fetches sent here consumer.commit(SYNC); // fetches returned here consumer.poll(100); // should return immediately, but will block on the next round of fetches because we don't check for available records before polling For 2), I'm a little unclear why you seem reluctant to change pollOnce. I'm pretty sure that the only place it's used is in KafkaConsumer.poll().
Should we clear the exception here? If not, then we'll have the exception thrown after the rebalance.
I'm not really clear why this is done in a separate loop. Are you trying to avoid the work of collecting valid records unnecessarily? Also, I think the loop below still has the OFFSET_OUT_OF_RANGE check.
Shouldn't this be invoked only after we've checked that this consume request matches the expected offset? I think the current code will behave incorrectly given the following sequence of events: 1. Fetch request sent 2. Rebalance 3. Seek 4. Fetch response triggers this code The callback is invoked, but the fetch request was for the previous generation and errors like OFFSET_OUT_OF_RANGE will not be correctly passed to the callback.
Not sure how using Collection would really help -- the element type for Maps would be `Entry<TopicPartition, OffsetAndMetadata>`, which won't implement any useful interface that would be interchangeable with a `List<OffsetCommitInfo>` where the `OffsetCommitInfo` contains both the topic partition and the offset and metadata. For mutability, in Copycat I just handled this at the application level -- subsequent updates are handled in a secondary map and they are merged once the commit completes. Would be nice to avoid having to do this if possible.
The purpose of the `Map<TopicPartition, Long>` was to avoid adding a new object type. But since we're doing that anyway what about just making the call: ``` public void commit(List<OffsetMetadata> offsets, CommitType type) ``` where now the `OffsetMetadata` class includes the `TopicPartition`? This is arguably no more complex than the original call in the case where you aren't giving metadata. Also does `OffsetMetadata` imply metadata about the offset whereas in fact this is both the offset and metadata? Other options would be `OffsetCommit` or `OffsetInfo` or `PartitionOffset`. Don't have a strong feeling on this one.
@ewencp Regarding making `OffsetAndMetadata` mutable, does that make sense if we do defensive copies of the passed in map as is being suggested? It seems to me that ownership rules would be inconsistent in that case and we would be doing plenty of allocation with the map copy anyway. It's also worth saying that due to boxing, we were already allocating `Long` instances, so the number of allocations doesn't change, it's just the size of each instance.
@ewencp As you say, it does hinge on whether this `commit` overload is used often or not. If it is not, then having this _and_ having `commitWithMetadata` seems excessive. You guys have a better handle on this, so I'll leave it to you. :)
I don't think it's _that_ big a deal to have to allocate the `OffsetMetadata`. And certainly the performance overhead of the allocation isn't a concern. I only care about the verbosity because the vast majority of use cases only care about the offset and not the metadata, and we're making that large fraction of cases harder. And would OffsetMetadata then be changed to be mutable, so it's convenient to just maintain the map where I update only the offset in that struct? Or do all my updates to that map (which I probably update for every single message processed) require a `new OffsetMetadata()`, bloating those statements and making them less clear? Or do I just maintain the `Map<TopicPartition, OffsetMetadata>` and have to convert it every time I call commit? On the other hand, maybe most users don't even specify the offsets manually anyway and the concern here is unwarranted since 99% of the cases are handled by `commit(CommitType)` and `commit(CommitType, ConsumerCommitCallback)`? In other words, I'm worried because I want the very common case to be clean, easy to read, and concise. I'm not yet sure whether this change would actually affect that common case.
You could call the class Offset (since the metadata is just an optional field).
Uggh, type erasure. You're right, we couldn't have both. It's ugly, but we could also use a different name, e.g. `commitWithMetadata`.
@nehanarkhede Yup I can review and check it in once it is rebased.
A better way is to do `offset.longValue` on the left side.
I feel enforcing users to create OffsetMetadata upon commit brings some overhead, especially in practice most people do not want to embed any metadata with their commits. I would like to propose an alternative solution regarding the API: commit(Map<TopicPartition, Long>, ComitType, String /\* Commit Message _/) commit(Map<TopicPartition, Long>, ComitType, String /_ Commit Message */, ConsumerCommitCallBack) And the commit message will be used as the metadata for all the partitions included. It is based on my assumption that users usually would have a single commit message (i.e. the metadata string) per each commit call; if they want to have different messages for different partitions (for example in CopyCat @ewencp), they can call commit() multiple times with the metadata strings on each partition. This of course may cause more round trips for all sync-commit, but I would suggest people to use the following pattern: commit(async); commit(async); ... commit(sync); // last call Since now the commit calls are all ordered.
This seems like an odd way to accomplish this. I guess it works ok, but seems like a lot of jumping through hoops to get the remote file. Is this just to avoid saving a copy locally? If we need to do stuff like this, it might be better to turn it into a utility in ducktape.
Right, in that case something like ``` num_log_lines = int(node.account.ssh_capture("wc -l %s" % ConsoleConsumer.LOG_FILE)) ``` might be better since it'll avoid sending the whole file. In this case it shouldn't matter since it should be empty anyway, but the general principle I'm trying to get at is that we should do what we can on the remote machine where possible rather than streaming entire files via SSH to process them locally.
Oh, nevermind. I didn't notice the API change.
Looks like nioSelector.connect takes a String not an Int.
Thanks for fixing. I changed the API and forgot the doc. Selector used to take a broker-id as identifier when it was used just for client-broker communications. Once we needed it for inter-broker, we needed more flexibility and changed that to String.
Yeah, that's a good point. Scratch that idea.
@guozhangwang Yep, sounds good to me.
@SinghAsDev Since KafkaConsumer has only one thread, even scheduled tasks have to be executed in that thread, which means the user has to wait for them. Since you can't really control when the tasks will be executed, in the worst case, it could turn a non-blocking call into a blocking one. And I don't see why error handling can't be handled asynchronously. For updating regex subscriptions, I wouldn't think it too much of a big deal even if we just ignored failures and waited for the next metadata update, though it would be easy to implement retries with backoff (I think we do this for heartbeats already).
I think requestFuture.value() is the same as value.
It's a minor thing, but I would move the acquire/release to the public subscribe method.
@SinghAsDev Haha, I can understand why this is a little confusing, but the network layer considers the request a "success" if it gets a response. However, that doesn't mean that there wasn't an error code in that response. It might be nice to handle this at the network layer, but it seems to me that there wasn't a generic way to check for errors. Each response object had the error code at a different location in its schema, so the only thing we could do is pass the response back and let the application determine if there was an error. The one case where we might be able to infer errors generically is by checking ClientResponse.wasDisconnected(), but I don't think we do this either (I've forgotten if there's a good reason for that).
@SinghAsDev I think that works! I wasn't sure if we needed to handle errors in the MetadataResponse itself, but it looks like the topics are only added to the Cluster if they had no error in the response.
@guozhangwang In KAFKA-2388, I think the plan is to remove the ability to subscribe incrementally (instead you have to provide the full list), so this would be consistent if we end up accepting that proposal.
I originally thought the semantics of unsubscribe() is blacklisting. For example: subscribe("PageView*") unsubscribe("PageViewTopicA") will subscribe to all page-view topics except page-view-A. Arguably this can be done solely in subscribe() with"^" in regex, but I feel this semantics is more natural for users.
@SinghAsDev @guozhangwang I wonder if we could support blacklist filtering through a second argument to subscribe. For example: ``` java void subscribe(Pattern whitelist, Pattern blacklist); ``` This might get around some of the complexity mentioned above.
Passing in the current listener seems a little weird. I wonder if we're trying a little too hard to reuse the subscribeTopics method in SubscriptionState. If instead we had a method like SubscriptionState.subscribeMatchingTopic (or something like that), then we wouldn't need the flag and we wouldn't need to pass a listener. You could change this line to this: ``` subscription.subscribeMatchingTopics(topicsToSubscribe); metadata.setTopics(topicsToSubscribe) ```
@SinghAsDev you are not increasing `pos`! It should be `topicsToSubscribe[pos++] = topic`, right? In fact, I would use a list as below: ``` List<String> topicsToSubscribe = new ArrayList<>(partitions.size()); blahblahblah .... subscribeTopics(topicsToSubscribe.toArray(new String[0]), true); ``` But I am fine with the array too.
This is an interesting discussion. I don't have the full context of these discussions, but I can see the argument for keeping things simple and letting users rely on the regular expression for achieving their goals. I am not sure if it's any simpler to provide two separate regular expressions than just one with the appropriate negation. The latter is easier to write a unit test for too.
Would it have to? I would see each call to subscribe as replacing the previous one. Perhaps if we wanted to support multiple regex subscriptions, we could do: ``` java void subscribe(List<Pattern> patterns); ``` And we could require users to use '^' for blacklisting. This would be consistent with the changes proposed in KAFKA-2388.
To be consistent with KAFKA-2388, this should probably be: ``` java void subscribe(Pattern pattern, ConsumerRebalanceListener listener); ```
format: ``` if (exception != null) throw exception; ```
formatting: no need for curly braces here
format: no need for curly braces
formatting: no need to curly braces
It does not appear that `KafkaConsumer` (or rather the `SubscriptionState` class it uses) allows using both `subscribe(...)` and `assign(...)`. Given that line 133 was recently changed to `assign`, then shouldn't line 167 be changed as well to: ``` restoreConsumer.assign(Collections.<TopicPartition>emptyList()); ```
Sounds good. The proposed `unsubscribe` will clear the state set by `assign` and `subscribe` methods, so that will address my concern. Thanks!
See, here you should evaluate what is the semantic that `paused` should have. If we want to return a **snapshot** of the paused `TopicPartition` then it's better to do: ``` return Collections.unmodifiableSet(new HashSet<>(paused)); ``` If we want to return a **dynamic** view of the `paused` then it's better to use: ``` return Collections.unmodifiableSet(paused); ``` In either case, we should return an unmodifiable view of the set, because it's not very nice to expose a mutable field directly to callers as above.
I thought about that too. Probably doesn't matter much and it's always kind of nice to have the code inline. As for using DefaultOffsetCommitCallback, either way is ok for me, though I doubt there's much chance of confusion between the auto and manual commits since users would (at least should) probably only be using one of them.
yep, especially given the current producer behavior.
Again, lines L#118 to L#123 can be replaced by: ``` assignment.keySet().retainAll(userAssignment); ``` same effect, as we want the intersection of `assignment` and `userAssignment`
nice fix! this has been bothering me.
There is a corner case in regarding the listener here: say a ListenerA class was previously used in the consumer, and now it wants to subscribe with another ListenerB class, after calling subscribe(), before sending a join-group request, a rebalance could be triggered upon, for example partition change, which will then use the new ListenerB class already. Not sure if it will be an issue though.
Again minor, but maybe IllegalArgumentException
I'd suggest to surround this command with an if-condition as below (if passing the argument as a collection, one more reason to change): ``` if (!topics.isEmpty()) requestUpdate(); ``` My reasoning is: why call `requestUpdate()` when there's no topics, right? Maybe nobody will ever call `addTopics` with an empty collection of topics so this optimization step is unnecessary. ;)
Lines L#93 to L#98 can be replaced by: ``` assignment.keySet().retainAll(subscription); ``` same effect, as we want the intersection of `assignment` and `subscription`
As we can "unset" listener to a `null` value then it's better to protected calls to `listener` against NPE, that involves checking `if (listener != null)` before calling (shrug).
Ya, sound just about right. :) update: I mean, it's preferable to fail fast when the assigment is tried than to let it blow up only when many lines after that, in a totally different context.
oops, you right, my fault. :( I glanced over really quickly, but with the prospect of doing a more comprehensive review/reading (incl. compilation, etc) on Monday. :) Well, so we can rewrite it as: ``` for (Iterator<TopicPartition> it = assignment.keySet(); it.hasNext(); ) { TopicPartition tp = it.next(); if (!subscription.contains(tp.topic())) it.remove(); } ``` This simplifies the code a tidy bit, at least. No need to get `entrySet` as we are only interested on the keys. ;)
Cool, never mind.
Does mirror maker support multiple consumer configs? A quick glance at the code suggests it only supports one.
Should be more specific about the type of error being caught -- catching all exceptions should be reserved for very special cases, like protecting the top stack frame of a thread to avoid uncleanly exiting the thread. I suspect that here you specifically want to capture `CalledProcessError`, which indicates an issue running the command on the remote host and/or `ValueError`.
You shouldn't need to pass in `consumer_timeout_ms` like this -- since it's a field on the object calling `render`, it should already be available to the template.
This is fine to leave like this since this is the current behavior of `BackgroundThreadService` in ducktape, but shall we file an upstream issue/patch to make it always have at least an empty list? This is annoying to have to have in here, and it's actually quite weird how `BackgroundThreadService` _starts_ with an empty list, and only when you call `wait()` does it set it to `None`.
As we've been discussing for other tests, this is not sufficient. I saw topic creation fail and found a leftover mirror maker process still running. Looking at the log from the previous run, it failed with an exception saying the process took too long to exit. I think in this case it might have been because I increased the consumer.timeout.ms, but this is just generally a problem. I think we need to catch the exception and `kill -9` the process if it doesn't exit gracefully.
I'm confused -- why wouldn't you run as a service with multiple processes? If you hand all the mirror maker processes the same config, they'll form a consumer group and balance the work between themselves.
This ended up printing out ``` AssertionError: num_produced: 1000, num_consumed: 869 ``` So I went and checked the console consumer log. Sure enough, I found this was the first message reported: ``` [DEBUG - 2015-08-20 16:44:53,298 - console_consumer - _worker - lineno:177]: consumed a message: 131 ``` So for some reason consumption didn't start with the first message. I ran the test and collected the `VerifiableProducer` log and it claims all were acked, they all went to partition 0, etc. I also logged the number of messages the `VerifiableProducer` claims were acked and that is 1000 as well. It seems something is still funky with this. How many times have you tried running to reproduce this? It sometimes passes for me, but maybe 50-60% of the time is failing. (By the way, it's kinda weird that the console consumer's per-message log goes to the test log, but verifiable producer is sent to a separate log file...)
Both of these are currently failing for me in a local virtualbox cluster. The first looks like a legit failure since the data doesn't seem to match, the second looks like maybe it's failing to create a topic perhaps because the previous test didn't tear down properly? The log for the second one indicates the topic already exists. ``` ====================================================================================================================================================================================================================================================================================================================================================================================================================================== SESSION REPORT (ALL TESTS) session_id: 2015-08-19--004 run time: 1 minute 40.540 seconds tests run: 2 passed: 0 failed: 2 ====================================================================================================================================================================================================================================================================================================================================================================================================================================== test_id: 2015-08-19--004.kafkatest.sanity_checks.test_mirror_maker.TestMirrorMakerService.test_end_to_end status: FAIL run time: 1 minute 2.808 seconds Traceback (most recent call last): File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 81, in run_all_tests result.data = self.run_single_test() File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 130, in run_single_test return self.current_test_context.function(self.current_test) File "/Users/ewencp/kafka.git/tests/kafkatest/sanity_checks/test_mirror_maker.py", line 94, in test_end_to_end assert len(self.consumer.messages_consumed[1]) == self.num_messages AssertionError -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- test_id: 2015-08-19--004.kafkatest.sanity_checks.test_mirror_maker.TestMirrorMakerService.test_lifecycle status: FAIL run time: 37.728 seconds Traceback (most recent call last): File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 78, in run_all_tests self.setup_single_test() File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 122, in setup_single_test self.current_test.setUp() File "/Users/ewencp/kafka.git/tests/kafkatest/sanity_checks/test_mirror_maker.py", line 58, in setUp self.k2.start() File "/Users/ewencp/kafka.git/tests/kafkatest/services/kafka.py", line 55, in start self.create_topic(topic_cfg) File "/Users/ewencp/kafka.git/tests/kafkatest/services/kafka.py", line 115, in create_topic node.account.ssh(cmd) File "/Users/ewencp/confluent/ducktape.git/ducktape/cluster/remoteaccount.py", line 79, in ssh return self._ssh_quiet(self.ssh_command(cmd), allow_fail) File "/Users/ewencp/confluent/ducktape.git/ducktape/cluster/remoteaccount.py", line 206, in _ssh_quiet raise e CalledProcessError: Command 'ssh vagrant@worker4 -o 'HostName 127.0.0.1' -o 'Port 2202' -o 'UserKnownHostsFile /dev/null' -o 'StrictHostKeyChecking no' -o 'PasswordAuthentication no' -o 'IdentityFile /Users/ewencp/kafka.git/.vagrant/machines/worker4/virtualbox/private_key' -o 'IdentitiesOnly yes' -o 'LogLevel FATAL' '/opt/kafka/bin/kafka-topics.sh --zookeeper worker2:2181 --create --topic topic --partitions 1 --replication-factor 1'' returned non-zero exit status 1 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ```
Not sure how many mirror maker tests we'll end up having, but would it make sense to have a `MirrorMakerTest` utility like the `KafkaTest` one, or does that end up being too minimal to be worth it (looking back at the `KafkaTest` one now, it looks like it's now just a few lines of code...)
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Docstring doesn't match the class
Any reason this isn't in `setUp` since it's needed for every test? Also, is there a reason `MirrorMaker.start()` isn't using the `wait_until` to wait until the node comes up? Seems like all callers of `start()` would want this functionality.
What you had is fine.
In version 0, we should only allow passing in a single BrokerEndPoint.
Even though UpdateMetadataRequest_v0 has identical structure as LeaderAndIsrRequest_v0, the set of brokers used are slightly different. In UpdateMetadataRequest_v0, we pass in all live brokers. In LeaderAndIsrRequest_v0, we pass in all live brokers that are the leaders. So, we can keep the names as they are.
This field should be named live_leaders.
This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
Good catch. It's probably too late to change that though.
It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
response version should be 1.
response version should be 1.
Yeah, I think that's fine. We probably need a JIRA for any other built-in assignors we want to ship with 0.8.3 anyway, e.g. I assume we'll have a copartitioning implementation for kstreams.
I would prefer defaulting to range just for consistency. We have seen similar cases in the producer where the behavior of partitioner's hashing function changes a bit, causing offset manager migrated for mirror-makers and hence resetting offset and data duplicates.
Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
Any reason for using an empty list here rather than `null` as a sentinel? The empty list approach seems like it could lead to confusing results if you have a programmatically generated list which can sometimes be empty. Right now it's not a problem since we only expose `listTopics` and `partitionsFor(oneTopic)`. But wasn't there a proposal for something like `partitionsFor(String... topics)`, in which case this could affect the public API.
Currently our configs are still going to pass-in a single partitioner which is then be used as a singleton list, hence we use `getConfiguredInstance` here.
Is this just temporary until we add better support in the configs for multiple assignors? I'd imagine we need to think through the exact semantics, if ordering matters at all, etc. Is the plan to eventually just switch this to a comma-separated list of class names? One thing I found with Copycat was that the more things that needed to be configured via the same config dictionary, the more problematic Kafka's standard approach to configuration became because you could easily hit cases where there were conflicting settings. Not sure if a) that'll be an issue here or b) if we even want to support assignors that have _that_ much config, but something worth thinking about before committing to this specific approach to specifying assignors.
It's just not great having multiple paths that make the same type of request if possible. But I see why we at least want different handling of this request/response, and there was another JIRA to make this exact change anyway.
I suppose you can make this final as well since quota1 and 2 are final. Another nit, can you add a message to the assertions? Otherwise, assertFalse will print a generic failure "Expected true received false" which doesn't really convey much information about the failing assertion.
It is probably not enough to just turn off OP_WRITE at SASL completion time. After completely sending a challenge token, the client needs to turn off OP_WRITE. Otherwise, while waiting to receive the next token from the server, the client will be busy checking in the selector.
It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
Could we make sun.security.jgss.native a property in the broker/client config file? In general, it seems that other than the jaas config file, it's better to specify other properties from config file instead of system properties.
The one we are keen on is PLAIN. We will be using SASL with SSL, so PLAIN gives us the simplest secure authentication without having to distribute certificates for mutual client auth. Yes, a separate PR makes sense so that this one can be committed soon. I will raise another JIRA.
Changed it locally (and in one other similar place).
There is the following in the constructor, so the thread can be null. ``` if (!isKrbTicket) { // if no TGT, do not bother with ticket management. return; } ```
Changed it locally.
Changed it locally.
I think so, changed it locally.
Changed it locally.
Changed it locally.
Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
Harsha has done this.
That's right, changed it locally. With the current code, this doesn't make much of a difference in practice, but it could lead to bugs in the future.
Changed it locally.
Harsha address this, I believe.
I added a TODO about this, we probably need to solve it in a subsequent release.
I removed this as it wasn't being used.
This class is surprisingly similar to org.apache.zookeeper.Login, have we copied from the same source? ;-)
I moved `transportLayer.removeInterestOps(SelectionKey.OP_WRITE);` from `case COMPLETE` to here in my latest PR.
Harsha did this.
Harsha removed the check.
Harsha did this.
Checked with Jun and this is fine.
That's right, changed it locally.
I believe this is fixed in my next PR.
I believe this is fixed in my next PR.
That's right. If we are still waiting for a new token to be completely received, we will need to turn off OP_WRITE.
This seems to have the same issue as in SaslClient in that we need the logic to turn off OP_WRITE here too. Suppose that the server tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the server receives the next token from the client.
It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
This is a no op.
not be => not be able to
newuntil => newUntil
It seems that t is never null. So perhaps it's simpler to just start the thread after t is created.
Is this test needed? It seems that loginContextName can never be null.
It seems that we need to set the login time during the initial login as well.
It seems that we should setLastLogin() in setLogin() instead of here.
It seems that both loginContext and mode can just be a local variable.
It doesn't seem that the client needs principalBuilder.
@ijuma Sorry, I don't know of a standard way of doing this,
OK, looking deeper into this, there is a difference: if someone else had called `setConfiguration`, `getConfiguration` would return that while here we override the value of configuration with the JAAS file. Neither option is ideal, but that's because of the global nature of this setting. I think just using `getConfiguration` is probably better, but I thought I'd mention it here for completeness.
Let's also add "socketChannel.socket().getInetAddress().getHostName() must match the hostname in principal/hostname@realm"
probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance.
That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
Do we need to set OP_READ? It seems it's always on.
We need to turn off OP_WRITE when saslServer is complete.
It seems that we can just use one level of if/else.
I think this could just be if(!SecurityProtocol.values().contains(securityProtocol))
Does that cause issue when a sensor/metric is added concurrently during removal? For example, 1. removeSensor(n1): complete until line 173. 2. a new sensor is added and we add a metric of the same name (as the metrics to be removed in step 1). 3. removeSensor(n1): complete the rest of the steps. After step 3, we may have removed the metrics added in step 2. Or step 2 will fail when adding the metric.
Perhaps we can lock the Sensor first and then lock the base metrics instance whenever we are removing a sensor. During addMetric, the sensor is locked first and then the Metrics object. If we follow the same pattern when doing removeSensor, we should not have any deadlocks right? What do you think? I'm still open to handling this in KAFKA-2419, but I want to be sure we have an acceptable solution.
Where do you cleanup the childrenSensors object? Otherwise we will maintain a reference to the Sensor objects always.
We might want to add a min version check as well, based on #986 changes.
This is added in #986 as well. The later patch to go in will have to rebase, or we can extract out the common code as a separate PR. I am fine with either way. FYI @becketqin.
Is this constructor used at all? If not, I'd not include it one should generally provide a message explaining more.
I'd also consider removing this one too if we are not using it.
This can also happen when topic exists but offline, right? or when we simply fail to connect (network issues, etc)? I like the specific error message, but I'm concerned it may be misleading if there can be other causes to the error (and user is certain the topic exists...)
Interesting design. If I'm understanding the code correctly, the get() returns a future that only gets triggered when you've reached the end of the topic. It copies the offsets out for the desired key, and returns them. So that "guarantees" that you have seen all messages in the topic, including any that might have been in-flight when the caller called get(). Is that right? It's not a complete guarantee though, right? There might have been some messages stuck a producer's retry loop somewhere. Or, messages that have been written to the master but not all the in-sync replicas yet.
Does this mean you manually assign this consumer to read all partitions? So the consumer group id doesn't matter or is not used? I couldn't see where a consumer group was being set. That means that each instance of this code consumes the entire topic, right? Which is exactly what you want. I ask because I have many use cases where I want a consumer to get all partitions. We currently do it by trying to create unique consumer group ids, but that is kind of annoying.
Does it still make sense to have the if/else here? we log debug anyway and the exception should contain enough information to figure out the type of error.
I see the problem with original approach after looking your test case. Now I think the new approach is correct.
Ah - you are not doubling the elapsed time because you are actually doing a modulo on the window size. That said, I think the current code should still be correct. Note that in your test you haven't actually created three samples because you didn't call record at the 60 second or later mark. i.e., if you debug through you will find only two samples. So the "current" time is taken off now minus the `lastWindowMs` of the "current" sample which is the second sample and that ends up being 105 seconds for me (which is correct because the current sample has not rolled over due to the absence of a record).
I had to stare at the `SampledStat` code most of today. I actually think the original code is correct. The elapsed time should be the time elapsed in the current window plus the previous (non-obsolete) windows. It seems this is exactly what the current code is doing. Your change I think would effectively almost double the actual elapsed time. Maybe we can discuss this offline.
It looks like there is an extra whitespace after return.
nit: those lines can be rewritten (simplified?) as: `return (topic != null) ? topic: metrics.sensor(name1);`
Nit: I don't think illegal generation is possible in JoinGroup.
strictly speaking, you don't need this yet. Still needed when we evolve I suppose
while this is fine, perhaps we should follow the same pattern for getters in these 3 new classes.
if these cant be null, then your checks can be simpler. return configKey.equals(that.configKey) && configValue.equals(that.configValue)
should this be null? perhaps throw an IllegalArgException
Is this only errors? I suppose we will need this even to return a successful response. Perhaps call this responseCode/responseData
Unclear why we're mixing some parameter substitution inline, and leaving others to the final substitution with the `args`. Made it confusing to figure out what was going on with the `JMX_PORT`, which looked like it had been lost since it wasn't in `args` any longer.
Do we need a newline here, either at the beginning or the end? Depending on who happens to edit the `console_consumer.properties` file last and whether their editor leaves newlines at the end of files, it seems like this could break.
Since these two branches do the same thing and half of the inner conditional is the same, it seems like you could just combine it all into one condition and only have one copy of that code.
More generally, you might find it easier to ensure that once you have the nodes allocated to you, they are all assigned a version, even if just the default `TRUNK`, before doing anything else with them.
Could be simplified to `not hasattr(node, "version") or node.version > LATEST_0_8_2)`
These two statements don't have the same semantics. Previously we would block until the first line of output, which here probably also guarantees metrics are being reported via JMX. From what I can tell here, it looks like you could easily be running `start_jmx_tool` before the metrics are ready. I don't think the previous version is great, but I think it at least provides the guarantee since we're piping logs elsewhere -- one line of output should indicate consumption has started.
This doesn't seem like something that should be in this class -- the node is owned by this service, but is passed into the method. This seems more appropriate to be implemented once in the `KafkaService`.
Delete this block - this was a specific check for ensuring log output in the test_console_consumer
It's fine to create a new file under the kafka system tests for now. In general though, it's OK to submit PRs against ducktape and advance the version - releasing new versions is a fairly lightweight process, and we pin the kafka system tests to a specific version of ducktape anyway.
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
I think `file_exists` is duplicated from `test_console_consumer.py` We should probably put this into something like `remoteaccount_utils.py` and reuse
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Flesh out the docstring here - it's important to have good docstrings in these test methods since the docstrings are actually propagated up to the test report as the test description.
Sounds OK to me since it is used more than once.
Might be no need to make this a field since it's only used in two test cases.
nit: rename this.to -> this.rawToKey
nit: the test case names are a bit too long, could we make them: testPutGetRange / testPutGetRangeWithDefaultSerde testRestore / testRestoreWithDefaultSerde We can ignore the key value types since they are not for specific purposes.
My minor concern is that KeyFactory and ValueFactory may be only specific to key-value stores, not not any general stores; for example for database stores you may have some functions like ``` withSchema() ``` that defines the data types for each column but not using "withKey / Value" any more. But since it is only for future improvements let's revisit this nested mechanism later.
Does this factory need to extend key-value store factory? It seems a general in-memory factory, not specifically for key-value stores, right? And same for OffHeapFactory.
And if they are not used yet, we can probably remove them for now.
Why did you change this to a `Set`? It seems unrelated to the ticket.
Yes. We can change them for consistency. That's a very good point.
Thanks! Will push this shortly.
Glad to help! :)
@guozhangwang No need to divide by 2 in the for-loop as it hops by 2 each iteration, so when it reaches `keyValue.length - 2` the next value of `i` will be keyValue.length. ;)
@guozhangwang **Forgive me if I misunderstood this method, really**. But if the list is composed of key-value pairs, wouldn't this for loop goes up to the the middle of the array? Plus, if `tags` is `"a","1","b","2"` the final map will be `"a" -> "1", "1" -> "b"`. Is this the right intend? Besides not reaching "b" -> "2", it will make the previous value a key in the next iteration. If not, I suppose we could rewrite it as ``` for (int i = 0; i < tags.length; ) { tagMap.put(tags[i], tags[i+1]); i += 2; } ``` Again, forgive me if I understood everything wrong.
You can just do i+=2 in the for loop.
Actually a more general question is that for assign(), is checking subscription.isEmpty() sufficient or not. Today we allow subscribe(empty_list) whose semantics is different from unsubscribe(), but they will leave the same state in subscription map.
I don't think an IllegalStateException gets thrown when empty lists come into the picture. Consider the following scenario: ``` java // assume polls may be interleaved consumer.subscribe(Arrays.asList("topic")); consumer.subscribe(new ArrayList<String>()); consumer.assign(Arrays.asList(new TopicPartition("topic", 0))); consumer.assign(new ArrayList<TopicPartition>()); consumer.subscribe(Arrays.asList("topic")); ```
Good point. 1. Subscribe(empty) should still trigger a rebalance by sending a join-group request with empty subscription, so that it will then be assigned with no partition, but still live in the group; 2. Unsubscribe() should, in the future, trigger a rebalance by sending a leave-group request and reset its generation. 3. Pause() does not trigger a rebalance, but just stopping the fetching of some assigned partitions. We can leave this in the future ticket.
I think the semantics of subscribe(empty-list) should be similar to pause(all), but leave the consumer still registered as member of the group with coordinator; for unsubscribe() the consumer means "do not talk to coordinator anymore" and moving forward we may add a leave-group request (there is already a ticket I think). As for now let's keep the original approach to check the assignment upon each commit call; thoughts? @hachikuji @onurkaraman
"implements ProcessorSupplier<String, String>" with String, String for typesafe check.
"implements ProcessorSupplier<K, V>" with the generic types.
@rajinisivaram I think @guozhangwang has observed unnecessary empty stub files cluttering the code base in the past, and is suggesting that as a pattern to avoid. Correct me if I'm wrong, but the way this logic is structured, it looks like like very little extra effort to add a default properties file as soon as non-empty defaults are needed (add the file, and switch to `self.prop_file = self.render(...)` Since this is such a minor edit, having an empty stub file in place doesn't really buy much. As for rendering missing templates as empty strings in ducktape - I don't think this is the right approach, since it would hide error conditions and potentially cause confusing behavior. For example, if the user's intention is to use a nonempty template file, but the location is wrong, he or she should receive an error (easy to diagnose) than potentially start up the service with different settings than intended (harder to diagnose).
As mentioned above, to avoid empty dummy files, we can just do something like this for now: ``` self.prop_file = "" self.security_config = SecurityConfig(security_protocol, self.prop_file) self.security_protocol = self.security_config.security_protocol self.prop_file += str(self.security_config) ```
Calling `maybe_start_jmx_tool` after each line that gets read from the producer process doesn't seem quite right. I think we want the behavior to be: - start producer "asynchronously" - start jmx tool asynchronously - wait for producer to finish - process each line of producer output I think it would look something like: ``` cmd = "same_as_before &" # now the cmd is "async" node.account.ssh(cmd) wait_until(producer is alive) self.start_jmx_tool(node) wait_until(producer is finished) for line in node.account.ssh_capture("cat /mnt/producer-performance.log"): # same as the previous for loop ```
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
@lindong28 Now it's a lot clearer what you meant in your original email. You're basically saying that since `RemoteAccount.ssh()` blocks, `RemoteAccount.ssh_output()` blocks until all output is collected and `RemoteAccount.ssh_capture()` drives the whole process despite acting as a generator, there's no way to start a second process between when you start the first and when you start reading its output? I think it makes a lot of sense to provide support for that. More generally, I think the current set of `RemoteAccount.ssh*` methods could use some cleanup and potentially renaming to simplify things. Originally I thought the blocking `RemoteAccount.ssh()` would be the most widely used, but it turns out we've found more and more uses for capturing the output. @granders, a patch just to add Ducktape support for the approach @lindong28 is looking for should be quick and easy. I think it can look a lot like `ssh_capture`, but it'll have to return an iterable object after invoking the ssh command rather than treating the entire method call as a generator using yield. It might even make sense to just return the subprocess object and allow the user to do whatever they want with it, so the example would look something like ``` cmd = "same_as_before" proc = node.account.ssh_async(cmd) self.start_jmx_tool(node) for line in iter(proc.stdout.readline, ''): # same as the previous for loop ``` I think there are cleaner ways to expose this, but given how many `ssh*` variants we're ending up with, we may want to hold off on introducing much more until we sort out how to capture all of them with a minimum of methods.
This test doesn't seem to correspond to the code -- the context object provides one TopicPartition that you can set as the one causing the error, but then this pauses two partitions.
Can you remove `{}` since there is only one line after `if`? Otherwise LGTM
I wonder if the id check is sufficient. If a broker was reprovisioned with a new IP address, for example, we'd probably want to update this collection with the new Node.
Thanks @enothereska and @ijuma , I'm convinced.
Right, with the `LinkedHashMap` we add two additional references per `Map.Entry` instance when compared to a `HashMap`.
Can we just use a LinkedHashMap for nodesEverSeenById so that the keyset iteration ordering is guaranteed? With this we can then remove nodesEverSeen and perhaps rename nodesEverSeenById to nodesEverSeen.
Why not instantiate `Random` once in the constructor? Or use `ThreadLocalRandom`.
You can use `<>` to rely on type inference, eg `new HashMap<>;`,
Thanks for changing this. A couple more minor suggestions: - We can use this `Random` in the assignment of this.nodeIndexOffset (creating a `Random` without a seed involves a `System.nanoTime` and an atomic update of a JVM global variable). - All the fields that are assigned in the constructor should be made `final`.
@enothereska @guozhangwang Sounds good. If we're planning to implement Ewen's proposed Now solution, then the local Time reference will go away anyway.
For all the callers of poll() will pull `time.milliseconds()` to get the current timestamp right before this function call except `Sender.run()` which will use the pulled timestamp to check buffer expiration as well as setting producer requests' create / send time before calling `poll`. So if we remove this timestamp from `KafkaClient` interfaces we will introduce one more system call for each sender iteration. Given this I think this is OK to keep two longs in the parameters.
@enothereska What do you think about my suggestion above? I think it's a little confusing to have NetworkClient.poll() both accept a `now` parameter and update using an internal `Time` reference.
Yeah, this makes sense. You could always update the entry in `nodesEverSeenById`, make `nodesEverSeen` a `List<Integer>` containing node IDs, and only add a node ID to `nodesEverSeen` when `nodesEverSeenById` doesn't contain the ID as a key. Then looking up a random node requires using both, but it's cheap and shouldn't even be happening most of the time.
It seems a little unfortunate to use Time internally while still supporting all the methods which set the timestamp directly. Since the only call where we actually use this reference is poll(), perhaps we could drop the local reference and change the signature of poll() to: ``` java public List<ClientResponse> poll(long timeout, Time time); ``` This is still very-low impact and would solve the issue KAFKA-2615. Of course, the bigger change would be to allow the local reference and to refactor the other methods to avoid sending the timestamp. If we could figure out a way to do that without requiring excessive calls to time.milliseconds(), I think it might be preferable since it would clean up the API and possibly avoid the need to pass Time around everywhere.
This would require a separate KStreamVoidTransformProcessor, but I feel it worth the internal cost for simpler public APIs.
The functionality of process() now is completely covered by transform: users can define a transform function with return type R be "Void" and add a dummy "return null" in the end of the function. And then in KStream we can add public void transform(TransformerSupplier<K, V, Void>) to replace the "process()" call. Having both process() and transform() might be confusing to users, so I would suggest we just remove process() here.
I think we still need this check. There may be a rebalance between the time a fetch response is handled and the time it is returned to the user.
Could we use ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG etc instead of hand-coded strings? It is less error-prone for possible future changes.
This is because we are inheriting a Scala class, and moving forward we should remove this with the Java version `org.apache.kafka.copycat.util.ShutdownableThread` that can be moved into `o.a.k.common.utils`. We can leave it as is for now.
With the type promotion changes I think you'll need to pass in both the original value and the projected value to this method and this assertion will have to change to something like `assertEquals(expectedProjected, projected)`.
You can reduce this code a lot by using `SchemaBuilder.type(readerType)` to handle all these cases instead of using `SchemaBuilder.int8()`, `SchemaBuilder.int16()`, etc.
Expected value here should be a list of longs.
What about promotion? We need to actually perform the type promotion. It isn't valid, for example, to use an `int8` field's `Byte` value directly for a `float32` field.
I think we just want to get rid of the special handling for logical types, but if it stays, the handling here needs to change. Any user defined schema can have a name too.
This is repeated in many of the methods, and I think there are a few other parts of the schema that you want to check are equal. Can we lift this up into `projectRequiredSchema`? I think you also want to check that the name, version, and parameters are equal. Then each of these `projectX` methods only needs to check any pieces that are specific to it, e.g. the key and value schemas.
It's not critical, but you might want to catch any exceptions from this `project` call and wrap them in another `SchemaProjectorException` so you can include info about which field failed projection.
Not sure about the terminology here. Reader and writer don't make sense in this context since nothing is being read or written. Maybe source and target? Also, it's possible the intent might be clearer if `writer` and `record` were next to each other in the argument list since `record` should be in the `writer` format and being converted to `reader` format.
This list is small anyway, so I'm not too worried about the cost, but this seems wasteful. Seems like this promotion check should be a simple lookup in a precomputed table.
If the record is null, this isn't doing any schema compatibility checks besides the check for optional/required check at the top of this method. This is causing the logical type test to fail.
We should emphasize that this partitioner should be stateless as it can be shared across topics / sink nodes.
How about "Custom partitioning is an advanced topic" -> "If not specified the underlying producer's {@link org.apache.kafka.clients.producer.internals.DefaultPartitioner} will be used to determine the partition".
nit: we can keep the send() call without the partitioner argument which will then call this function with null.
I feel this optimization is not necessarily since calling `ByteArraySerializer.serialize()` does not introduce much overhead, but requires other instantiations of this class to override two functions.
Hmm, good point. Maybe just letting `send` throw the TimeoutException is OK.
`KafkaProducer.partitionsFor()` could block up to max-block-time when the metadata of the topic does not exist, so if it returns null it means the producer cannot fetch the metadata within the block time period, which should not be common. Also the same function will be called in `send()` and if it returns null we treat it as an unexpected error.
Typo, should be `Class.forName`
Probably not necessary to report consumer startup time in this test? Also `start_consumer` is a misleading name, since it doesn't actually start the consumer :) Also - on line 88, change `self.consumer.stop_node(node)` -> `self.consumer.stop()`
Could we skip some of the checks below by setting the initial value? ``` boolean needsReadToEnd = configState.offset() < assignment.offset(); ```
always what? I can't even :)
Could we get rid of the backoffOnFailure variable and just use: ``` java if (isLeader() && needsRejoin) ``` It might make the code a little easier to follow if that's really the only case where we want to backoff.
@auradkar Just a **tip** (no need to change, up to you): you can use `Collections.singletonList()` instead of `Arrays.asList` when dealing with only a single element collection. ;-)
an non-empty => a non-empty
Let's name these methods so they sound like actions to be taken by the SinkTask. So instead of `public void backoffMs(long backoffMs)`, I think it should probably be `public void backoff(long backoffMs)`. A bit of a nit, but since this is public (and very permanent) API, I'd like to get the naming right.
I think I get what you mean, but this is incredibly ambiguous and if someone didn't work on a connector before, they may not get it at all. If I understand correctly, what you are saying is: "As a connector developer, you may have certain operations on external systems that may fail and you may want to retry (for example, an update on a database table may fail due to locks). You will need to know how long to wait until retrying. Please use this method to know how long to wait instead of creating your own configuration. This will give some standardization in our system". Did I even get it right? If not, we need better explanation for sure :)
Shouldn't this be `Math.min`? The timeout passed into this poll method includes at least the next commit time. If you use `Math.max`, won't we miss that timeout if the `backoffMs` value is too large? (or vice versa)
Same here -- should probably just be `public void rewind(Set<TopicPartition> partitions)`. Also, is there a reason we need both `offsets()` and `rewinds()`? Seems like they accomplish basically the same thing, but with slightly different semantics. Why not have just one mechanism for requesting offset changes to the framework, and on each iteration, the `WorkerSinkTask` can handle checking if there are offsets that need to be reset, reset them, and then clear out that list? The only benefit I see right now is that by having one of them be stateful, you've moved a bit of the work of maintaining the offsets to the framework, but I'm not sure that small benefit to the task developer is worth having nearly duplicate functionality.
I submitted a PR for KAFKA-2711. I think we do need clientPrincipalName, and I hope this is clearer with the changes in that PR: https://github.com/apache/kafka/pull/390
Second parameter should be `serverConfigs`
`isServer` is not used
We are setting the default in the config already, we should not duplicate it here IMO. If the config is missing, we can probably throw an IllegalArgumentException.
Is this a valid restriction for the broker? Would there be cases where multiple mechanisms may be required? In the original Kafka Security proposal, there was mention of one port supporting multiple SASL mechanisms. I don't know how common that is though. Probably not for this release, but it may be worth thinking about how we would do that in a compatible manner if we decide to do it.
We already have a `serviceName()` method that returns the `serviceName` variable. This method is different as it's getting the values from the config or JAAS file. I think it's useful to use a different name. I used the `get` prefix, but we could use `find` or some other prefix.
we are not using getters and setters. I think this should be named as serviceName
It seems to me that we could have a single `createSaslClient`. Most of the code is the same apart from the mechanism, clientPrincipal and the exception message.
In unusedProperties(), we probably want to use the originals variable instead of the originals() method since we don't want to record those properties as used there.
it treated => it is treated
it treated => it is treated
Thinking about it twice, maybe this requirement is a bit too restrict: for example we may have a low-traffic topic with N partitions and a high-traffic topic with N \* M partitions and we still want to join them as partition1 with partition1-M, etc. Admittedly this will bring other problems when we change number of partitions, but maybe worth considering.
Could we move these two functions to `org.apache.kafka.common.utils.Utils`? And we can then also remove the duplicate sort function in `DefaultPartitionGrouper`.
Just to be clear, I think we can just add INVALID_GROUP_ID to be handled together with the other two, while keeping the unexpected error check.
Although theoretically we should not see any "unexpected error", I think it is a good sanity check moving forward if we changed the code but forget the update the error handling.
nit: move this line to line 28 to be grouped with other o.a.k imports and leave line 29 empty.
Style convention: this should be called `value()`.
We don't need this on client side. Auth to Local is useful on the broker side where we enforce authorization to convert a principal into a local. On client side we are not using this.
Is this still true? I don't see logic any more that sets this based on security_protocol
I feel INFO level is fine as in many cases people will keep alerting on warn / error logs.
Am I misreading this or will this, combined with the `TimeoutException` from `maybeLeaveGroup` cause this to always throw a `TimeoutException`? Some tests seem to be throwing it, so I think this might be the case and not the behavior you want here.
I wonder if this section would make more sense to average users if phrased in terms of flow control? I see pause() as less about prioritizing partitions and more about implementing back pressure. (By the way, would it make more sense to have pause and resume accept a collection instead of varargs? Currently pause/resume are the only remaining methods which take varargs)
Not on this line, but the paragraph below says that the seeked position will be used on every rebalance. In fact, we always reset to the latest committed offset or using the reset strategy on rebalances.
never mind then :)
Is there a way to make this sentence bold? :)
Maybe "storing offsets outside Kafka"? Otherwise its not obvious how "Managing your own offsets" is different from "Manual offset control"...
I would maintain the terminology of "subscribe" to topics and "assigned" partitions, and say: "manually specify the partitions that are assigned to it"
I thought configuring consumer groups is not mandatory? (i.e. "consumer can configure a consumer group" but not "must").
should read: note that offsets _are_ always committed.
Couldn't these just be methods of TestMirrorMakerService and then you could just pass `core_test_action=self.hard_bounce`? Seems more intuitive to me to keep it as a method since it only seems to be used with the one class.
Are we missing the ApiException here? I.e. error returned from servers that are not recoverable.
Right, now I remembered, we had this issue in Kafka Streams Java Doc as well. For consistency how about just add the prefix for each exception? Admittedly for some of them it is not necessary.
also awesome :+1:
I don't have a good place to put this... on line 208, if commitSync was succesfull, we still need to call workerThread.onCommit().
Would be very helpful for users to get some information on which record. At the very least which topic, but perhaps key / value too (at "debug" level perhaps)
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
Why do you need separate `kill_consumer` method and a `stop_node` method? Or maybe just make the naming consistent with your change to `verifiable_producer.py` and call this `kill_node`
Maybe we can also check if the revoked partitions are equal to the current assigned partitions.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
Hopefully we can remove this `sleep` as well
Not sure if it's related, but there was a bug in `SslTransportLayer` that was fixed yesterday that would cause the producer to time out sometimes while waiting for metadata response: https://issues.apache.org/jira/browse/KAFKA-2801
"If no selection operation is currently in progress then the next invocation of one of these methods will return immediately unless the selectNow() method is invoked in the meantime." I read that to mean that that `poll(Long.MAX_VALUE)` will not return immediately since a `selectNow` was invoked in the meantime . Might be worth a unit test to verify that it works as expected.
I'm wondering if we also need to delay the call to `client.wakeup()`? For example, consider the following sequence: 1. disableWakeups() 2. wakeup() 3. poll(0) 4. enableWakeup() 5. poll(Long.MAX_VALUE) The underlying wakeup on Selector would effect the `poll(0)`, but we would suppress the exception. Then there would be nothing to wake us up from the `poll(Long.MAX_VALUE)`.
Since log.error(.. ex) will print the stack trace already, may be we can save re-throwing the exception again. EDIT: if we want to stop the whole process by throwing the exception, we can then save log.error().
I think this is equivalent to `for node in self.nodes`, which might be clearer
Also minor `minOffset` -> `min_offset` `maxOffset` -> `max_offset`
Super minor, but you can also express this as `return {handler.node: handler.current_assignment() for handler in self.event_handlers.values()}`
A little confusing to set `local_temp_dir = "/tmp"` since it in theory should never be set to `/tmp` I think this line is actually not necessary since `local_temp_dir` can be set inside the try block, and be available to the surrounding scope
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
A bit out of topic: since most of the time the restore consumer's position will be very close to the producer appending LOE, to achieve some batching we could set the restore consumer's max bytes to some larger value (by default it is 1K in `ConsumerConfig.FETCH_MIN_BYTES_CONFIG`)
You are right, this is fine. I was confused about the checkpointedOffsets in standbyTasks before.
It seems that we don't need to maintain principalBuilder in SaslChannelBuilder since both SaslServerAuthenticator and SaslClientAuthenticator ignore principalBuilder in configure().
Yeah we should retry, what I meant is that we can still reschedule at (now + interval).
Personally I would prefer not special handling this case with the different schedule time, as this could change the commit interval unexpectedly for users; instead we can just move forward with the regular commit interval, and if the next commit fails again it may be fine still.
Right, as you said since we already did the check at `run()` it is probably OK to just leave this case as is.
Shouldn't need this line, it's handled by the superclass's constructor.
Does it make sense to set `self.kdc` in the constructor? And then having non-None self.kdc would be part of the logic in `has_sasl_kerberos`
This could use a docstring e.g. `"""afsadfasd"""`
Maybe just check that `minikdc` is not None here
We would like to avoid wildcard import in the code base.
I think you will need a space between `kafka_principals` and `self.extra_principals`
Why adding a suffix? Is there any problem if the topic name is equal to the store name? This breaks ktable.
Why not replace the `initialized` check with a null check? In that case, `value` needs to be volatile and `initialized` can be removed.
Good point, I think we should add `this.nano += TimeUnit.NANOSECONDS.convert(autoTickMs, TimeUnit.MILLISECONDS)` in `nanoseconds()` as well.
My reasoning is that the tests that expect the clock to auto-tick would be affected if the code under test changed from `milliseconds` to `nanoseconds`. Am I missing something? And is there a reason to only auto-tick `milliseconds`? `FastClock` is an interesting idea, it seems to depend less on how often `milliseconds` is invoked by the code under test. It seems more realistic too (in a sense, it's like reducing all timeout values by a multiplier).
Shouldn't this also autoTick `nanoseconds`? It seems like it could be confusing if someone changed some code from using `milliseconds` to `nanoseconds` and suddenly the test would start failing.
I think that code got in by mistake. There is a PR by @rajinisivaram for supporting SASL/PLAIN, but it hasn't been merged yet. Support for SASL in system tests was also contributed by @rajinisivaram and maybe it assumed the presence of the yet unmerged PR.
Looks like dead code here
Minor style point: I'd probably put as a method inside ZK, but that's a bit subjective really.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Note that Kafka only supports kerberos as the SASL mechanism.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
looks good, thanks
It might be worth adding a note (similar to the justification you outlined for me) on why we're overriding the default zk timeout.
This is causing a merge conflict, and it is now invalid to use star imports. Should be a minor fix, but there's a sizable list of imports that this needs to be replaced with.
Do we need this and other similar constructors? Throwing exceptions with no message tends to be not as helpful as one would like and it would be nice to not allow it altogether.
Maybe I'm missing something, but this doesn't appear to be used anywhere.
Yeah, the throw is what I was looking for.
We can use JUnit "expect exception" here. For example in SchemaBuilderTest.testInt64BuilderInvalidDefault.
nit: renamet to "ktablePartition"
Should be final.
Do we really need this method? It seems like we could pass the relevant information in the `update` method. Not yet sure which way is better, but I'd like to consider the option.
Maybe worth making it clear that expiry is not enabled.
`expireMs` should probably be a `long` since we use it as if it's never null a few lines below.
@rajinisivaram Thanks for the detailed explanation. Yeah, I was basically wondering if topic expiration was a "good enough" fix for all of these cases. You may have some unnecessary logging until a deleted topic is expired (for example), but it seems like it wouldn't be too bad since the expiration timeout is 5 minutes, which also matches the default metadata refresh interval. Since we're not attempting to fix the problem of log spam while a message for a deleted topic is queued (which seems like the most likely source of excessive metadata error logging to me), do you think the early removal still makes a big difference in practice? If so, then it may be worth keeping.
Longer explanation from before: https://github.com/apache/kafka/pull/645#discussion_r47489229
It seems this test is always using `Cluster.empty()`, which means it never tests state transitions from a topic that is in use to a topic that falls out of use and gets `UNKNOWN_TOPIC_OR_PARTITION` errors.
I'm not sure how significant it is for the timeout to be a multiple of the refresh interval. The scheduling might not ever align anyway since it depends on poll() getting invoked at the right time. I also don't see why a separate mechanism would be needed for a hard-coded value. We're not expecting high granularity, just a way to avoid the cache growing unbounded over time. My concern is that we are technically changing the semantics of `metadata.max.age.ms` for the producer. Before it only controls how long we wait before refreshing metadata; now it also sets an expectation on the frequency of writes to each topic. Admittedly, the change should be transparent to the user, but it feels like an unneeded dependence.
nit: use `<>`.
This confused me for awhile because setting the expiration to `now` seemed odd. Thinking about it more, is it possible there are really only 3 states and we don't need to track any timestamps? I think they would be: - Active and needs a metadata refresh (What is currently called `TOPIC_EXPIRY_NEEDS_UPDATE`) - Never remove from topic set (Currently `TOPIC_EXPIRY_NONE`) - Failed one lookup and is now candidate for removal (current setting timestamp to `now`). And I suppose there is another implicit state -- removed or missing. So I think we could simplify to a simple enum, which might also help explain the state transitions better.
Nitpick: a slightly nicer way to write this is: ``` java for (Iterator<Map.Entry<String, Long>> it = topics.entrySet().iterator(); it.hasNext(); ) { ... } ```
This might be better named `TOPIC_EXPIRY_NEVER` since it indicates it should never expire, not that you have no info about expiration time yet.
3 seems quite a small number. It might be good to use 20 or something if there isn't a good reason not to.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
lagging is actually the correct word here. We are consuming messages that are outstanding because the consumer is lagging behind the latest data in the broker.
Shouldn't the first argument be `KeyValueMapper<K, V, K1> selector`? We only care the aggregation key.
Alternatively, we can change the first arg `KeyValueMapper<K, V, K1> keySelector` and the second arg `KeyValueMapper<K, V, Long> valueSelector`. If we define special value selector classes, `LongValueSelector<K, V>` whose apply method returns `long` (not `Long`), `DoubleValueSelector<K, V>` whose apply method returns `double` (not `Double`) and so on, we can overload the `sum()` method and allow summing over different data types (and avoid object overheads), I think. In this case, SumSupplier is no longer a subclass of AggregatorSupplier.
remove "on a window basis"
remove "on a window basis"
remove "on a window basis"
"top-k records" -> "top-k values"
"top-k records" -> "top-k values"
I think we might want to consider dropping some of these `log.debug`s to `log.trace`. Some of the logs in error conditions make sense at `debug`, but logging every fetch request and response at `debug` might make changing from `info` to `debug` a bit overwhelming.
Not introduced in this patch: "is non" => "as non"
I think you mean `\"FileStreamSinkConnector\"` here. Also, I know it doesn't work for the `FileStream` case, but should we support also dropping the `Sink` or `Source` part? I don't feel like I have a good feel for how frequently we'll have both implemented. For example, while you could implement both source and sink for JDBC, I'd guess that the vast majority of users are looking for the source connector and so that is likely the only JDBC connector class available.
I know there's a backwards compatibility issue here, but any value in making the option just `connector` here? Of course we're always referring to a class, but settings like `FileStreamSink` read more like "this is the connector I want to run" rather than "this is the class of the connector I want to run".
I know we're violating this a few places (due to the initial code import), but I think we want to avoid converting to `*` imports.
Tiny nitpick: `TopicPartition.toString` does what you are doing manually here, so you could simplify it by just saying: ``` java "Batch containing " + recordCount + " record(s) expired due to timeout while requesting metadata from brokers for " + topicPartition ```
You are right, they are actually the same.
Could this be completely fixed by ``` int startIdx = Utils.abs(this.randOffset.nextInt(Integer.MAX_VALUE)) % nodes.size(); ... for (int i = 0; i < nodes.size; i++) { int idx = (startIdx + i) % nodes.size(); } ```
I think the split between the two classes is fine, just wanted to understand better what the division is. Re: varying single parameters, this is currently not possible in ducktape and unfortunately hard to work around, which is kind of annoying since it would be nice for cluster scheduling purposes to know the full set of services when you first instantiate the class but the need to vary parameters per-test requires deferring that process until you invoke the test method.
Was this intentional or just for debugging? Verifiable consumer can generate a huge amount of output, so its pretty risky to save it, especially if we don't have a way to ensure it is at least compressed first. We definitely always need to grab it to do verification, but not necessarily save it by default with the output (which would happen even if the test was successful).
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
I don't think this is a problem yet, but we should start thinking about these `Service` classes as at least semi-public interfaces. I know we rely on them in muckrake (although perhaps not this particular one yet) and I know others are starting to/planning to/want to be able to build tests on top of the pieces included in Kafka. While these definitely aren't the same as our client APIs, I think we should make an effort to provide some degree of compatibility.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
@granthenke Ahh, great point! I guess that is both nice to enforce and also annoying in this particular case.... Seems like we did some significant reformatting under the initial checkstyle addition as well, which unfortunately messes up git-blame, but I guess it's a one-time cost.
@granthenke Heh, I didn't even mean for the actual credit. But sometimes it's useful in tracking down where an issue was first introduced, the reasoning for the way a particular chunk of code is written, etc. It's an inconvenience, not a serious problem, especially for a set of changes this size.
Seems like a lot of the changes in these files are just re-spacing? Not sure if we want to do this, although it is a pain that they are 2-space indented matching the Scala code despite being java files. In particular, there are at least a couple where the imports haven't even changed (which is why I'd expect them to make it into this patch).
Nitpick: I'd call this `deserialize`.
You don't need this block because `ObjectOutputStream.close` closes the stream you pass to it.
If you use this, you can also remove the `byte[] byteArray = null` variable as you can just return from within the try statement.
This and the other deserialize method have a lot of duplicated code. We could have a method that takes an InputStream with the shared logic.
You should use try with resources here too.
You can use `getResourceAsStream` instead of `getResource` which makes some of the steps redundant.
Could we try to avoid this copying if we assume it always wraps an array? I.e. ``` if (data.hasArray) return data.array(); ``` My syntax may not be perfect, and we need to double check the starting and offset in the backed array, but the general idea is as above.
Also not clear why "numSegments - 1" here.
oh, mapping keys... Isn't it confusing? I don't see why we need it.
The changes in these configs looks good, but the options still aren't used _outside_ the config classes. A good way to be sure we've switched everything over to use the configs parsed by this class is to make sure there are no other cases where the config names (i.e. "offset.storage.topic", "config.storage.topic", and "offset.storage.file.filename") do not exist anywhere else in the code anymore.
We probably shouldn't use internal class names in the descriptions here (or the following config docs and in StandaloneConfig). Something like `Kafka topic to store connector offsets in.` is probably good.
nit: "name" => "joinThisName"
This check doesn't seem right.
ah, you are right.
This class, `ProtoUtils`, has `public static short latestVersion(int apiKey)`
Could the check above be removed all together? This check looks like it would cover all cases. Even if there is a gap or jump in versions.
Nit: missing articles & punctuation in this description. "not available in buffer" -> "not available in the buffer". "available currently in buffer else return empty" -> "available currently in the buffer, else return empty"
I think you need to do something like `assert not self.worker_threads[self.idx(node)-1].isAlive()` after this since `join()` always returns `None`.
If this is allowed to shutdown gracefully now, should the subsequent `join()` on the worker thread have a timeout? Otherwise it could hang indefinitely if the worker thread doesn't exit properly. I think some of these unlimited timeouts have carried over from some initial test code I wrote originally, but they really should have timeouts.
No need to check null. We always have to forward oldAgg and newAgg, anyway.
Doing `store.put(key, newAgg)` in `if (value.oldValue != null)` and `if (value.newValue != null)` is inefficient. Consider doing it once.
indicate the client support quota => indicate that the client supports quota
the message format => that message format
Fetch response v2 is actually different from v1 since the message format is different.
Hmm, I'm having trouble reasoning about the shutdown in this case. In the existing cases, we'd have `WorkerSinkTask.stop()`/`WorkerSinkTask.awaitStop()` handle invoking these, which also tries to ensure this thread has exited and calls `SinkTask.stop()`. But in this case, this thread calls `WorkerSinkTask.abort()`, which only calls `SinkTask.stop()`. How would we manage a case where, e.g., work is rebalanced by the DistributedHerder, which would trigger the Worker to stop a task (from the Herder's thread)? Then we'd potentially have `SinkTask.stop()` called multiple times if it hit this `catch` block, right? I think the responsibility of which thread invokes certain methods might be getting confused here. (Admittedly, it is tough to get this right since the responsibilities of different threads is confusing in this code.)
Can we move this check before consumer.poll()? This can detect the rebalance exception before another poll timeout.
That's OK too.
It does seem like we could pass the node id to `RequestSend` without much issue.
I don't think this check will be correct either. For example, what if the `configState` was updated, a rebalance runs and results in the `configState.offset()` and `assignment.offset()` being synced, and then a whole new set of `configState` data is pulled in without a rebalance being performed (one may be requested, but if the rebalance and assignment process doesn't complete, the test here would pass, but the assignment would be out of date). The existing check should already validate that `configState.offset() >= assignment.offset()` anyway, right? Shouldn't we be unable to join a group and get an assignment with the specified offset unless we have the configs up to that offset? Or, I guess we can get one if the assignment has failed, but then we could just check `assignment.error()`? I think if you wanted to get read-after-write without any possibility of error codes and avoid rebalances, we might need to track offsets in a more complicated way (e.g., track what offsets we used when re-joining that generated the assignment vs what is the latest offset we've read up to).
Given that this is now supposed to return a specific subset, you might want to update `check()` to return the list so the tests can validate the output.
What we are getting here is not necessarily the bootstrap nodes that the user has passed as it depends on what the caller passes (the current implementation does that, but it could easily be changed). 3 options I can think of: - Leave the current implementation, but change the name of the field to something like `initialNodes` - Add an additional constructor parameter called `bootstrapNodes` - Leave as is
Passing the bootstrap cluster to Metadata sounds good. That's better than passing an additional constructor parameter called `bootstrapNodes` to `NetworkClient`.
Interesting. In that case it doesn't seem to important in the short term, it's only relevant once the exceptions become user facing.
My reasoning in Connect was that we wanted to be able to isolate things that Connect was aware of and was truly unable to handle from other stuff that could bubble through. What this _should_ mean (assuming Connect handles things properly...) is that you can just catch ConnectException, even in embedded mode, and anything else is pretty much unrecoverable anyway (so catch RuntimeException and log it, but you can't really do anything about it). In Connect this is kind of important because you might get random exceptions from connectors (and the libraries they are using) so I wanted something that would be as predictable and isolated as possible. I haven't thought through the implications for KStreams though. Not sure you necessarily need anything quite as strict, and as long as the expectations about source/sink topics possibly throwing KafkaException is clear, maybe that's fine. Just wanted to make sure there was a consistent policy.
Looks good, thanks.
Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
This is the same code that is in `NetworkClient.correlate`. Maybe we can make that a static public method and reuse it.
This is just a `Map`, so we can't use `getString`. I wanted to change `configure` to take a `Config` type for this reason, but it would break API classes unfortunately.
It seems that we have an existing issue in the handling of case INITIAL. If we can't completely write all bytes of the sasl token, we have to rely on the next call of authenticate() to finish writing the remaining bytes. However, when the write completes, we will go to the initial state and try to send the token again. It seems that we should be transitioning to the INTERMEDIATE state after the write completes. The same issue seems to exist when transitioning from SEND_MECHANISM to RECEIVE_MECHANISM_RESPONSE, if we can't write all bytes in SaslMechanismRequest in one send call.
Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
Could we just use configs.getString and avoid casting? There are a few other places like that.
This should be in the "assigned in `configure`" section of fields.
I think I'd configure this right after creating the callback handler instead of in this method.
We should use interpolation instead of string concat here.
I wonder if more of this code is generic and should be pushed somewhere else.
If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
I was thinking it was odd that this wasn't reusing `flushAndCommitOffsets` since they are basically the same. But I see the exception handling is different, so that makes sense. But then I noticed there's no call to `onCommitCompleted` if there's an exception during the flush, which we do for every other path during commits. _Then_ I realized that this is actually for the special case of closing, and that the callbacks that are invoked by `commitOffsets` in this special case are weird since the seqno will never match and it is probably always getting logged as an "error" at debug level. This isn't critical since it's just at debug level, but should we have `onCommitCompleted` check for the sentinel value and ignore the callback in that case? But also, I think the seqno handling is kind of unnecessary now. I think this is a holdover from possibly 2 separate things. First, we previously handled offset commit differently with different threads. Second, the semantics of offset commit in the new consumer were very unclear at the time this code was original developed (that was back when it wasn't even fully implemented and I was trying to sort out what semantics we wanted, so ended up being somewhat defensive in this code). I believe it is the case now that you cannot have callbacks for offset commit come back out of order (despite the fact that they are async) and that since we guard offset commits with a check on whether we are currently committing, we can only end up with multiple because we explicitly allow commits to expire to allow a newer one to be submitted. But will this ever actually help? Because of the way the protocol works, won't the offset commits just get queued up anyway? Would it make sense to adjust this so we have to wait for the commit to finish regardless, but that we might do something like pause processing if it takes too long? Basically, since we don't do it synchronously, we allow processing to continue since it's nice to not have to stall during the commit, but at some point if we couldn't commit, we shouldn't just cancel that commit, we should wait until it actually completes. (And, of course, we also need better handling if the commits repeatedly fail.)
As now we only perform `commitSync` during closing, we do not need to update the `lastCommittedOffsets` here.
Minor, since now the exception is displayed in the first log message. We can change : to .
We should not return here as the connector needs to retry when put() is invoked. So we should call put() even with empty message batch.
This is using the "try-with-resources statement" so it will take care of the close.
This doesn't seem accurate -- this class no longer provides logging, it seems like maybe this has moved to `InMemoryKeyValueLoggedStore`.
Does it make sense to call `errors.record()` here as well? If so, maybe we can remove the KafkaException case above since it would be doing the same thing.
This might count in the micro-optimization category, but if there are no interceptors (which will probably be a majority of the time), maybe it would be better to store null and skip the interceptor invocation? This is less of a concern for the consumer since records are handled in batches, but the producer interceptor is called on each record pushed.
I wonder if it would not be better to have a `catch (Exception e)` that does both the recording of errors and calls `onAcknowledgement` followed by a series of `isinstanceof` checks (in Scala, this would be a bit nicer as pattern matching for catching exceptions is the same as standard pattern matching, but oh well). Duplicated logic like this inevitably leads to bugs.
Oh, good to know that they've changed the behaviour since 1.6.0 to make this work (i.e. if the last parameter is unused and it's a Throwable, then it's interpreted as a Throwable instead of a parameter).
Actually I think it works: http://www.slf4j.org/faq.html#paramException.
This is a little annoying, but you do that the Throwable is also a parameter. As far as I can see, there is no overload that takes a Throwable _and_ parameters. I assume that's why Anna did it like this. However, I didn't test to see if slf4j does a runtime check on the arguments to look for a Throwable (it would surprise me if it did).
Maybe use log parameters instead? ``` java log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), t); ```
Code convention nitpick: there should be a space before the colon.
I wonder if we should move all of this into a new private method that takes the interceptor callback and intercepted record. The reason is that it's a bit easy to make a mistake and use `record` and `callback` instead of the intercepted ones (with the current approach).
You can remove `: {}` as we are not passing any args anymore, that is, we are calling the second method instead of the first: `public void warn(String format, Object arg);` `public void warn(String msg, Throwable t);`
It may be worth explaining what happens if both a subclass and superclass have a mapping (the subclass mapping is used).
I'd probably just `return error` here (I'm not a fan of `return`, but `break` isn't much better and I assume you wrote it like this to avoid a call to `getSuperclass()` in the very common case where we find the `Errors` on the first attempt). While I am nitpicking: space after `if` is missing.
Tiny nitpick: you can declare error in this line too as it's not used outside the loop anymore
No batch should have been drained.
ANY of the following (not followings) Also, I think we can improve the formatting a bit. How about the following: ``` * A destination node is ready to send data if: * <ol> * <li>There is at least one partition that is not backing off its send * <li><b>and</b> those partitions are not muted (to prevent reordering if * {@value org.apache.kafka.clients.producer.ProducerConfig#MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION} * is set to one)</li> * <li><b>and <i>any</i></b> of the following are true</li> * <ul> * <li>The record set is full</li> * <li>The record set has sat in the accumulator for at least lingerMs milliseconds</li> * <li>The accumulator is out of memory and threads are blocking waiting for data (in this case all partitions * are immediately considered ready).</li> * <li>The accumulator has been closed</li> * </ul> * </ol> ```
Maybe this is a bit pedantic, but it seems to me that this is about ensuring message ordering even when there are failures and retries, not just `sending` in order. One could claim we already send in order now, but that's not enough.
one liner? partitionsInFlight = sendInOrder ? new HashSet : null;
Should we just assertTrue result.readyNodes.size() > 0? Ditto in line 348.
Also discussed offline with Becket, but this test can probably be simplified significantly
Discussed offline with Becket - rework this patch to avoid the null checks elsewhere; i.e., make the accumulator more explicitly aware of the `sendInOrder` requirement
Does it make sense to make a version of this that matches the old contract so all the existing calls don't need to change? Something like: ``` java public ReadyCheckResult ready(Cluster cluster, long nowMs) { return ready(cluster, null, nowMs); } ```
I think the requirement should be stronger, we should check the subject has a KerberosPrincipal to skip the kerberos login. Else we can run into a situation that there is an alternate login context that has nothing to do with Kerberos and we'll fail.
I'd clarify to sth like: > 2) use general data types (here: JSON; but can also be Avro generic bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
Same clarification as above.
What about: > Demonstrates how to perform a join between a KStream and a KTable, i.e. an example of a stateful computation. > In this example, we join a stream of pageviews (aka clickstreams) with a user profile table to compute the number of pageviews per user region. I think, for all these examples, it is less important to summarize what exactly the example is doing ("we join pageview stream with a user table to achieve XYZ") but which techniques we're demonstrating ("how to join kstream with ktable"). Users will rarely look for how to compute pageviews by regions, but they will surely want to see how they can join "their" KStream instances with "their" KTables.
I'd clarify to "same pre-loaded"
"over text files": This is confusing because we're not using text files anywhere. What about the following: > Implements the WordCount program that computes a simple word occurrence histogram from an input text. > Assumes the input text is read from the Kafka topic "streams-lines-of-text", where the values of messages represent lines of text.
Why no lambdas in this example? (We use lambdas in e.g. `PageViewTypedJob`).
Same for the other examples where we sleep before closing.
To be consistent, we should also change the job ID (further above in the file) to say "typed": ``` java props.put(StreamsConfig.JOB_ID_CONFIG, "streams-pageview-typed"); ```
I'd clarify this to: > Demonstrates, using the high-level KStream DSL, how to read data from a source (input) topic and how to write data to a sink (output) topic. > > In this example, we implement a simple "pipe" program that reads from a source topic "streams-file-input" and writes the data as-is (i.e. unmodified) into a sink topic "streams-pipe-output". > > Before running this example you must create the source topic (e.g. via `bin/kafka-topics.sh --create ...`) and write some data to it (e.g. via `bin-kafka-console-producer.sh`). Otherwise you won't see any data arriving in the output topic.
I realize that the "streams-file-input" topic is used for multiple purposes in the upcoming quickstart/demo instructions. In this case, feel free to keep the input topic name as is.
I'd clarify to sth like: > 2) use specific data types (here: JSON pojo; but can also be Avro specific bindings, etc.) for serdes in Kafka Streams. To make it clear that this example does not showcase Avro usage.
I'd rename "streams-file-input" to "streams-lines-of-text".
As an optimization I'd also add as the first check: ``` java if (other == this) { return true; } ```
The `KeyValue` class allows null values for `key` and `value` (at least I didn't see input validations such as throwing IAE in its constructor when either key or value are null). So we must guard against nulls / NPEs here.
Nitpick: We don't need to explicitly check for `other == null` -- `instanceof` (which is required for the casting logic anyways) will return false if its first argument is null.
Rather than prefixing each metric name with the topic, I wonder if we should use a tag for the topic? This is how we handle node metrics in o.a.k.common.network.Selector.
Do we want to always enforce "latest" for AUTO_OFFSET_RESET_CONFIG? I think there are some cases where a user may want to consume from earliest.
We should also assert that the return value is correct.
I would suggest using `try with resources`: https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html
Java doc for lifecycleListener.
Java doc for lifecycleListener
StreamThread.getName() will return with a prefix of "StreamThread" already. EDIT: nm, this is just too minor.
Also, `n >= records.size()`? Seems we could clear `this.records` and use this fast path if they are equal. Would also remove need for checking `iterator.hasNext()` below.
We should not modify the set after assigning it to a final field. We should assign it once in the `static` block. This ensures that other threads will see the value correctly. It may also be a good idea to wrap it in `Collections.unmodifiableSet` to ensure accidental mutation doesn't happen.
Not sure if it's a problem in practice, but I thought I'd mention it.
One issue with this is that older clients are meant to work with newer brokers and if additional internal topics are added, this could get out of sync.
I missed the part about "messages," so yeah, no confusion about the listTopics() API. I think it still may be worth clarifying that messages from internal topics will only be included if those topics are explicitly subscribed to.
One other minor note: for whatever reason, the new consumer uses the term "records" instead of "messages." We flop back and forth between these terms even in this class, but it would be nice to start to settle.
Fair enough. Probably the risk of users accidentally subscribing to the offsets topic offsets any inconvenience for users who are actually trying to do so.
@gwenshap meant that `kafka.common.Topic.InternalTopics` should be removed in favour of the `INTERNAL_TOPICS` defined in this PR.
Yeah, that would be a nicer way to do it.
@vahidhashemian, yes, that's what I mean.
Do we really want anything related to internal topics to be client side? This could change in brokers from version to version and the clients should still work. I understand that for now we have no way to get that information, but we will soon (KAFKA-3306). I imagine removing the client side list would be part of the cleanup once thats available. So whatever exists in the mean time should be private so we don't need a deprecation cycle.
Yes, this should on an internal package (eg `common.internals`).
Immediately should be lower case.
spelling -> recrord -> record
I think it was my suggestion to add it. I was going off the pattern for tagged node metrics in Selector.
I feel we do not need the "topic-" prefix for the tag value as it will be shown as "tag-key"-"tag-value" already.
Either way works for me. Now I'm thinking about why the Selector metrics had the "node-" prefix. Maybe it's only because the node ids were simple integers before.
We should read the metadata inside the while loop since it could change.
This is not introduced in this patch, but we should only set `topicNotReady` to false in the `else` condition, i.e. when there is metadata exist, and its numPartitions == topicMetadata.size().
It's not critical since these expect calls don't depend on strict ordering, but it is nice to order the expectations temporally to make the tests a bit more readable. In these cases, it's nice to at least put the expectation of the per-record commit after the expectation that records are sent.
Smaller test case with custom ConfigDef for this test? This is super dependent on config docs used elsewhere in a way that'll make it a real pain to update as any changes are made to that ConfigDef.
You don't get a warning if you use a deprecated field/method/class from a deprecated field/method/class.
In that case you should get a warning. Looking at what you pasted, it does say: ``` Note: /home/mjsax/workspace_kafka/kafka/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java uses or overrides a deprecated API. ```
Could use the parameter variable here. MAX_BLOCK_MS_CONFIG
Could use the parameter variable here. METADATA_FETCH_TIMEOUT_CONFIG
An alternative might be to let verifiable producer accept just one compression type. Then in your test case, you could create separate instances, each with a different compression type. Seems a little more intuitive that way to me.
Now that `message_validator` methods are being used here in VerifiableProducer, it is odd that with the existing validators the behavior is now: If valid, append original value (possibly parsed into an int) else, append None That actually seems pretty surprising. Previously, if the value was not parseable, an error would be thrown immediately.
The logic for a valid topic name is actually a little more complicated than this. The server side validation exists in `kafka.common.Topic.scala`. We also shouldn't duplicate the logic. So if we move it to the clients package, removing the Scala version and replacing its usages would be good too. Also #898 (KAFKA-3219) may change this logic a little bit too.
My suggestion would be to move the logic from the core module `kafka.common.Topic.scala` to the client module. And then replace all usages of the old Topic class in core with the new version added in clients. `org.apache.kafka.common` is where we want all common client and server classes/logic to live.
This approach seems pretty weird. Are we modifying state in `ConfigDef` during validation? I feel like there are a few different issues with this -- it won't be thread safe, it ties state to the `ConfigDef` that shouldn't really be part of it, and it allows different config validations to get conflated. Why would we even be modifying the config keys in validation? Seems like validation should only generate `ConfigValue` objects.
Naming this the same as the one in `WorkerTest` is causing failures in `WorkerTest` because the search for the connector by reflection finds both classes.
Fails checkstyle, needs to be final
Also, I think in the exception thrown if `configEntries == null` should mention whether the system property is set or not as this would be helpful for people who thought it was set.
The idea is that `Configuration` can be set in other ways (see the PR discussion), so we fail if we can't find the configuration (a few lines below), not if we can't find the system property.
I think we should provide some context on the exception here.
It seems you can move this line after line422.
Do you actually want this at info level? This looks like the one case that will add log messages at the default level. Deletions ideally aren't all that frequent, but on worker start up we'll log all the deletions on INFO level as the config log is consumed to catch up.
I've been thinking about this and I wonder if this is the best way to handle this. It feels a bit error-prone to pass a `RecordMetadata` with almost nothing set. Have we considered passing the topic and nullable partition via the `exception` parameter? It's then easy to explain: if an error occurred, the second parameter is set, otherwise the first parameter is set. This would imply having a custom exception type that would include topic, nullable partition and the original exception.
Got it. However, we probably want to make the api in interceptor consistent with producer Callback. Similar things can happen in the producer callback where we don't have the full metadata, but just the topic/partition. Currently, we just pass a null RecordMetadata. It may be useful to preserve the available metadata there. It's just that changing the exception type in Callback is an incompatible api change.
The issue with passing topic/partition through exception is how the user would know which specific exception class to cast to. Also, when there is an error, the topic in general is available. However, partition may not be. So, you would still have the issue of including partial metadata. Passing any available metadata through RecordMetadata seems more natural to me.
@junrao The user would not have to cast because we would change the type of the second parameter from `Exception` to e.g. `RecordSendException`. This class would have 3 fields: `topic`, `partition` (nullable) and `cause` (original exception). It also means that if we ever need to add additional fields for the error case, it's easy. Wrapping and unwrapping the exception can be a bit annoying, but it's a common pattern in Java (`Future.get` throws an `ExecutionException`, which wraps the original exception for example).
Should have thought of this before, but `CollectionUtils.groupDataByTopic` already does this.
nit: maybe utility methods can be moved to the bottom of the class.
Does this really need to be `Serializable`? Same for the other comparators above.
nit: could this be private? A few more of these below. Since `StickyAssignor` is a public API, we need to be a little extra careful about what we expose. If it is exposed for testing, perhaps we can move it to a utility class in `consumer.internals`.
Wow, that's annoying. No harm making the Serializable I guess, but perhaps we can just add the `serialVersionUID` so that we don't need the suppressions.
It might be better to use a Kafkaesque schema definition.
I realize that these values are the same that are used in the consumer protocol, but perhaps we should just copy the data so there is no unneeded dependence.
Hmm.. why not always clear it? The behavior becomes a bit less predictable if it depends on state which is not part of the current rebalance.
nit: this could be a normal "for each" loop.
Couldn't we could just iterate through the collection and ensure that each list equals the previous one.
Is the goal to have a version number for this schema? If so, perhaps we could just add it to the sticky assignor schema. Something like this: ```java public static final Schema STICKY_ASSIGNOR_USER_DATA_V0 = new Schema( new Field("version", Type.INT16), new Field("previous_assignment", TOPIC_PARTITION_ASSIGNMENT_V0)); ``` That would remove the dependence on the consumer protocol. If we add a version, the next question is what kind of compatibility we are attempting to provide. Forward compatibility is probably the only useful thing to provide in this assignor. When you're doing a rolling upgrade to the newer assignor version, there's no way to ensure that only the newer member becomes a group leader, which means backwards compatibility is not enough. So if we can't have forward compatibility, then perhaps we can just leave the version out. To get it, we would basically need the parser to just ignore version numbers which are higher than it expects.
Seems we're missing the second purpose.
Maybe it's worth adding a code snippet which shows how to use the sticky assignment with the rebalance listener? It's a little different than with "range" and "roundrobin" from memory.
nit: seems you can use `new ArrayList<>`
You can replace this method with `new HashMap<>(source)`
OK. Let's leave it as it is for now. My larger point is that the sticky assignor should be a pure function. It takes inputs (subscriptions) and generates outputs (assignments). There shouldn't be any need to carry state between rebalances. It sounds like that is just a convenience for test purposes. In a follow-up, we should try to improve this.
Might be better to just propagate the exception and fail. We wouldn't expect to actually hit this in practice.
Unless I'm misunderstanding something, it seems like we're giving the full group assignment to every member in the group. I expected instead that each member would only receive its own assignment for the current generation and that we would aggregate the individual assignments on the leader when we received the group subscriptions. If we send all the assignments, then the overall overhead grows quadratically with the number of members in the group.
But you can simulate a previous assignment by setting the previous assignment in the subscription, right? You don't actually need to have executed the previous assignment.
Yes, I think we ought to use a Kafka schema definition even for the user data so that we can avoid dependence on java-specific serializations.
This is also related to the following PR https://github.com/apache/kafka/pull/1015 that uses sentinels in a number of places. It would be nice to be consistent on -1 versus null.
Not sure about this. I'd suggest using NO_TIMESTAMP and similar sentinels for the compatibility case.
Yeah, it seems to me like we should remove it.
Should this be `error.message()` like a few lines above? Same question for other cases where we are still using `error`.
Interesting. Which is more useful host:port or id? If the latter, let's just leave as is. Otherwise, then changing it here and other places sounds good.
I was mostly trying to get rid of the word `Node` because it's a bit redundant when you look at the log messages.
When I suggested it, I thought we could do a bit better, maybe something like `(id: 5, www.example.com:9123)`, but maybe that's actually worse.
Should the second sentence be "Will find new coordinator and retry"? That seems to be what we use everywhere else.
Thanks for identifying this issue. Piggybacking on OP_WRITE may not be the best way to fix this though. I was thinking, if connected is true, we can probably just call socketChannel.finishConnect() and register the socket channel with OP_READ, which is what PlaintextTransportLayer.finishConnect() and SslTransportLayer.finishConnect() will do. Then, we don't need to change the logic in poll.
Yes, that may not be enough since we need to propagate the connected state through Selector.poll(). Could you try the following? In Selector, we maintain Set<String> immediatelyConnected; In connect(), we make the following changes. If connected is true { immediatelyConnected.add(id); key = socketChannel.register(nioSelector, 0); } else { key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT); } In poll(), we make the following changes. 1. if (hasStagedReceives()) || immedidatelyConnected.size() > 0) timeout = 0 This will make sure that select() will return immediately. 2. After the call to addToCompletedReceives(), we will add the following code. for each id in immedidatelyConnected get the KafkaChannel from id kafkaChannel.finishConnect() this.connected.add(id) immedidatelyConnected.clear();
@becketqin : Yes, if it's a bug, it's going to be hard to auto fixing it in the client code. Just propagating the exception to the caller is probably the best that we can do.
@zhuchen1018 : But the issue is that we already processed the response. Next time, when we come back to handleCompletedReceives() again, we will get a response intended for a request different from what's in the head of inFlightRequests.
like above, no need for security protocol here
Since this is simple consumer, I think security is not applicable here? We can probably remove security-related fields here.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
This line is failing checkstyle. I think we need a space after the first semicolon.
Would it be worthwhile to also add an empty collection at the end of the map as well to make sure that case is covered? You could use LinkedHashMap to ensure order.
It would be better to either introduce a method to verify this condition or to change `connectionFailed` to return `false` if there is no `ConnectionState` for this node. The former seems safer.
@becketqin : Yes, you are right. It does seem that checking connectionFail state achieves the same thing. It's just that using muted partitions seems simpler and avoids having to pass in networkClient to recordAccumulator, which is a bit weird.
This check existed before in the original KIP-19 patch but was removed to address https://issues.apache.org/jira/browse/KAFKA-2805. I am not sure if it should be added back.
Is just checking connectionFailed enough? For example, the connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight.
@becketqin : I was thinking that another way to fix this is the following. In NetworkClient, we add a new method nodesWithInflightRequests(). In sender, before calling abortExpiredBatches(), we call networkClient.nodesWithInflightRequests() and pass those nodes with inflight requests to abortExpiredBatches(). Then in abortExpiredBatches(), we can avoid timing out those partitions whose leader node has any inflight request. We probably also have to move the abortExpiredBatches logic to after the call to client.send() to pick up new inflight requests added in this iteration.
Is just checking leaderNotConnected enough? For example, the leader connection may be fine, but a batch can't be sent to leader because the max inflight requests to leader has been reached. In this case, it seems that we can timeout a batch in the accumulator before those that are still in flight. Also, would it be simpler to implement this based on muted partitions? If a partition is muted, we know there is still an outstanding request for the partition and we can disable expiring batches from that partition. This only applies when guaranteeMessageOrder is true, but is probably what we care about anyway.
Not sure why we want to close the batch here since we will close the batch in batch.maybeExpire().
Since there is no action in run_produce_consume_validate, the test will start producer and consumer and then stop them right away. So the test will probably producer a very small number of messages. Maybe we should make sure to at least produce some set number of messages? Take a look at compression_test.py as an example.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
@MayureshGharat, it was removed because the performance impact was unacceptable, the JIRA has more details: https://issues.apache.org/jira/browse/KAFKA-2950
This causes a checkstyle failure
Some `@param` descriptions are missing.
Typically, I would prefix such an implementation detail with, doh, "Implementation detail: ....". This highlights that the information is not part of the API contract, but rather a FYI.
Typo, should be: "an ever-updating" counting table"
Question regarding naming consistency: Also, we perhaps want to rename "aggregate key" to "count key" or sth, given that we renamed e.g. `aggValueSerializer` to `countSerializer`? (No strong opinion on this one. If we assume developers will be used to "aggregate key" because that term is used for all the other functions where one needs to pass in a selector, we should keep the current terminology when talking about `count`.)
Same as above, I'd prefix with "Implementation detail: ..."
Do we need `sleep` here and is it even sufficient to ensure we get the right behavior? I figure something like changing the format, then making sure at least one new message is produced + consumed would be the necessary and sufficient condition for proceeding to the subsequent change.
I think you also need to remove the trailing `{}`. Exceptions get logged without requiring anything in the formatter string if they are the last trailing argument.
Got it. Thanks. The patch LGTM.
Yes, that may work. It just that it would be good to avoid duplicating the same code in two places.
Thinking a bit more. This is a bit tricky since we probably can't just continue here. For channels like SSL, we need to do the handshake after the socket is connected. Currently, the handshake will be triggered immediately after the connection is established through channel.prepare() and this has to be done in the same poll(). Otherwise, the selector may not be able to select the key again for initiating the SSL handshake. This applies to all those immediately connected keys not coming from the selector. So, to get around this issue. We can probably create a new KeyIterator that iterates both keys in this.nioSelector.selectedKeys() and those in connectableChannels. The iterator can return a <key, alreadyConnected> tuple. For keys coming from selectedKeys(), alreadyConnected will be false. For keys from connectableChannels, alreadyConnected will be true. Then, we just need to change line 291 to check if (key.isConnectable()) || alreadyConnected) and leave the rest of the code as it is. This way, keys in connectableChannels will be handled in the same as way as those from selectedKeys().
@llowrey : In the interest of time, @ijuma helped produce a new patch in https://github.com/apache/kafka/pull/1094. It would be great if you can verify if it fixes the problem. We will make sure that you get the acknowledgement when we commit the patch.
@llowrey, it would also be interesting to know the environment where you can reproduce this. I tried writing a test that connects 1 million times to a Java EchoServer and `connect` always returns false.
This can be tricky since finishConnect() can throw an IOException. This is dealt with in the while loop starting at line 293. We can deal with it here again, but we will end up duplicating more and more code. Have you considered iterating both keys from selector and immediately connected keys together? That complicates the iteration a bit, but will avoid code duplication.
Since this is basically just a pass-through method anyway, there isn't that much to test here -- you could simplify this test just to use a simple set of connectors you create yourself. There's a lot of code in this test when all you really want to see is that the set makes it back out of the call to ConnectorPluginsResource
You can just do something like `return new ArrayList<String>(Worker.getConnectorPlugins())`. No need to manually copy with a for loop.
Question: Does `LogAppendTime` really provide processing-time semantics? My understanding is: - `CreateTime` = Kafka producer time = **event-time semantics** - `LogAppendTime` = broker time - System.currentTimeMillis() = WallclockTimestampExtractor = Kafka consumer time = **processing-time semantics**
While we're making this timestamp change, we should also add a docstring to `WallclockTimestampExtractor`, explaining that, if my understanding is correct, it yields processing-time semantics (= time when records are being consumed/processed).
Could also have a cross-reference to `ConsumerRecordTimestampExtractor` that says "If you need event-time semantics, use `ConsumerRecordTimestampExtractor`" or sth like that.
The docstring could also have a cross-reference to `WallclockTimestampExtractor` that says "If you need processing-time semantics, use `WallclockTimestampExtractor`" or sth like that.
Also: missing full stop (`.`) at the end of this sentence.
We really need a docstring here. `ConsumerRecordTimestampExtractor` enables event-time processing, which is a crucial functionality for stream processing. Also, the name `ConsumerRecordTimestampExtractor` (which IMHO we should keep) does not hint at "hey, if you use me, then you'll get event-time processing in return". Idea: > Retrieves built-in timestamps from Kafka messages (introduced in [KIP-32: Add timestamps to Kafka message](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message)), thus providing event-time processing semantics. > > Here, "built-in" refers to the fact that compatible Kafka producer clients automatically and transparently embed such timestamps into messages they sent to Kafka, which can then be retrieved via this timestamp extractor; i.e. these built-in timestamps are different from other timestamps that the user may have included in the _payload_ of the Kafka message. However, I remember that KIP-32 actually defines: > (From KIP-32) > Add the following two configurations to the broker > - message.timestamp.type - This topic level configuration defines the type of timestamp in the messages of a topic. The valid values are _CreateTime_ or _LogAppendTime_. The docstring idea above only covers CreateTime semantics (= producer-time), not LogAppendTime (= broker-time). So we may need to correct the docstring idea.
Its fine. Just checking :)
Also, `consists` should either be preceded by a `that` or it should be changed to `consisting`
Oh, and a question just for my understanding: Initially I would have suggested that `branch` should perhaps be named `partition` but then I realized that `branch` is different from (say) Scala's `partition`. Notably, we ignore/exclude any data records that do not match any of the criteria = no catch-all bucket for `branch`, although this behavior does exist in `partition`. I suppose we don't need any such `partition` method? Or, why did we go with `branch` instead of `partition`? (I understand `branch` to be a combination of `partition.filterNot`.)
As you would expect, Kafka has both forms. It does seem to favour the `Create` form more when looking at `KafkaConsumer` and `KafkaProducer`. It's a bit sad because we are unlikely to ever achieve consistency by choosing something different from the Java style guide (contributors will invariably use the Java recommended style and it's hard to spot all of these during code review).
"of [an] ever-updating ..."
"of [an] ever-updating ..."
"of [a] windowed..."
"of [a] windowed..."
Ah, I see the difference: I suppose "ever-updating" refers to this method returning a _non-windowed_ KTable? If that's right, then we should perhaps clarify this a bit in the docstring (it's easy to miss -- these methods all have similar names).
Typo: "into a new instance of [a] windowed {@link KTable}." Also: Full stop `.` missing at the end.
To be consistent, we should say "into a new instance of [a windowed] {@link KTable}".
"Combine values of this stream [...]": I'd clarify the role of the key(s) with regards to the two streams that are being joined.
Typo: "new instance of [an] ever-updating KTable" Also, we do we say "ever-updating"? I'd remove it. Also, in other parts of the docs we do not mention "ever-updating" when we talk about KTable (see e.g. the `aggregateByKey` variant right above this one).
I'd clarify to: > Process all elements in this stream, one element at a time, by applying [...] This one-at-a-time clarification is important (think: low latency processing).
Sounds good to me (i.e. future PR).
`KStream` => `{@link KStream}`
Typo: "_of_ key-value pairs"
Perhaps clarify to: "by transforming each element in this stream into zero or more elements with the same key but with new values." Also, perhaps we should add an example? > Example: ("alice", "hello world") could be transformed into two new records such as ("alice", "hello"), ("alice", "world"). Note how the key stays the same as in the original record but (1) there are now two output records and (2) their associated values have changed from the original value.
Why the qualification that says "that generates stateful processors"? AFAIU nothing prevents the user from supplying a state-less transformer (supplier), even though yes, you can also provide the names of state stores when calling this method.
Perhaps clarify to: "by transforming the value of each element in this stream into a new value in the new stream"
Sorry, out of KIP scope... never mind.
There are some reason to move from varargs to Collections? It's breaking a lot of projects.
Does it make sense to do this check for all types on read? INT64, INT32, etc
Negative array size is something the protocol wiki doesn't really talk about in detail. The current implementation doesn't allow it but perhaps it should. From a consistency perspective the array specification for bytes and string allow -1 to indicate null. Should arrays in general follow that pattern? This change was made as part of #1095 but may need to be discussed in a KIP unless null was always intended to be -1.
If the goal is preventing generic "BufferUnderflowExceptions" I think its worth adding. Doesn't need to be this patch if you don't want to though.
Yeah, I don't think it's worth doing it for broker properties at the moment.
"consumer group management" => "group management" for generality? "When the session timeout expires..." => "when a consumer's heartbeat is not received within the session timeout, the broker will mark the member as failed and rebalance the group".
Yeah, I think you're right -- the `dev0`, including the number, are so we can release intermediate snapshots to PyPI if needed/helpful such that they stop being a moving target. (In practice, we haven't actually released to PyPI yet; however, we may want to figure out how to include that in the release process moving forward). However, I'll let @granders have final say here since he was the one who figured out how the python versioning should work.
@gwenshap @ijuma Yes the leading part of the version should match version in `gradle.properties`, so it looks like we still want `0.10.0.0`
I am actually asking the question, @ewencp or @granders would know the answer.
The 0.9.0 branch has `version=0.9.0.2-SNAPSHOT` so it makes sense.
I would move all of these apart from the first two to the declaration of the fields themselves.
Makes sense @ewencp, not sure how I missed that sentence when I read it originally.
@ijuma I think this question is addressed below -- pausing partitions in the network thread effectively is backpressure and turns the network thread into a pure heartbeating thread while processing is being performed. You can also, of course, choose to buffer as much or as little as you want by adjusting when you decide to pause the collection. I'd say the current docs explain this well enough, though I think a few code examples (in the somewhat defunct examples jar) would be the most helpful way to show how to make this work in practice.
While you are there, can we fix this one? it is " Running at : " + miniKdc.getHost() + ":" + - miniKdc.getPort()
I don't think we want \* imports...checkstyle should catch this on a clean build.
This closes the task since the second parameter is true -- I don't think we can just skip over this step entirely. I'm also not sure what we expect the semantics for flush() to be if offset commit is not enabled (and this might need clarification in the KIP as well). The flush() call here may not be superfluous if the connector uses that as a signal that it should flush and commit offsets (all in the destination system).
Looks a little odd to use the offset commit interval as the poll timeout when commits have been disabled.
Worth noting the need to specify offsets when starting and after rebalance. Otherwise it'll just use the auto reset strategy.
nit: i'd prefer a new testcase. this doesn't really have anything to do with 'testOldDataHasNoEffect'
In current design (KIP-19), expiration of a batch in the accumulator is to avoid holding the batch forever in the accumulator when the partitions has no leader. If the batch can be drained, that means it can still make progress. In that case, we probably don't want to expire that batch.
This is much better. Thanks!
A docstring for this method would be good :)
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
just noticed this - I think we want `clean_shutdown=False` here to ensure we really kill the process if the normal attempt to gracefully shut it down failed
@hachikuji Yup, you're right -- I missed the fact that the overridden `needRejoin` takes into account the data in `subscriptions` and was only accounting for the base class's `rejoinNeeded` variable. Makes sense now.
Does this always work if you're the leader during a rejoin? The flag gets reset by `JoinGroupResponseHandler` but the leader will still need to complete the sync group phase. These two steps are combined, but using `ConsumerNetworkClient.poll(RequestFuture)` to wait for the entire operation to run can invoke the internal `ConsumerNetworkClient.poll(timeout)` multiple times, which in turn runs delayed tasks. So with bad luck with timing I think you may execute the delayed task between the two requests and this `needsRejoin()` check won't work.
I think if you had the right time.sleep() right before this response you could trigger the issue I raised. But given that the sleep needs to happen in the middle of the `poll()` call, not sure how we'd test it.
Do we still need maybeCommit at line 369 then? EDIT: never mind.
For functions that include the StreamPartitioner instance, better add "... and a customizable {@link StreamPartitioner} to determine the distribution of records to partitions".
I think we only use a wrapped WindowedStreamPartitioner if keySerializer is not null and is instance of WindowedSerializer? Otherwise we just rely on the DefaultPartitioner.
Nit: users with the high-level DSL may not be familiar with "sink", so we should reword it from the Processor API. How about "the function used to determine how records are distributed among partitions of the topic"
Nit: "..and producer's {@link DefaultPartitioner}".
Coordinator changes are _usually_ associated with disconnects, but not necessarily. We have a `coordinatorDead()` function in `AbstractCoordinator` and I was thinking we could make a call to `client.failPendingSends(coordinator)` or something like that at the same time that we set the coordinator to null. I think that would make the behavior the same as the existing code. There may be in-flight sends to the coordinator at the time that we call `coordinatorDead()`, but that is true currently as well, and at least this would prevent any new requests from being sent.
Sounds good. I'm also ok with a more incremental change if it ends up being more complex than I suggested.
Rather than trying to control when and when not to fail unsent requests, I wonder if we should use "request.timeout.ms" to selectively fail only those requests whose timeouts have expired? Then we could remove the `failUnsent` flag everywhere and just call something like `timeoutExpiredRequests()`. In that case, we could remove SendFailedException entirely and use TimeoutException. What do you think? The only problem is that I'm not sure whether this could cause strange behavior on coordinator fail-over. For example, suppose that we try to send a commit just before the coordinator crashes. In that case, the commit could get stuck in the request queue. We may find the new coordinator and the old coordinator may even come back before the commit's timeout has expired. In that case, if the commit eventually is sent to the old coordinator, then we'd suddenly see a NOT_COORDINATOR_FOR_GROUP error, which would cause us to find the coordinator again unnecessarily. Maybe all we need to address edge cases like this is a way to fail any pending requests to the coordinator when we find that it has failed.
@SupermanScott This change looks ok, but this is failing checkstyle: ``` :connect:runtime:checkstyleMain [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:280:1: File contains tab characters (this is the first instance). [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:282:21: '}' should be on the same line. [ant:checkstyle] /Users/ewencp/kafka.git/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java:293:21: '}' should be on the same line. :connect:runtime:checkstyleMain FAILED ``` Looks like simple indentation cleanup. You can easily check that these are fixed by running `./gradlew :connect:runtime:checkstyleMain`.
To simplify this, you could also just do `return assignmentSnapshot != null ? assignmentSnapshot.connectors().size() : 0.0;`
`schemaType` is null means that the value type is not supported by the Connect's Data API. May be we should throw an exception with a message indicating this.
Should we preserve the error message? Otherwise the user can't actually tell why this happened, they'll see the same error message for both stale config and forwarding errors. It's pretty important to expose enough info to tell what's going on because in cluster mode the user making the request may not have easy access to the worker logs.
Do you want `@DefaultValue` or whatever the appropriate annotation is? Same for rest of the `forward` flags.
i.e. defaults on the query params will get rid of this weirdness (although at least it is isolated here)
Also it can be static, as it's thread-safe. Or an alternative option. In terms of flexibility, it's wise to move initialization to configure() method. This way you'll be able to retrieve some jackson-specific options (if necessary) from the "props" Map.
I recommend to add special handling for JsonParseException - just log it instead of rethrowing. If such an exception is not handled properly, consuming may be blocked with any non-json message - just text, for example. I got this while playing with Kafka locally: just one simple "dummy" message from console client brought tons of exceptions to my log.
My approach is shared here: https://github.com/stepio/kafka-json/blob/master/src/main/java/org/stepio/kafka/common/serialization/JsonDeserializer.java
This can be final.
Just to clarify - is `self.prop_file` necessary? One justification to keep it is that it provides a way to override configs. But if the only way to pass in custom producer configs is to set or append to the `prop_file` field after constructing VerifiableProducer, that's a pretty confusing mechanism.
oops, thanks for catching that :)
Also - this method gives the ability to construct different configs for different nodes - so it seems like the logic for setting `self.security_config` doesn't belong here since it is independent of the node, and would have unintuitive behavior if it did become dependent on the node? (e.g. configuration of one node affecting configuration of other nodes)
I see - fyi you can actually retrieve idx from the node: `self.idx(node)` and the node from the idx: `self.get_node(idx)` Since these are more or less interchangeable, I'd lean toward making the method signatures consistent
It seems problematic to pass in `self.prop_file` here `self.kafka.security_config.client_config(self.prop_file)` since `self.prop_file` is now a function and not a string
We do not need to have a separate `KTableForeach`, since it does not generate any new KTable object and hence no need for `view` etc. Instead we can just reuse `KStreamForeach` inside `KTableImpl`.
we can narrow the try/catch block inside the if block.
That is right, since Kafka Streams processor topology traversal is Depth First, when we finished one child route we need to go back to the next child route, and hence resetting the the currNode.
Verifiable consumer necessarily prints something for every record right? I would think that could easily grow larger than the debug log. But in any case it doesn't look like anything is collected by default for VerifiableConsumer, so I'm not sure why any other log file size would be relevant for comparison...
We probably shouldn't change this default to true -- we should override it in the specific test we need by calling `self.mark_for_collect(consumer, 'verifiable_consumer_stdout')` where `consumer` is the `VerifiableConsumer` instance. stdout for verifiable consumer can result in _huge_ log files, so collecting them by default will result in very large archived data for some tests.
I see, I guess `type` in `ConfigDef.convertToString` can be `null` due to this call, but why would we even use `ConfigDef.convertToString` in that case? It seems weird to use something from `ConfigDef` to do conversions when we know there isn't a type or `ConfigKey` associated with the field.
Not a big deal, but since you're using both key and value, you might want to just iterate over `entrySet` instead of `keySet`.
Like I said, not a big deal and I'll commit as is. But by using entrySet, I didn't mean not to use key at all, just that something like: ``` for (Map.Entry<String, String> entry : configKeys.entrySet()) { String configName = entry.getKey(); Type type = entry.getValue(); ``` would avoid doing the extra lookup and would still let you use the key directly as needed.
Here `topicToTaskIds` contains both internal source topics (should not log compacted) and state changelog topics (log compacted), we should treat them differently. Actually we have created the internal source topics before triggering the partition grouper, and here it is triggered again in case the number of partitions has changed from the logic of partition grouper.
`ObjectMapper` instances should be created once and cached, normally.
Style convention is to separate words by underscore.
Yeah, the logic seems right to me.
We want the exception to be thrown in either case right? If requestTimeoutMs is greater than either sessionTimeOutMs or fetchMaxWaitMs an error should be thrown? I think this a difference between the english meaning of "and" and the programatic meaning of "&&".
Do we still need this? It seems unused.
My understanding is that Jun is suggesting that we should set `checkHC` to true for the Java client if message format is 1.
Both `GZipInputStream` and `SnappyInputStream` read the header in the constructor, so it would make sense to me to remain consistent in that respect.
I think the approach used by Snappy is a bit better in that it throws a subclass of IOException so that existing IOException catch blocks can catch the exception while allowing more specific catch blocks to exist.
OK, I see that you are treating these exceptions differently.
Can you please elaborate why we no longer read the header during construction? It seems to me that `checkHC` could be a constructor parameter and then we could keep it as a private and final variable and less changes would be required. But maybe I am missing something. Note that public and mutable variables are generally avoided in Java.
To be consistent with the rest of the code, this.flg can just be flg.
Ditto above: "... to select the new key". And use `KeyValueMapper<K, V, K1>`.
nit: "Create a new key from the current key and value."
With the above change, this would be `new KStreamMap<>(new KeyValueMapper<K, V, KeyValue<K1, V>>) { apply(k, v -> new KeyValue(mapper.apply(k, v), v))}`
As for implementation, we could just call toStream().selectKey(), which will add two processors instead of one, but maybe more illustrative on the topology.
Since a `AbstractTask` contains exactly one `ProcessorStateManager` and one `ProcessorContext`, we can just pass the context from `AbstractTask' to the constructor of ProcessorStateManager and remove this map.
Why are we returning a new instance of `ByteSerializer` everytime we call `serializer`? (I think I had this question before.) Same for the deserializer part.
I think I would say the second sentence as: "This avoids repeatedly sending requests in a tight loop under some failure scenarios." As it's phrased, it could be interpreted as meaning that we would repeatedly fail the same request.
nit: ".. select the grouping key and the value to be aggregated".
nit: {@link KGroupedTable}
If we can defer the creation of the repartition sink node and source node to KGroupedTableImpl, then we do not need the name in `groupBy` as it can just use the name of the resulted aggregated `KTable`.
Since we introduced that during the 0.10 development cycle, yes please change that too. I did a search and it seems to be the only case in the clients jar that uses that convention. Everything else (and there are many examples in the security configs) uses the `DEFAULT_*` approach.
IN `CommonClientConfigs`, it seems like we use `DEFAULT_...` instead of `..._DEFAULT`.
I think this should be used in the define method below too.
Ah I see, makes sense. Somehow the snippet show it is from `onJoinComplete`.
Minor: maybe move this to initialization? ``` java int newTaskCount = configs.size(); ```
Nitpick: this parameter is no longer a map
Yeah, sounds good. And as we discussed offline, maybe we could even go as far as constructing the taskIds inside the ConfigBackingStore: ``` java public void putTaskConfigs(String connector, List<Map<String, String>> configs); ``` This would save the need to do the taskId verification step, though I'm not sure if there was a good reason to keep taskId generation within the Herder implementations.
This method had me worried at first because it doesn't seem to do anything to guarantee there's only one connector's tasks in the map. We might be able to switch the internal representation here to also be a list, although the way we collect them makes it natural to maintain as a map and then convert to the list (especially given a compacted topic). Maybe a good solution is an assertion here that the connector name matches as expected? I think we're sort of covered by the unit tests that test that compacted topics work even if random sets of task configs get cleaned up, but right now I'm not certain we test the possibility that we have two connectors with tasks that have been compacted and are in an inconsistent state.
@Ishiihara Do we still need this check? Based on the usage above and since the method is private, it seems safe to assume that the tasks all match the connector.
If it can't return null, what's the harm in checking? completeTaskIdSet will still NPE if taskIdSet is null, so I think that being sure is very important.
Thanks @omkreddy. I should have expanded the diff to see the docs.
I don't, @guozhangwang may know.
That is right, thanks @omkreddy .
We should remove the emit calls in all the windowing specs for 0.10.0.0 as it is not implemented yet. @mjsax Do you want to do that in this PR, or me / @miguno can do that in a separate PR.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
Nit: extra empty line.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Actually it seems that `StandaloneHerder` is missing the check.
Should we be checking for null here? It's probably OK as this is only called from `abortIncompleteBatches`, but I thought I'd ask.
I don't have a strong preference, it seemed a bit more resilient to changes elsewhere, but it is a private method as you said.
TBH, I do not have any better suggestion as well. Maybe it is OK to have it the way it is.
That would work for me, but I'm OK with leaving as is as well.
NIT: On a separate thread it was mentioned that we do not follow getter convention. Would it make sense to stick to that here as well? Maybe @ijuma will have some suggestion here.
We don't need this for files, right? Just for directories (because of `file.deleteOnExit`)
We should update the Scala `TestUtils` to call this method.
This change is not needed.
Typo -- or is that Italian? :-)
That's a good idea. Note: Kafka does not use this JUnit functionality yet (i.e. no use of ExternalResource, ClassRule, Rule as far as I can tell). @ijuma: Would it ok for us to introduce this? There's no additional dependency etc., it's just using a new JUnit feature that was introduced in 4.7 (we're on 4.12).
This method could be replaced with the use of a Junit Rule and TemporaryFolder, i.e., `@Rule public final TemporaryFolder folder = new TemporaryFolder() ... logDir = folder.newFolder() `
I'd consider making this extend org.junit.rules.ExternalResource - it can then be used as a JUnit ClassRule or Rule. The benefits being that the JUnit framework takes care of startup and shutdown
nit: "which provide underlying producer and consumer clients for this {@code KafkaStream} instance".
Are both host groups really needed? Does the one that contains ':' handle both? We have regex in other places in the project that do similar parsing we may want to keep in sync: - https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L53 - https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/cluster/BrokerEndPoint.scala#L27 - https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/cluster/EndPoint.scala#L29
I see. I think I'd rather have this static field in `TestDigestLoginModule`. Also, it can probably be final since we assign it once only. That way its role is a bit clearer IMO.
Should be more precise. See discussion about `HoppingWindow` and `TumblindWindow` in #1250
I would prefer `advance` instead of `hop`. As we discussed, `slide` should not be used to avoid confusion with sliding windows and `offset` would overload Kafka's usage of this term.
Remove double blank.
Remove double blank.
Remove double blank.
Remove double blank.
This will probably be subjective, but I'm ok with "hop" for now.
You could phrase it differently: "Move the window by the given advance size." or "Advance the window by the given timespan." (or similiar)
I would prefer `advanceBy(long advance)`
This is out of the scope of this PR but may just be OK do to together: since now we have added the toStream(map) function to KTable, we can actually get rid of this TODO.
I personally prefer consistent naming, ie, both `hop` or both `advance`.
`hop` vs `advance` is subjective :) I am fine with `hop`, too; it's called a `HoppingWindow` after all. `advance` is just a term that is quite common in literature so I am somewhat used to it.
Haha, fair enough. Another option would be to actually use the node objects in the set. Anyway, just a nitpick.
@hachikuji Actually I asked for a name which made it clearer that this object contains numerical indices, and not actual node objects... :)
Ah, I was actually thinking that any message (in particular a consumed message?) after a shutdown complete would be a thing to raise an error over. Really just a sanity check, might not actually be of use.
I don't think we need this method since tests can just access the `self.nodes_clean_shutdown` field directly. Also, from line 234 I can see `nodes_clean_shutdown` is actually a list of numeric ids, so `nodes_clean_shutdown` is a somewhat misleading name.
Would it make sense to validate here that idx not in self.nodes_clean_shutdown? (although we might want a set instead of a list). Seems like it should be an error to receive a valid message after receiving a "shutdown_complete" message
Is it really worth having this catch here? I think it's best to just let the exception propagate. Any method under test can throw an unknown exception after all.
I think we can.
Right now I think we should keep the (high) sleep times as is. But in general we may want to lower the sleep times in this PR. I don't know what a "good" value would be (preferably we'd be able to use `TestUtils.waitUntilTrue` eventually) -- using sleep just s*cks -- but I think Jenkins should be running a bit faster than Travis. Perhaps we should give it a try with `5s` rather than `10s`? That said, we perhaps don't want another iteration on this code, given that the 0.10 vote already started.
Hmm. I feel the `final` would be worth capitalizing the var name.
Can we use something like `TestUtils.waitUntilTrue`? This approach will make tests unnecessarily slow.
It isn't about blocking vs non-blocking. It is about how a Kafka broker behaves to a non-authenticated request on SASL ports... we need to test it (since it is the requirement of this JIRA). We hope that SASL behaves exactly the same as PLAINTEXT, but we don't know that it does without a test.
@ijuma is on the way to London now, so I'll jump in for a bit :) What we mean is that the whole point of the test is to show that the broker can reply to an ApiVersionRequest on the SASL port before doing the handshake. Current test doesn't really validate that. @ijuma suggested simply opening a socket (low level java type, the kind we use in SocketServer tests) to the SASL_PLAIN / SASL_SSL port, sending an ApiVersionRequest and checking the result. Does that make sense? We are open to other suggestions on how to validate this patch.
@gwenshap looked into this and showed me why it doesn't work. Could we use a plain socket to send the api versions request to avoid the issue? It's a bit difficult to verify that we are doing the right thing with the current test. For the second question, the current thinking is that we will do that after 0.10.0.0.
Shouldn't we be using `SASL_PLAINTEXT` as this point? Also, it may be worth including a test with `SASL_SSL` as well for this case.
I saw the client doesn't use SASL, and I know that if it would, the test would fail because our current SASL client tries to authenticate before sending ApiVersionRequest. However, the requirements for this patch were to allow clients to send ApiVersionRequest to SASL port before performing SASL authentication. So we need to test them...
Probably a good idea to check if SASL_SSL for the second case and throw an exception otherwise (in case we ever add another SASL_\* protocol).
Alternatively? > @return a {@link KTable} with same key but different value type as this {@link KGroupedTable} that contains aggregated values for each key
Ditto above. I would recommend having consistent explanations here.
Nitpick: Is there a better term than "join-tuples"? Perhaps we should highlight a bit more that that we return tuples of values (but e.g. not of keys).
That is correct. Though for "windowed KTable", I think it is simply a list of tables where each table represents the aggregate results for that window. As for the docstring we are only talking about key value types for windowed operations, while talking about semantics for non-windowed operations, which is inconsistent. More specifically, for non-windowed aggregation, if we say: `returns a {@link KTable} that contains records with unmodified keys and values for type <R> that represent the latest (rolling) aggregate for each key.` Then for non-windowed aggregation, we should be equivalently informative: `returns a windowed {@link KTable} which can be treated as a list of {@code KTable}s where each table contains records with unmodified keys and values for type <R> that represent the latest (rolling) aggregate for each key within that window.`
nit: space after comma.
This is not introduced by this PR, but I think it is more natural as "a new {@link KStream} instance." Also how about "that contains" (or containing, again either is fine and we just pick one throughout the class files) "the same records as this {@link KTable}. The records are no longer treated as updates on the primary-keyed table anymore, but rather as normal key-value pairs in a record stream." Ditto below.
Add one line after "primary-keyed table.": Each record in this stream is an update on the primary-keyed table with the record key as the primary key. Since we use the word "update" in other docstrings in this class we need to make it clear what an "update" is.
Nit: how about "a new {@link KTable} ... as this {@link KTable}"? Ditto for all `through` calls of KTable and KStream
Why the wording is different for this function and the one above? I think no matter if it is windowed or not the return KTable object should have similar semantics, but only different on windowed or not.
Nit: the same key.. ditto below.
Nit: we sometimes use "that contains", and sometimes use "containing". Either should be fine, we can just make it consistent. Also: "joined records computed by the given {@link ValueJoiner}, one for each matched record-pairs with the same key and within the joining window intervals." Similar below.
Suggestion: "and transformed values with type {@code R}".
In practice `T` may still be the same as `V`, for example sum / max / min. So how about "with same key type and aggregated value type {@code T} containing ...". Ditto below.
"The time... or both." This may be confusing for developers as it seems they can set the window size parameter as negative values. I recommend removing this sentence as the follow-up example is sufficient.
When a stream is created from multiple topics, we do not have any ordering semantics across those topics, but within a single topic we still follow the within-partition ordering.
"Only paranoid survives" :) I am not native speakers, but I think no comma before "containing" is fine.
Let's keep it as is then.
Question: What is the contract/the semantics, if any, for the order in which we turn multiple topics into a single stream? IMHO we should be a bit more explicit here, which includes the option to say "We _do not_ guarantee xyz..." /cc @guozhangwang
What about sth like: > @return a windowed {@link KTable} with same key type whose values represent the latest (rolling) count for each key IMHO "latest number of values of a rolling count" is a bit confusing (quite a lot of nesting, and number vs. values vs. count).
Rewording suggestion: "a new {@link KStream} with the transformed key and value types"
Good call, we can actually remove this parameter for code simplicity.
Instead of moving this check internally to the loop, can we add an `else` call after the original if condition that, if `topicToTaskIds` is not empty, log a single warning entry that `Topics {} do not exist but ...` with `topicToTaskIds.keySet().toList()`. This will not swamp multiple warning log entries.
Thanks. Any chance of adding record.NO_CHECKSUM and record.NO_SIZE? We have several places in the code where we call the constructor with -1 and I think this will improve readability.
Not really sure. I feel like this is breaking the contract currently. On the other hand, the behavior its useful for (being able to check exit flags, or do anything else that requires waking up) is already possible given the current behavior...
The implication here is that wakeup won't work if you've fetched data and keep calling poll() when you have max records set small, right? This seems like it could be a problem for anything that takes a long time to process messages since the wakeup may be an indicator that the application needs to shutdown...
Any reason we need this constructor? In the past we haven't added constructors without a message, to help encourage more understandable exceptions.
Nitpick: `assertTrue` can be used instead of `assertEquals`.
This and maybe other fields can be final.
Can you please remove the extra space after `=` from here (while you're changing this file).
Are you going to update the other two to remove unnecessary operations? That's all that's left and then I can merge the PR. Thanks!
I think the message needs to be updated.
requestHeader.apiKey() can just be apiKey.
@junrao We are relying on fall-through for the second case, so it's not straightforward to make that change.
`apiKey` is of type `ApiKeys` while `requestHeader.apiKey()` returns a `short`.
Hmm, I am not quite sure what the new state GSSAPI_OR_HANDSHAKE_REQUEST is for. It's making the same call as the HANDSHAKE_REQUEST and there is no code to change saslState to GSSAPI_OR_HANDSHAKE_REQUEST.
Both should be final.
It's just noise. They can make sense in cases where it's not obvious that an empty implementation is desired, but `configure` is not one of those cases.
This is a nitpick, but perhaps we should not use `at the same time` in `close` or `configure` because it won't actually happen at the same time (it will most likely happen sequentially, but we don't have to specify that).
Maybe an `assert` for unit tests (are `asserts` common in Kafka code base?). As `WrapperSerde` is private, nobody else can use it -- so no need to check for `null` IMHO.
We don't need any of the 3 lines above right? We do it all inside the finally.
Removing last element from waiters may be wrong -- for example, some other conditions may be added to waters before timeout. We probably need to iterate through waiters to remove this condition.
@MayureshGharat in general we may have some requests that timed out. Thus we have to remove the specific object from the waiters queue, right? To do that, we probably need to replace `Deque` with something like `ConcurrentLinkedDeque` for waiters, or use lock to protect waiters.
Good point @zhuchen1018. We should probably update `waitTime` in that case too.
Hmm, maybe even better: ``` java long startWaitNs = time.nanoseconds(); long timeNs; boolean waitingTimeElapsed; try { waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS); } catch (InterruptedException e) { this.waiters.remove(moreMemory); throw e; } finally { long endWaitNs = time.nanoseconds(); timeNs = Math.max(0L, endWaitNs - startWaitNs); this.waitTime.record(timeNs, time.milliseconds()); } ```
I was talking about the delayed interrupt time. Ideally, the shorter it is, the faster the test terminates. So I thought we could use 1s instead of 2s.
It's a bit better if you move this inside the `try` and remove the `return` from the catch.
You can pass `numPartitions` to the `ArrayList` constructor so that the internal array is allocated with the right size.
Is there a reason why we are using `no smaller` instead of `greater than or equal to`? I generally prefer to avoid logical negations in messages.
I think this is going to end up throwing an exception if it executes on a worker that isn't running the connector. `reconfigureConnectorTasksWithRetry` needs to only be executed on the worker actually running the connector since the first check it does to check if the connector is running throws an exception if the connector doesn't exist. I think you want to call `worker.setTargetState` without the check, but still maintain the check around this call. The current tests miss this for two reasons. First, they exercise pausing but not resuming. Second, the result is not a crash, but log noise and rescheduling the next attempt to reconfigure the connector.
I don't think we want this in the base class -- its risky as it leads to accidental sharing as we had before. Instantiating it once statically in `SourceConnectorConfig` and `SinkConnectorConfig` seems fine, but keeping it here seems like we're inviting people to accidentally define additional parameters on it. It looks like we only use this in one place (in this file) so probably not a big deal to remove it.
Minor: would it make sense to make this use varargs: ``` java public static ByteBuffer partitionRecordBuffer(long offset, CompressionType compressionType, Record... records) ```
Minor: it seems like this test would be a little simpler if you start subscribed to topic1 and then switch to topic2.
Seems like this could be made `final`.
nitpick: it's a little weird to use `Boolean.TRUE` instead of just `true`.
It's not really clear to me why we need a test case with two consumers.
I'm not sure this makes sense. The offsets for each group are isolated, so `consumer2` would actually start from position 0. I think a better test case would be the following: 1. Start a single consumer with autocommit disabled. 2. Read 5 records. 3. Call unsubscribe(). 4. Verify that no offset commit request was sent. To be honest, this might be overkill, but I wouldn't complain if it was present.
Much appreciated. I probably should have done this when I created the first of these test cases, but thanks for helping to clean things up.
Similarly, everything up to the fetch (i.e. coordinator lookup, join group, and sync group) are pretty much the same in all of these methods. Maybe we turn it into a function (e.g. `prepareRebalance`).
Should have caught this before, but we should make sure the callback gets invoked as well.
There is a `StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG`, we can just use that.
I wonder what will happen under parallel unit test environment like Jenkins, for `KafkaProducerTest`, `KafkaConsumerTest` and `KafkaStreamsTest`, won't the global counts be messed since AFAIK they are executed under a single JVM? cc @ijuma
Is it safe to call this multiple times? If not, we would not want to do it if `state == STOPPED`.
@guozhangwang Tests are executed sequentially in each JVM. Parallel execution is achieved by forking multiple JVMs.
Thanks @ijuma , obviously what I learned about gradle unit tests are totally wrong.
@davispw I think this was just done defensively since the ExecutorService means some state will be mutated in another thread (you'll notice the Kafka*BackingStore implementations aren't synchronized). Since we're only touching the `executor` field here and it should be thread safe anyway, I think we can drop the synchronization.
30s maybe? Timeouts like these should be fairly liberal, we really just want to bound how long we're willing to wait in the worst case, but usually it shouldn't block for any significant amount of time. (All "user" callbacks for this class are internal to Connect, and they don't do anything that should take a long time).
It is probably good to always have ms in the name variable name that represents time in ms. Probably not a big deal here since this is local variable.
`clientId` is not used
Instead of `new MetricName("batch-size-avg", "producer-metrics", "", tags)`, I think we should use `metrics.metricName("batch-size-avg", "producer-metrics")`, right? The doc says `Please create MetricName by method {@link org.apache.kafka.common.metrics.Metrics#metricName(String, String, String, Map)}`. Same for other MetricName instantiation.
`start` is not used.
I am thinking that, without any idea of how the ratio is affected by this config, users will choose either 0 or MAX_INT. Not a big deal for me if you don't bother to get it.
Would it be better to provide default value, probably 1, for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Would it be better to provide default value for this configuration? Otherwise this patch will break existing tests/tools that use `ProducerPerformance`.
Hmm, so actually I think the problem here is that we need to be able to parse generally because these can technically be passed in as `Map<String, ?>`, so the type is unknown. Another option is to replace this parsing with `ConfigDef`, which will handle both the `String` and `Integer` cases since its designed to handle both properties files and `Maps` passed in directly from code. To be honest, I think the only reason we didn't do this before is because this code came with the initial patch that was prototyped outside of Kafka and couldn't use internal APIs.
Uggh, yeah, I forgot about this. We kind of inherit some annoying types from Kafka's config setup but I tried to ensure we're using String types where possible. It gets a bit hard to figure out what is valid where -- the JsonConverter actually gets passed a `Map<String, Object>` as that's what is returned by `AbstractConfig.originalsWithPrefix`, but in practice they are all `String` so type erasure allows this to work...
Why are these removed? If this still passes, I'm surprised -- these are required configs in DistributedConfig.
This works fine, though if you wanted to simplify, the key thing here is just that you are using a `Map<String, String>`, so you could get rid of the resource file and just create it on the fly: ``` Collections.singletonMap("schemas.cache.size", "1"); ```
We should probably mention that due to consumer design restriction, currently we only allow one stream throughout the topology to be created from regex patterns.
We do not need to mention "partition" here since it is supposed to be abstracted from users, ditto below.
nit: forward. EDIT: I realized it may be inherited from the overloaded function, we could fix both.
nit: "Only one source node with regex subscription pattern can be defined in the topology, and there is an existing pattern: {}". EDIT: actually, could we union multiple patterns with `[pattern1] [pattern2] ..`? https://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html Note that if we do this, we need to deserialize data differently based on which sub-patterns its topic name matches to.
I think you want to null out these fields after you call `dispose` because using them after this will lead to undefined behaviour (a NPE is preferable in case of misuse).
Right, it's indeed an option which is much safer. Still I think we better throw here by following reasons: - It sounds semantically correct more. Callback and Future should be used for providing result of asynchronous(background) processing. However, these two TimeoutException occurrs while a KafkaProducer is still doing synchronous(foreground) processing and that(kafka producer has to do some foreground processing before it appends record to the accumulator) is the reason why a caller of `producer#send` is forced to wait until the result turns out, so the caller should receive the result of that call in a synchronous way. - Assuming this is going to be shipped with 0.10.2.0, making a breaking change on behavior isn't preferred but allowed if we leave a correct note on "breaking changes" section. - We can expect this breaking change is relatively less-harm with expect to most users who uses producer in sensitive situation would already using it like below. Users may see some new error logs but it still doesn't break the whole processing(maybe I'm biased, feel free to leave objection if any :D) ```java try { producer.send(record, (metadata, exception) -> { if (exception != null) { // logging } }); } catch (RuntimeException/* or maybe KafkaException, TimeoutException whatever */ e) { // logging } ```
You are right. Never mind.
Actually, at line 387, the batch may or may not already been closed, and we should only call `close()` only when it is not closed yet.
`compressor.close()` will re-fill the wrapper message metadata by re-computing crc, etc; it does not result in correctness issues, but may incur unnecessary costs, since they are not just "no-op" when called again.
Nitpick: in Kafka, we generally set the field where it's first used unless it's an if/else where the field could be set in either.
A `raw type` is being used here, that should generally be avoided.
Why don't we extract this loop into a separate method that takes an interface like: ``` scala interface WaitPredicate { boolean test(); } ``` Then we can reuse the logic from the different variants.
If we have a common default, it may make sense to have an overloaded method without `waitTime`.
`minExpectedRecords` or something like that may be clearer.
nit: full-stop after the description.
Will do this, ideally before feature freeze, but definitely before code freeze. Stay tuned.
We shouldn't really be using the `ordinal` in this way IMO. We should expose a method in the `RecordLevel` class to return a boolean in this case. For example, the invocation could look like `recordLevel.shouldRecord(config.recordLevel)`. Also, using the `ordinal` internally can work, but it's a bit opaque (i.e. if people change the order of definition, they break the code). One often adds a new parameter to the enum instance to make it clearer.
This should disappear with my suggestion.
Instead of doing this, we can simply override `toString` in each enum. However, if we go with my suggestion of renaming the enum values, the `toString` will be the right one by default, I think.
Thanks for the updates. Looking better. :) One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally. Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums. Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it).
Is this intentional? Ditto below.
Done now, thanks @ijuma
INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful.
Could this result in NPE? May be we can directly throw an IllegalArgumentException here.
`shouldRecord`? `maybeRecord` sounds like it would record it for you.
We can use the `replace` overload that takes a `char`.
We can use the `replace` overload that takes a `char`.
A couple of things (applies to the same code a few lines below): - Is there a reason you changed `Map<String, String>` to `HashMap<String, String>`? - We can pass `1` to the `HashMap` constructor to avoid allocating a 16 entries array when we `put` `topic`.
I probably would have these 12 lines in a `consumerProps` method and also reuse it from `getConsumerConfigs`. But if you think it's better this way, then that's OK.
Is ignoring the right answer? Would it be better to fail fast? If not, at least a warning should be logged.
Nit: maybe we should call this `PRODUCER_DEFAULT_OVERRIDES` and the other one `CONSUMER_DEFAULT_OVERRIDES`.
Can you please explain why it's better to warn than to fail-fast in this case? Just want to make sure I understand the reasoning for the choice.
For cases such as this, I think it's good practice to have a temp variable where the map is populated and then wrap it in a Collections.unmodifiableMap while assigning it to the static field. That way we're sure the map is not modified accidentally.
In fact, even the producer code is pretty similar, so you could have a methods like the following which is called by both `getBaseConsumerConfigs` and `getProducerConfigs`: ``` java private Map<String, Object> clientProps(Set<String> configNames, Map<String, Object> originals, Map<String, Object> overrides) { Map<String, Object> props = filter(configNames, originals); // enforce streams overrides if not specified by user for (String keyName : overrides.keySet()) { if (!props.containsKey(keyName)) props.put(keyName, overrides.get(keyName)); } } ```
I was thinking that this should be `Collections.unmodifiableMap(tempProducerDefaultOverrides)` to ensure that it is never mutated after it's assigned.
Nit: rename to "repartitionForJoin"? Since we have the other repartitioning before aggregation in another class.
Minor typo "will is"
Nit: rename to `doStreamTableLeftJoin` to differentiate with stream-stream join.
Also, instead of adding an extra operator node, I'd suggest we just do the checking within the operators themselves to reduce virtual function call overheads, for example see `KStreamKTableLeftJoinProcessor.process`.
As mentioned above, we should defer all the checks of null keys to "repartition (before join or aggregation)", and the "join / aggregate" operators themselves. I think we are already doing this for most cases, but just double checking.
Since `KStreamAggregate` and `KStreamReduce` does not expect key to be null, for example: ``` // the keys should never be null if (key == null) throw new StreamsException("Record key for KStream aggregate operator with state " + storeName + " should not be null."); ``` We should filter out null keys after applying the selector.
Hey @dguy , actually after thinking about it again, I realized that the `selectKey` can be used before either aggregates, or joins, but it could also before any other operators. So enforce removing nulls at this stage is not safe (similarly for `map`, which could also change the key to null). Instead, we should filter nulls in 1) `repartitionIfRequired`, as if the key is null, it is meaningless for repartitioning since it can go to any partitions anyways, and 2) in `KStreamAggregate / Reduce / Joins`, that if the received record key is null, ignore them (instead of throwing exceptions), since repartitioning may not necessarily happen before the aggregation or joins. Thoughts? Sorry for the back-and-forth opinions btw.
I see. Could we do something like this: first assign the partitions for all internal topics as the writing topology's number of tasks, i.e.: ``` // for all internal source topics, // first set the number of partitions to the maximum of the depending sub-topologies source topics for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) { Set<String> internalTopics = entry.getValue().interSourceTopics; for (String internalTopic : internalTopics) { Set<TaskId> tasks = internalSourceTopicToTaskIds.get(internalTopic); if (tasks == null) { int numPartitions = -1; for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> other : topicGroups.entrySet()) { Set<String> otherSinkTopics = other.getValue().sinkTopics; if (otherSinkTopics.contains(internalTopic)) { for (String topic : other.getValue().sourceTopics) { List<PartitionInfo> infos = metadata.partitionsForTopic(topic); if (infos != null && infos.size() > numPartitions) numPartitions = infos.size(); } } } internalSourceTopicToTaskIds.put(internalTopic, Collections.singleton(new TaskId(entry.getKey(), numPartitions))); } } } ``` And then update the Cluster metadata with `Cluster.withPartitions`, then in the `ensureCopartitioning` call, if after the first for-loop, `numPartitions` is still -1, it means all topics in this co-partition group are internal topics, and then in this case read from `metadata.partitionsForTopic` and took the maximum among all of them; and then later after calling `prepareTopic`, the metadata will be updated again with `metadata.withPartitions()`.
nit: Since we are reusing this function for aggregations as well now, better rename to `createReparitionedSource`.
As I mentioned before, we can do a filtering of null keys here as well since if the key is null, it is going to random partitions and hence meaningless for joins anyways; for example we can add a private `addFilteringSink` in `TopologyBuilder` that filters the null keys. Hence the later filtering on the join operators are only for cases where repartition did not happen before the join.
Not sure this really clarifies the process. The sentence "The assignment of topic partitions will be assigned to consumers that subscribed to the topics in a round robin fashion." doesn't add any detail. I'd say something like "When subscriptions differ across consumer instances, the assignment process still considers each consumer instance in round robin fashion but skips over an instance if it is not subscribed to the topic. Unlike the case when subscriptions are identical, this can result in imbalanced assignments." and then include the example you provided here.
Same for all such calls.
We can use a separate ticket to centralize these helper functions into `org.apache.kafka.test.TestUtils`, would you mind filing a JIRA? Example functions like `getProducer/Consumer/StreamsConfig`, etc.
Thanks. I prefer to avoid using `Atomic*` classes when we don't need atomicity guarantees. I introduced a `LongRef` class in the broker for this reason. I think the right solution long-term is probably to introduce a class to complement `Time` that caches the last returned value so that one can choose when to refresh the value (for performance reasons). This was suggested some time ago in a different PR. For this PR, as per your suggestion, I think using a private static class with two public fields is probably the way forward (since Java doesn't have tuples).
We should probably avoid string concatenation if `prefix.isEmpty` (since it's the common case).
@rajinisivaram Sorry should have caught this before -- instead of `time.sleep` here, using `wait_until` on the checks for errors below and then moving the assertion for `self.producer.num_acked == 0` to below those `wait_until`'s might make the test more robust. We'll still have a timeout on the `wait_until` calls, but it can be a lot more conservative and the test may be able to finish a lot faster.
Do we need 3 Kafka nodes for this test? Seems like it doesn't rely on multiple Kafka nodes.
Yes, this seems fine then.
I think it's because of: ``` * <p> On some systems, closing a channel releases all locks held by the Java * virtual machine on the underlying file regardless of whether the locks were * acquired via that channel or via another channel open on the same file. It * is strongly recommended that, within a program, a unique channel be used to * acquire all locks on any given file. ```
Sorry for so many messages, but I think we are leaking a file descriptor here so the right fix may be more involved.
It seems like this method doesn't return an exact `size` for the RocksDb implementation so we need to spec it differently if we want to include it. We should also consider the right name and whether we actually want to expose it. Let's see what @guozhangwang thinks.
Should we check `newPair != null` before checking the key and value for nulls? A user might want to have their `mapper` return `null` in the case they want the value to be skipped.
I think `will go` should simply be `go`.
This is an improvement, but seems a little verbose. How about this? ``` // execute delayed tasks (e.g. autocommits and heartbeats) prior to fetching records. // It is crucial for "at least once" delivery semantics to ensure that offset commits can // only be sent prior to updating the position in ``fetchedRecords``. ```
Generally speaking, it is not guaranteed that the data passed into the serde classes will never be null, and hence when initiating the interface developers should keep in mind to handle that case. As for Kafka Streams specifically, there are some cases that it should actually guard against null values as they are meaningless, which I think is KAFKA-3836 created for.
It would be nice to be consistent and use the thread in both cases. Something like the following, maybe? ``` java Thread thread = Thread.currentThread(); if (thread.getId() != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, thread.getId())) throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access. Request accessing thread is " + thread + " and it is already being accessed by " + currentThread.get()); ```
You're right, I got confused by the misleading `currentThread` name. Hmm, seems like it's not easy to include the thread name for the thread that is currently holding the lock.
@bharatviswa504 @ijuma Couldn't we just change the thread ID that we store to an `AtomicReference<Thread>` to store the entire `Thread` object instead of the ID and make `NO_CURRENT_THREAD` `null`? You would just need to be careful to `currentThread.get()` once and once you verify it is non-null you can `getName()` safely. In fact, this is best effort concurrency detection, so it's even ok to call `currentThread.get()` twice as long as we're careful about null values in all cases.
Originally we were just thinking about notifying the user, not necessarily giving them additional help to track it down (ideally you don't need this as you have a clear threading model and consumer ownership), but obviously that's not always the case. If we can get the name included too, that'd be ideal, so I'm open to changes as long as we're convinced it otherwise maintains the same semantics.
Basic patch looks fine, but this code formatting won't pass checkstyle. After cleaning up you can verify it passes with `./gradlew clients:checkstyleMain`.
@ewencp Yeah, we can do that and I was debating whether I should suggest it. I wasn't sure if we wanted to make a change that could impact the common path so that the error message could include the thread name for the `currentThread`. You reviewed the original commit that introduced `acquire` and `release`, so you are in a better position to judge. :)
Nitpick: if you pass the deserializers via the constructor, it's a bit more concise. This applies to all tests.
We don't usually assert on exception messages as it makes tests a bit brittle. This applies to a few other tests too.
Nitpick: if you move the fail inside the `try`, then you no longer need the `return` in the catch. This applies to a few other tests too.
The usage of `()` to delineate values is a bit weird. Maybe we can replace it with `'` or something like that.
Is it actually useful for `metadata.awaitUpdate` to throw an exception? Maybe it should simply return a `boolean`.
It's a public method, true, but `Metadata` is not considered public API (it's annoying that we don't make this crystal clear via package names). Still, you're probably right that it's not worth potentially breaking people for this small improvement.
and IMO `if(!isStopping()){throw e;}` can cancel this noise.
@hachikuji Why don't we catch the exception here? If the task is being stopped, a wakeupexption is kind of noise in log especially for the application which needs to monitor error log.
You need to be careful about ordering here and how you check this. The old code first validates the IDs aren't the same and then tries to set the value (because the only way the IDs shouldn't match is if no thread is currently in a Consumer method call). This new code tries to set it first, and if it fails, it assumes that it is still set when comparing the IDs. However, if the initial call fails because another thread is accessing the Consumer, but then it finishes and calls release(), then calling `currentThread.get().getID()` will fail because it will have been reset back to `null`/`NO_CURRENT_THREAD` and you'll get a `NullPointerException`. Same goes for the subsequent call to `currentThread.get().getName()` in the error message. I think you want to call `currentThread.get()` _once_, and hold onto that value. No matter what happens, if at some point during this method call the value was a different thread, then we should trigger the `ConcurrentModificationException`. The problem is that you can't be guaranteed you'll perfectly capture the thread that was in conflict because `compareAndSet` doesn't let you know what the value was if it wasn't the expected value. So I think the error message creation just needs to be careful about this -- it's possible we see a conflict, but we cannot actually get the `Thread` object that caused the conflict (we couldn't do this with IDs either -- calling `currentThread.get()` when creating the error message could, by that point in time, return -1). I think the JDK8 version of AtomicReference may have methods that let you accomplish this, but atm we're stuck with JDK7.
`threadId` is no longer used.
I would probably name this `reconnect.backoff.max.ms` to keep things consistent and to make the units explicit.
Should probably update the variable name to have `MS` as well.
For the streams-broker communications I think it would make sense, yes. We could open a follow up JIRA, probably doesn't need to hold back this KIP.
Nit: change the parameter name for `repartitionForJoin` and `createReparitionedSource` as `topicNamePrefix` since repartitioning is not necessarily followed by materialization.
Since the `name` here is the parent name, it may not reveal information for people looking at the store names, for example if you have `stream1.filter().join(stream2)` then the store names would be `KSTREAM-FILTER-xxx-this` and `KSTREAM-FILTER-xxx-other`, which are quite misleading right? Instead, we could use for example `thisWindowStreamName + "-store"` and `otherWindowedStream + "-store"`.
For KStream-KTable join, the passed-in storeName of the table could be null, for example: `stream.join(table1.join(table2))` in which case the returned table from the table-table join has storeName `null` as it is not materialized anywhere.
Any reason why you don't just add these conditions to the if statements above? i.e., `if (before <= 0 )`
Should we explain the magic number `1` here? Why `1`? Can it be any other positive number? One way to clarify would be to use a "label" variable like `int anyPositiveSize = 1;`, then call `JoinWindows.of("join-0", anyPositiveSize)`.
Ok - yes makes sense. Thanks for the clarification
ok - i still think `timerStartedMs` is a better name. i can't think of anything else
Why call it `now` and not `then` (or `previous`); `time.milliseconds()` give the current timestmap, ie, "now"
We can't convert the value returned by `nanoTime` and expect it to have the same semantics as `currentTimeMillis`. The specification says: ``` java This method can only be used to measure elapsed time and is * not related to any other notion of system or wall-clock time. * The value returned represents nanoseconds since some fixed but * arbitrary <i>origin</i> time (perhaps in the future, so values * may be negative) ```
I think this method would be clearer like so: ``` private long computeLatency(long now) { return Math.max(time.milliseconds() - now, 0); } ```
I'm not sure why this needs to be a field. It looks like it is used to measure latency, but then why not just use a local variable around the code that is being timed?, i.e, ``` final long start = time.milliseconds() // do some computations computeLatency(start) ``` Also, the name doesn't seem correct as it isn't really the current time. It is more like `timerStartedMs`
Actually, there are a few other tests that use this too. I'll file a JIRA to clean it up -- it's not reliable enough anyway (and maybe some of the tests can just drop the use of the thread and mock the necessary bits instead if that doesn't make things even messier).
Can we just drop the `ThreadedTest` now? We no longer have a `ShutdownableThread` which is what `ThreadedTest` relies on.
What is the reasoning for not throwing an exception if the condition is not met after the timeout? That would make the tests more concise.
Maybe mention the advantage of using this over `Thread.sleep`.
Is this a good default timeout? The default timeout in `TestUtils.waitUntilTrue` is 5 seconds.
Consider adding some small back-off time (say 100 millis) to avoid busy looping and possible log polluting.
Could we just overload `producerConfig` function that takes three parameters, for bootstrapServers and key-value serdes like we did for `consumerConfig` function? Any specific reasons that we want to have a direct constructor for string and integer serdes? Also nit: use `/** .. */` for doc strings.
@shikhar This is needed because these tests reuse a common `expectPollInitialAssignment` which _only_ handles getting the member into the group and returns zero records. The subsequent expected poll call will actually retrieve the message and cause the records to be put into the sink task.
I think `kafkaOffset` was incorrectly changed to `Long`. We'll always have a Kafka offset, so it should be `long`. Also, the current version breaks compatibility since the old signature constructor is no longer available.
Don't we need to be careful about the timestamp? I think since `ConsumerRecord` returns a `long` for this, couldn't it be the sentinel value `-1` if there is no timestamp? I think we need to check for `NO_TIMESTAMP` or if `timestampType()` is `NO_TIMESTAMP_TYPE`.
Just my humble opinion, but I prefer the constructors over the builder since it makes it easy to see which fields are needed and it's generally less verbose. But there are some situations where a Builder may be more helpful, so I think it makes sense to offer that as an alternative.
It would probably be helpful to include the old constructor. At least I can imagine users having test cases where they construct SinkRecords directly and it would be nice not to break those unnecessarily.
I think your IDE is adding extra braces. (I know this is a bit annoying between Apache Kafka & Confluent projects since they use different styling. iirc default IntelliJ styling works for Kafka if you are using IntelliJ)
I think I mentioned this offline, but I'm not sure I would deprecate this yet. I think there are a number of tradeoffs here, this is the set that come to mind immediately: Pros: - One builder class, although it has a bunch of per-field methods, will beat the combinatoric explosion of possible constructors when the number of fields increases (as it is now). - Less error prone for users as they don't have to try to dig out the right constructor to use that has just the right set of fields. If there are any fields with the same type (schema and key/value fields are the ones that come to mind), users may accidentally pass the wrong variable as that argument and not be caught by the compiler. - Easier compatibility story -- as long as the setter for each field continues to exist in the `Builder`, we're good. Cons: - Deprecating relatively new APIs. May be more painful for us to maintain them, but we probably shouldn't be overly aggressive since we want folks to have a stable API to work with. - Overhead of builder -- I think in a lot of cases this is not a concern as the object is so short lived that collecting it will be very cheap. However, consider that this _can_ be very hot code -- in some cases creating the SourceRecord might be _the majority_ of the work involved in moving messages (e.g. Schema is fixed and a primitive type, so there's not even any other conversion happening). Is the expectation that those use cases would use the single all-fields SourceRecord constructor directly? - How many variants of the constructor are realistic? We're definitely not expecting to add more fields to messages (that type of change has to come with a _huge_ functionality win to ever make it past KIPs). Also, at least a few fields only make sense when combined (e.g. `valueSchema` + `value`). I'm not sure the number of variants is that high (but maybe I'm wrong -- I'm guessing you raised this as you started copy/pasting all the existing ones to include timestamp). Heh, not great when my pros/cons list ends up with an equal number of items on either side, but these issues probably warrant further discussion. (I do also very much appreciate keeping an eye out for ways we can cleanup/simplify the API and make the connector developer experience better.)
Nit: "Assigning tasks to streams clients: ..."; also better to be `log.debug`.
OK, fair enough.
nit: ``` Returns all the application config properties as key/value pairs. The config properties are defined in the {@link StreamsConfig} object and associated to the processor context when it was first initialized. ```
nit: ``` Returns all the application config properties with the given key prefix, as key/value pairs stripping the prefix. The config properties are defined in the {@link StreamsConfig} object and associated to the processor context when it was first initialized. ```
Nitpick: space missing before `conditionDetails`.
Nitpick, how about: ``` java public static File tempDirectory(Path parent, String prefix) { final File file; prefix = prefix == null ? "kafka-" : prefix; try { file = parent == null ? Files.createTempDirectory(prefix).toFile() : Files.createTempDirectory(parent, prefix).toFile(); } catch (IOException e) { throw new RuntimeException("Failed to create temporary directory", e); } file.deleteOnExit(); Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { Utils.delete(file); } }); return file; } ```
I would say something like: ``` java /** * Wait for condition to be met for at most {@code maxWaitMs} and throw assertion failure otherwise. * This should be used instead of {@code Thread.sleep} whenever possible as it allows a longer timeout to be used * without unnecessarily increasing test time (as the condition is checked frequently). The longer timeout is needed to * avoid transient failures due to slow or overloaded machines. */ ```
Nitpick: `maxWaitMs` would be a better match for Kafka's naming convention.
`, and default message for failure` is no longer needed.
Could use `equalsIgnoreCase` directly.
I said it incorrectly before, and I actually meant that you can use `org.apache.kafka.test.NoOpKeyValueMapper` in this case as we do not really care about what aggregate key / value to use. Your proposal is also fine, while I was just trying to reduce duplicated code :)
Why not use the copy constructor? It could even be done inline: ``` java for (Map.Entry<String, Object> entry : new TreeMap<>(this.values)) ```
I think you did this change to trigger Jenkins. I'd probably revert it. If Jenkins has a transient failure, it's OK to leave it and let the reviewer run the tests locally.
This scenario can happen when a rebalance migrates some tasks from one thread to another, within the same process / JVM. And I think we should still try to close the channel while shutting down the processor state manager, since otherwise the number of channels will just grow indefinitely due to rebalances that move tasks around, although very slowly.
As discussed in the JIRA, I still think we should use `FileChannel.open`.
And nitpick: there should be a space after the comma.
Is the current implementation vulnerable to the following race condition? 1. thread1 grabs `CHANNELS` lock, get the channel object from the map; and then exists the `CHANNELS` lock; 2. thread2 grabs the lock, release the lock on it, close the channel, remove from the map. 3. thread1 now calls `channel.tryLock`, which throws `ClosedChannelException`.
This should be final.
And same question for the other uses of `TestUtils.tempDirectory` in this PR.
Ah, I see that the state directory is being purged here. But I think this @After-approach may not work if the test crashes or the JVM terminates before `shutdown` is being called, right? (e.g. due to Jenkins woes) I think it would be safer to purge the state dir prior to the run.
At least we try to not abuse Thread.sleep() ;-)
Yeah, I don't know if we do (depends on whether locking is a bottleneck here). And even if we did, it makes sense to do that separately instead of this PR.
CLHM was the baseline algorithm for Guava, though is much faster due to leaving G before optimizing the port. If you later investigate this in-depth, I'd suggest Caffeine now since it includes a superior eviction policy and tons of features. Cheers.
I just read through `MemoryLRUCache`. It is not thread-safe and will corrupt itself because a read causes a mutation of the LRU history. (I made the same mistake early in my career when fixing performance problems leading to exploring caching in-depth, so its an easy oversight to make) A read/write lock is a very expensive mechanism and most often the incorrect lock type to use. For short critical sections it is more expensive than an exclusive lock. By using a `ReentrantLock` or `synchronized` you'll have both correctness and higher performance. As is, I strongly urge you to correct this before merging. You don't have to use a caching library, but the code is very broken.
Why not simply: `TestUtils.tempDirectory("qs-test")`? That will use the OS's default temp directory (more concise and better behaviour IMO).
Thanks for the explanation. It seems like `purgeLocalStreamsState` should really be using `java.io.tmpdir` instead of `/tmp` if it wants to have that safety net.
There is https://github.com/ben-manes/concurrentlinkedhashmap. Guava and Caffeine (Java 8 required) also have Cache implementations with the same underlying behaviour.
@dguy you'll need a lock for `putIfAbsent` too since the individual locks in put/get are not sufficient (e.g., value could change in between get and put).
After the iterator is returned, the lock is released, then the stream thread can still close the RocksDB store while iterator is not closed yet, right? An alternative approach I was thinking, is to keep track of a volatile "RocksObjectCount" for all associated objects, including RocksIterators / WriteBatch / etc, and let the close() call to only proceed when the count becomes 0. Note for WriteBatch since it is only going to be created by the same stream thread it does not actually matter, but just in case it will be created by different threads in the future. But there is still a race condition if the alternative approach is not carefully implemented such that after the query thread has called `validateStoreOpen()` in `range()`, but before the iterator was created, streams thread can kicks in and delete the store. So we need to increment the count first before checking if store is closed.
Good point, thanks for clarifying.
Fair enough. Thanks for the validation.
Another nit: you can use == to test enums. It reads a little nicer, but more a matter of preference.
I think we should stick with host:port for the near term unless there is specific requests in the KIP feedback.
My inclination is to provide more flexibility to the user, but add some guidance on what a "typical" setting would look like (think: `host:port`).
keep this one and call `print(keySerde, valSerde, null)`
Actually `this.name` is still the processor node name, not the KTable name as mentioned in the JIRA. However, after thinking it a bit more, I feel there are a few corner cases when using the state store name may be cumbersome: even after KAFKA-3870 and KAFKA-3911 is merged, still not all KTables will be backed by a state store (for example, a KTable generated from another KTable.filter, which is just a view of the other table). And if we call `filter` on both of these KTables, they will actually share the same state store names, which are confusing. So just using the processor node name, admittedly are not very intuitive for users, may be the least bad solution here.
We should have 4 overloaded `print` methods, ie, this one should not be replaced but kept.
Hmm, I thought you planned to use the "parent processor node name" as the prefix if it is not specified, which seems more user friendly to me? EDIT: actually my bad, nvm.
It seems a bit ad-hoc to have this environmental variable for one test only.
Is this change intentional? Ditto below.
I think this change is debatable. You make it a bit more visible that the lock is held when mutating the structure, but some of the logic is duplicated. I'd leave it as it was, personally.
nit: We use singular verbs in other functions (e.g. line 54 above), would be better to be consistent.
On a second thought, the overhead should be minimal: a few young gen objects only. So mvn.
We've had a report of large amounts of memory being used by `KafkaMbean` with empty maps. This makes it worse, so I don't think we should do it.
Yes, we don't use `Qty` as a suffix. We sometimes use `num` prefix and other times `Count` suffix (it would be nice to be consistent, but we're not there yet).
Nit: this line is too long.
This exception is no longer possible since the constructor is taking `ObjectName`.
We are slightly changing the behavior here: if the metric does not exist, previously it is just an no-op, while now we will create the metric first then deletes it.
Since `addAttribute` always calls `getMBean`, `this.mbeans` should always contain this metric already after `addAttribute`, right? Ditto below at line 83.
Making timeouts configurable could be a good idea, but it's better done in a general way in its own PR.
If using hamcrest matchers, I think it's better to use static imports to make them more concise.
We typically avoid the `get` prefix in Kafka, so `objectName` seems fine.
This method name doesn't follow Kafka code conventions.
Wouldn't you also need to handle default values? Presumably you're also assuming that there's only one version of a schema referenced anywhere in the schema? And that the schema parameters (e.g. the scale used by Decimal) don't matter? I realize that Decimal isn't a good example of where this could come into play, I'm just trying to highlight that I'm not sure the current API really addresses the full problem.
Does this actually need to be public? It looks like the SchemaBuilder uses names anyway and expects resolution to work internally. Adding another Schema class seems like its going to get confusing.
Also need to change `keyValueStore` method to return `StateStoreSupplier<KeyValueStore>`
This should be: `return aggregate(initializer, aggregator, windows, aggValueSerde, windowedStore(aggValueSerde, windows, storeName))` - it is probably one of the reasons you have test failures
Can remove the first two null checks as they are covered in the new overloaded `aggregate`
See above, this shouldn't be a generic type. It should be `KeyValueStore`
@duy @jeyhunkarimov Got it. So I think the question is whether we want to partially solve the "not be able to have finer-grained caching" problem here or just keep it as is and wait for a more focused shot later in another ticket. I think we can argue that it is OK to be "soft" and potentially change it later when we have another mechanism for fine-grained caching. My concern, though, is that by opening the pandora box of allowing users to do caching like this we are not long complicating the internal implementation (which, as we talked, is OK as long as this tech debt can be simplified later), but also about exposing the internal classes like CachingXXStores to public user interfaces, and this box is hard to be closed later. Anyways, my take is that eventually the finer-grained caching support should be added, but not by exposing the CachingXXStores to users.
Synced with @dguy offline on caching, the current semantics "only dedup on forwarding if it is the default CachingXXStore with wrapper RocksDB" looks OK to me. I think we need to re-consider the unification of caching with dedupping after this since now with user customizable stores this caching mechanism is narrowed to default state store suppliers only.
make params `final`
nit: is this change necessary? They are originally sorted alphabetically before.
@dguy I see your point, but I am not sure if we should remove it. Personally, I prefer short stack traces and even if there is some code duplication, I think each public API method should the checks right away. Assume, this overload get's rewritten at some point and does not call the other `reduce` anymore, we might miss to add the check. This is of course just my personal opinion...
i think this should be `maybeForward` - `checkForNonFlushForward` indicates to me that it is just going to check and return if it should forward.
Can remove the first 3 null checks as they are covered in the new overloaded `aggregate` method
Can remove the first two null checks as they are covered in the overloaded `reduce`
You can remove this check as it is now covered in then overloaded `reduce`
@guozhangwang i'm not sure why we would want to enforce caching? Perhaps the custom store is already an in memory store? Why would we cache that? Perhaps there is some other reason why they don't want caching for a given store.
ok - i see what you are saying. That probably feeds into another issue, though. I know we don't support it now, but the global caching on/off switch is probably too coarse grained. I think we should be able to decide on a per store basis if we want it to be cached or not. I think this has already been asked for by at least a couple of people.
My concern with this approach is that it isn't very flexible, i.e., i either have caching on or off, and that if i'm using any custom stores (and there might be a mix of custom/non-custom), and i don't need/want the custom store to be cached, then i need to turn it off for everything.
maybe `then the records in the second batch` is a bit better
There's no benefit in using `StringBuilder` for something like this. Using `+` will have the same effect (the case where `StringBuilder` helps is when there's a loop.
Sorry for not catching this earlier, shall we try to be consistent when it comes to `toString` output? Here's an example of a recent Java class: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/ClientResponse.java#L67 What do you think? Also, it would be good to include an example of the logging output.
Typo: "which not matches with"
don't think we need to log here . If a topic is not matching to the pattern than why bother logging here.
Not sure if we want to provide information on a topic on which user does not have permissions.
Could we just add a toString function to ProcessorNode / SourceNode / SinkNode so that this code just becomes: ``` print(node) { // node.toString // also include listing children names foreach(child <- children) print(child). } ``` And also you do not to maintain the mapping as well.
I tried to print the topology with the current code in WordCountDemo, and here is the output: ``` ProcessorTopology[sourceNode=ProcessorNode[name=KSTREAM-SOURCE-0000000006,sourceTopic=X-KSTREAM-MAP-0000000002-repartition-->node=ProcessorNode[name=KSTREAM-AGGREGATE-0000000003,StateStores[Counts,],--> sourceNode=ProcessorNode[name=KSTREAM-SOURCE-0000000000,sourceTopic=streams-file-input-->node=ProcessorNode[name=KSTREAM-FLATMAPVALUES-0000000001,--> node=ProcessorNode[name=KTABLE-TOSTREAM-0000000007,--> node=ProcessorNode[name=KSTREAM-MAP-0000000002,--> node=ProcessorNode[name=KSTREAM-SINK-0000000008,sinkTopic=streams-wordcount-output,--> node=ProcessorNode[name=KSTREAM-FILTER-0000000005,--> node=ProcessorNode[name=KSTREAM-SINK-0000000004,sinkTopic=X-KSTREAM-MAP-0000000002-repartition,--> ] ```
I see that you are actually printing the sink nodes. I'm wondering if this if condition is necessary since in line 85 this function will be skipped anyway.
I feel there are still some room for improvements. @theduderog's feedbacks would be very helpful, while here are my two cents: 1. Start a new line with different levels of indent as we go through the streams granularities, streams->threads->tasks->topology, etc. 2. For the topology specifically, the parent-child relationship seems incorrect from the `-->` hierarchy, so I'd suggest reducing the verboseness a bit by removing `sourceNode[..., sourceTopic,` etc, and also just displays the child name instead of using `-->`, for example take the above topology: ``` KSTREAM-SOURCE-0000000000: topics [streams-file-input], children [KSTREAM-FLATMAPVALUES-0000000001] KSTREAM-FLATMAPVALUES-0000000001: children [KSTREAM-MAP-0000000002] KSTREAM-MAP-0000000002: children [KSTREAM-AGGREGATE-0000000003] KSTREAM-AGGREGATE-0000000003: stateStores [Counts], children [name=KSTREAM-FILTER-0000000005] KSTREAM-FILTER-0000000005: children [KSTREAM-SINK-0000000004] KSTREAM-SINK-0000000004: topic [X-KSTREAM-MAP-0000000002-repartition], children [KSTREAM-FILTER-0000000005] KSTREAM-SOURCE-0000000006: topics [X-KSTREAM-MAP-0000000002-repartition], children [KTABLE-TOSTREAM-0000000007] KTABLE-TOSTREAM-0000000007: children [KSTREAM-SINK-0000000008] KSTREAM-SINK-0000000008: topics [streams-wordcount-output] ```
`.. about this topology by performing a breadth-first traversal of the DAG from source nodes. This is useful in debugging scenarios`.
`.. of this instance`.
This should probably live in the `benchmarks` folder.
Oh, this is the service, that usually lives under `services/performance`.
Ack, makes sense. I'm fine with either approach, although looking at the next few lines it doesn't look like there's a good, single place to reset it.
Do we still need `needsJoinPrepare` now? It seems to me that we need to call `onJoinPrepare` whenever `needRejoin` is true.
If this exception is thrown all the way up to the user, we should update the java doc string as well.
`state` is shared between threads. I think this block needs a `synchronized(AbstractCoordinator.this)` (and same below)
Might be worth noting what needs synchronization (accessed by heartbeat thread and user thread) and what is safe. I was having trouble sorting out which methods need synchronization and which ones don't. Seems like the shared state is `heartbeat`, `state`, `client`, `time`, `groupId`, `retryBackoffMs` (where `time`, `groupId`, `retryBackoffMs` are threadsafe)
Don't need to copy the future here. We're synchronized so the couple of lines can be reduced to `client.poll(joinFuture)`.
Yeah, I was thinking that just resetting to null is fine, but it needs thinking through to make sure we don't leave anything else in a broken state due to half-completed operations.
Won't that result in a dead coordinator and be handled by the first `if` block.
Nitpick: I'd call this `getOrCreateFileChannel`.
In fact, I think I'd remove all empty lines all the way to `autoCommitIntervalMs` as I don't think they add much.
I don't think we need these two empty lines.
I'd suggest we move it as part of the `public <K, V> KTable<K, V> table` call or just make it protected, which can still be accessed by the extended `KStreamBuilder` class. The main reason is that as a public function, its semantics will be very confusing to users who observe this function in java doc but do not really know how to use it.
nit: we are not "overriding" them, but duplicate them with the prefixed props right? If the user has already applied the prefix then this function would mostly be a no-op.
This is used for both producer and consumer, so we'd better name it `props` not `consumerProps`.
We should add doc string that "for properties user specify both with and without the prefix, the one with the prefix will be used, only for BOOTSTRAP_SERVERS_CONFIG it will ignore the prefixed one but always try to use the non-prefixed one, since currently KS is only supporting to read / write from the same Kafka cluster", etc.
For these messages in the case where the fetch does not match the current consumer state, it might help to clarify them by stating that the fetch is stale. It took me awhile to figure out all the cases when looking directly at this code; a user just seeing the log message probably isn't going to fare so well. The one here and the one in the `partition.errorCode == Errors.NONE.code()` case could probably both have "stale fetch request" added somewhere in the error message.
Good, `LinkedList` is rarely a good option.
Nitpick: would `CompletedFetch` be a better name? You seem to use that as the variable name in some places.
I was looking at the `append` code and it seems a bit brittle. It assumes that: - The collection returned from `PartitionRecords.take` is safe to mutate even though the latter returns `Collections.emptyList()` if `records == null` - That `part.take` will always return at least one element This is fine today, but it may be worth making it a bit more robust to refactorings.
Yeah, that could work. It could be confused with a collection having records to drain, but it's a local variable, so I'll just let you pick something (you can keep the current name if you like as well). :)
Yeah, that works. I was even thinking of just extending SerializationException with that information and leaving the existing constructors. But I don't think it matters much whether it's a new ConsumerSerializationException or (the slightly awkward) SerializationException wrapping another SerializationException with the info just placed directly in the message.
A quick look shows that other code has access to e.g. `topic` and doesn't include it in the exception message. Seems like having the fields there could help with better exception messages.
OK, so users just have to call `poll` more times to get all the exceptions. That seems like an acceptable compromise for the reduced memory usage.
The other thing is that the `InvalidRecordException` is a bit scarce in details. It would be useful to at least include the partition and offset like we do for the `SerializationException`.
Oh, we handled this in `throwIfOffsetOutOfRange` previously.
Would it not be easier to track all the overloads if we had a consistent order for parameters? That is, `SubscriptionState` and `Metrics` would always be the first two parameters.
Well, we wouldn't want to just remove that clause -- in the case of `SerializationException` you want to maintain the `SerializationException`, but if there's some other `RuntimeException` (which there can easily be for serializers that aren't aggressively catching exceptions and converting to `SerializationException`) then you still need to convert it to a basic `KafkaException`. I think you could do this: ``` try { // parse record } catch (SerializationException e) { throw new SerializationExceptionException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } catch (RuntimeException e) { throw new KafkaException("Error deserializing key/value for partition " + partition + " at offset " + logEntry.offset(), e); } ``` as long as we're confident there aren't any other `KafkaExceptions` we'd want to handle differently (i.e. any other more specific types of `KafkaException` where we'd want to preserve the same type instead of generalizing to `KafkaException`).
@hachikuji Yeah, I guess. The record parsing is a big chunk of code, but I guess there aren't really any other possible exceptions I can see there.
Nitpick: double space after `private`
There's an interesting edge case where `record.isValid()` could throw a `IndexOutOfBoundsException` if the buffer size is smaller than 4 (i.e. we fail when we try to extract the checksum from the buffer). Obviously, this means that the record size field was itself corrupt (since it should never be that small), but it can happen. I'll file a separate PR for that.
There's a space missing after `offset`, I'll fix it before pushing.
Yes, indeed. Jason suggested (offline) the same thing. He said he was going to push a commit with that change.
Which actually raises the question: why `InvalidRecordException` isn't a `KafkaException`? We could introduce `KafkaException` in the hierarchy and remain backwards compatible.
I can maybe understand a null value, but is a null key something we want to support? This is also making me wonder whether we should be implementing some mandatory namespacing.
Copy/paste bug: should be "source."
The JIT can easily inline this method, so it doesn't actually do anything. The BlackHole implementation in JMH is a lot more complex: http://hg.openjdk.java.net/code-tools/jmh/file/cde312963a3d/jmh-core/src/main/java/org/openjdk/jmh/logic/BlackHole.java#l117
I think it would be good to include a message giving context before we start listing unresolved issues.
Haha, to clarify, I was trying to suggest moving this line above null check. Something like this: ``` java FetchMetrics topicFetchMetric = this.topicFetchMetrics.get(topic); if (topicFetchMetric == null) { topicFetchMetric = new FetchMetrics(); this.topicFetchMetrics.put(topic, topicFetchMetric); } ```
Is this necessary? There should only ever be one instance of `FetchManagerMetrics`, and it already exposes a method `recordTopicFetchMetrics`.
As mentioned above, I think this is the right idea, but shouldn't we just call: ``` java sensors.recordTopicFetchMetrics(entry.getKey(), metric.fetchBytes, metric.fetchRecords); ```
This class can be `private static`.
This is a micro-optimization, so feel free to ignore. An alternative idiom is to call `map.get()` and check the result against null. This works because we know the value will never be null and it saves a hash lookup.
nitpick: could we move this below the record() function? It would be nice to keep the class fields together.
This condition should probably be like a few lines below: ``` java if (subscriptions.getSubscribedPattern().matcher(topic).matches() && !(excludeInternalTopics && TopicConstants.INTERNAL_TOPICS.contains(topic))) ``` So we would want to extract that condition to a helper method perhaps.
I think we should probably include information about the received `kerberosName` in the error message and use `principalToLocalRules.toString` instead of `toString`.
It's useful to have a toString in any case, but I'd probably include the name of the class `KerberosShortNamer` in the `toString`.
Nitpick: maybe the rules should be at the end since they could be long. Something like: ``` java throw new NoMatchingRule("No rule applies to " + kerberosName + ", rules " + principalToLocalRules); ```
Nitpick: we typically do this like: ``` java return "KerberosShortNamer(principalToLocalRules = " + principalToLocalRules + ")"; ```
Since now the `StringBuilder` is being appended to directly, the `b.append` here would cause duplication of the accumulated content I think
Also we can just pass in the `StringBuilder` as an argument rather than create a new one here
The second newline should be left for the caller, as it otherwise causes an extra line before 'Dependents' in the enriched RST
The order in each group should be based on... `ConfigKey.orderInGroup` :-)
I think this should be a single line break
perhaps a better name is `getConfigKeyRst()`
Groups in particular may not make sense in some cases. Some connectors have only a handful of options so grouping them isn't particularly useful.
I figured if you're calling `toEnrichedRst()` the new 0.10 fields are expected to be set
Need to check if `group` is `null` in both `k1` and `k2`. Using this on, e.g., `DistributedConfig` from Kafka Connect doesn't currently work.
`group` could be `null`
At this point we have a lot of repeated code with `toRst()`. Would probably make sense to refactor to share the majority of the code for each config's main set of fields.
this doesn't seem right -- the JIRA suggests that the group should be a section header for all config keys with that group
Currently this is reverse order. Probably better to just do `return Integer.compare(k1.orderInGroup, k2.orderInGroup)` here.
these 2 lines shouldn't be here
The dependent key name should be in backticks. And perhaps comma-delimited rather than tabs.
Also we should omit the Dependents item from the list if they are empty.
need a newline after the group before the underline, and a double newline after the underline
There's a trailing comma issue
Given that we always set the future to `null` here, is it correct that the reason that we do all of this logic is to potentially use a future initialised via `lookupCoordinator`? And is there a point in actually setting the `findCoordinatorFuture` in this method? If I understand the aim correctly, it could be clearer if we did: ``` java RequestFuture<Void> future = findCoordinatorFuture; findCoordinatorFuture = null; if (future == null) future = sendGroupCoordinatorRequest(); ``` But I may be missing an important detail. :)
as above, though this one seems inlinable
`orderInGroup` param is duplicated for key & value converter
Looks like `maybeAutoCommitOffsetsAsync` already checks this flag.
This doesn't seem too interesting as a test case since we already unsubscribed. Maybe the second `subscribe` can go before the `unsubscribe`? Then we can verify that we still own tp0 and tp1.
The extra parameter seems a little more annoying than the effort it saves (which is really just checking a couple flags). If we really wanted to avoid the redundant checks, maybe a better way is to add a private `doAutoCommitAsync` or something like that, which simply sends the request assuming the coordinator is known and autocommit is enabled. Then the two `maybeAutoCommit` calls could delegate to `doAutoCommitAsync` after doing the checks themselves.
nit: the previous location seemed a little better since it was listed next to `autoCommitIntervalMs`.
Similar to below. Maybe `testManualAssignmentChangeWithAutoOffsetCommitEnabled` is a more descriptive name.
Aha, I missed the significance of the parameter. We actually want the commit to happen regardless of the next auto commit deadline. So maybe we can refactor as you suggested and have the public version accept no arguments and simply send the offset commit. Maybe we should call it `maybeAutoCommitNow` or something like that? Then the existing method can delegate to this method when the commit deadline has been reached.
I think we would end up skipping the offset commit anyway since we check for empty prior to sending it. In fact, we could probably simplify this to just: ``` java if (this.autoCommitEnabled) this.commitAsync(); ```
One other thing. Would it make sense to move this call to the `else` branch below (just before the call to `assignFromUser`)? Otherwise, we'd double-commit down the `unsubscribe` path.
Interesting. So this seems to expose a couple problems. 1. It looks like we allow the `OffsetCommitCallback` provided by the user to be null, so we probably need a null check in `ConsumerCoordinator.OffsetCommitCompletion.invoke()`. 2. For automatic commits, we use a logging `OffsetCommitCallback`. See `ConsumerCoordinator.maybeAutoCommitOffsetsAsync()`. We should probably use the same callback here, so maybe we need to make that method public and invoke it instead of calling `commitAsync`.
What I'm looking for is test cases which would have caused this code deletion to fail. Perhaps the main case we're missing is one which asserts that the assignment remains the same after a subsequent call to `subscribe()`, and only changes when `assignFromSubscribed` is called. Glancing at the code, I'm also not sure we have a case which covers a subsequent call to `subscribe(Pattern)` as well.
I'm surprised that this didn't cause any unit test failures in `SubscriptionStateTest`. Haha, that probably shows that we have some gaps in our coverage. Maybe we could have test cases which validate that the assignment does not change with a call to `subscribe`, but does when we call `assignFromSubscribed`? Another one for regex would be nice as well. These cases are probably covered in `KafkaConsumerTest` already, but it's helpful to have unit testing as close as possible to the implementation.
nit: unneeded newline
I think I'm still not getting why we're trying to hook into the metadata updates in this patch. As I understand, the main point is to hold onto the current assignment metadata until the onPartitionsRevoked() hook. Unless I'm missing something, it shouldn't have anything to do with when we receive topic metadata (which can happen basically at any time).
nitpick: we usually include a space before and after ":"
@vahidhashemian Feel free to ping me on Google chat if you want to talk about this. It would probably go a little faster that way.
When the user calls subscribe(), we should set a flag in SubscriptionState saying that a rebalance is needed. As of yet, the user's assignment hasn't changed. When the user calls poll(), we check the flag and begin the rebalance. Once the rebalance completes, we set the new assignment and begin fetching. I'm not sure I see a way for any fetches from the old assignment to reach the user.
@vahidhashemian Are you concerned about the fact that we will have some unneeded metadata cached? That doesn't seem like a problem as long as we clean it up eventually (i.e. on the next scheduled refresh).
Hmm... Yeah, maybe the call in the metadata listener is sufficient. But we definitely don't want to fetch metadata for all topics. Also, it seems unnecessary to request a metadata update blindly. I think `metadata.setTopics` was previously requesting the update only if the topics are not already contained in the Metadata.
True. I went ahead and did that in KAFKA-3807. I'm not sure which will land first, but feel free to do it here.
Here too it seems like we can use the generic version of `prepareOffsetCommitResponse`.
Maybe just one more case missing. If we call subscribe with a new pattern, the assignment also shouldn't change.
That's true, but perhaps that could be a call to `clearAssignment` directly from `unsubscribe`? If we only clear the assignment here, then it wouldn't be cleared until the user calls `poll` after calling `unsubscribe` which doesn't seem quite right.
This is a change in semantics, right? Before, we would never expire a connection that has been processed during the `poll` because we use `currentTimeNanos`. After this change, if something in `poll` takes long enough, we could end up expiring a connection in `completedReceived` for example.
`NetworkReceive.UNLIMITED` sets it to unlimited. Although it seems like a questionable choice, seems like it would be easier to just use `Integer.MAX_VALUE` for such cases since it's never unlimited really.
this problem also affect replication node fetch process? our server occur a error: [2019-04-05 23:59:46,084] WARN [ReplicaFetcherThread-1-1], Error in fetch kafka.server.ReplicaFetcherThread$FetchRequest@b22a64c (kafka.server.ReplicaFetcherThread) java.io.IOException: Connection to 1 was disconnected before the response was read but return to normal after a reboot kafka server.
I think we just leave the expiration of the next connection in the next poll() call.
There's still a usage as `applicationId + "-" + topic` in the `SinkNodeFactory` subclass. The structure of that class is now a bit odd as the `applicationId` is passed as a parameter, but it's a non-static so it captures the parent reference and actually uses it for at least one other member of the parent class (`internalTopicNames`). I'm fine committing as is if this is consistent with trunk since then a clean up could easily be cherry picked if that was desired. But it looks like this patch and what's on trunk differ significantly. This method name doesn't even seem to exist on trunk? Are you sure you want to diverge so wildly? It's going to make any more backports/cherrypicks really annoying...
This seems like it's a kind of weird restriction. I guess it'd be odd to use the same name for an internal topic and another, though if you know its going to be prefixed it might not be great to not be able to use that same name.
I think the other thing that's a bit confusing is how when you start a connector it needs try/catch, but stopping a connector doesn't (and handles its own failures). I get that this is because we have the extra `DistributedHerder.startConnector()` wrapper of `Worker.startConnector()`, but it feels asymmetric and confusing when reading this code.
This was probably left by mistake.
cc @rajinisivaram I'm not sure we should have this here - i.e., a unit test failure isn't particularly helpful if SHA1PRNG is unavailable (since the user may be fine with the null default).
@dguy EDIT: nvm. Since this is pure in memory, it will not matter at all when upgrading in a rolling bounce since it will be lost anyways. I confused myself yesterday night about what to keep compatibility during rolling bounce.
I think we should consider add a byte for indicating versions so that we can have better upgrade patch in the future in case we are evolving the serialization format. Similar rationales for versioning RocksDB state files and subscription metadata, or anything that involves serdes.
I am wondering would it be better to have this logic in `kafka.Kafka.main()` instead of in the exception? Also it seems that after calling shtudownSystem() the thread will continue to run in this case, which is different from the previous behavior, i.e. system.exit() does not return. If we do so we need to inspect all the cases where we call `System.exit()` and make sure all the threads will exit after we change it to throw the exception. By putting this logic in `kafka.Kafka.main()`, we can still call System.exit() in any of the Kafka thread after catching the `FatalExitError` exception and it is guaranteed the thread won't run anymore.
If there are multiple FatalExitError thrown, do we want call `System.exit()` only once instead of creating a thread for each of them? Maybe we can just log the error and return.
It should be 'false' by default
I think @becketqin was suggesting that we can add a method to `Kafka` that would cause the thread waiting on `KafkaServer.shutdownLatch` to resume and call `System.exit()` with the appropriate exit status.
Ideally, we would exit the loop after a certain amount of time.
I was not insisting :) Just wanted to know if you want to use 10000ms or want to use 5 times the consumer timeout. Sounds good.
Perhaps a better option for nulling `findCoordinatorFuture` is to do it in `coordinatorUnknown()` since that is called on every iteration of the poll() loop (and in the heartbeat thread).
Looking at this a little more, I think @ijuma is right that the shadowing is unnecessary.
@rajinisivaram Yeah, that may be better. Initially I was trying to keep the management of that field somewhat confined, but seems we need to break that anyway.
Great. I will review and merge #1808 first and this PR then can be rebased.
Yeah that is fine. My bad.
`isEmpty` is a little nicer than `size() == 0`
Here for the property, if we pass, a single script without parameters, it works. Like ssl.truststore.password=/usr/iop/current/kafka-broker/conf/password.sh Whereas if we provide a script with the parameter it will not work. Like if we pass as ssl.truststore.password=/usr/iop/current/kafka-broker/conf/password.sh ssl.key.password Here we can add a line String[] execCmd = trimmed.split(" "); before calling sell.execCommand(execCmd); So that user has an option he can provide a simple script or he can provide the single script to all SSL properties by passing options along with the script. Let me know your thoughts on this.
I like this cleanup, but I think we still need the `null` check. Since it's possible for the value to be `null`, we should probably be defensive about it. Or were you thinking that we should just let the `NullPointerException` occur and kill the connector? Something in the middle of these two cases might be to log a warning so hopefully the connector developer can fix their code. (The only reason we even need to validate this is due to the `SinkTaskContext.offset(Map<TopicPartition, Long> offsets)` variant, the single partition variant with `long` obviously doesn't have the same issue.)
Always a toss up with test code like this whether to try to make these helper methods with more options or just clearer but a bit repetitive by using completely different methods. This seems fine for now, we might want to rethink/refactor if we end up with another one or two methods similar to this.
`joinThisName` is used in the store of `thisWindow` which is to be queried by the other stream, so my personal understanding is that: 1. for inner-join (`rightOuter` = false, `leftOuter` = false): both window-store has `JOINTHIS_NAME-store`. 2. for outer-join (`rightOuter` = true, `leftOuter` = true): both window-store has `OUTERTHIS_NAME-store`. 3. for left-join (`rightOuter` = false, `leftOuter` = true): the left window-store `THIS_NAME-store` and the right window-store `OUTERTHIS_NAME-store`, since we will join with `null` if the right window-store returns null (hence "outer"), but not vice-versa.
Since you've noticed that these don't depend on any instance state, do you think it makes sense to move them to a separate file (e.g. `ConnectorUtils`)? The `instantiate` function below would be another candidate.
Yeah, that's fair. Just thought I'd mention it since it stood out while reviewing the patch.
It seems we never use the return value here? I also wonder if we should just remove the `connector` argument. This is only invoked from 2 places and I don't see a good reason to have used the list of connectors provided in the rebalance callback, which is the only place we don't use the `stopConnectors()` variant. We might want to validate the set in the callback and complain in a log if something looks wrong, but we always want to shutdown all connectors that are actively running. It would also mean we wouldn't need to check whether the connector exists in `stopConnector(String connName)`.
The return value is fine, I just noticed that it wasn't used, which made it feel unnecessary. Doesn't really matter either way as it can be added/removed easily. Re: `stopConnectors()`, rebalances will always revoke the entire assignment (at least as things stand today), so the two are equivalent. To be honest, this one is a bit up in the air anyway -- today we rely on the entire assignment being revoked, but in the future we may do a sort of incremental rebalancing. In that case, it isn't clear whether we would be able to use the callback to notify the listener exactly which ones are _actually_ being revoked or if that's too application-specific.
Not part of this patch, but this name seems a little misleading. In addition to stopping the connector, we also remove it from the connectors that are managed by the Worker. I'm wondering if it makes sense to separate the notion of connector _installation_? Maybe that would get around some of the awkwardness with removing the connector when we're just restarting it and needing the set copy in `stopConnectors()` above.
I'd err on the side of not adding configs for now and only add them if we find a need, so this seems fine.
I wonder if you also considered moving startup and shutdown into the `Worker`? The advantage is that we already have an executor there and then we'd get the parallel implementation for `StandaloneHerder` as well, though admittedly that may not matter as much.
Actually `Worker.startTask` is what I was referring to. All we do is submit the `WorkerTask` to an executor. I'm trying to understand the benefit of the parallelization.
nit: This check seems superfluous (and is inconsistent with the collection of connector callables).
@hachikuji Did you misread `startTask`? It directly invokes `Worker.startTask` afaict.
They are using the same underlying enum, but `TaskStatus.State.DESTROYED` would probably be clearer here.
It looks like we always run with effectively infinite timeout since we rely on the timeout for individual connectors/tasks. We can probably just remove the timeout values and in `bulkRun` use the `invokeAll` variant that doesn't have a timeout.
Using `UNASSIGNED` might be confusing here since the connector/tasks have been assigned at this point. I think we should either stick to current state or target state here even though it isn't user visible as it'll make it clearer what's going on. (Relatedly, but not really important, `action` might be better named `targetState` or something like that given it is using the `TaskStatus.State` type.)
Well, if you want to match the use of `DESTROYED`, `RUNNING` probably makes the most sense since that is the target state you want the connector/task to be in. But I'm not picky, either one works.
Cool, if we are worried about concurrent updates then the current pattern is good. I had the sense further requests being added is not possible while in `halt()` but I don't immediately see this prevented in any way.
Shouldn't catch the exception since https://github.com/apache/kafka/pull/1778/files#diff-10752971682d4575c93ddec45e0553f0L762
You probably want to also change this to `Throwable` in the vein of https://github.com/apache/kafka/pull/1788/commits/6c6f6761a3e08ad04b2659386eaf6e288d36e546
Shouldn't this result in a change to a failed status? I think we'd either need to do that here (if thread safe) or have some way to communicate it to the original thread? We can defer this to another patch if we already weren't handling this case properly.
You also get futures back when you submit them, so you can also get that barrier by waiting on the futures. But this seems fine too, I don't think collecting the entire set would cause any problems.
This could probably also be an exception since this shouldn't be called with invalid target states.
An alternative would be to also attempt to use `System.identityHashCode` to break ties in a way that would be consistent (up to the point that the hash code can be guaranteed to also be unique, which isn't perfect either).
I think this is a throwback from before rebasing against https://github.com/apache/kafka/pull/1778/files#diff-10752971682d4575c93ddec45e0553f0L755 We shouldn't need to catch any exception here.
I don't doubt it works but it's not obvious if one has to check the implementation of `ConcurrentSkipListMap.doPut()` to get confidence. Actually, a better argument - https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html#compareTo(T) states: The implementor must ensure sgn(x.compareTo(y)) == -sgn(y.compareTo(x)) for all x and y.
To a caller they are equivalent though. But I don't have a strong opinion here, and I guess avoiding unnecessary exception overhead makes sense.
Aha, good point.
> Question: How about we retire the executor in the Worker. It's only used for starting tasks. That executor is used for all the work tasks do, not just starting (`WorkerTask` is a `Runnable`)
Upon checking out the code, actually the only purpose is for the running of the tasks, and not starting at all :-) It could definitely do with a better name... and naming for the the threads with a `ThreadFactory` (we should do that for the `bulkExecutor` too)
It's probably more of an issue now, but I think we may have already not been entirely thread safe with the methods we can call when starting connectors/tasks. In particular, `reconfigureConnectorTasksWithRetry` can invoke `addRequest`, and `addRequest` expects to already be locked but `reconfigureConnectorTasksWithRetry` doesn't guarantee that. I'm guessing we've missed that issue until now because that happens rarely and I think the only thing that could conflict with it would be external HTTP requests. I think the only other piece that needs to be protected as fallout from this parallelization is the step where we call `configBackingStore.putTaskConfigs` in `reconfigureConnector`. The backing store is not thread safe (and neither is its underlying `KafkaBasedLog`).
Hmm, I'm wondering if there's some reason we shouldn't make `addRequest(long, Callable, Callback)` synchronized and remove the synchronization from all the other HTTP call handling methods (`connectors`, `connectorInfo`, `putConnectorConfig`, `requestTaskReconfiguration`, `taskConfigs`, `putTaskConfigs`, `restartConnector`, `restartTask`) which are currently `synchronized` - they pretty much just call out to `addRequest`.
@ewencp Actually I'd probably prefer to use separate explicitly named classes. For example: `StartConnectorCallable` and `StopConnectorCallable`. The reuse of the status enum seems a little confusing.
I think this can be written as a for loop over `requests` followed by `requests.clear()`.
I think nicer to just `return requests.first()` here
I meant a for-each loop, which avoids having to `pollFirst()` in 2 places ``` java for (HerderRequest request: requests) { request.callback().onCompletion(new ConnectException("Worker is shutting down"), null); } requests.clear(); ```
@hachikuji I suppose that depends on whether you find booleans eventually become confusing at the call site.
Just a note. This may need consideration together with #1707 where the metadata age may subject to change during producer startup.
This logic should probably be put in `RecordAccumulator.abortExpiredBatches`. Otherwise, if we put this logic here and we call RecordAccumulator.abortExpiredBatches() from another place, RecordAccumulator.abortExpiredBatches() would still return partition to be expired even if metadata has expired.
Discussed offline. This can instead be the producer's retry count. If the retry count is zero, then we will have to allow for at least one metadata request past its max age. So the staleness threshold will be: `retryCount * (backoff + requestTimeout) + maxAge`
I'm not terribly thrilled about a hard-coded retry limit here. Let me think a little more on this but I understand that there may not be a much better way.
I'm not sure about this wording change. The producer does wait until the send is declared `complete` as far as I know. And once that is done, it's up to the kernel to send the data even if the JVM dies.
Do we want to encourage the confusing `-1` value? I think it's intentional that it wasn't mentioned there. We could perhaps say "also known as" or something like that without promoting its usage.
That's not what I see when I look at the code. The following code populates `completedSends`: ``` java if (channel.ready() && key.isWritable()) { Send send = channel.write(); if (send != null) { this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); } } ``` `channel.write` looks like: ``` java public Send write() throws IOException { Send result = null; if (send != null && send(send)) { result = send; send = null; } return result; } ``` And `send` looks like: ``` java private boolean send(Send send) throws IOException { send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed(); } ``` Why do you think we are not waiting for the request to go down the OS layer? I don't see any unflushed JVM level cache/buffer in the code above.
@rajinisivaram, the reason I was asking is that @cotedm was only able to reproduce this issue on Centos 7, which made it seem like it could be related to the OS network stack as well.
@ijuma I have been able to reproduce it on Windows, Mac OS, and Ubuntu. More [here](https://issues.apache.org/jira/browse/KAFKA-3129).
For testing, we might want to be able to use `MockTime` here. Not sure if this would be easy with this (not sure how to change it either -- to be honest)
This might be instable in Jenkins.
Why do we need this? Seems to be a wrapper for `NetworkClient`, but does not add too much value IMHO.
nit: revert this file back to what it was originally as the only thing that has changed now is the order of the imports
So this remains an issue for me, we shouldn't remove this test.
I think we can just add these configs as part of the PR.
I don't think we should add these configs to the public interface. Developers can already add Consumer & Producer Configs to the StreamsConfig - they both have all of the configs defined here and we should just use those. For example have a look at `StreamsConfig.getProducerConfigs(...)`
I don't think this is necessary to add here. AFAICT `StreamsKafkaClient` is only used in `InternalTopicManager` and it can just be constructed there
I think we should, and with #1792 maybe this test needs to be updated a bit again, but still by default (user does not overwrite anything) the changelog should be compacted.
Personally i feel adding `final` to any locals etc that don't change is good practice. It makes it immediately clear that the value of the field/local is fixed.
If only Java made it possible to declare immutable local variables and method parameters in a more concise way!
For some test cases we may want to use the same `MockTime` object on both server and client, for example in window store changelog truncation (cc @dguy ). So instead of creating the object internally we may want to pass it through parameters.
A better way is to first call `this.mockTime.milliseconds()`, then `this.mockTime.sleep(1000)` then call the `milliseconds` again with the second batch.
For followup: I think it's worth elaborating a little that this heuristic can diverge a little over time from the ideal as partitions are fetched in different orders and partition leadership changes.
Do we actually need this? It seems that we wait until `inFlightRequestCount == 0` and there are no unsent data in the accumulator.
Does this actually buy us anything here? If `forceClose` is false, then doesn't that mean that there are no more in-flight requests? I think we still increment the in-flight request count even if the client doesn't expect a response.
nit: Would it be clearer to initialize this to `time.milliseconds() + CLOSE_TIMEOUT_MS` and avoid the sentinel check? It doesn't seem too worthwhile to avoid the system call.
The call to `closeGracefully` can result in some responses being returned from the brokers. Is it intentional that we do not invoke the completion handlers? I think this probably makes sense since anything awaiting the responses has probably shutdown by now, but wanted to check.
nit: prefix "task [%s] "
nit: unnecessary extra space.
Add the stream task id prefix here as well for both exception message and the warning log entry.
Thanks for the explanation. A bit subtle as you had said. :)
nit: could we rename this boolean to `initialized`? And then when checking this flag is not true: ``` throw new InvalidStateStoreException("Store: " + storeName + " may not currently available, since the hosted thread is not (re-)initialized yet"); ``` Also we may need to update the `ReadOnlyKeyValueStore` and `ReadOnlyWindowStore` about those possible thrown exceptions and how to handle them (e.g. backoff and retry).
I think this should only be done after the store is in a valid state, i.e, after restore. Otherwise there is a chance we can try and query or write to the store before it is ready
can we move this down such that it is the last line? Again, just making sure that the store has been initialized before we put it into the map.
I think i said this before, i think it is cleaner to bail out early if the topic is null. We then don't need to check again and for the main case, i.e, topic is not null, the code is outside of an `if` statement ``` if (topic == null) { this.stores.put(storeName, store); return; } ```
my preference is to always use `{..}` for `if` . Without them it reminds me of the goto fail bug!
You might want to swap the order of the logger args and the `count`/`max` args. There are multiple overloads in `slf4j` and this is only one of them. There's also `(String)`, `(String, Object)`, `(String, Object...)`, and `(String, Throwable)`. The `(String, Object...)` version is actually the one you want to use as the most general version, the others exist purely as an optimization.
Also, not sure if we can make this easy, but it'd really be ideal if we could keep the logging line as a single line of code without requiring additional logic by the caller. (Maybe even if this requires allocating a special object to do that.)
Would it be worth having the rate limiter have 2 levels: the first level logs every error and the second logs every nth error? My concern is that simply cutting off logs entirely once we hit a certain # of messages can mask later messages. You don't want to mask those entirely, you just want to cut them off. I think any approach using timestamps is probably going to get too complicated. But I think still printing every 1000th message would be useful so you eventually see the problem.
One extra line.
@eliaslevy Since this part is covered in other unit test case, we want to remove redundant coverage to leave the unit test as succinct as possible.
Note that the `getPort(url)` call a few lines above can fail with a `NumberFormatException`, so we probably need to put that inside the `try`.
The point is that `getPort(url)` is outside the `try` block.
I think we probably want to include both `host` and `url`. Maybe something like: ``` java log.warn("Removing server {} from {} as DNS resolution failed for {}", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host) ```
I don't mind diff noise if it makes things better btw (even slightly)
That's where I'm referring to: `String fieldName = targetField.name();` but I didn't follow too many levels of redirection ...
This does not look pluggable to me : a concrete implementation of the password verifier is hardcoded rather than say instantiating the class from a config entry.
Is this always true? Can't the method be called with `schema` set to null? (The method does check for that condition.)
~~Perhaps all of this logic should be within the `if (schema != null && schema.name() != null) {` block on [line 714](https://github.com/apache/kafka/pull/1872/files#diff-84083875888fce192c216d574b13163cR714).~~
Actually, I now understand why the logic is where it is, and why the logical conversion doesn't need to be done. However, I still think the above logic using the schema's default and/or checking whether the schema is optional needs to only be performed when the `schema` is not null.
@shikhar Is there a downside to covering the default value case for the 3 other logical types? Seems like these tests are cheap and protect us in the future from any change in logic which may or may not be type-specific.
Hmm, do we need this log entry here? Since `flushInternal` is only called by `flush`, which is called by the `StateStoreManager` which is already logged.
Similarly, I'd rather log it in the caller function where we could have more context logged (e.g. which thread, which task, is closing this store). And currently since it is only called in `StateStoreManager` which is already logging this event as `log.debug("task [{}} Closing storage engine {}", taskId, entry.getKey());` I think we do not need here.
To be consistent with other logging entries: `... Committing all its owned tasks`. And capitalize on line 509 below as well.
Could we move this log to `AbstractTask`, before `store.init(this.processorContext, store);`? Also we can use the same logging prefix as: ``` ("task [{}] Initializing store {}", store.name()); ``` And since we already log the loggingEnabled information in `Stores`, I think we do not need to log it again here.
To be consistent with other logging entries (also `{}` is more efficient I think): ``` log.trace("task [{}] Start flushing all stores upon committing its state", id()); ``` And ditto below: ``` log.trace("task [{}] Start flushing its producer's sent records upon committing its state", id()); ```
Soon @ijuma, soon ;-)
People might call it with an invalid `storeName`, so it will never exist. I think it should throw in this case. @guozhangwang - thoughts? Yep, we just had a long conversation about this on another PR. Stream re-init is not really exceptional and people should back-off and re-try in these cases. Though, i could argue for throwing an exception here, too :-)
They have a different meaning. `null` means that the `storeName` doesn't exist - perhaps it should throw an exception in this case. `StreamsMetadata.NOT_AVAILABLE` means the streams haven't (re-)initialized
We probably want all these `Long`s to be `long`s.
If we did as I suggested above, then we could make the inverse of this as the loop condition.
nit: this could be a `break` also.
@becketqin It's ok with me, but is it worth removing the unneeded break after `awaitMetadataRefresh`? I also think it's a little clearer if the check for `isDone` is done first thing after calling poll() as I did in the snippet above. More generally for this problem, I think we should have some better options for doing these loops generically without so much pain when we finally switch to Java 8.
These timeout loops are indeed painful. This one could be structured a little more nicely. For example, there's probably no need to check the result of `awaitMetadataUpdate`; we can just let the loop logic handle the timeout. Also, it might be more natural to `break` after first checking `future.isDone`. That might make the timeout check in the middle unnecessary.
I was thinking something like this: ``` java long nowMs = time.milliseconds(); long deadlineMs = nowMs + timeout; do { RequestFuture<Map<TopicPartition, OffsetAndTimestamp>> future = sendListOffsetRequests(timestampsToSearch); client.poll(future, deadlineMs - nowMs); if (!future.isDone()) break; if (future.succeeded()) return future.value(); if (!future.isRetriable()) throw future.exception(); long remaining = Math.max(0, deadlineMs - time.milliseconds()); if (future.exception() instanceof InvalidMetadataException) client.awaitMetadataUpdate(remaining); else time.sleep(Math.min(remaining, retryBackoffMs)); nowMs = time.milliseconds(); } while (deadlineMs > nowMs); throw new TimeoutException("Failed to get offsets by times in " + timeout + " ms"); ``` Not sure if it's any better though. If so, only marginally.
Ah, you're right. I misread the second check.
We can do this in a separate JIRA, but there is a TODO in this class to send the list offsets requests in parallel when resetting multiple partitions (currently we reset each partition separately, which means one ListOffset request for each partition). Should be an easy refactor after the changes from this KIP. I'll open the JIRA.
Shouldn't we pass the time remaining before the timeout to this call? Similarly, we should take the timeout into account when backing off after a failure.
Can we cover the other error cases? For example, INVALID_REQUEST and UNKNOWN_TOPIC_OR_PARTITION. It also looks like it's possible to receive no error, but an unknown offset. Having all of these test cases protects us when we change the code in the future.
Can we combine the trace message above (which prints the node that received the message) and this one so that we know which response came from which node? Maybe the easy way to do this is to move the parsing of the `ListOffsetResponse` into the `onSuccess` call in `sendListOffsetRequest`.
Stale docstring (copied from reassign_partitions_test)? Perhaps unecessary, since you have a detailed docstring in the test method
Could also do `ms += ", ".join(missing_list[:20])`
It would be best if this test used a replication factor of 2. With a replication factor of 1 we will have no regular replication traffic occurring when the producer writes messages. It would be good to have both throttled replication and non throttled replication happening at the same time.
We don't mind if it takes longer. Technically it could be a bit shorter though, so for stablity I'd probably suggest something like: `time_taken > estimated_throttled_time * 0.9`
I suggested `alive` to be consistent with the method being called on the consumer. The main reason is that `init` and `start` are a bit too similar and it's a bit difficult to distinguish between the two. Anyway, ok to leave as is if you feel strongly about it.
`consumer messages` should be `consume messages`
Maybe we should say `Consumer process took more than %d s to become alive`? Because the other timeout has the word `start` in the name, it would be good to avoid using it here as it might be confusing.
Actually, it seems more than one figure needs updating. Makes me wonder if we should actually remove them altogether and let people read the code (which can't go stale) instead.
Probably worth extracting this variable. It seems like it would be easy to update the one in `wait_until` (maybe because we have transient failures) while forgetting to update this one (causing confusing logs).
@dguy `dbDir` is referenced in RocksWindowDBStore's segment; and by default it is `package private` not `private` since otherwise the build will break.
@guozhangwang, Damian has made the field less accessible than before. Protected allows package access and subclass access while package private does the former only. Since the build is still fine, I'll merge this.
Capitalize 't' Also, died _due_ to
`while` seems to be missing
I think the implicit assumption is that the parent "foo" doesn't exist (yet), right? If so, shouldn't we assert that it actually does not exist prior to continuing? Or do we feel `TestUtils.tempDirectory();` is sufficient? (I think it is, but still wanted to ask.)
Sorry for the dumb question, but I am curious if ``` if (partitionsPerTopic == null) return that.partitionsPerTopic == null; ``` Wouldn't be ``` if ((partitionsPerTopic == null && that.partitionsPerTopic != null) || (partitionsPerTopic != null && that.partitionsPerTopic == null)) return true; ```
This is a checkstyle failure (use `./gradlew testConnect` to run all those tests and checkstyle without having to run the entire, very long test suite).
Got it. Thanks for the explanation.
Should we have a case for the old consumer too? I think we can delete one of the 0_9_0 tests, since there are so many of them.
Is`atexit` is the right approach here? The registered function will only run when the entire ducktape process finishes (i.e. _after_ all ~215 tests finish). Use of the `tempfile.mkdtemp` will still avoid path collisions from concurrent processes, so maybe it's good enough. Another possibility that at least results in immediate cleanup: put directory removal into `clean_node` (but check the containing directory is present before removing to avoid errors)
I think truststore and ca could be stored in a single directory. The files are related and are created together.
Now that we have a way to work with `Configuration` instances without using a singleton, I think we should try to move the `Configuration.getConfiguration()` call to the edges of the application. So, perhaps, it would be better to remove this and any other method that hides the loading of the singleton.
In `reLogin` we have `loginContext = new LoginContext(loginContextName, subject);`. Shouldn't we be passing the `jaasConfig` there? Also, I'm a bit unclear on why we pass a `subject` since we didn't in the initial construction.
Even though the config is invalid, the passwords may be valid, so it seems safer not to include them. It would be nice if the sensitive entries in the JAAS config would be communicated in some way to improve debuggability (in the future). Btw, a nit: we seem to use inconsistent capitalisation of the word JAAS in our messages. It would be nice to make that consistent.
@rajinisivaram makes sense. I was thinking if we can break the JAAS config file and made them into client config properties. But just passing JAAS config file will make it easier and extensible.
@rajinisivaram can we not provide keytab an principal name instead of passing the jaas config. Its still another config management todo for the users.
Nit: unnecessary new line.
I think I'd just do: ```java List<String> lines = asList(loginType.contextName() + " { ", jassConfigProp, "};") Files.write(jaasConfigFile.toPath, lines); ```
Maybe the `testConfiguration` methods should be called `checkConfiguration` to distinguish from the test cases. Similarly for `testInvalidConfiguration`.
Good that we have a test case where the expected output is hardcoded instead of generated by the test (avoids issues where a bug in the construction of the string can cancel out a bug in the parsing code. :)
It turns out that passing the size of the destination array makes `toArray` slower, so passing `0` is both more concise and faster. Source: https://shipilev.net/blog/2016/arrays-wisdom-ancients
Maybe we should update this to mention the error case for the server.
I'd suggest throwing an runtime exception since the `storeNames` function should never be called for `KTableRepartitionMap`: it is more efficient for detecting errors.
`Collections.emptyMap()` is the right one to use. `EMPTY_MAP` should give you a generic warning.
I think you can just use `partition < partitionsCount` instead of using `compareTo`. Similar in line 558.
I'd probably say "in case the number of partitions has been increased" instead of "in case a partition expansion has taken place".
I wonder if it would be better to fail in `waitOnMetadata` instead of having the logic in two places.
Just keep in mind that it's important to be careful not to simplify to the point of being incorrect (e.g. a successful response is not the same as a response containing metadata for a given topic).
Raising the `UnknownTopicOrPartitionException` changes the behavior of the producer. The difference is that the previous `IllegalArgumentException` would be raised to the caller of `producer.send()`, while this exception will be passed to the send callback. For Kafka Connect, this means that sending data to an unknown partition will be handled silently (well, with a log message) instead of failing the task. That might not be what we want since it basically results in lost data. I'm wondering if it would be safer for now to raise this as a generic `KafkaException` so that we keep the current behavior.
This line is failing checkstyle.
Oh, nevermind. I didn't see the following line.
nit: this is not introduced in this patch, but it'd better to add the log prefix as other logging entries do.
nit: could you add a logPrefix indicating the threadID
Not sure. PENDING_SHUTDOWN indicates a clean shutdown while this lets the thread fail.
We should add `else` and throw `StreamsException` with cause using original `NoOffsetForPartitionException` (ie, `ex`). Furthermore, the error message should explain in detail what happened and how a user can fit it. Something like: ``` No valid committed offset found for input topic T (partition P) and no valid reset policy configured. You need to set configuration parameter "auto.offset.reset" or specify a topic specific reset policy via KStreamBuilder#stream(...) or KStreamBuilder#table(...). ``` The reason for throwing an exception is, that Streams has no change to start reading a topic/partition in a meaningful way. The user needs to fix this issue! This is also current behavior, because `NoOffsetForPartitionException` would be raise for `auto.offset.reset=none` and missing committed offsets, too.
The state transition is not great at the moment for passing exceptions to the user so any help there is appreciated. What @bbejeck says makes sense.
The fallback should be taken from `StreamsConfig` and not be earliest all the time.
Why do we need to update the configs? By using seeks we can simply ignore the underlying configed value right? More specifically, as done below we can check if `consumer.committed` returns anything or not, and if no AND the reset policy is specified, we just use seeks to the specified positions.
Ditto here: use `entrySet()`.
Does this need to work with ipv6? Might be worth comparing with `ClientUtils.parseAndValidateAddresses`.
Since this is a fairly complex assignment process, I wonder if it would help to break it down into smaller functions (maybe one for each step?). Otherwise, this is going to be a pretty intimidating chunk of code for newcomers.
nit: maybe iterate over `entrySet()` instead.
this won't work with ipv6 addresses, I think there are some helper methods for this is org.apache.kafka.common.utils.Utils
nit: the `T` parameter is unnecessary.
Ditto here: seems we don't need the key? Same for the nested loop over `topicGroups`.
Seems this could be a function as well. For example: ``` java Map<TopicPartition, PartitionInfo> partitions(Map<String, InternalTopicMetadata>); ``` (I'm looking for small independent chunks of code that can be taken out of this function.)
typo: computer -> computed
Seems you can replace this with this: ``` java topicPartitions.addAll(partitionsForTask.get(id)); ``` Same below.
Are the two loops below equivalent to this? ``` java taskIds.addAll(state.activeTasks); taskIds.addAll(state.standbyTasks); ```
From `SaslConfigs#SASL_LOGIN_CLASS_DOC`: > For brokers, login config must be prefixed with **listener prefix** and SASL mechanism name in lower-case. For example, **listener.name.sasl_ssl**.scram-sha-256.sasl.login.class=com.example.CustomScramLogin This code is only using the SASL mechanism name in lower-case as the prefix. Assuming this observation is in fact a problem, I think it raises a broader issue, which is that the listener (`SASL_SSL` vs. `SASL_PLAIN`) is not known at this point in the code (or at least it isn't readily available).
@rajinisivaram The login callback handler class isn't getting its #configure(Map<String, ?>, String, List<AppConfigurationEntry>) invoked. Perhaps it might be better to treat the login callback handler class the same way the client and server callback handler classes are treated, which is to create/configure them in SaslChannelBuilder? Note that the login callback handler class is potentially used both on the client side **and** on the server side (it is used on the broker when the mechanism is the inter-broker protocol).
All these tests could also be parameterized - would be a lot less code, but would likely need reflection to look up the constructor.
While we should have the call to `super` above, do we really need this given that the default `available` implementation in GZIP is not very helpful? i.e., ``` super.available(); return inf.finished() ? 0 : 1; ```
Yeah, either way works for me. Seems unlikely someone using a bleeding edge library like streams would be on an ancient Slf4j 
Should this be debug level? Seems like this could be really spammy.
You can actually use "{}" here for the prefix. Slf4j supports this: http://slf4j.org/faq.html#paramException. Same for the other cases.
you dont need the `String.format` here would need `%s`->`{}`
This method is not `synchronized`. So there could be a race condition here. I think it should be: ``` final NamedCache cache = getOrCreateCache(namespace); final LRUCacheEntry result = cache.putIfAbsent(Bytes.wrap(key), value); maybeEvict(namespace); return result; ```
This seems wrong, we have 2 `STDOUT_CAPTURE` and no `STDERR_CAPTURE`. Why did the first one change to `STDOUT` since we're still redirecting stderr to the file? (It's also not obvious to me why we changed this to use `tee` but it seems to not affect anything critical here.)
From Frank's reported stack trace it seems not the case but I'm wondering if the following can happen (this is a slightly modified procedure that you described before): 1. Put a dirty entry into the cache for key = 1 2. Flush called, the dirty entry's dirty bit is set to false, and the the listener function fires. 3. Inside the listener function: 3.1. cache eviction runs, and the entry is the oldest, since its dirty bit is false `flush` is not called while this entry is removed from `cache`. 3.2. more dirty entries go into the cache 3.3. cache eviction runs again and flushes due to dirty entries. At this point key = 1 is in the `dirtyKey` set, but not in the cache and the IlegalStateException is raised.
One suspect I have is on the `CachingKeyValueStore.delete` function, which just writes a `<key, null>` into the cache but does not really remove the key-value pair in the underlying store (is this a bug?). Could this cause the above issue eventually? Also why we are writing a `<key, null>` into the cache instead of just calling delete on the cache itself? They underlying `serdes.rawValue(null)` may throw an exception if it does not handle `null` values.
I guess my main point is that, like Damian said, if we add the `dirtyKeys.remove` function we may hide some bugs that will make it even harder to detect in the future, so I'd rather bite the bullet and make it more detectible and fix it than adding this one line change. Other changes LGTM and we can merge as is.
minor: seems like we got rid of the `new_consumer` parameter in the other cases below when using the default.
Oh, I see. Well, either way works for me.
Is it necessary to make this an interface? It seems there is only one implementation.
Yeah, tbh I wasn't sure either since I hadn't reviewed those patches yet and wasn't sure of the state. I mentioned this to @hachikuji today as well. His thought was that since https://github.com/apache/kafka/pull/2264 (which is actually only 1 of a couple of patches for KIP-97) is quite large, it might make sense to get it merged first. We're pretty sure this is the only KIP that will potentially be affected by it. @hachikuji has also taken a pass at that one, so if we merge it and you need guidance on updating the patch, he can probably give direction pretty easily. I just checked and there are some minor merge conflicts, but nothing too crazy, so my guess would be that it'd only be a bit more work to layer on the extra bit of compatibility work.
Yeah, protocol changes definitely need a KIP. Probably makes sense then to split the bug fix into a separate patch.
Since the is specific to v2+, the constructor used doesn't even really need the `responseData` parameter -- if there was a top-level error it seems there will never be response data so we can just use a dummy empty list in `OffsetFetchResponse`.
style nit: normally we'd use braces around blocks unless they're a single line
Aren't you still missing setting the error code field on the struct in this case though? The pattern that seems to be used elsewhere, e.g. in `MetadataResponse`, is to make the constructor that takes the version contain all the fields as arguments as well as the version. Then all the decoded fields are kept as member variables and written regardless of whether that version contains them, but only written to the struct conditionally. For example, `MetadataResponse` has some code that looks like this in its constructor: ``` this.clusterId = clusterId; // This field only exists in v2+ if (struct.hasField(CLUSTER_ID_KEY_NAME)) struct.set(CLUSTER_ID_KEY_NAME, clusterId); ``` (after having constructed the `struct` with the correct schema). I think if the current code is working, it's just lucking out on `NONE`'s error code being `0` or something. I wouldn't think it would work as is since the field doesn't have a default value defined.
I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
Kind of annoying that the response doesn't give us an instance of `Errors` directly.
nit: not really sure we need two separate constants even though they are separate fields in the struct.
This will be a little annoying to handle when we incorporate the client compatibility KIP since we'll have to check for the presence of these errors at both levels. One option might be to enhance the parsing of the response to check for the presence of one of the top-level errors in the partition data. If it is there, we could insert it at the top level as well. Currently I think we just put `Errors.NONE` at the top level for old versions.
Maybe we should enforce a minimum version number when querying all partitions? You can look at `ListOffsetRequest` for an example of this.
nit: add a space before the `:`.
Wonder if there's any harm retaining the top-level error regardless of the version. Seems more consistent with how we handle the case of constructing from a `Struct`.
Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
Haha, I'm not sure whether we're saying the same thing. My suggestion was to blindly treat the exception as a top-level error. In other words, take the error code from the exception and use it as the top-level error code for new versions, and as the partition-level error code for old versions.
Yeah, we're still feeling out the best patterns for handling older versions.
Seems just as efficient to me, especially since we only throw the first error.
I was mainly concerned that we'd need to check errors in both places, but I think we're good now since we ensure that top-level error codes will always appear at the top level (even for older versions).
Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed.
Perhaps: > The record consumed from the input topic has an invalid (negative) timestamp, possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, or because you are reading from a pre-0.10 topic after upgrading the Kafka cluster to 0.10+. [...]
Why not check `context.timestamp`? Checking the message of the exception is very brittle.
Maybe we can also add sth. like `possibly because an older versioned client is used to send input topic messages to Kafka that do not have timestamps encoded` ..
The restore consumer needs to override one param: `consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");`
This doesn't pass checkstyle -- you need spaces before and after the `+`.
I have no strong opinion on close-parameter vs streams config. So ok with me.
State that `A timeout of 0 means to wait forever`. 1. Also, I feel it's better to use 0 as the default value in the above function to be aligned with Thread.join() semantics? 2. Could we return a boolean indicating if the instance has been successfully closed or it is timed out, besides the warning log? Users can then program around it instead of only doing operational actions based on logs.
I think the intended method to call would be ``` Thread.currentThread().interrupt() ``` Same with line 258 below.
As an alternative, which might align better with Kafka in general, would be to set the timeout via `StreamsConfig`. This keeps the API clean. @enothereska argument that `close()` should not have any arguments is quite valid to keep APIs consistent within Kafka.
Why do we need an atomic here? `close()` should be called single threaded only, right? And if I miss anything, we do we not need to use atomic to switch from "created" to "running" in `start()`.
Yeah this is observed in another PR review, and should be fixed by now. cc @dguy
Just realized, that the method does use `synchronized` keyword anyway... it's guarded against this already. Same for `start()`.
This is not introduced in this PR: we can try to get the record collector in the constructor and cache it instead of trying to access it every time. I checked all the access patterns of this field and they should not be modified dynamically.
It's a bit confusing, but it can: http://slf4j.org/faq.html#paramException (if I understood your point correctly).
Also, are we fine with the config logging being bumped up to `info` (which is what `logAll` does) vs `debug` (which is what it was here).
Also, you're missing a space here that causes the build to fail. I can clean up on commit, but you'll want to make sure you run tests (even if just `./gradlew clean testConnect` to restrict to connect tests). Running from an IDE won't run extra stuff attached to the test targets such as checkstyle.
Hmm, so I realized the issue with this is that you lose the context of which task (or connector if we fix the other instances) is even listing this config. With `ProducerConfig` or `ConsumerConfig`, just listing the classname in `logAll()` usually works fine since you're often just creating one instance. Here, we'll just get `TaskConfig` in the output. Do we think it's enough to just have the surrounding context from previous log lines? It does look like we log which task is being created pretty close before this. But there is also a `ConnectorConfig` that is parsed before the `TaskConfig`, so I'm not sure how obvious it'll be that the two are tied together. (MDC could probably help a lot here.)
Personally I'd suggest we only keep a skip sensor for user to monitor correctness since the the "processed record sensor" is well covered in the throughput sensor already. And for that skip sensor we can use `Rate()`. For example, as in `record-error-rate` in `Sender.class`.
`Count is a {@link SampledStat} that maintains a simple count of what it has seen.` So with this stat, its value will be increased for the window period, then suddenly drops to zero, then start rising again. So it's hard to alert on such a metric, on the other hand `Rate(Count())` will record the average rate per time unit (here second), so users in practice can easily set a threshold for alerting, and even if they want "zero tolerance", setting the threshold to be 0 can still satisfy their needs.
Yeah, I'm not sure we need to provide the synchronization either. Serializers are currently assumed to be synchronized by the producer, for example. So there is some precedent for that assumption, although it only needs to be thread safe if you use it from multiple threads. Currently Deserializer doesn't need to be since the consumer is single threaded anyway. @kkonstantine Do we know of a case of one that isn't thread safe already? iirc JsonConverter uses synchronized caches which is the only state I can think of that it has.
This formatting is going to fail checkstyle. There seem to be other failures as well. For the changes in this patch, run `./gradlew clean clients:test` to make sure everything will pass.
Could also be `final`
Is this line intentional? Unit tests normally don't need to print out to console.
We should not include this along with the unit tests since it's not a unit test.
Should we treat it as a WARN level entry? I think this situation can actually happen frequently in a real production cluster and is transient anyways, hence I'd suggest changing it to `log.info("Skipping assigning topic {} to tasks since its metadata is not available yet", topic);`.
can you use the Utils.newInstance here . https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L312
The use of this field makes the test class quite confusing IMHO.
This fails checkstyle
Hmm, we seem to be sanity checking a) that we are assigned this partition and b) the user code is not jumping ahead of the current position without actually performing a seek. Is this right? If so, these seem like things we should warn about if a connector is trying to do that since it indicates the connector is almost definitely broken.
nit: Indicate that this needs shallow iterations on the entries.
nit: Indicate that this needs deep iterations on the entries.
This statement is a bit misleading, how about "to the format indicated by the given magic value".
"a 4 byte size," needs to be removed.
a 4 byte size, an 8 byte offset => an 8 byte offset, a 4 byte size of the record
Why are these checks being removed? They were added because people would sometimes make the group names conflict, which results in confusing error messages from the group coordination protocol.
Since the motivation for using this probably isn't too obvious to someone who isn't familiar with the implementation details, it'd probably be worth expanding this explanation a bit to say when you would use it.
Perhaps say that the repartitioning will happen automatically, i.e., user doesn't need to do anything.
You could also use `MemoryRecords.EMPTY` here (I think).
To clarify, what I meant is just to move the initialization of the records variable from above to here (since we don't actually need it outside of the loop): ``` List<ConsumerRecord<byte[], byte[]>> records = fetchRecords(builder.build(), Errors.NONE.code(), 100L, 100 * i).get(tp); ```
@enothereska is right, if you make that additional change, I'll merge your PR. Thanks!
Should it be "...consumption...is coupled"? Currently it is "...consumption...are coupled".
I don't think we need these prevTasks and standbyTasks for this test. You can just pass `Collections.emptySet()` to the `Subscription`
Any reason why this block of code isn't in the `if(tasksByTopicGroup.get(topicGroupId) != null)` block? After the `for(..)`? If it is not null then it is going to have some tasks, right? So numPartitions will always be > -1
Is it worth treating these differently instead of including them in the UNRESOLVED list? People that look at JIRA release notes for a version would probably think the issue was fixed when it really hasn't.
Also, this is failing checkstyle because there is no space after the comma. I think there are a couple unused imports in this class as well (you can check the jenkins build for more detail).
nit: the other log message had a little more information (i.e. it described what the code was doing). Maybe this message could be "Reading to end of log offsets {}" or something like that.
Minor: would be good not to lose this information from the logs. It's probably fine to print the whole map of end offsets instead of iterating through them by partition though.
This one is still not using a tab.
Given that this is rare, do we really want to return early instead of doing the copy? I am asking because it seems a bit surprising to return less than `max.poll.records` if we have fetched the data already.
Thinking about it, this may be the root of the problem. We are removing from the head of the ArrayList, one at a time. For each removal, we cause all the elements to be shifted in the underlying array, which is very inefficient if n is not small.
Is this right? I thought it should be `position += limit`.
Nevermind, I was thinking of `position += n`, but `limit` is actually the result of that (bounded by records.size).
I have a slight preference for doing the copy in this case if it's reasonably simple. But I'll leave it to you (maybe I'm being paranoid :)).
It doesn't seem like we need this variable. Why not assign the result of `createFetchRequests` and call `size` on it? Less mutable variables that need to be tracked.
This condition is reversed, we should call `pollNoWakeup` if a fetch has been sent.
While you're here, I suggest fixing `fetchablePartitions` so that it doesn't do two `remove` calls on an `ArrayList` (in the worst case, it has to shift all the elements in the underlying array twice). We could pass a predicate to `subscriptions.fetchablePartitions()` and make it more efficient.
Nit: This should be called `maxPollRecords` as we don't use the `get` prefix for methods like this.
My bad, thanks!
Hmm, we can't use shallowEntry.offset() since not all messages will be retained. Perhaps we could just maintain maxOffset as we add messages to retainedEntries.
nit: let's group all o.a.k.s.p.internals imports together, before the `StateRestoreCallback` for alphabetical ordering.
And again - i'd say it is definitely worth having what i mentioned above
I know this is only a test class, but returning `Collections.emptyMap()` would be the nicer thing to do here
I would restructure all these fields like: ```java private final long startMs; private final long sleepTimeNs; private final long targetThroughput; private long sleepDeficitNs = 0; private boolean wakeup = false; ```
Why was `FileNotFoundException` removed from the signature? It can be useful to list specific subclasses that may be worth handling separately from the general `IOException`.
I think there might be a bit more to it than this. Do we need to consider the flushing of the StateStores and the producer, too? I.e, we have state in the state stores that is going to get flushed - that state is related to the offsets that we are not committing. That state is not-only going to be flushed to disk, but also written to the changelog topics. Although, without EOS i'm not sure we can do much. We can't guarantee that the state hasn't already been flushed to the store and the changelog, i.e., eviction can happen at any time.
Should be AtomicInteger (see explanation below)
Preferably int (and the declaration should be ideally after `at`)
The priority queue defines priorities primarily based on user defined delays (`addRequest` takes a `delay` as argument. The sequence number should break ties between requests that have the same priority, which is a specific time in the future). Because the priorities depend on user defined delays, such priorities might correspond to the creation time of a request, but this is not required. Thus, `compareTo` needs first to compare deadlines, and if they are equal use the sequence number to break ties.
Since `StreamsException` ultimately implements the `Serializable` interface, could you add `serialVersionUID` as well as other constructor functions? You can take a look at `TaskAssignmentException.java` for example.
@ijuma The password is used to check the integrity of the truststore. If password is not specified, the integrity check is not performed, but the truststore can still be read and used.
I see, thanks for the explanation. OK, so this means that if someone mistypes the key for the truststore password, integrity checking is disabled and the unused config warning is the only clue to the user. Probably OK.
I understand that. My point was if you have a typo in the config name. In that case, you'll end up with a null password and integrity checking will be disabled even if the truststore has a password (in this test, the truststore password is set when we save it in `TestSslUtils.createSslConfig`). I found that a little surprising, I thought this was only about the case when you didn't want to set a password in the truststore. In any case, you'll at least get a `unused config` warning and that should hopefully be a clue.
In a batch of records, if the first record failed, it will cause all the rest record's callback to add a warn entry and hence swamp the log file. I feel it is better to just modify line 95 to sth. like "error sending to topic and partition, will not updating offset of this partition anymore and this exception should be eventually thrown to the user".
nit: Can we just merge these two test into one? I feel the second is almost a super set coverage of the first.
Any reason why you did revert the order? It seems more natural to me (but this is nit) to change the state after the threads got started, ie, (1) start thread (2) change thread state (3) after all threads got started change KS state.
Probably want to make this a `ConcurrentHashMap` or `synchronize` access to it. It can be modified etc from multiple threads
This should be `synchronized` too as you could have other started threads updating the map at the same time. Perhaps just wrap the map with `Collections.synchronizedMap(...)` and then remove `synchronized` from `StreamStateListener.onChange(..)` Sorry for the back-and-forth.
Hmmm... Even if we use `ConcurrentHashMap` I guess this would not be enough. In `StreamStateListener#onChange()` we check the state of each thread to compute the new state for KS -- this most be atomic. Using `ConcurrentHashMap` would not prevent a race condition there. So maybe `StreamStateListener#onChange()` should have `synchronized` keyword -- it a single instance we hand into into each thread.
This is the callback from the `StreamThread`s so it will be called from multiple threads, i believe. See the inner class `StreamStateListener`
Good argument. That let's keep as is.
Ok would prefer to remove it.
Actually it looks like it is `synchronized(stateListener)` in `StreamThread`. But even that seems a bit wrong. I think the synchronization would be better to be done in this class rather than elsewhere
The state that is being updated is private data of `KafkaStreams`. It should be responsible for synchronizing access to its data, not external classes.
I had a similar thought first. But `StateListener.onChange()` is only called within `KafkaStreams#setState()` which is synchronized.
I don't immediately see the difference between the removed and added code.
Guess just a matter of taste... No performance issue of course.
Nvm, I think it is not worthwhile anyways.
I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
I doubt it would add much to the size of this patch if you want to do it here. A follow-up would work as well. I'm anxious to have a good general approach in place so that we have clear patterns to follow going forward.
I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,
In this case, we're using the presence of the `offsets` field to indirectly determine that the version that was used was 0. I'm not sure we'll always have something so convenient, so I've been wondering if we need a way to determine the version that was used more directly. For example, we could have `ClientResponse` or even `AbstractResponse` include a field for the version.
I think we need to pass this exception to the future instead of raising. Would be good to have a test case if we don't already.
We didn't have it before, but maybe we should add a null check here for more resilience in the future.
I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required.
I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably.
I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry.
Also, it would be good to verify that `flush` still behaves as expected if called without `close`.
Should we move closing non-assigned tasks into this method too? Would be a cleaner workflow -- close all not reassigned tasks (regular and stand-by), than reuse suspended tasks and create new tasks.
Could we return a boolean indicating if this task is removed or not? Then in line 964 below we can instead check that this map should then be empty since we have removed both recycled tasks or non-recycled ones (in line 863).
Can we use `UnsupportedVersionException`? The record is valid, it is just that the produce request version doesn't support it.
Yeah, I think that works also. Actually, now that you mention it, another place we could do this validation is `ProduceRequest.validateRecords`.
nit: may as well store `sorted(configSnapshot.connectors())` in a variable since it's used twice.
nit: missing closing parenthesis.
This first argument (`1L`) can be missed, given that that's the only line in which we pass 2 arguments. I'd add one argument per line.
Here too, if the one above changes.
Hmm, this one is kind of interesting. I was thinking pause would actually stop things, but I guess for sinks we can actually just pause the consumer. At first I thought we wouldn't want this because `pauseAll`/`resumeAll` were initially used just to pause the consumer when data was rejected with a `RetriableException`, which could, e.g., happen even if data is still being processed simply because a buffer got full or something. But these methods are also used to just pause work when the target state is paused. This makes me wonder if we're also not doing quite enough in those cases before marking the task status as paused -- in fact the connector may still be processing data, we're just not feeding it anymore. I think we at least need a flush + commit, and to be really sure (e.g. in the case of the HDFS connector) we'd probably need to actually `close()` as well.
Is this ok to do for both reasons this would be called? Do we actually want to request a commit when in the case of the connector throwing a RetriableException? That'll result in the connector being forced to flush all its data.
Oh, I did miss one case: if we are closing while the coordinator is failing over. Hmm, in this case, we could still send any _unsent_ offset commit requests after the coordinator is rediscovered... I'm not sure if this case is worth trying to handle though.
Seems the only thing we really care about is offset commits when shutting down. As long as we send the LeaveGroup, it's probably fine not to await its response (because of one of your previous patches). Because we now have the check for `pendingAsyncCommits`, I'm wondering if it's actually necessary to await all pending requests from the coordinator? At least if we keep the check, maybe we could ensure that we are not in the middle of a rebalance since that would unnecessarily delay shutdown.
@rajinisivaram I guess so. I was trying to come up with an excuse to keep shutdown fast and eliminate some bookkeeping  . By the way, I'm not sure it makes sense to distinguish between async and sync commits since `wakeup` can interrupt a synchronous commit before returning.
This is where we leave the group. Any offset commits which haven't yet been sent will definitely fail after this, so I think we need to move that loop into the try block.
Hmm, seems like you forgot to remove the `remainingMs` in the `while` condition.
Thanks, that makes sense. However, I think there may still be a problem. Any unsent requests will be targeted to the current coordinator at the time of `close()`. Discovering a new coordinator in the loop below won't help us unless we can retarget those unsent requests. What's more, we have logic in `AbstractCoordinator.coordinatorDead` to explicitly fail unsent requests to the coordinator.
If the coordinator changes during close, the queued offset commits targeting the old coordinator will fail, and the pending commit count will decrement. Right? So we lose those offset commits. Is that a big deal? If you use async commit, and close before you are sure they are finished, that is a risk the user must accept. Also, this is no worse than the behavior before the patch, right? Am I right in my understanding that there were no guarantees around offset commits during close even then? If this is true, then the current patch is reasonably optimistic: if your coordinator does not change within the close, we will wait upto the timeout for pending offset commits to finish. If the coordinator does change, then your commits going to the old coordinator will fail.
Do we want this to be `error` or `warn`? Maybe `error` is right, so take this as a question. :)
Shouldn't this be `timeoutMs - (time.milliseconds() - startTimeMs)`? Also, it's not too big of a deal, but the checks for `Long.MAX_VALUE` seem like overkill.
If we don't need to track the pending offset commits, maybe we could move this into a `AbstractCoordinator.close(timeout, unit)`.
I'm not actually sure we need this. The method `ConsumerNetworkClient.pendingRequestCount` handles sent and unset requests, so it seems sufficient to just wait until the count reaches zero to be sure all outstanding offset commits have been sent.
Might be worth mentioning that this method also sends a new offset commit when autocommit is enabled.
I can't make out the difference between this test and the previous one. looks identical from the code.
Got it. So in the autocommit mode, the offsets will be committed on close. Further, any asynchronous commits pending will wait until the timeout before the close proceeds. If the coordinator cannot be found within the timeout, the close proceeds without committing anything. The default timeout in all cases will be 30 seconds. I think it make sense to put this in the java doc.
Not sure why this was changed.
Nit: seems like the interrupted check should be done before we compute the remaining time (from a clarity point of view).
Nit: you can call `Thread.enumerate` directly. Also, it would be good to assert that `threadCount` is < than `threads.length`.
Now that this logic is not tied to offset commits, could we move this loop into `AbstractCoordinator.close(timeout)`? That would make it available to `WorkerCoordinator` as well.
If there are pending async commits, then the coordinator must be known (because we explicitly fail all requests to the coordinator in `coordinatorDead`), so I'm not sure I see the value of rediscovery here. However, I think there is some value in calling `ensureCoordinatorReady` prior to invoking `maybeAutoCommitOffsetsSync` and also prior to sending the LeaveGroup.
The point is that the coordinator discovery logic below is unneeded.
I think this should be synchronized.
nit: seems we could move this to the caller and remove the `requestTimeoutMs` parameter.
Since we don't do coordinator discovery in this loop, do we need to reinitialize the `coordinator` variable? If the connection fails, then pending request count should drop to 0 already.
Why we want to make it static? I feel it is not worthwhile as we now have to send client and metadata parameters into the class.
We can also remove "public" here to make it package-private.
I don't think a reference to `protocol_api_keys.html` is required here; because that file is loaded as a server side include (SSI) inside `protocol.html`. I would prefix the anchor labels with something like `The_Messages` (which is the referred main section name) instead to make them uniform. The hyperlinks should work fine after fixing this.
Tab should be replaced by 4 whitespace characters.
Another tab here that should be replaced.
Looks like a styling issue is failing the build. A whitespace is missing before and after a couple of`+`'s on this line.
The timestamp is not necessarily the one in the ProducerRecord (if LogAppendTime is used), so it would be good to clarify that.
We should probably mention the timestamp that was added in 0.10.0.0.
Please use string interpolation. There are a few other places like that.
Hmm, I think I'd prefer two separate maps with two separate fields in `LoginManager`. It makes things more explicit and easy to understand in my opinion (even though it's a bit more code).
This is an existing bug, but `mode` should be `loginType`.
if you have an unsigned ~~8-bit~~ 64-bit data source
if you have an unsigned ~~8-bit~~ 16-bit data source
if you have an unsigned ~~8-bit~~ 32-bit data source
Also out of date here.
I think this is outdated based on the addition of schemaless support.
`final` and initialize in constructor instead? Doesn't seem to depend on the config at all.
Incredibly unlikely to be a problem, but this should be `>=`. Actually, this is a bit risky anyway because one timestamp is from the driver machine, the other is from the worker machine where the connector is running. (We ntp the workers during setup, so they are reasonably in sync. I can't remember the exact details, but I think we had issues with this previously due to clock skew; iirc it was in code that generated security certs on the host and then tried to use them on the workers.)
nit: we can define a static ``` private static final String[] NO_PARENTS = {}; ```
The parameters can be `final`.
Maybe we can create a JIRA for tracking, but it's not that important for now. Since the common parts that can be consolidated may be not much: in the actual topology building process we need to set the internal topic names, set copartition topics etc which are not needed for the topology description building at all. What I was originally thinking is the the topology building process may be extending from the topology description building process with its additional functionalities like I mentioned above, but I am all hand-wavy on the devil details now.
You are right. NVM.
Any reason why we don't just stick with return `AbstractNode` from these overrides? I don't see anywhere that we need it to be a `Processor` etc
if we make this `<String, TopologyDescription.AbstractNode>` then can we do away with the casts below? I think we know that they will always be `AbstractNode`
if you added a method to `NodeFactory` `String[] parents()` then in `SourceNodeFactory` you implement it by returning `new String[0]` This code becomes ``` String[] predecessorNames = nodeFactory.parents() for(final String predecessorName : predecessorNames) { ... } ```
Maybe add an `addSubtopology` method? I don't really like the idea of a) exposing, and b) mutating an internal collection.
Rather than doing `if(nodeFactory instanceof blah)` could we add an abstract method `ToplogoyDescription.Node describe(...)` to `NodeFactory` and then implement it in the subclasses. My skin crawls when i see `instanceof` :-)
nit: personal preference so feel free to ignore. But i'd ditch the `else` in this case. I don't think it adds anything
why bother with this? `NodeFactory` is private in `TopologyBuilder` so it is not going to be anything else. Plus it wouldn't matter if we used the interface rather than `instanceof`
same as before. Would be much nicer to add a method on the abstract class rather than using instanceof
@amethystic I believe Jun is suggesting that it is abnormal for `readFully` (i.e. if you can't fill the buffer, then something is wrong). I think a case can be made for that. I think the downside is that the error messages may not be as good as if the callers do the check themselves. The upside is that we avoid the situation where the caller forgets to check. We'd have to verify that these semantics are right for `FileRecords.readInto` since it doesn't perform any checking atm.
This is not correct, it should be `buffer.remaining`. It seems simpler to just do `while (buffer.remaining())` with an early exit in the case read returns `-1`.
What's the purpose of this warning? It doesn't seem needed.
Maybe the name should simply be `readFully`. And `buffer` should maybe be `destinationBuffer`. And `startPosition` can maybe just be `position` for consistency with `FileChannel.read`. Finally, we need a mechanism to indicate that EOF has been reached. Maybe a `boolean` is good enough. The usual way of returning an `int` is a bit weird for the case where some bytes are read and then EOF is reached.
A bit unclear why we need this. In my mind, `readFully` should be as close to `FileChannel.read` as possible with the exception that it attempts to fill the buffer while end of file is not reached.
I think we probably want a `do/while` loop here. There should be no difference in behaviour, but it seems to model the problem better (i.e. we first do a read and then we check if there is still space remaining in the buffer. Maybe: ```java long currentPosition = position; int bytesRead; do { bytesRead = channel.read(destinationBuffer, currentPosition); currentPosition += bytesRead; } while (bytesRead != -1 && destinationBuffer.hasRemaining()); ```
The issue with using a real file is that you can't test the scenario that we are trying to fix: `FileChannel.read` returns before the buffer is full. See `FileRecordsTest.testTruncateNotCalledIfSizeIsBiggerThanTargetSize` for an example of a mocked fine channel.
The boolean makes it easier to do the check, but doesn't help with cases where the caller forgets. So the exception is better from that perspective.
Yes, it seems that -1 means that we are trying to read beyond the end of the channel, which probably should be treated as abnormal.
@junrao, that's an interesting suggestion. If we do that, various `if (buf.hasRemaining())` checks in some of the callers no longer make sense.
+1 on Jun's suggestion if it works. If not, it might make sense at least to return a flag indicating whether the read completed.
Maybe we should add one case where `position > 0`.
It would be nice if the calling code could pass a string with context for what is being read. That way we don't lose information like: ```java if (buf.hasRemaining()) - throw new KafkaException("Failed to read magic byte from FileChannel " + channel); ``` For the case above, we could pass "magic byte" and then the error message would read: "Reached EOF before filling the buffer with magic byte..." or something along those lines.
I don't think we need the `null` checks.
@throws IOException If an I/O error occurs. See {@link FileChannel#read(ByteBuffer, long)} for detailed information on the exception cases.
I don't think we need this `null` check either.
Maybe add at the end `until there are no bytes remaining in the buffer or the channel has reached end of stream`.
Nit: "The file channel position..."
I am not sure what you mean by `test multiple reads` here. I think we're testing two things here: 1. That the first read doesn't cause a side-effect to the channel that could prevent the second read from succeeding. 2. That we can read into a smaller buffer than what's in the file channel. "multiple reads" is a little unclear as it sounds like we are causing the underlying file channel to do multiple reads, which I don't think we are.
Seems like `assertFalse` would be more appropriate here. There are a few cases like that. Also, it would be good to verify the buffer contents.
It would be good to verify that the buffer contents are correct as well.
Seems like this is the same in `testReadFullyOrFailWithMultiReads`. Maybe we can extract it to a helper method.
Is this name correct? It seems like the buffer is never filled in this test.
It would be good to verify that the the buffer is populated correctly.
We should include `startPosition` in the message.
`must` -> `can be specified ... if the no-arg constructor is called and hence it is not passed during initialization`.
You can click on the below links named "Details" to see the failure message. Also try using `./gradlew checkstyleMain checkstyleTest` to check locally.
Will this ever happen? I think `Utils.newInstance` is guaranteed to give you an instance of the passed in `class` type. Ditto below.
I looked at it, and I think the scenario is a bit different in `public <T> T getConfiguredInstance(String key, Class<T> t)` as in here, where `Utils.newInstance(Class<T> c)` is called and the passed `Class<?>` is directly from the config, where in this case the config value is a `String` and we directly use that in `Utils.newInstance`.
OK, let's keep the change to that one field for now.
As above for this and the next ctor
As above for this and next ctor
Why add this and the next ctor? They aren't used anywhere. We should add them when/if they are needed
There is a `KeyValue` class that we can reference here.
Yeah, figured this out the hard way when I tried to implement it. Still feels like there ought to be a simpler pattern, but I'm appeased for now  .
Mentioned offline, but it would be good to understand the need for these fields to be `volatile`. It seems that the use of the latch is sufficient to ensure visibility from the memory model perspective.
What makes this difficult to follow is that `value()` depends indirectly on the fields set in `produceFuture.set()` above. I think this is ok here, but I'm wondering if a separate refactor could make this less obscure. Something like this perhaps: 1. Pull `ProduceRequestResult` out of `FutureRecordMetadata`. 2. Pull the latch out of `ProduceRequestResult` and into `RecordBatch`. 3. Each instance of `FutureRecordMetadata` can have a reference to the latch instead of `ProduceRequestResult` 4. Make `ProduceRequestResult` immutable and only construct it when the result is ready. 5. Add a `FutureRecordMetadata.complete(ProduceRequestResult)`.
Not from this patch, but it seems we could make this field `final` if we pass the topic partition into the constructor.
What is the value in separating the `set` and `done` methods? The one place where I can see that they are separated by some other code it looks like moving the `set` next to the `done` doesn't affect the tests at all.
Ok. Makes sense. And for compaction, windows do have the same start timestamp, so the topic does not grow infinitely anyway (same as for non-windowed KTable).
I guess doing this incrementally will not work. What about one (or multiple) PR that add `final` wherever possible over all files (maybe package by package to split it up in multiple PRs)? It's dumb work, but I would volunteer... If we do in incrementally, it also distracts from actual code changes for a PR and add noise that makes reviewing harder.
I know this is a bit opinionated, but ... I think we should make an effort to make all locals `final` where possible. It is just a few extra keystrokes (that intellij can do for you!), and it helps to eliminate a class of bugs.
I am not sure if it would be better, but it feels like this is the kind of tweaking of templates that should be done in the templates themselves based on forwarding parameters to the template via the class members. In other words, if there are bits that should be swapped out, maybe we should encode those as methods on this class or simply conditionals in the template instead of doing regex search & replace here.
I was going to leave this, but since there's one other change to be made, I think you can just do: ```java recordsLag.add(this.metrics.metricName(name + "-max", ``` And similarly for `avg`. That avoids having to compute `toString` on the topic partition again.
Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
Is the check (madeProgressLastPoll && dataInBuffers) necessary? dataInBuffers is caused by no memory in the memory pool. It seems that it's simpler to wait for the default selector poll time, which is what we do when the pool is out of memory in other cases.
Would it be simpler to check channel.isMuted() instead of channel.isInMutableState()? Then, the latter can be a private method in KafkaChannel.
So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read.
previous message => previous receive
Selector is shared between client and server. So, it's better not to mention server here.
@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states: 1. Handshake 2. Authentication 3. Waiting to receive a message (receive == null) 4. Received partial message size (receive != null, buffer == null) 5. Received size and partial message body (receive != null, buffer != null) 6. Muted after receiving size due to OOM 7. Explicitly muted 8. Disconnect The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState` is unnecessary if muting is done on allocation failure and that makes the code simpler.
It's just more proactive, right? If you're out of memory, you're not going to be able to do anything (beyond the handshake) on any channel anyway. I think the value is that instead of going through a more polling unnecessarily only to end up muting all the channels, you can just do so immediately.
Similar to the mute in `poll()` - the mute could be delayed until a buffer needs to be allocated? It is possible that the channel already has a buffer allocated, in which case, we want it to complete read.
Not sure about this. `SslTransportLayer#hasBytesBuffered` returns true if there is any data in `netReadBuffer`. If more data is needed to unwrap and no data arrives from the client, I think the handling of `keysWithBytesBuffered` results in a tight polling loop with timeout=0.
I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated.
If this code was only called from tests, then channels would remain in `explicitlyMutedChannels` forever :-) It is actually called by the broker - mute/unmute to control reading from the channel and hence the need to track explicitly muted channels.
You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
Since`keysWithBytesBuffered` was cleared earlier, it needs to be populated regardless of the status of staged receives. I think `"if(..) { keysWithBytesBuffered.add(..); }"` should be done outside the outer if that checks staged receives.
The current implementation of `addToCompletedReceives` moves receives from staged to completed state if the channel is not muted. I think it will better to replace `!channel.isMute()` with `!explicitlyMutedChannels.contains(channel)`. Buffers have already been allocated for the staged receives, so we should allow them to make progress and release the buffers.
I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state).
There are two if statements - one just above this one sets timeout to zero and that needs to check `dataInBuffers`. This one is just unmuting and resetting `outOfMemory` flag. Not sure why this needs to check `dataInBuffers`.
Nit: rename to `shouldThrowOnInvalidTopicNames`
this four lines can just be one line: `Topics.validate(topicName)`
Nice tidy up of this test class :-)
Nit: `"... DelegatingPeekingKeyValueIterator"` -> `"..." + getClass().getName()`
add `final` (also all other methods below)
would be nice to mark all params final whilst we are changing this
I'm not convinced we gain anything by adding another level of inheritance here. It removes one method that doesn't do much anyway. My preference would be to leave it as it was. If we are going to use inheritance then i think we should keep the hierarchy as shallow as possible.
I am confused. If we change `store.range(hello, "zooom");` should `hasNext()` not return `true` (start and end are inclusive). Thus, to me it seems the test is wrong and there must be a bug in the iterator code.
"InMemoryKeyValueIterator" -> `getClass().getName()
Ack. Overlooked this change :)
Why do we need "duration" or "maintain" ? Can't we say just ...the window retention time...
max timeDifferenceMs later than -> at most ...
Same here and below.
nit: typo "ths"
This adds an additional `get` when compared to the previous solution.
Not sure why this is being changed.
`lesser` -> `less`.
The idea is to not mutate the map after assignment, so the previous code was fine.
nit: if you use `Utils.mkSet`, then you won't need the conversion below.
Could we have one more test case similar to this, but in which the paused partition already has a position? This verifies that `updateFetchPositions` does not overwrite the current position.
Can we make these fields final? I know they weren't before, but as you are changing this
Does this need to be abstract? Can't we just have the impl of it here? i.e., the `SessionWindow` and `TimeWindow` implementations are identical.
why does this message differ, saying "short / string", but the rest just state the type. I think the string part is superfluous.
About `ProcessorContext`: yeah definitely not for this PR, just throwing it out here for discussion. About `punctuate`: that is a good point, and it makes me thinking if we should just change the return type from `R` to `void` then (and I know we need a KIP for that..).
I was thinking we need to do something about the `ProcessorContext`, too. The interface is quite broad. I think what you have suggested is ok, but I'd probably like to see `register` moved elsewhere, too. At the moment the `ProcessorContext` is accommodating initialization and processing, which means we have methods on it that aren't valid in all circumstances. IMO it would be nicer to have more focussed interfaces that make it less likely that you can do something that isn't allowed. The interfaces can all be implemented by the same object, of course.
I think this should be four different tests. One for each of the methods you are testing. I probably said this before, but it is much nicer if you can just read the test names to understand what should/shouldn't be happening
I'd probably extract this to an inner class. I just find it a bit unwieldy having an anonymous class of this size. I find it a bit distracting. But i'm not overly bothered either way. Just a suggestion
Should we throw an exception here? A ValueTransformer should never call this.
I stand by what i said, but I'll leave it up to you. It isn't a deal breaker for me!
This is the JIRA describing the motivation, just FYI: https://issues.apache.org/jira/browse/KAFKA-3519
nit: use string interpolation. Also probably good to add the groupId to these messages.
I'm not sure this works. The purpose of the `joinedSubscription` field is to remember the exact list of topics that were used when joining the group. If a metadata update arrives after the rebalance has begun, then we can notice the fact that the joined subscription does not match the current subscription and we can trigger another rebalance. With this change, we will no longer be able to detect this case, which means that consumption from a topic matching the subscribed regex will be delayed (perhaps indefinitely). It seems the behavior we want is to only add to `joinedSubscription` those topics which were added to the assignment by the leader.
@guozhangwang A metadata fetch could return in the middle of a rebalance (i.e. with the JoinGroup on the wire). In this case, we may end up discovering a new topic that we didn't know about before and adding it to our subscription (see `subscribeFromPattern`). We are able to catch this case currently because although the subscription has changed, `joinedSubscription` has not. With this patch, this is no longer true.
> Just clarifying: After the group has formed, both leader and follower can still trigger a rebalance: leader will trigger a rebalance if the existing topics number of partitions has changed (including the topic is deleted); follower will trigger a rebalance if the subscription has changed (both due to a metadata refresh with regex pattern or user called subscribe again). Is that right? Yes, right. > And if we change the consumer coordinator to allow passing regex to the leader, I think joinedSubscription can be removed completely and only leader need to trigger rebalances unless users call subscribe again on any of the consumer member, is that right? The leader will still have to deal with the potential for a metadata update during a rebalance, so I'm not sure we can remove `joinedSubscription`. At least we won't need this funky logic to try to change `joinedSubscription` after the rebalance though.
Add to the end, "as long as they still match the subscribed pattern"
nit: "is" -> "were"
I'm puzzling a bit over whether we should be using `joinedSubscription` here instead of `subscriptions.subscription()`, or whether it matters. Seems it should be `joinedSubscription`. Suppose that `joinedSubscription` contained only [A] at the time of rebalance. Topic B is then created by the leader and assigned. Before the rebalance completes, the consumer discovers topic B, and `subscription` is updated to [A, B], while `joinedSubscription` is still [A]. The consumer then receives the assignment containing partitions from both A and B, but `addedTopics` will be empty (since B was already added to `subscription`). The consumer will then notice that `joinedSubscription` is [A], while `subscription` is [A, B], and request an unneeded rebalance.
`ClientRequest.destination` is the broker id.
Same as above: need to check `clientResponse.hasResponse()`
This probably needs to check `clientResponse.hasResponse()` otherwise it could throw `NullPointerException`
nit: this log message doesn't seem too useful
I see @mjsax has already a PR for that, great!
This would unfortunately break the python script, but it's ok i can fix that. Thanks.
In line 58 above, in `recordLatency()`, we need to mention that `If the passed sensor includes throughput metrics (e.g. it is create via the XX function), the throughput metrics will also be recorded as a single occurrence of this event.`
nit: `if (advance <= 0 || advance > size)`
nit: this can be a single line.
I don't think this one needs to change. AFAICT it is only `transform` that has the issue
When some expected external topics do not exist or do not have metadata updatable, current strategy is that Streams will not create any tasks of its "affected" sub-topologies, including the changelog topic and repartition topic as well, and will expect another rebalance to trigger once these topics shows up. If the source topics are input ones for the "head sub-topologies" then no tasks will be created and no internal topics will be created.
If that was the case wouldn't we get an exception due to the topics not being co-partitioned? We should have a test for this case.
seeing as you are restricting visibility (thanks!) this one could be private
This is an expensive check. It needs to be done only at the start of a connection, not for every receive. I think it would be better to trigger the check from one of the channel/transport layer classes.
This and the read below. There is no guarantee that either of these non-blocking reads is going to complete here. So the checks would only work depending on which reads complete.
and -> a
base -> based progress -> progressed
and -> a
nit: We use a single space after a period throughout the rest of the code. It would be good to stay consistent.
Yes, this looks good to me.
Seems to be covered by `shouldAssignMultipleReplicasOfStandbyTask()` already
As above: add more "randomness"
Nit: line too long
Thinking about this, I guess we could improve `StickyTaskAssinger`. If I am not off, load balancing on stream basis is not optimal -- but I am also not sure if the effort to improve it is worth it... If we extend this test to assign more tasks, let's say 12, client `p2` will get 7 tasks assigned and `p1` get 5 tasks assigned (while it would be better to assign 8 tasks to `p2` such that all 3 thread get 4 tasks each). The problem is, that the capacity factors are not considered: `p2` should get twice as many tasks assigned as `p1` -- but the algorithm says only "more" -- and this more is determined be the diff of the capacity (ie. in this case `p2` will get at most 2 more tasks assigned than `p1`. Or maybe my analysis is wrong (I did not run the code and step through it.)
Thanks for clarification. Does make sense now -- I did not consider the "task pairs" heuristic.
Any thoughts about this @dguy @guozhangwang -- this is a different issues than the newly created JIRA (even if we might be able to subsume it with it). The issue described here would also be there, if we only assign stateful standby tasks.
`hasItem(task01)` -> `equalTo(Collections.singleton(task01))`
Can't you use `createClient`? This call confused me... (same below)
Just wondering if this test is good. Seems like a "RoundRobinAssigner" might compute the exact same assignment. Some more "randomness" would be good IMHO. (Not a must though.)
It's not about capacity, is it? It's about having not task that does not have the task assigned (as active or standby). -> `shouldNotAssignStandbyTaskReplicasWhenNoClientAvailableWithoutHavingTheTaskAssigned`
Does this have to be a different variable name than line 131 above? Same below.
This line (314) could be renamed to `expected Key` as well.
A record. Same everywhere else An record is mentioned
an -> a
an -> a
records to it, and reading all records from it, such that
We should actually remove the reference to `KTable` as we don't currently support it
I know this is a bit picky, but we (mostly @mjsax and i) are trying to encourage everyone to use `final` where possible
We could use `@Test(expected = LockException.class), ditto below.
use `try-catch` instead of `expected` annotation -- not a single line test.
Maybe we should clarify that we invoke the callback and deallocate after the iterations (because we close the batch here, one could say that we have done part of what one may consider expiration).
nit (sorry): seems ungrammatical when the error message is appended. How about this? ``` "Expiring " + recordCount + " record(s) for " + topicPartition + ": " + expiryErrorMessage) ```
Ah, you are right. Sorry.
How about `completeExpiration` or `expirationDone`? Also, do you think we should add safeguards to ensure that the batch can't be completed more than once? Maybe at least we can add an assertion that `expiryErrorMessage` is null in `RecordBatch.done`.
Should we close the task first before re-initialize it to another StreamTask? Ditto below.
Could we just initialize the task in `setUp` with the testCache so that we do not need to explicit re-initialize it here? If not we need to at least close the task before re-initialize it.
Unify "create task" code with `shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseTopology` -- it's almost the same and both test cases can use the same topology structure.
wrap with `try-catch` instead of `expected` annotation -- more than one line test.
Nit: somehow I don't like `blah`. Otherwise LGTM
Should this be simply `builder`? The current name sounds like it builds the definition.
Nit: in `parseValue`, we changed this to `NO_DEFAULT_VALUE.equals(key.defaultValue)` due to findBugs warnings.
Maybe we should say "and will be removed in a future major release". That makes it a bit less ambiguous.
Since it's tied to the deprecated name, the error message should probably include both the deprecated and new name. Otherwise it'll be more difficult to track down the source of the conflict.
Nit: space before `:`.
As you said offline, this is really getting out of hand. Should we introduce a builder of some sort? We can do it in a separate PR, but it would be good to do it before the next release so that we can remove the two overloads that have been introduced.
It seems like we lost the case where we should set the value to the `key.defaultValue`. You more thoroughly handle other cases (deprecated values, no default value), but I think this line was removed and should still be at the tail end of the branches that have been added to this logic.
Nit: space missing before `:`. There are other cases like this in the file.
Maybe we should have a line for the old consumer as well. Regarding your question about the cluster id, the following line needs to be conditional on `from_kafka_version <= LATEST_0_10_0`: ``` assert self.zk.query("/cluster/id") is None ``` The code is just checking that the cluster id is generated after an upgrade from 0.10.0.x or lower to 0.10.1.x or higher.
Nit: space missing before `timestamp_type`.
I miss @shikhar!
I thought so, but wasn't excited with the grouping and it's not a long line anyways.
I wonder if it makes sense to propagate optionality and default values when recursing. I.e. if a parent `Struct` was optional all the flattened fields resulting from it should be optional. And in the absence of a default value on a child field if there was a default parent `Struct`, use that `Struct's field value as default flattened field value.
nit: unneeded newline
Seems like a no-op
Seems to fit in one line
I see. I do not have a strong preference actually. But I remember we use singulars not plurals in many other classes and just wanted to be consistent. If it is actually the opposite case I'm happy to have them all to be plural.
{code..} than -> A client supplier that..
This line is a bit misleading, as it is referring subject and object to the same class. I'd suggest just remove this line.
nit: {@link KafkaClientSupplier}
For consistency: {@link KafkaStreams} instance
recommended; ditto below.
nit: reorder according to ordering of params.
Same for `addThroughputSensor`.
; note if brokers have version 0.9.0.x or lower, blah blah..
does always apply -> always applies
There are no ..
I meant to have all "singulars" for consistency, i.e * Creates a * Starts the * Shuts down this * Does a clean up I'm OK with imperative style.
Since this is usually the entry point where users would call `toString`, the indent is used for wrapping it. For example, `toString()` prints ``` Kafka Streams Thread Tasks Topology States ``` `toString(">")` prints ``` >Kafka Streams > Thread > Tasks > Topology > States ``` We do not need to let users override the internal ones, only the entry point of nested prints.
Nit: too many blank lines.
`fail` is not required. Maybe, it would be better though to have a try-catch around this (and use `fail`) and remove `expected` annoation (using `expected` should only be done for single-line tests).
maybe - `shouldNotAllowOffsetResetSourceWithDuplicateSourceName`
I did have a closer look into the code. You are right. I also double checked and `Serde` does actually not implement `Configurable` (so there will also not be two calls what would be bad). Sorry for the confusion -- and thanks a lot for pointing out that it is correct as is!!
Why do we need to augment this function here? I.e. by the time the stream task is created, we should have populated the `sourceByTopics` map with the pattern matched topics already, so I'm not sure if the additional computational logic is needed? cc @bbejeck .
Thanks for the explanation, makes sense.
@jeyhunkarimov Sorry for jumping in a little late. @guozhangwang is correct about the test. An example for unit-testing with regex defined topics using the `TopologyBuilder` is `TopologyBuilderTest#shouldSetCorrectSourceNodesWithRegexUpdatedTopics` (line 675)
`Serde` implements `Configurable` -- the object `o` in `((Configurable) o).configure(originals());` is the serde object.
It's actually pretty elegant! :)
Nit: Can you insert this method further down -- we want to order method overloads with regard to number of parameters -- it simplifies to keep track of what overloads are there.
This can be reverted.
Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)
The test is fine -- but what's the value in testing overwrite the default extractor two times.
guaranteed -> guarantees
topics -> topic. This may well be elsewhere in the java-doc, too
Make all params `final`
Here also, would be great if you could make the params `final`
And again with `final` if you don't mind
`final` ? All of the fields should be `final` really
This method can be package-private
We should make this `final`, too
+1 to what @mjsax said. The `source` should never be null. So you should change the `StreamThreadStateStoreProviderTest`. It just needs to have the topic name extracted to a field on line 73. And then that same topic name used on line 189 in `new TopicPartition(...)`
Nit: no "." at the end Please update everywhere.
remove "(if any)"
`return stream(null, null, keySerde, valSerde, topics);` Do the call directly instead of the cast.
update to `return stream(offsetReset, null, null, null, topics);` to avoid too many indirections. To this for other overloads, too, please.
This should be the only method with actual code. All other overloads should call this one.
should not have an implementation but call overloaded method.
Should not have an implementation but call overloaded method.
remove this line -- not required.
I'd also consider extracting: `source.getTimestampExtractor() != null ? ...` into a local as the line is quite long and it will make the code a bit easier to read.
No reformatting, please
No reformatting, please
keep this but rename to `defaultTimestampExtractor`
add line: `TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();` and change to `RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
Nit: "default {@link TimestampExtractor}[,] and" The rule is "A and B" (for two things no comma), but "A, B, C, and D" (for three or more things, use commas)
check `source != null` not necessary. In doubt add an assertion.
i'd probably extract lines 121 -> 130 into a method, i.e, `findSourceNode(...)` Also, we -> if
using `assertThat` is nicer as it gives better failure messages. `assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))` in other places, too
typo: kStreamhould... -> kStreamShould In fact i'd probably rename these methods to begin with should, i.e., `shouldAddTimestampExtractorToStreamWithOffsetResetPerSource` etc
as per previous `assertThat(..., instanceOf(...))` would be better
please fix this: `use {@link } instead`
Nit: add missing `.` at the end.
Nit: missing `.`
Nit: add `final` twice
nit: add `final`
Nit: add `final`
Nit: add `final`
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the "right" place. Thanks a lot. :)
For backward compatibility, we need to keep the old default value. Btw: we don't do any ordering here yet -- just above. so no need to reorder anything here.
As above: need to keep default value.
you can simple call `serde = defaultKeySerde()` here.
`.configure()` is called within `getConfiguredInstance()` already -- you can remove this line (I know this pattern was there before, but it's wrong -- can you please fit it :)) this can be a single liner within try-catch-block: `return getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);`
Make all locals `final`
Ack. By bad.
It would be nice if you made these `final` while you are doing this change.
There is no need to test this as it is calling the same method as above.
I'm not sure what this test has to do with `StreamTask`? To me this test should be in `TopologyBuilderTest`. You don't need a `StreamTask` in this case to check that the `TimestampExtractor` was assigned to the source
I just realized, that we use different wording for topics as array of Strings and topic pattern: "there is no ordering guarantee" vs "there are no ordering guarantees" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO.
Sorry -- mixed it up with `table`.
key -> topic
I am still confused, about `source` being `null`. In the original code (L121) `source` is handed to `createRecrodQueue` and must not be `null` -- because this was never an issues before, I am still puzzled. why it is now.
Can you please add `final` wherever possible.
add `final` wherever possible
apply `final` wherever possible (also within method)
What's the deal with the `name` attribute instead of `id`? From what I can gather about html versions, `name` isn't actually valid in HTML, even HTML5, and `id` is the correct attribute to use.
No need, if it's valid then the `name` version is fine. I'm probably just out of date.
This method is the source of a lot of debugging hell... Should we maybe consider logging each of the cases separately, with corresponding info about the decision (i.e. whether `currinFlight == 0`, it's blackedout, etc)? It's beginning to look like the more info we have on this path, the better.
nit: an object -> an object has associated partition offsets that can be ...
Yes. Got confused about the correct order...
nit: remove `this` and move one line down -- first initialize with parameters, than everything else
add `final` to all
what about the case that both if's are `false` -- can this happen? If not, we should throw an exception
nit: maybe it's more helpful to use the error directly here since no one knows the codes.
We can probably output the error instead of the error code (the field name should be changed appropriately.
Similar questions below.
Is it possible that `key.compareo(endKey) > 0`? Just trying to not hide any potential issues with relaxed condition.
Why this and other 4 tests below cannot reuse the `runGenericBenchmark` common function? It seems it is not necessary to use a separate thread for these tests to distinguish them from others that can leverage this common function.
Same here - this block looks the same as other blocks of code. Perhaps extract into a method used by all
We should also validate (in our tests) that the user cannot modify the underlying stream because, as I understand the proposed semantics, "an unchanged stream" is an invariant we want to guarantee as part of the contract.
Nit: add `<p>` to get an empty line -- otherwise, generated HTML does not render correctly. Same below.
please add `final` for parameter and wherever else possible (i.e., also for vars within this method, and all other classes you add in this PR)
IMHO the current docs don't make it clear what the difference to e.g. `map()` is.
@miguno Actually, we do not enforce this guarantee. For many UDFs, we just hand in a reference to key and/or value and if the user would alter the object, we would not notice...
I'd suggest to use a more descriptive test name, e.g. in the form of `shouldDoXYZ`.
You should also compare `expectedValues`.
Sorry for that -- You are of course right. `final` only for iterator loops...
add `final` twice
use `try-catch` plus `fail()` for this line and remove annotation `@expected` -- right now, for example, a bug in `builder.stream` raising `NullPointerException` would not get detected. ``` try { stream.peek(null); fail("Should not allow null Action"); } catch (final NullPointerException e) { // expected } `@expected` should only be used in "single line test" for which it is clear that no exception can happen before the actual tested code.
add `final` twice
Nit: remove `this` (we try to avoid `this` wherever possible)
you don't need this. Junit gives you a new instance of the test class for every test method
initialize the `KStream` here and make it `final`
This can be initialized here and be `final`
It would be better to do the assertion in the test rather than here. It will make the test clearer.
I don't think we need this. You can just inline the creation of the `List<KeyValue>` see above.
You can remove this `assertThat` and the loop below as you've already proven this is true in the test above. So no need to assert it again.
`topics created using through() method` -> `topics used in/by through() method`
Nit: remove empty line ;)
Can you please format this differently. No line should be longer than 120 chars and please start a new line after each sentence (otherwise reviewing is quite cumbersome). Thx
typo: `will does not`
`.` at the end missing
`usually` -> `default` (I hope that people *usually* change the default to avoid unwanted state recreating if `temp` gets wiped out :) And: closing `)` missing at the end.
`topics created with the through() method` as above
`[because] the tool`
`statestore` -> `local stores` (to use same terminology as above)
I would remove `will`: `This tool reset[s] offsets` same below
Seems like you want `try/finally` instead of `try/catch` here. It may be worth being conservative and having a wider `try/finally` and nulling out the outer variable if we have used the buffer successfully. Something like: ```java ByteBuffer allocatedBuffer = free.allocate(buffer); try { ... // after successfully using the buffer allocatedBuffer = null; } finally { if (allocatedBuffer != null) free.deallocate(allocatedBuffer) }
@satishd I re-read your code and I see that you are only de-allocating on exceptions instead of using a finally like in my suggestion. They both work (in my suggestion, we null the outer variable once we have used it to prevent it from being freed). One difference is that in your approach, we could end up calling `free.deallocate` twice if the first invocation throws an exception (which should not happen normally).
Seems like we can put this back into the previous line.
Can we not do: ```java if (appendResult != null) return appendResult; else { ... } ```
Sounds like a good idea
We don't have to do this here, but as I'm reading this, I wonder why we can't push this logic into the builder. For example, we can add a builder method `requireTimestamp(boolean)`.
Nit: you can also add `final` here: `for (final Map.Entry.....)`
Yes. It's not severely important, but it's the code style we try to introduce throughout the whole Streams code base: `final` everywhere if possible.
Nit: add `final` (please apply this wherever possible -- of course only code you change :) )
Could we have one warning log entry instead of multiple lines for a single exception? It will help with log file greps / etc I think. nit: `TopicPartition` / `OffsetsAndMetadata` classes have their own `toString` function that can be used, so we just need to use that, so printing the map itself should be fine.
Instead of showing the time-since-last-poll, should we have the max-time-since-last-poll and average-time-since-last-poll? These two metrics are more informative and stable than the time-since-last-poll since they are measured over a time window.
So we may have one extra `time.milliseconds()` for every poll. Not sure if this will be a problem. I recall that there used to be performance issue due to extra `time.millseconds()` in `consumer.poll()`. Do you remember this issue? But it seems we have to bear with this extra call in order to have this metric.
nits: not sure if we should `:` after `Invalid value` in the error message. Otherwise LGTM. Thanks for the update.
I personally don't think it makes sense to wait for the metadata to propagate to all brokers. A broker may be partitioned away from the controller, but not the clients, for example. I would prefer to make the consumer and producer smarter when they get stale metadata from a random broker.
I'm hoping KIP-117 will do that eventually.
it also depends on what you want to check. asking the controller will tell you what state brokers should eventually be in, checking any random broker will tell you what state that particular one is in now. i.e. if you want to make sure its fully propagated you'd have to check all the brokers.
Sounds fine then (what else could you do if you don't even know who the controller is :)).
The case where you should fetch the metadata from the controller is if you expect the metadata response to include the topic that was just created. For example, if you create a topic, it returns a success and _then_ you ask any broker for the metadata, it may be missing the topic that was just created (since metadata updates are asynchronous). I haven't checked the code to understand if this matters or not.
im not *100%* sure, but i think you want this to also hit the controller. i hit the controller in my personal client, and @cmccabe is doing it here https://github.com/apache/kafka/pull/2472/files#diff-7378a806dbf302c1e7a9098c4780d2a8R283
Also, `GzipOutputStream` doesn't seem to call `flush` on the underlying stream: ```java public void close() throws IOException { if (!closed) { finish(); if (usesDefaultDeflater) def.end(); out.close(); closed = true; } } ``` So, seems pretty safe to follow that example.
Regarding your latter point, I think you are conflating two different things. This `close` method does the same as `flush` in that it calls `writeBlock`. So it is correctly implemented. The thing that changed is that for the underlying stream, it calls `close` without calling `flush`. Anyway, we can be conservative and do a `out.flush()` before `out.close()`. I think that's clearer than calling the `flush` since `finished = true` already.
OK. So strictly speaking this patch changes behavior in the sense that `out.flush()` is not guaranteed to be called. I would have kept it just to avoid unnecessary problem in the future if I were to write this patch. I really don't think an extra line of `out.flush()` would be a problem. But since the patch is already committed, I would't bother to change it as well since there is very little chance this can be a problem.
To clarify: the PR hasn't been merged yet. Note that if this change were to cause a problem for LZ4, we would have the same problem for GZIP (due to the JDK implementation). In other words, we have to ensure that the underlying output stream has a good `close()` implementation anyway since we support GZIP as well.
The convention (even though not enforced by checkstyle) is for `parent.checkForest(sensors);` to go in the next line for the Java code.
Should this return a `DataInputStream` or simply an `InputStream`? Seems like wrapping into a `DataInputStream` is not relevant for this class and can be done by the caller. Same for the output case.
Nit: I think we should replace `authFailed` with `authenticationFailed` everywhere to avoid ambiguity (`authFailed` could be misinterpreted as `authorization failed`).
I'm a little uncertain about allowing overrides blindly. For example, I don't think we'd want to allow automatic offset commits to be enabled. On the other hand, it looks like we do apply worker-level configs blindly, so maybe it's fine.
The additional validation doesn't hurt, but it might be more natural to move this check into the `if` below since we don't actually cast unless the condition is true.
this could be: `assertThat(serializer.serialize(topic, null), nullValue())`
We have a rule w.r.t using `@expected` the rule is that it should only be used for single line tests where you can guarantee that the exception can only come from the line that is being executed. In all other cases you should use `try{...}catch(...)`
this one, too
these calls aren't needed, right? `close` does nothing anyway. And if they did do something, then you'd probably want to do it in `tearDown` rather than here.
make this into a different test, i.e., `shouldSupportNullInFloatSerde` or similiar
i'd prefer we use asserThat, i.e., `assertThat(value, equalTo(deserializer.deserialize(...))`
nit: please use `final` everywhere
I was also wondering how you ran into these NPEs. This is an internal class and the configs should never be null as there are default values.
Are we sure we want to make them required config for `KerberosLogin`? I noticed that we have actually defined default value for these configs in `SaslConfigs.addClientSaslSupport()`. These default values are used by producer and consumer. And if these configs should be explicitly provided by user, should we have a class that extends `AbstractConfig`, define these configs as required, and use this class in KerberosLogin to handle user-provided properties, in the same way that ConsumerConfig is used? This would allow us to throw exception in case of missing config using a unified mechanism.
Minor but I'm not sure if we'd prefer the builder's toString or the underlying request struct's toString as was formerly the case.
Thinking about it a bit more, it seems struct's toString is the way to go. It's harder to mess up by using the underlying request struct. Builder toStrings are updated manually while struct toString just traverses the underlying struct so it will automatically pick up wire format changes.
Instead of waiting in all cases, I'm wondering if it makes sense to check the result of `lookupCoordinator`. If the future has failed, then we backoff.
Out of curiosity, why do we do this here? In the normal case, we only update the position of the duplicate buffer, it seems.
I am fine -- it's not a critical code path (no offense, but I am even fine without the change)
Actually the version that takes an empty array is faster. The other option is to have a concat method that takes multiple arrays. We maybe even have that already in the utils classes.
@ijuma @original-brownbear Yes. It's an array because it's passed into `TopologyBuilder#connectProcessorAndStateStores(String processorName, String... stateStoreNames)`
`System.arraycopy` is quite a low-level API and easier to make mistakes. The way I'd write it if performance is no concern: ```java final ArrayList<String> stores = new ArrayList<>(); stores.addAll(Arrays.asList(storeNames1); stores.addAll(Arrays.asList(storeNames2); stores.toArray(new String[0]); ``` A matter of taste as you said though.
It makes a difference for code that catches `KafkaException` instead of `RuntimeException`. We'd have to check every usage to make sure it was OK.
Yes, it does. The copy constructor is used below. ```java Map<String, Object> result = new RecordingMap<>(values(), prefix, true); ```
I am not sure about this. It's a really dangerous method as it implicitly casts in an unsafe manner.
Why are we changing the exception type here? That could affect calling code. Also, if there's a good reason to do this, we'd want to include a message in the exception.
Ah. Right. I did miss that!
My bad, I hadn't noticed that the rethrown exception was actually different (subtle name difference). I blame the small screen on my phone. ;)
`InterruptedException` is checked, so we can't throw it.
Should this be a warning or even `info`? It doesn't seem like there's much for the user to do in this case.
Is this punctuate function necessary? Since we do not `schedule` in the init functions I think this punctuation will never be triggered.
Yep, i understand there are other `assertNextOutputRecord` methods. I'd probably replace them too (not in this PR!) . I feel using `assertThat` is better in this case as we are just comparing the `ProducerRecord`. I don't see the need to have a method with multiple assertions when it can all be done in a single assertion.
This can be removed if replaced with `assertThat` as mentioned above
Looks like these could be replaced with: `assertThat(driver.readOutput(...), equalTo(new ProducerRecord<>(topic, null, timestamp, key, value)`
I have been debating whether I should change this every time I saw it. :) I guess whoever wrote it liked the fact there was a regular pattern followed by each line.
you can remove the cast to `double` now that you are using `1024.0` same below on line 737
I think we can call the one-parameter `produce()` in line 144 above as well
We don't allow a third option [elsewhere](https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1288-L1292) in the code, so probably we shouldn't here either.
adding `final` in every argument here was just noise but with not much added value. Given that we don't write constructors like this in Connect I think we should revert and add the new argument without the `final` specifier.
Should we remove this class? Currently they can only be 2 types of connectors, source or sink connectors. This seems to introduce a third implementation of the `abstract Connector` class, but testing seems sufficient without it.
```suggestion WorkerSourceConnectorContext(OffsetStorageReader offsetStorageReader) { ```
I'm not convinced we should allow this
Nit: let's avoid unrelated line additions.
Can you enclose the body of if block with curly braces ? Same with else block. It is easier to read.
Why use the delegate? Why not just call the methods on the `WorkerConnector`'s fields from within the `SourceConnectorContext` and `SinkConnectorContext` methods? E.g., @Override public void requestTaskReconfiguration() { ctx.requestTaskReconfiguration(); }
I know this line hasn't been changed, but the `this` variable refers to the `ConnectorContext` that does not have a `toString()` method. Might be better to refer to `WorkerConnector.this` instead, so that it behaves like old line 87.
At one point these were anonymous classes, and the delegation perhaps made a bit more sense. Now that they are inner classes, it seems like it would be a lot less confusing and simpler if the `DelegateToWorkerConnectorContext` class were extended by the `DelegateSinkConnectorContext` and `DelegateSourceConnectorContext` classes. Doing this has several advantages: 1. removes nesting/delegation and eliminates the chance of getting into an infinite loop 2. eliminates duplicate code in these classes 3. simplifies the context classes quite a bit 4. makes it more obvious that the sink connector context has nothing extra over the base 5. makes it more obvious that the source connector context only adds the offset storage reader 6. simplifies this code a bit (see below) By keeping `DelegateToWorkerConnectorContext` class non-abstract, we're actually able to maintain backward compatibility by calling initialize when the connector class is neither a source or sink connector: This code becomes simpler, too: ```suggestion if (isSinkConnector()) { SinkConnectorConfig.validate(config); connector.initialize(new DelegateSinkConnectorContext()); } else if (isSourceConnector()) { connector.initialize(new DelegateSourceConnectorContext(offsetStorageReader)); } else { connector.initialize(new DelegateToWorkerConnectorContext()); } ``` This seems a little strange, but we're actually not checking whether the `Connector` instance passed into this `WorkerConnector` is only an instance of `SourceConnector` or `SinkConnector`. If it is neither, prior to this change the framework would still initialize it, and I think we should maintain that arguably strange behavior just to maintain backward compatibility.
Is it possible to trigger infinite loop: raiseError -> reconfig -> raiseError -> reconfig ...
Changing the test to expect a failure.
Good call. I'll update the code to throw an exception like in `Worker` when creating a source and sink task.
`sumCapacity` can be removed as well.
nit: add `final`
nit: add `final`
Nit: keep the text of L672 and here in sync.
Nit: use `{ }` for the loop body.
This is not required as contained in the check next line.
Nit: go with single parameter per line.
We should mention it gets deprecated by two functions: 1) to get the static full topology description, one should use `Topology#describe()`; 2) to get the runtime thread metadata, one should use `#localThreadsMetadata()`;
`final` is for the var `activeTasksMetadata` (not for the method). We try apply a "use `final` whenever possible" policy. It's just some nit.
Nit: Maybe `Use {@link #localThreadsMetadata()} to retrieve runtime information.`
Nit: maybe `Returns runtime information about the local threads of this {@link KafkaStreams} instance.`
Can't we simplify this to: ``` for (final StreamThread thread : threads) { threadMetadata.add(thread.threadMetadata); } ```
Nit: add `final`
nit: add `final` We try to use `final` where ever possible. Please also add below (including the for-loop).
Good point. We should keep app.id for the log messages. So only add `final` in line 291 and apply `threadName.replace(applicationId + "-", "")` in line 300.
I guess, we can get rid of local var `threadName` and just use `threadClientId`.
The duplicate `appId` as mentioned in the JIRA comes from `threadName` (not `clientId`). As we add `appId` in the call to `super(...)`, it is contained in return value from `getName()` (line 291). Thus, it might make more sense to remove the duplicate in line 291 (we should also remove the added `"-"`, see below). Please also make the variable `final` in line 291: `final String threadName = getName().replace(applicationId + "-", "");`
nit: add `final` to parameter (there are some more like this -- please update all).
I'm wondering if we can not add this method and slightly modify the one above it that is just about identical? i.e., this is just a thought but i think it could look something like this: ``` interface SourceCollector { void collect(final SourceNodeFactory factory); } private void findSourcesForProcessorParents(String[] parents, SourceCollector sourceCollector) { for (String parent : parents) { NodeFactory nodeFactory = nodeFactories.get(parent); if (nodeFactory instanceof SourceNodeFactory) { sourceCollector.collect((SourceNodeFactory) nodeFactory); } else if (nodeFactory instanceof ProcessorNodeFactory) { findSourcePatternsForProcessorParents(((ProcessorNodeFactory) nodeFactory).parents, sourceCollector); } } } ```
nit: space after `//` and also de-capitalize the first word.
Do we lose anything if we use `Set` instead of `Collection`? Seems like set is the right semantics.
It's a fair point that `Cluster` is public and we should be careful about what we add there.
I was wondering about that too. It seems consistent with the other things we have there.
Nit: add braces.
mvn, got some answers on SO..
Blank line can be removed.
Maybe we should deprecate this method in favour of `partitions` with an explanation.
I think this `if` block can be written this way too so we can avoid the extra nesting (since Java performs short-circuit evaluation for `&&` and `||`): ``` if (subscriptions.isAssigned(tp) && subscriptions.isOffsetResetNeeded(tp) && !resetOffset(tp)) failedPartitions.add(tp); ```
Rather than reset estimation in `Sender`, would it be possible to do that here? Reason being - the actual splitting isn't explicitly done. It is done (IIUC) by virtue of the fact that the estimation has been reset. So it would be clearer if that logic is fully contained in this method. I haven't looked closely enough to verify that this would still work, but the point remains that the `split` is effective only if the estimation has been reset (again, IIUC).
It would help to add some detail to the method name to highlight this.
Nit: would generally prefer to see this made explicit (i.e., `drain` twice) as opposed to loop (which in case of some bug could in the worst case be non-terminating).
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
Can we also have some boundary case tests for the other branches in that method? Since it's not obvious that `now` can be before the other values, it's good to have test cases for that.
@hachikuji is suggesting that `requestIterator` should return an empty iterator instead of `null` to be consistent with the other methods.
Seems there's no need for this method to declare `ConcurrentLinkedQueue` as the return type. You can use a normal `Collection`, which would then allow you to use `Collections.emptyList()` instead of pointlessly creating an empty queue.
Re-reading, he did say that, but I think his main point is that it would be good to be consistent. :)
It is a shame we have to do it like this, but i don't see a viable alternative
anyways this could be in a separate PR just throwing it out for discussions @sjmittal @dguy @mjsax We can merge the PR as is if other reviewers could give a thumb up.
I think we can just use `consumerConfigs.getInt(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)` will automatically return its default value if it was not set.
Ack. Will have a detailed look into this when doing KIP-129
I think there is a small risk that if the lock was kept by someone else for more than rebalanceTimeout but it was released later, then this task will be "lost" as it is not picked by anyone. But I think it will be covered by KIP-129 anyways. cc @mjsax
In `StreamsConfig` we do populate the map from consumer / producer's default values first then use user specified values to overwrite, so it should be safe to `config.getInt` where `config` is of type `StreamsConfig`.
https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L453 Note that inside the function it calls `originals()` which is parsed on the consumer config in which default values will be auto-filled when it is not specified by the user.
Rather than this i think it would be better to add: `tempConsumerDefaultOverrides.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, "300000");` to the `CONSUMER_DEFAULT_OVERRIDES` map in `StreamsConfig`
I'm not sure this is correct. Lets say we have 2 tasks. Currently, once the first task is successful we will sleep for `backoffTimeMs`. This shouldn't happen, i.e., once the task was successful `backoffTimeMs` should be set back to the initial value and we shouldn't sleep before we attempt to create the next task. We need to track how long we've been rebalancing in total and use that to determine if we should give up (rather than the `backoffTimeMs` In the event that it times out i think we should probably throw an exception so we leave the group, the thread shuts down (releasing any acquired locks), and some other thread can pick up the work.
Sorry - i'm reading it wrong. I was failing to see that the backoff is done in the outer loop. Anyway, i still think the `backoffTimeMs` should be reset after each successful task creation. As for tracking the total rebalance time... we could just get the time at the start of `onPartitionsAssigned` and make sure that `MAX_POLL_INTERVAL_MS` hasn't passed since then. Yes you probably could rely on the `CommitFailedException` to be thrown by poll. My concern is that we carry on for some reason and some tasks are never processed. In which case it would be better to find a way to bail early so they can be re-assigned.
It seems like we are not totally consistent in the naming convention: `readRawUnsignedVarInt` versus `readRawVarUnsignedInt` below.
This is a security risk because the right bit pattern can cause an infinite loop. We need to stop after a certain size. Same for `sizeOfVarlong`.
My bad, the parameter is just an `int` and not a buffer. Thanks to @hachikuji for correcting me.
Do you need this `int` cast? I thought I removed that in my branch and it worked fine.
Since the type expected by `writeByte` is `int`, it seems like the previous code was OK? Otherwise we do two conversions.
I was checking the code in the protobuf repository and it turns out that the optimised version of this code is pretty complex: https://github.com/google/protobuf/blob/master/java/core/src/main/java/com/google/protobuf/CodedInputStream.java#L2473 I think it makes sense to start with the simple implementation, but we should file a JIRA for considering the optimisations in their implementation.
Can this be a `byte`.
We should probably say `update the buffer position` instead of `update index`. Same for other similar cases.
I think `update the index` could be made clearer since the `DataInput` interface doesn't mention an index. Same for other similar cases.
Is the `& 0x7f` needed here? The protobuf code doesn't seem to do that: ```java if ((value & ~0x7F) == 0) { buffer[position++] = (byte) value; totalBytesWritten++; return; } else { buffer[position++] = (byte) ((value & 0x7F) | 0x80); totalBytesWritten++; value >>>= 7; } ```
Actually I think #2456 does not update the printed html for marking these as deprecated, so we still need to rebase + merge this PR after that one is merged.
You probably need to do the same in `toEnrichedRst()` and `toRst()`
I think marking it as deprecated in html would be better than skipping it. So if #2456 is definitely going in we can close this PR then.
)by -> ) by (space needed)
We don't need the BUFFERPOOL prefix here since it's already defined in the `BufferPool` class. Thanks for the PR, will hopefully review it soon.
Yeah, it's not pretty. However, reducing the concurrency of `BufferPool` in the common path is not desireable. We definitely need to handle OOMs correctly, but they are relatively rare and it's OK if that path is slower.
If an OOM is thrown from here, it seems the available memory would be already decremented and not added back.
We can probably remove this line `restoreAvailableMemoryOnFailure = false`.
The name `restoreAvailableMemoryOnFailure` is a bit weird because we should always restore available memory on failure. Maybe we can name it `hasError` and set it to `false` right before `return buffer`.
I am not sure it is useful to have a static variable for this string since it is used only once. It is also different from the code style in `Sender.SenderMetrics` which assigns sensor name directly. Maybe we can preserve the existing style for simplicity and code style consistence.
Good point. I think it is still worth keeping the optimization, although typically the the producer will only allocate poolable batch size, so the actual memory allocation should not happen very often.
Should we also check that ``` final long start = SessionKeySerde.extractStart(bytes.get()); final long end = SessionKeySerde.extractEnd(bytes.get()); return end >= from && start <= to; ``` Although the current impl of RocksDBWIndowStore naturally checked that for us, it does not guarantee all underlying store impls guarantee that.
@guozhangwang @dguy we cannot guarantee that all the entries for one key will necessarily precede the entries for the next key. The following code still fails with this patch, and only returns `0001` and `0003`, since the key for `("a", "0005")` will come after the key for `("aa", "0004")` ``` final RocksDBWindowStoreSupplier<String, String> supplier = new RocksDBWindowStoreSupplier<>( "window", 0x7a00000000000000L, 2, true, Serdes.String(), Serdes.String(), 0x7a00000000000000L, true, Collections.<String, String>emptyMap(), false); windowStore = supplier.get(); windowStore.init(context, windowStore); windowStore.put("a", "0001", 0); windowStore.put("aa", "0002", 0); windowStore.put("a", "0003", 1); windowStore.put("aa", "0004", 1); windowStore.put("a", "0005", 0x7a00000000000000L - 1); final List expected = Utils.mkList("0001", "0003", "0005"); assertThat(toList(windowStore.fetch("a", 0, Long.MAX_VALUE)), equalTo(expected)); ```
This is a slightly different test. The window is much larger to ensure all the entries are in the same window and the last timestamp for (a, 0005) is such that it causes the resuling composite key to come after the one for (aa, 0004)
Note that `WindowStore.fetch()` should return `WindowStoreIterator` where the key is `Long` indicating timestamp and `value` is the value.
Could you point me to the constructor of `<String, Integer>`? Can't found it in this file..
since parallelism in streams task based, this would create a lot more RocksDB threads than available processors if each task maintains its own state store.
Should this change not be more "radical" calling `KStream#peek()`? In order to resolve the `PEEK_NAME` vs `PRINT_NAME`, we can add a private `peek()` that take the name as parameter.
The importance of the topic name depends on the serializer being used. For example, if you are using an Avro Serde with the Schema Registry, then the topic might be the subject name. So in this case it is quite important.
The reason of this function is that we'd reply on the overridden `Object.toString` function to print it, and if the received key-value happen to be `byte[]` we'd like to deserialize it otherwise `byte[].toString` will just print user-unreadable stuff.
Part of the JIRA is to remove KeyValuePrinter class after it was replaced with `ForeachAction`
Also as stated in the JIRA, it is worth exploring whether it is cleaner to collapse the implementation of `KStreamForeach` and `KStreamPeek` into a single class, with an flag indicating whether to `context().forward(key, value)` after `action.apply`.
I wonder if this could be generalised further? Probably a broader discussion, but `KStreamPeek` is really the same as `KStreamMapValues` (there may be others). They just don't share a common interface for the action. I personally feel this would be a better rationalization of classes etc than combining `KStreamForeach` and `KStreamPeek`
No worries. All good. :)
At the moment, within `ForeachAction` the `context` is not accessible. Even if we have some plans to change this it does not help you, as we don't have any timeline for the change.
could this be a bit clearer? sth like: > both input streams need to use the same partitioner
it is preferable to have descriptive test names, i.e., `emptyIteratorAlwaysReturnsFalse` `emptyIteratorPeekNextKeyShouldThrowNoSuchElementException` etc
Can you split this out into 2 tests please?. One for each method that is being tested. Thanks
I did not find any `toString` in `RocksDBStore` so I added one in `Segment` (which can then expose the id, which is not something that `RocksDBStore` knows about).
Seems like it would be better to add a `toString` to the segment or superclass.
This could be final.
`Integer.toString` is a slightly more concise way of doing this.
I fixed this one to use the constant before merging.
nit: `final` also the next line
remove this and just put a `fail(...)` inside the `try{..}catch`
i.e., add `fail` after this line
Can you update this, and the above test, too -> remove `expected = ...` and use `fail`
Would you mind "fixing" this test to? i.e., using `@Test(expected = SomeException.class)` is bad form for tests with more than a single line. The exception could potentially happen anywhere and my be masking actual bugs. In these cases it is always better to use the same pattern as below, i.e., ``` try { performExceptionThrowingAction() fail("KABOOM! shouldn't get here"); } catch (SomeException e) { // pass } ```
To be clear: we only need to do this if `(!records.isEmpty())`, right? But I guess you are suggesting that it's easier to reason about if we just do it at the start of the method? Generally, calling that method should be cheap but since we have to acquire a lock after your changes in this PR, it may make sense to avoid calling it if not necessary. Let's see what @hachikuji thinks.
Yeah, that's what I mentioned above. Maybe we could keep the `disableWakeups` API, but make its behavior final? That is, once disabled, it cannot be re-enabled.
I was trying to remember since I know I did do it that way at one point. It could have been that we thought we needed `disableWakeups()` anyway for `ConsumerCoordinator.close()`, though we could pass the flag around as well. Maybe I just thought it was ugly.
I think I prefer the approach using the extra parameter in `poll()` if we can convince ourselves that it is safe (i.e. that there are no completion handlers which could either miss an expected wakeup or propagate one where it wasn't expected). I looked over the code and couldn't find any such cases.
Yeah, that's my preference. Not sure if @ijuma thinks otherwise.
The current solution seems more direct than what I was suggesting, which is good. A bit unfortunate that we have to expose `maybeTriggerWakeup()`. I guess we could keep the synchronization around `pollNoWakeup` that you've added and still do the `hasFetchedRecord()` approach. Probably it would look like this: ``` if (fetcher.hasFetchedRecords()) { client.poll(0); return fetcher.fetchedRecords(); } ``` The downside is that it `hasFetchedRecords()` may not be trivial to implement since it would require doing the work that `fetchedRecords()` is doing, but without updating the position. Given that, exposing `maybeTriggerWakeup` doesn't seem too bad.
`hasFetchedRecords` avoids the cost of populating the map returned by `fetchRecords`. No locking is needed for that. So, if I understood the suggestion right, it would look something like: ```java if (fetcher.hasFetchedRecords && client.hasPendingWakeup()) client.poll(0, ...) ``` I guess by calling `poll(0, ...)`, we don't have to expose `maybeTriggerWakeup()`.
i'm wondering if this one should be at `info` level? It would make it easier to find out when a thread has finished rebalancing. Right now it is a case of looking for some other message that happens during processing. Seeing as rebalancing shouldn't, in theory, happy that often, i think info would be ok.
Should probably raise a JIRA for this otherwise we will end up losing the performance gains that we got from batching everything else.
Making a copy is important as we can run into deadlocks if the user synchronizes on the received map. Someone actually reported one such issue.
Seems like we should just remove this method if it's the same as `Map.toString()`.
I'd keep `FileNotFoundException` in the signature as it indicates a specific error as opposed to the more general IOException
I think we should remove this method since it's no different than `join` and is only used in tests.
minor performance nit, it might be worthwhile to have something like SessionKeySerde.inWindow(bytes, binaryKey, from, to) to avoid creating three bytebuffer objects for each comparison.
Nit: add `final`
Nit: add `final`
Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
We really need a bug fix release for this! \cc @guozhangwang
For suspended tasks, could the closure process be simpler? For example we have already closed the topology as well as committing the states, etc. Ditto below.
nit: "active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"
nit: config is an overloaded term in the code, you might prefer to name this argument configInfos for instance.
Also no need to use upper case in words within the sentence. E.g. `@param configInfos config infos to read errors from` ` @param callback a callback to add config error exception to` `@return true if errors were found in the config, otherwise false`
Currently in this file the indentation style used is: ```java protected boolean maybeAddConfigErrors(ConfigInfos config, Callback<Created<ConnectorInfo>> callback) { ``` Still, once we move to single arguments per line it should be: ```java protected boolean maybeAddConfigErrors( ConfigInfos config, Callback<Created<ConnectorInfo>> callback ) { ``` I'd pick one of these. (First I confused `callback` for a local variable)
nit (optional): Since a variable is used as a shorthand for the return value of this method, maybe a more intuitive name would convey the meaning better. E.g. `hasErrors` makes more obvious it's a boolean (`result` can be anything).
No need to reorder imports.
No `final` needed. Also using names `configInfo` and `configInfos` respectively removes the need for abbreviations (here `cfg`).
nit: `err` is not used elsewhere. You may choose to iterate directly over the collection that is returned.
No need to reorder imports.
No need to reorder imports.
Same here, static imports need to go to the end of imports.
@kkonstantine The argument for `final` on method arguments is that you shouldn't reassign these variables (which is different from whether you mutate the values or not).
nit: i find it helps to space out the replayAll and verifyAll from other code as it helps visually break up the test into mocks/expectations and the method calls you are actually testing.
Method should not be `final`. Additionally the `final` keyword for method arguments and local variables is not required and does not improve readability of the code here. Indeed Java does not distinguish between readonly and read-write variables. But unless an anonymous class is declared (this requirement is removed after Java 8) or the variable is used further down in the code (improved readability) marking every single readonly variable as final does not make things better IMHO.
Kindly adjust your IDE's settings to order imports roughly as follows: ``` all other imports <blank line> import javax.* import java.* <blank line> import static all other imports ``` If you use IntelliJ this is something you'd add under: Pref->Code Style->Java->Import Layout or set equivalently for other IDEs. This way we won't shuffle imports around and we won't get diff noise, unrelated to this PR.
Blank line between description and params.
This can be removed as well. I'll do it before I merge.
Iterating over the entire list on every added field looks a bit suboptimal. Since we are using this in the Converters and we convert every batch of messages, this may have actual impact. Did you check what would it take to change the data structure of fields from List to Map (using field name as a key) and then use containsKey to check if the field already exists? It should be more efficient, but if this has too much impact on the code elsewhere, maybe it isn't worth it.
Be careful about changing this -- it should probably be a LinkedHashMap or some similar order-preserving Map if we do this. Depending on the serialization format, they may be sensitive to field ordering so being able to preserve the ordering will be important. Even within a single process a converter could end up randomizing schema field orders and breaking equivalence if an order randomizing map was used. The alternative would be to add an extra Set to SchemaBuilder, which requires extra allocations but doesn't require jumping through hoops to preserve the behavior of the `fields()` method.
We should probably mention in the docs that enabling idempotence will change the default configurations for retries and in-flight requests. I think it might also be worth adding at least a `debug` level log message in the code that we are overriding the defaults.
I don't have all the context, but isn't `3` pretty low? We don't do exponential back-offs, so the recommendation for no data loss is typically higher.
A more reliable way to check if a user explicitly sets the config is to check from config.originals(). Ditto for configureRetries().
Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all.
docstrings go below the class (unlike every other language). same with other changes in this patch
By the way, I think we have `Utils.mkSet` which does the same thing.
This doesn't seem to be used.
This doesn't seem to be used.
OFFSET_FOR_LEADER_EPOCH_RESPONSE is an inter broker request. So, we shouldn't add a throttle field.
Discussed this with @junrao. The main challenge with this option is having the top level error field for every response. This would probably affect a lot of code: 1. We would need to handle this top level error code everywhere. 2. A bunch of protocols that currently have a top level error code would no longer have them, so a bunch of code would have to be updated as well. So, it doesn't seem appropriate to do this as part of this KIP.
It's actually read and reset by the broker's network thread.
This is an inter-broker request. So, no throttling needed.
This is an inter-broker request as well and clusterAction should be true.
Our system test currently doesn't do perf validation well. It would be useful to just run ProducerPerformance and ConsumerPerformance and see if there is any noticeable degradation.
I wonder if there's a way to add this to `ResponseHeader` in a compatible way. It seems a bit annoying to have to add that to every response.
I think we'd want to override `parseResponse` for `API_VERSIONS` only.
I'm OK with merging as is, if you're concerned about removing the cases I suggested.
They are set to whatever is set in `StreamsConfig`, default is `earliest`.
OK, will remove before merging.
Is this intentional? cc @enothereska
I find this a bit difficult to follow. Could it be something like: ``` final Set<String> expectedMissingTopics = Utils.mkSet(INPUT_TOPIC, OUTPUT_TOPIC, OUTPUT_TOPIC_2, OUTPUT_TOPIC_2_RERUN); ZkUtils zkUtils = null; try { zkUtils = ZkUtils.apply(CLUSTER.zKConnectString(), 30000, 30000, JaasUtils.isZkSecurityEnabled()); while(expectedMissingTopics.removeAll(scala.collection.JavaConversions.seqAsJavaList (zkUtils.getAllTopics()))) { Utils.sleep(100); } } finally { if (zkUtils != null) { zkUtils.close(); } } ```
Yeah - you are correct. Which further highlights that i think it is currently too difficult to follow.
```java while(!stopped) ```
```java if (!(o instanceof HerderRequest)) return false; ``` catches both comparison with `null` and with an Object that is not based on `HerderRequest` and using this doesn't require potentially catching an exception. I also usually don't mind including a ```java if (this == o) return true; ``` at the very start. (optional)
Looking a little bit closer what `throttler` does, it seems that it uses its own condition variable internally for throttling. That explains the call to `throttler.wakeup()` below (that was expected to wake up the throttler). The problem (bug) of course is that this call to wait() here is not waiting on the same object as the throttler (one object is the `SchemaSourceTask` and the other one is the `ThroughputThrottler`). There are two alternative solutions to introducing a new condition (here `stopped`). 1. Just keep on calling throttler.throttle(): - if target throughput is <= 0, it will practically wait indefinitely until a thread calls `stop` -> `throttler.wakeup()`. - if target throughput is > 0, is will still wait for approximately 1/throughput time. 2. Since you can't reset the target throughput for a specific throttler (it's a constructor argument), discard the old throttler and create a new one with target throughput 0. Here that'd be: ```java throttler = new ThroughputThrottler(0, 0) ``` (the 2nd argument doesn't matter if the 1st one is 0). This new instantiation would still need to be wrapped with `synchronized (this)`. Now, all that needs to happen is to wrap `throttler.wakeup();` within a `synchronized (this)` block, in order to make visible to every thread the switch between throttlers. The implementation is far smaller than the explanation (3 lines if I can count well). Your call.
You'll hate me, but I see a tiny chance for `ClassCastException` that we can avoid.
Might want to consider just making this usable via a mixin class so we don't have to sprinkle these extra parameters everywhere (though maybe having to inherit everywhere is just as bad, but I think it could probably be added to a base class in one location and take effect everywhere).
@edoardocomar `Prepared` doesn't convey much meaning in terms of an externally visible metric. I imagine you chose it rather than `authenticated` since you intended it to work for `PLAINTEXT`. But `PLAINTEXT` doesn't go through this if-block since `channel.ready()` returns `true`.
What is the motivation for this change? It seems a little odd that we update the fetch positions for all assigned partitions if the position for one partition is `null`.
I think we should probably move this log statement since this method doesn't actually reset offsets.
That's right. Thanks for the explanation.
I think that's fine. No need to enforce it IMHO.
Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
We should check the keys too in every case in this test.
One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is.
`headers` is missing from here and from the other `append` that was added.
`front` seems to be redundant
looks like this isn't used
FWIW - this is probably something that could be tested easily with good usage of a real Mocking framework (rather than hand coded stubs). We should probably start making use of something in streams as it would save a lot of effort and with correct usage lead to a better overall design.
Is this the reason that `threadProducer` is not `private`. I'm not 100% sure, but if it is maybe there is another way of testing this so that it can be `private`, i.e., if you are using the per task model you know that the `clientSupplier` should be called n times. However, with the thread model it should only be called once.
Something to consider for a future PR: it's a bit odd that `MockClientSupplier` always returns the same producer when the contract is that `getProducer` returns a new producer. If we changed it so that it had the specified behaviour we would not need this class.
`assertEquals` should be used here and any other place where we are comparing numbers. `assertSame` checks for reference equality and it may return false if the number is outside the cacheable range even if they're equal (the various `valueOf` methods used by auto-boxing cache numbers within a certain range).
I'm not sure about this. It seems to be used only in the where we are using `WindowStoreUtils.INNER_SERDES`. I think i'd prefer to just create a new instance for each of those cases and do away with the `static final`
the `try{...} finally` has been removed
It should always be called from the same thread, though we've taken a simple approach to locking in this class where everything is protected as far as I can tell. This does technically introduce a behavioral change though, since `heartbeatThread` is being reset to `null` whereas it was not before, but this doesn't seem like a problem.
Since we acquire the same lock in `heartbeatThread.close()`, maybe we should just call `hb.close` within the lock to make the locking clearer. It's also consistent with `startHeartbeatThreadIfNeeded` and `disableHeartbeatThread`. We just leave the `join` outside the lock.
Yes, that matches my understanding and suggestion.
This was only ever done from the `Sender` thread, so I think it was OK as it was. Clearer with the `AtomicInteger` and probably OK since we'd hope `reenqueued` is not a bottleneck.
nit: add `final` to `Map. Entry `
I have no clue about this. \cc @enothereska @dguy
Nit: add `{ ... }`
There is an inconsistency with above code: `Map.Entry` vs `Entry` -- I guess we should use `Map.Entry` everywhere.
remove var -- only used once.
Might be excessive to add `final` everywhere. I'd say if it's self-evident that things aren't mutated I could skip final.
That makes sense. But what I don't know is, why we only consider `partitions` in the first place...
Nit: remove the variable -- it's only used once.
nit: add `final` to `String` (just to help us cleanup the code :)
Rather than having `try{...} finally` everywhere it might be better to keep track of the locked `TaskId` and unlock them in the `cleanup` method.
Please increase timeout to 30 seconds.
nit: we can just call the function with three params which will then use `DEFAULT_TIMEOUT = 30 * 1000L;`.
you should wrap in `try{...}finally{..}` so `context.close()` gets called when the test fails
Ah right, we only create the metrics object in this mock processor context, and in practice metrics is closed following another path. Thanks for the explanation.
I think we can just call `createKeyValueStore` and inline `createStore` inside `createKeyValueStore`. Also since all the calls in this class are the same, we can extract the `store` as a class variable.
Could we move `context` as this class's member variable, and then close the context in `@After`? All test cases seem to create the same object.
This won't get called if the test fails. I think it would be better to convert `driver` into a field and call `close` in an `@After`
@dguy @enothereska This `synchronized` here seems suspicious. Is it really the aim to synchronize on the listener instance when updating variables like `threadState`? Seems like a bug.
Thanks @dguy. That makes sense. @original-brownbear, maybe you can do a follow-up that does that.
It looks like there should only be a single `StreamStateListener` and `threadState` should be a member.
Nit: remove `this`
Nit: fix line break
Nit: also remove `this`
This also seems unrelated. It's in another patch that's being backported anyway, but probably shouldn't have made it into a cherry-pick.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Strictly speaking not a bug, just kind of confusing. And a reorganization of code could suddenly make this test fail despite it not currently representing realistic behavior. I guess it's a bug in the sense that the mock is not actually expressing the behavior we expect from it.
Hmm, not sure if this is being inherited from other tests in this class, but this isn't the behavior we'd expect. The logic is now somewhat confusingly split between `ConnectorPluginsResource.validateConfigs()` and `AbstractHerder.validateConnectorConfig()`, but since `connector.class` is missing, we expect a `BadRequestException`. This test only works because this answer doesn't match what would actually happen in `AbstractHerder`.
Given that we have both `ConnectorFactory` and `PluginDiscovery` which should be able to give you the full set of both canonical and short names, couldn't we just do a look up both on `connType` and `includedConnType` and validate that they are the same? It might require a bit of reworking (e.g. making `Map<String, Class<? extends Connector>>` available instead of `List<Class<? extends Connector>>` in PluginDiscovery), but it seems like a better solution since it'll address all the cases afaict. re: BadRequestException, we needed something in the 400 range and nothing else seemed appropriate.
The exception can still be `ProducerFencedException`. Just the name of the error code should change.
This was changed to`INVALID_PRODUCER_EPOCH` in the kip. Let's change it here too.
Not sure if this is clear, but all classes under `errors` are public API. As such, we should consider the naming of such classes carefully. It would be good to take a pass at all the names before the release and make sure they still make sense.
Adding this constructor is quite dangerous because it's almost the same as the one that takes `partition` (the only difference is that one is a `Long` and the other is an `Integer`)
Minor typo, `were` -> `where`? Also, `10 <= start time <= 20` might be slightly better notation.
it think it probably should be "ensure the co-partitioned topics ..."
Can we add an error message to go along with the error code? It already happens today that a `GroupCoordinatorNotAvailable` on the client is generally not enough to know what's wrong. We have to check the broker logs and hope that additional information is available there (and we had to add additional logging, after the fact, in some cases). Looks like this is about to become a bigger issue with this change. CreateTopicsResponse is an example of a protocol response that includes an error message along with the error code.
That is fine, just curious if we have ever thought about how users would leverage the APIs to determine which stores to query. We can discuss this in a follow-up JIRA.
Could we collapse the code path for having a queryable store name or not into the same function? For example: ``` filter(.. /*nothing*/) calls filter(.. (String) null); filter(.. "storeName") calls filter(.. storeSupplier); // if storeName is not null, otherwise pass null as well filter(.. supplier) do the actual impl, which checks if supplier is null or not ```
I wonder if in this case we should always materialize the join table. It would help us with resolving this bug: https://issues.apache.org/jira/browse/KAFKA-4609
I don't think this is in the KIP. I also think it should be named `queryableStoreName()` I think we generally don't use getXXX on the public API
Nit. `.` missing at the end.
This test went from checking for exactly one message, to checking at least one message. If you think that is fine (I dont have the full scope so I dont know) then LGTM! :)
Right, I was referring to the difference to the old assert and that the new check and error message contains less troubleshooting information than the old one, but I understand that it might seem irrelevant given that the producer part can be trusted, thus the "nit" :)
I think this is already the default.
Relying on the exception message seems fragile.
Seems like we don't need this variable? Same for other similar methods.
One concern is making things too magical. However, it does seem that there's no scenario where someone would want transactions without idempotence. So, I'm in favour of this suggestion.
We don't usually use JVM level asserts because they are disabled by default. Same for all other cases in this PR.
I'm not sure we want to throw the static exception since we won't get a stack trace (at least currently).
Mentioned offline, but it seems we don't need the `isInTransaction` check here. No matter the state, if the producer is fenced, we should raise.
Perhaps if the user configures a transactionalId, then we should enable idempotence automatically. We can raise an exception only if the user has explicitly disabled idempotence.
Seems like we could push these some of these checks in `TransactionState.beginTransaction()`. Same for the other APIs.
This breaks with an NPE when the user does not specify the partition explicitly in the `ProducerRecord`. We should use `tp` as the partition.
+1 to what @ijuma mentioned in https://github.com/apache/kafka/pull/2773: we can close the producer inside task.close if it does has one. And hence `closeProducer` could just be private and we do not need to guard potential bugs either.
Thanks for the explanation: checking per-commit is indeed easier. Moving forward we can even make them two separate PRs for other reviewers to easily review.
I'm wondering if we should move this config to LOW as it is already deprecated and hence would suggest users to never change it any more.
The diff for the rest of the code seems to have gone wrong. Hard to tell what has changed and what the diff is. Might be ok.
Spoke to @hachikuji offline. There are two things to be resolved: 1. Ensure that the good records before a bad records in the same batch is delivered. 2. Do not skip over a bad record. This patch does 1 but not 2. I have created KAFKA-5211 to address 2 and will submit a patch for that separately. For this patch we will just focus on solving 1.
Please update the test case as I suggested. Thinking about the current patch. If there is an exception parsing or validating one of the records, we will update `PartitionRecords.nextFetchOffset`, but we will not change the current position (i.e. what is returned by `consumer.position()`. That means in the next call to `poll()`, we will simply discard the rest of the records. So there is no behavior change here and my suggestion above simply makes the behavior explicit. You can confirm this by updating the test case.
The point is not that the next record is at a higher offset, but that the current position deterministically decides the next record to be returned. After we have caught the exception from the record at offset 1 (say), the position remains at offset 1. However, the next fetched record may be at offset 2. If we catch an error at that offset as well, the position will remain at 1 (I guess?) and the next record we try to return may be at offset 3. This makes the consumer less functional since the behavior of a fetch at the current offset depends on the history of previous fetches. The alternative is that the consumer always fetches from the current position. If the user wants to skip a message, they must explicitly seek to the next larger offset. Then the result from a poll is always deterministic. The user is in control and the consumer will not try to guess what they want to do. Whether by design or by accident, this is how the new consumer has worked up until now, so another thing to consider is whether changing this behavior will cause any compatibility issues.
@becketqin Yes, my preference, as mentioned above, is to deal with that problem separately. We should not make behavioral changes without first raising the issue at least in a separate JIRA. The unintuitive thing about the proposed behavior to me is the fact that although the consumer's position remains at the offset of the failed record, the next returned record will be from the offset after that position. You can see this in the test case below: the consumer's position is at 1, but the returned record is at offset 2. This makes the behavior less deterministic. It would be nice to maintain the invariant that the next fetched record is always the first record at an offset greater than or equal to the current position.
Nit: `new Runnable()` not required, you can just override `run()` from `Thread`.
Rather than sleep 12 times, you could wait for an `metadata.updateRequested` before doing the `update`.
It is not particularly critical, but my suggestion was to keep the same `for` loop, but add something like > while (!metadata.updateRequested()) Thread.yield() > metadata.update(...) or `Thread.sleep(1)` instead of `yield()`. Now each update corresponds to a request to update metadata from the `partitionsFor`. like it does in the real scenario. You need to make sure that the thread terminates in this case.
@lindong28 I think the 12 wait for updates in the loop may be too many since max.block.ms=10min? It will be good to ensure that the test doesn't leave the thread running even if the test fails.
To be realistic, the second parameter should be `Collections.singleton(topic)` rather than `emptySet`.
I think we need to skip all the code in this else block if the partition is no longer assigned.
@enothereska Yes, I think that is a better solution. But I think @hachikuji was right that we don't need to cover both if/else branches. We just need to cover the `parseCompletedFetch`
@enothereska The trunk code does not need to access `subscription.position`, instead it uses `PartitionRecords.nextInlineOffsets` which should be the same as position because the position is updated to this value every time after a successful `fetchRecords()`. The big try/catch is to make sure the the exception from `fetchRecords` will also be caught and not result in loss of non-empty `fetched`.
Mentioned offline, but a better solution (if Becket is right that the try/catch needs to cover this branch) would be the following: ```java fetchedOffsets = nextInLineRecords.fetchOffset; ``` Also, seems this variable shouldn't be plural.
@enothereska Looking at the previous patch, it seems like the scope of the try/catch is unnecessarily large. I think it only needs to cover the call to `parseCompletedFetch`. If we do that, then there should be no need to access `subscriptions.position` down this path.
Maybe we can go with this solution since it seems pretty safe. Shall we just remove the `log.debug` since we'll get that from the `drainRecords` call anyway? And let's run the system tests on this branch after that.
@mimaison It is true that the test would check only one class depending on the JRE. But it checks that the relationship between `java.vendor` and Kerberos classes matches the expectation in the code (for that JRE). The other unit test is checking if String comparison works, which is fine as a unit test, but it doesn't really test the actual System property based on the JRE.
It may also be useful to have a test that checks that IBM Kerberos classes are available if `Java.isIBMJdk` is true and `com.sun` Kerberos classes are available if false. In particular, you could check the classes `com.ibm.security.krb5.internal.Config` and `sun.security.krb5.Config` which are loaded in `SaslChannelBuilder`.
@rajinisivaram Just for curiosity I tried the IBM MacOS SDK. It has both classes. And neither work  with our test harness! Anyway that JDK is not our priority
EDIT: just realizing that we are re-throwing the exception anyways after re-closing the state managers. So this should be fine.
For standby tasks `init` is no-op, so we can just remove it? PS: logically we should call `resume` not `init` here, but the former is also no-op anyways..
Hmm, if standbyTask does not ever call `init()` then why do we want it to be an abstract class? I felt that we can either just remove abstract `init` and just let `StreamTask` has its own `init` function that is also called in `resume`; or if we do want to have a `init` abstract logic, we can also move ``` log.info("standby-task [{}] Initializing state stores", id()); initializeStateStores(); checkpointedOffsets = Collections.unmodifiableMap(stateMgr.checkpointed()); processorContext.initialized(); ``` At let constructor call this `init`. Personally I like the first option since we are already overriding the constructors anyways which always covers the `init` part, and just the stream task needs a reusable `init` part that is to be used in `resume`. But it's your call.
I'm just afraid that capturing any RTE that we have not thought about and re-close the state managers may hide some issues or even subsequently trigger some other issues.
nit: including the acked offsets to checkpoint as well.
Not sure what has changed here.
Not sure what has changed here.
I think we should introduce an interface, `Task`? Doesn't necessarily need to be done in this PR, though
Hmm, this pattern is a bit weird. As @guozhangwang initially said, it's as likely to introduce issues than fix issues. `closeStateManager` should just do the right thing. If we need to change the `catch` blocks, we should do it there.
nit: this is not introduced in this PR but, other places capitalize the first letter after log prefix.
I don't know what has changed here.
Or is your concern about `commit` throwing an exception? If that is the case, then the `try`/`catch` should just be around that method IMO.
I don't think we need this because we always compile against the current version. The runtime error won't happen unless `shouldPrintMetrics` is true.
maybe rename this to `setUp` and put the loop populating the array in this method
@original-brownbear I understand that part, but IMHO it's best if we stick to some basic guidelines for our tests/benchmarks. To reduce the GC impact how about we decrease the cache size to 5, and the number of records to insert 25 or so? Or just create an array inline with the keys and declare as a private variable? Either way, we should be able to do the work in the `setUp` method.
Sorry for being late on this. Populating static variables from a non-static method is generally not a good practice since it's generally not thread-safe. Given how JMH works, it's fine for those fields to be non-static right? Also, it seems more realistic since the data is isolated per run.
The second param seems redundant.
This newline is not needed.
This seems a bit misleading, and less informative. Maybe just "Adding active tasks {partitionAssignor.activeTasks()}"? Ditto below for standby tasks
What typo ?  Yes, it should be public -- it's meant to be a utility method (even if we don't use it atm).
I would keep this public method -- it can just create a consumer and call `private readValues`. Same for `readKeyValues()`
I'm not 100% sure if this should be `max` or `min`.
@mjsax With the recent changes on IntegrationTestUtils, `waitUntilXXX` will use one consumer instance only so I'm wondering if it could still return more than expected results. Also for the mock time issue, could we eliminate the non-determinism by not using `System.currentTimeMillis`? We can augment the `MockTime` to set manual initialization values if necessary.
Yes. there used to be wait/notify methods, but they were removed in a previous patch without updating the java doc.
Actually the old `hasPid` would always return false in this method since we reset the PidAndEpoch at the beginning, so we would always get a pid. Also, the `transactionCoordinator` check is also redundant since we always check whether there is a valid coordinator before sending requests that need it. So this is a good simplification.
That's just a mistake. Initially I had separate priorities for `ADD_PARTITIONS` and `ADD_OFFSETS`, but forgot to adjust the `END_TXN` priority after combining them.
Hmm, we're using a raw type here and a few other places. This is discouraged (type checking is disabled in these cases). If we don't want to propagate the generics when we use the superclass, we should probably drop them.
Maybe `Producer epoch...`. Also, not sure the exception message adds anything given what's already logged. Maybe we should remove that.
It would be good to have constants instead of hardcoding the fields in many places.
I know, but you can save it in a static final.
Can't we just `put`? If there are duplicates (which is rare), we will simply overwrite, no? Same question for other places where we do the same.
Hmm, seems like we are exposing a class from an internal package (`requests`) in a public class.
Since the code and enum id coincide exactly, is it worth having this map? Maybe we can add it later, if needed and simply look for it in the `values()` array.
Maybe it's worth not including this constructor. It's only used in tests and it's generally a good idea to provide a message with the exception.
Since asserts are not enabled in any environment at the moment, can we just do plain checks? Also, it's a bit nicer if we include a brief error message.
Is it worth having this class? Not sure there is much overlap apart from storing the fields.
Another reason for having these classes in common (i.e. KAFKA-5265) is that they can potentially be used by the Authorizer interface when we move it to Java.
Other enums in Kafka tend to use id for this.
Would `isUnknown` be clearer? I find that boolean methods without any prefix feel a bit ambiguous when reading them.
Hmm, we should probably specify that we can change the output of this String (i.e. parsing may break). I would have preferred if we didn't expose this as a public method and had a utility for it.
Needs to be updated.
Also , the 1-liner description "Transform the given value to a new value." needs updating.
ok - same thing three times. Maybe extract it to a method `verifyTransactionInflight`
nit: "MockProducer has already been initialized"
Do we need to add `this`? Also below
Here and everywhere else. Got doesn't sound right. Maybe something like: `shouldThrowOnInitTransactionIfProducerIsClosed`
"... as producer is closed" Same elsewhere
"Should have thrown as producer is already initiailzed"
Because it may hide a bug if we receive a negative value :)
Fixed this before merging.
Nit: I think we can fit this into two lines. :)
nit: I know it was already like that, but since we are now passing the actual class object, you might want to refer to the class object as `klass` (I like the keystrokes on this one) or `clazz`, which are common naming conventions when using class objects. Then call the String field `className` or similar. Of course JsonProperty will continue to be called `class`. Up to you.
public access? I can see this being accessed by another package too (such as `rest.resources`)
a `toString` override would be nice (as per KIP discussion)
Oh, and a typo which I would like to make KNOWN (or UNKNOWN?! ... I would pick a pun over clarity any day :) )
nit: Starting a message with lower case feels a little unusual.
Sure, no problem. It's a slim and harmless chance, that's why I mentioned.
`newInstance()` can throw `ExceptionInInitializerError` and `SecurityException` as well.
I think putting a `@JsonValue` annotation here should fix the capitalization issue, seems like it uses `name()` by default for `enums`.
Passing through Jackson seems to be the real test we need here (though checking `toString()` itself is also useful for logging elsewhere if we want consistency).
These aren't getting serialized by Jackson the way I think you expect they are: ``` $ curl -s http://localhost:8083/connector-plugins | jq [ { "class": "org.apache.kafka.connect.file.FileStreamSinkConnector", "type": "SINK", "version": "0.11.0.0-SNAPSHOT" }, { "class": "org.apache.kafka.connect.file.FileStreamSourceConnector", "type": "SOURCE", "version": "0.11.0.0-SNAPSHOT" } ] ``` The types are still capitalized.
Thanks, looks good. Yes, it's O(1), but a lot less efficient than returning a local variable. Take a look. :) ```java public V get(Object key) { Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1) & h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } else if (eh < 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) return e.val; } } return null; } ```
You can also replace `return segments.get(key);` with `return previousSegment == null ? newSegment : previousSegment`.
You could remove this `if(..)` block as `putIfAbsent` below will cover it
Yeah, my PR removes all of these in favour of existing constants: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that: https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192
We could change `DefaultRecord.readFrom` to take a `DataInput` (instead of `DataInputStream`), change `KafkaLZ4BlockInputStream` to implement `DataInput` and then only wrap if the returned value is not already a `DataInput`. That would remove a layer of indirection and if it's possible to implement `readFully` more efficiently in `KafkaLZ4BlockInputStream`, then it could be a win. If you have time, it may be worth a try.
I guess it doesn't harm.
same here. let's make all method params as `final`
nit: i don't mind if you use `final` or not in the methods, but it would be good to be consistent. Same in other tests
actually, rather than using an enum i think we could use a functional interface something like: ``` interface ProcessSupplierGetter { KStreamAggProcessorSupplier apply(Aggregator) ``` Please come up with a better name! we can then remove the `switch` from `doAggregate` and the enum
nit: `final` is redundant for `private` methods
You could probably create the `StreamsConfig` in an `@Before`
nit: no need to set `driver` to `null` each test gets it's own instance of the class, so it will always be null for the next test.
`null` is redundant
nit: static is redundant
Can we think of a better name for this? `pairs` doesn't really tell me anything.
This is not in this PR: I realized that in `KGroupedStreamsImpl#repartitionIfRequired`, in the return statement: ``` return KStreamImpl.createReparitionedSource(this, keySerde, valSerde, queryableStoreName); ``` We pass the `queryableStoreName` as the prefix of the repartition topic. That seems not correct to me? cc @enothereska since it seems from one of your previous commits,.
Looks like you can do these 7 lines in a `@Before` method and not repeat it in all of your tests
@cmccabe to clarify my point: if you have 3 futures and you want to use all of them, you need to a way to combine the 3 futures into 1 and handle the fact that each may fail separately. So, imagine that someone wants to use these 3 fields somewhere in their code after handling errors. I'll use Scala Future terminology next. The typical way to do these things in a non-blocking API is to map/flatMap over the future and pass the unwrapped value to other code. The code, in turn, may return a `Future` if it needs to perform some computation that is expensive, involves IO, etc or simply return a normal value. In the former case, `flatMap` is needed why in the latter `map` will do. If you have 3 `Future`s instead of `1`, you need to do 3 nested `flatMap`/`map` calls and pass the individual values to a method or create a class containing these values before passing them around. So, in general, it's easier if this is done for you. We can always introduce separate futures in the future if there's a case for handling errors separately. I can't think of a good reason to do that here though.
i guess i dont feel too strongly about either way. my initial thought was that effectively having 3 outputs from this would make having an external client cache these results more difficult, but i'm not sure that a) matters b) is even true since you could just cache the `DescribeClusterResults`
We should add this check in `ensureOpenForRecordBatchWrite`, `ensureOpenForRecordAppend`, and `close` as well.
Seems like we transition to AUTHENTICATE before the TLS handshake (if TLS is enabled). If TLS authentication fails, will we also get a warning then? I think that would be good, but just checking as previous proposals didn't cover TLS.
Besides knowing if the topic exists or not, it's also handy to have information about the number of partitions, replication factor, etc. Obviously it would be ideal if we could always rely on this code to create the topics with good configurations, but I meant that having that info to identify that, e.g., the user created a config topic with multiple partitions and that's why they'd be seeing weird behavior. Anyway, it's not critical, just thought it might help with debuggability.
This shouldn't be possible, right? It wouldn't make much sense to put a topic in the result if it didn't have a corresponding `TopicListing`.
Extra copy of the license
All the `null` checks at each layer of the call stack make me think that particular issue might be better handled with an exception. Not critical since this is all internal code, but seems like then we'd only need to check version compatibility in one or two places.
This is probably good at `INFO` level as it should be a one-time message (unless you can't create the topics, in which case seeing it repeatedly is helpful) and it's critical to normal operation of Connect.
While we're in here making changes anyway, might be nice to add debug logging indicating what partitions are being added. This will be helpful for debugging issues where the topics are created incorrectly. Maybe just log the `TopicDescription` as its `toString` seems to give all the relevant info.
If we do allow an `int` here, we should a) check for bounds and b) re-validate values after the cast. Otherwise you can end up with invalid results (which should never happen in practice given the range of sane values for replication factor, but down casts like this always warrant such checks). However, I suspect just shifting to using `short` all the way back to the configs is probably a better solution.
Hmm, would be good to get these constants out of here, but seems they don't appear in the clients jar yet. Not something we need to do now, but we might consider this for the future, and the fact that we have the AdminClient in clients might be good enough reason (though since these are really broker configs, I'm not sure how much we can do with them in the AdminClient).
Include class name here? This seems like it would be confusing if logged.
You can also do this more concisely as `topicsByName.keySet().removeAll(existingTopicNames)`.
Refer to the `processing.guarantee` config here.
Regarding `MetricNameTemplate`: makes sense. Regarding `*MetricsRegsitry`, I think even with these classes we cannot centralize them right? think: if we add a new metric in the future, and the developer need to add it both in the `*MetricsRegistry` and here; and if she forgot to add it in this class, nothing will break at compile time, so nothing is programmatically enforced.
Good point, thanks!
I'd suggest we do not print the stack trace in either case; instead, we can just print the exception's `name()`, which is sufficient to tell it is the locking issue.
Can you please fix this output too -- the tool does "seek to beginning" and does not set offsets to zero.
Not required. Client will be automatically closes, as we use Java's "try with resource" feature.
We typically don't re-order imports
Nit: maybe `("Topic: " + topic)`
could we move this `if(!dryRun)` to before `maybeSeekToBeginning`? Then we don't need the additional checks in `maybeSeekToBeginning` and `maybeSeekToEnd` you could just log the messages you want to log here or in another method.
Nit: why not use `boolean`
Nit: maybe `("Topic: " + topic)`
nit: add `final`
nit: `maybeDeleteInternalTopics` and `maybeReset...`
Nit: go we need to include the groupId? It's repeated in line 316.
Nit: add `final` to parameters
Nit: why not use boolean
But `toString` by default returns `name()`. So, I don't understand why we are overriding it.
Yes, it makes sense to return a range for an ApiKey instead of a single version. I was just wondering if this method is redundant given NodeVersions.apiVersionRange(ApiKey api). Also, it feels a bit weird for a public facing class to reference Protocol, which is not a public facing one.
We should specify that this name is a descriptive name and can change between releases. The id is guaranteed to remain the same for a given request type.
Seems like this file has a bunch of unintended changes.
`ApiKeys` -> `ApiKey`
name seems unused.
You can use `EnumMap`.
You can use `EnumMap`, which is a lot more efficient.
Answering here the question about name changes. This is probably the best example, it has been renamed a couple of times: `ConsumerCoordinatorRequest` -> `GroupCoordinatorRequest` -> `FindCoordinatorRequest`. Also, I'd like to rename `Produce` to `ProduceRecords` and `Fetch` to `FetchRecords` so that all protocol APIs are consistent.
Hmm, that's annoying.
Nit: `describeTopics(Collection<String> topicNames, ...)`
Fix this reference? There is no `AdminClient#describeTopic(topicName)` method
It is bad. The exception will just be logged and will never propagate back to the Streams Thread so we will end up losing data. This is why it was done the way it was.
I don't see how this is going to work as the callback is happening on the Producer's Send thread
The original approach is to avoid throwing exceptions on each of the record: for example, if you get a timeout exception on the request, all records in the batch will return the same exception in that callback, which will spill the log4j since we will get one error for each record.
I think we cannot fix the issue, that error are detected late, as we want keep async pattern. I guess the problem is, that `checkException` is done within `send` at the beginning -- this confuses used as they assume the current send request fails. Maybe we can do the check outside of `RecordCollectorImpl` ? Not sure -- might be hard to maintain. What we also can do, change the error message. Right now it only says "Error sending record to topic " -- maybe we can say "Aborting send record because a previous send returned an error"? I am also wondering, if the logged `topic` is correct -- should we not log `metadata.topic()` ? We could also buffer up all sent records and include the record that causes the error in the log/exception -- to point out which record did cause the problem.
Log this kind of stuff at `debug`, it's useful info to have
You might want change IntelliJ to use the `java.util.Objects.equals and hashCode (java 7+)` setting when generating `equals` and `hashCode` implementations -- it generates nicer, tighter code that plays better with our checkstyle.
What if it is a file? We didn't really talk about this, but it could potentially be a list of uberjars. Even if we don't want to support this here, at least log something if the entire path is going to be ignored due to not being a directory.
Private and call from the constructor? We only ever call this immediately after instantiating the class.
It might make sense to either a) get rid of the caching of aliases or b) fill the entries in proactively during loading of classes. Then we would be able to make `pluginLoaders` non-concurrent and make this class simpler to reason about since all data would be filled in during initialization.
Doesn't seem to be used for anything? Why not just log a message saying that it didn't contain any plugins? In fact, even if we save this here, it seems like we'd still want that error message since the lack of any plugins probably indicates an incorrect configuration.
maybe: `inputKeySerde` and `inputValSerde`
I am just realizing, that `other stream` is not right here either -- it's used for both input streams, as both will have the same key type.
The key Serde here, is for the new output key type, right? I think `input` stream is miss leading here.
Not sure, if we need this here either -- input stream and output stream do have same type -- and Serdes are actually used to write and read.
If we change the definition of the available memory here, we may have to change a few other places as well. e.g. `deallocate()`, `unallocatedMemory()`, etc.
I think they should be local to the `punctuate` method as they are only needed there. If it becomes a problem they can be optimized later. I don't see the point in optimizing before there is a problem, especially when it is making the code worse (IMO)
I'm not sure why `punctuator` or `punctuateDelegate` are fields? In fact, `timestamp` shouldn't be a field either. All three of the them are only used in the context of `punctuate(timestamp, punctuator)` so there seems to be no reason for them to be set as fields. We could just construct the `Runnable` in the `punctuate` method
We should just throw an "InvalidArgumentException" here.
Since you are touching this class, I noticed use of `getCanonicalName`. Do we want to change to `getName` here too? It doesn't really matter, it's only used for logging, but I mention it for consistency.
I'd prefer that we pass in `this.name` here instead of `null`. In the method it delegates to we should do `Objects.requireNonNull(streamName, "streamName can't be null");` Same for other methods that are passing null
Not done as part of the PR, but... Can we pass `new PrintWriter(System.out)` here instead of `null`
I'd really like to discourage passing `null`. We can have a `KeyValueMapper` instance that we pass here and also throw an exception in the method that is delegated to if the `KeyValueMapper` is `null`. Same elsewhere
As i said above, we should `requireNonNull(mapper, ...)`
`PrintForEachAction<>` to remove warning. Also in the `print` method
You only need to crate that instance once, right? It can be a member of the class
I think we should just create this as a field on the instance? I don't see any reason to create it everytime. Also makes the code cleaner
nit: this is unused
Yes, that would make sense to me.
I think what you have here works fine. Thanks @umesh9794 for your PR.
Maybe add an assert checking `streamsString` contains the app-id. But considering this method will be removed, I'm also fine with it as is.
Instead of pulling the value out with a regex, what do you think of `streamsString.contains("appId")`. Although what you have works as well.
I think I'd just do a check at the top and say `if (props == null) return new String[0];` or something like that.
One of the possibilities for a corrupt record is an error transmitting over the network (the TCP checksum is only 2 bytes). We could recover from this error by discarding the current batch and refetching from the failed offset. The downside is, well, we have to refetch. In practice, I assume this would be super rare, but maybe it's still worth allowing for the possibility? For parsing errors, refetching may not help, so caching the error seems fair.
We can do it in a follow-up if you prefer. I was thinking it was as simple as setting `isFetched` to true, but I could be wrong.
I think caching the exception may actually be OK. For both `SerializationException` and `InvalidRecordException`, we've overridden `fillInStackTrace` anyway.
Should we set `lastRecord` to null here? Otherwise it seems like we might misinterpret which record had the error.
Mentioned offline, we can probably move move the conversion to empty string into the `OffsetAndMetadata` constructor so that we always handle this consistently.
Note to self: remove this conflict marker before merging.
I changed the cast to `Long.valueOf`.
Yeah, I was thinking it was fine to remove it since users should not create an instance of this apart from tests and it's been deprecated for a few release cycles. Or if we think the method is useful, then it would make sense undeprecate it.
`long` -> `Long` is a binary incompatible change.
Forgive the bikeshedding, but I was curious why this was located here instead of instead `KafkaConsumer.pollOnce`.
I guess we could pull this into the `partitionsAutoAssigned` block.
null check not needed it is inside ensureExtended
To avoid this instanceof check on hot path, as with KafkaClient, you can change the private Deserializer<K> keyDeserializer; private Deserializer<V> valDeserializer; to Extended versions, and on construction wrap them, thus removing instanceof checks on hot path.
This one too should synchronize on `TransactionManager.this`.
Think you might have forgotten to remove some debugging here
It may be worth handling overflow here as people could pass `Long.MAX_VALUE` for `waitTimeDuration`.
The parameter names need to be updated.
Nit: we use a single space after a period.
Nit: `enqueue` would be a little clearer.
"will be 0" should probably be "should be INVALID_SHUTDOWN_TIME". I can fix this when merging though.
This signature should be consistent with the consumer and producer. For example: ```java public void close(long timeout, TimeUnit timeUnit) { ```
nit: add `final` (same below)
The test name is not self describing: what about `shouldAlllowToSpecifyRocksDBConfigSetterClassAsString`
This test case doesn't seem different than `testInvalidRecordSize`.
Seems we can revert this change. We'll hit the InvalidRecordException below if there is additional data, so this is really more for iterator sanity.
Sorry for the confusion. I thought `hasRemaining` made sense initially, but then I realized that the name should be more suggestive of its usage. I'd prefer something like `ensureNoneRemaining`, but it's not a dealbreaker for me.
@hachikuji asked you to change the name originally: ```text hachikuji 5 days ago Contributor nit: the name is a little awkward. How about hasRemaining to match ByteBuffer? ``` :)
Both of these cases are still testing the same thing. I think you are intending to set an invalid record count, but this is actually changing the size of the batch (i.e. the size in bytes). So whether it is 2 or 10, we're validating the same path.
The try/catch is over the whole block, so the message seems a little odd. If this can only happen while reading the headers, we should probably move the try/catch to `getHeaders`. We should also probably rename `geHeader` to `readHeaders` or something like that.
nit: We could probably just pass `sizeOfBodyInBytes` into `readFrom` and this function.
Hmm... Seems rather wasteful to decompress another time just to validate the record count. What I had thought is that we could hook the logic into `DefaultRecordBatch.RecordIterator` so that we can do this validation as part of the single pass we do over the records when validating. As a matter of fact, we seem to already do part of the validation there. If the number of records that we consumed is greater than the count, then we raise `NoSuchElementException`. That should probably be changed to `InvalidRecordException` and then we just need to check for the underflow case.
Hmm... This check seems to depend on the buffer only containing data from this record. We should either duplicate the buffer and set the limit according to the expected size of the record, or verify that the position delta (i.e. the difference of the final and initial positions) is equal to the expected size.
My point is that any of the calls to `readVarint` or even `buffer.position()` prior to reading the headers could also underflow. It seems like it would be useful to raise `InvalidRecordException` consistently in all cases (not just for headers).
This is a little odd. I wonder if we can instead add an abstract `hasRemaining()` method and use `InputStream.available()` and `ByteBuffer.hasRemaining()` to determine whether there are still bytes left. cc @ijuma
Yeah, @ijuma is right. Unfortunately, we may have to live without this check until we have a better input interface to work with. For a little additional context, we have found that the overhead of throwing `EOFException` is excessive and we took pains to eliminate it, so it would be unfortunate if we still need it in the end. The consequence of not having this check is that a buggy or malicious producer could write additional data at the end of the batch which would not be reachable. It pollutes the log and may be a difficult issue for a client developer to debug, but it doesn't actually cause any problems that I can think of, so maybe we can live with it.
I think perhaps the error message should be more generic. An invalid number of headers is one possibility, but it could also just be incomplete data.
nit: add `final` also in L616
Nit: I was thinking that the variable name should be `batchHeader` and this string seems to confirm that.
We should add parameter names here.
Nit: maybe this can call the newly introduced `withTransactionalRecords`.
Since this is a public method, we should probably validate that `magic` is not > 1. Same for `recordOverhead` and `keyOffset`.
EDIT: nvm, I think I understand it now.
Should we add the check in the `Sender.completeBatch()` as well to note call split in this case? Otherwise if the producer was sending uncompressed messages and one of the message in a batch a too large, it seems the producer will not fire the callback with correct exception. This would probably be a rare case because a big message will typically get sent in a dedicated batch if compression is none. But it is theoretically possible if user configured the producer batch size to be larger than the max.message.size.
This wording could be improved: "Batch splitting cannot be used with non-compressed messages, NOR with message format versions v0 and v1"
Looks like this is causing checkstyle errors.
This is really inefficient if `buffer` is a `DirectByteBuffer` and it's not small. The bulk `put` method performs better by doing a JNI array copy if it's larger than 6 elements. It's also less code.
This message should say "Consumers earlier than 0.10.1.0..." since KIP-74 was part of 0.10.1.0.0.
netrics => metrics
Since `RECONNECT_BACKOFF_MS_CONFIG` and `RECONNECT_BACKOFF_MAX_MS_CONFIG` are defined in this class too, is it really useful to pass the key and value from every usage? It seems like every one of them passes the same configs.
Nit: maybe we should include this in the previous check
When Dana implemented exponential backoff, she disabled it for Streams and Connect. I don't think there's a good reason not to enable it for those two and hence this PR. In the Streams case, I think those two configs are actually used for the `StreamsKafkaClient`.
I'm not sure of the original motivation behind setting it to `50`. But if we are going to define it as `1000` i think it might be better to remove this and `reconnect_backoff_min_ms_config` definitations from here as they are the same as what is set in the `ProducerConfig` and `ConsumerConfig`, so it seem pretty pointless overriding them
There was also a test failure that seems to show that this may not be the right fix.
Thanks for the clarification @hachikuji
I'm not sure if we should set `requestRejoin` in the base class (`CoordinatorResponseHandler `). For example, `HeartbeatResponseHandler` also extends from it, but for that request if we get a disconnect, we should just mark the coordinator as dead in order to re-discover it; and then after new coordinator rediscovered retry sending heartbeat request and if that succeed just proceed as normal. Setting it here will force heartbeat request disconnection to also trigger a join group.
I think @hachikuji 's suggestion may be better: do not call ``` AbstractCoordinator.this.rejoinNeeded = false; ``` in `JoinGroupResponseHandler#handle()`, but in `SyncGroupResponseHandler#handle()`.
On a second thought... `future.complete(syncResponse.memberAssignment())` above will trigger `joinFuture.addListener`'s `onSuccess`, which will enable the heartbeat thread right away, and hence there is a (very small) race condition. I think it is safer to just move the the above line inside `onSuccess` (line 395) to set it before enabling heart beat thread, and we would not need `AbstractCoordinator.this` prefix also.
@dguy @hachikuji if it sounds good to you I can go ahead and make this change while merging.
This could also potentially be simplified to iterate over their fields together, which is probably cheaper than than the map lookups by field name. (In fact, *strictly* speaking it should be safe to just use the field from the original in the copy since they are supposed to be exactly identical anyway.)
Nit: var should be named `deserializeValue`
Nit: please add `final` to all local vars and method parameters
Nit: get rid of empty line: `public void configure(Map<String, ?> configs, boolean isKey) { }`
partitions is not used
Is this okay? If committed == null it means no offsets get committed for this partition in `getCommittedOffsets`, and we should check that we do not get any data in `partitionRecords ` but currently this check will always pass even if `partitionRecords.getValue().size() > 0`.
Nit: we should probably include a little more detail. Maybe something like: ```"Found invalid wrapper offset in compressed v1 message set, wrapper offset '" + wrapper offset + "' is less than last inner record offset '" +lastOffsetFromWrapper + "'and it is not zero."```
Nit: instead of `older`, maybe we should say `certain versions of librdkafka`. The Java client never does this for `v1` as far as I know.
Should be `post 0.10.0 Java clients`, I guess.
Thanks for the explanation. The iterator returned by synchronized connections has to be synchronized by the callers. The code of `SynchronizedCollection` that seems to be used by `getPrincipals`: ```java public Iterator<E> iterator() { return c.iterator(); // Must be manually synched by user! } ``` So, it looks like we don't really have a choice here.
nit: extra "for"
Yes, that's what I was thinking. Then we can remove the global references and use the same pattern consistently.
Don't think readability would suffer. For `MockProducer` is might also not be bad atm if `close()` is not called in case the test fails with an exception and thus `close()` is never called. It just feel "right" to make sure to call `close()` for all classes that are `Closable` -- even in case of failure. This might also be more "future prove" in case `close()` implementation changes at some point. But I also don't insist on the change.
One issue is that the exception thrown by this method is `InvalidTopicException`. In this context, `IllegalArgumentException` seems like the right exception to throw to be consistent with the other things we check. Maybe we can catch the exception and rethrow it. Also, it would be good to mention it in a @throws clause.
No need to repeat `is thrown` in the 2 lines above.
I think this would be clearer if called `isSendToPartitionAllowed`. At first I thought this method was sending to a partition. :)
Nit: why not `failIfNotReadyForSend`? One character longer, but reads a bit better. :)
Yes, it makes sense. Good catch.
It doesn't look like this does anything -- afaict the `ZookeeperService` doesn't currently render the `log4j.properties` file and it won't use the same one as Kafka since it's on its own node.
I changed a few test names to match the rename for the methods they are testing.
We may as well use the more specific type `TimeoutException` given the name of the method.
I don't think priority HIGH is a good choice for this. I guess, using broker default setting is good enough. MEDIUM (ie, "please double check"), might be better IMHO. Nit: can you pleas add this in alphabetical order (with whatever level we use).
Nit: can you pleas add this in alphabetical order.
I understand that. My question is whether there is some thinking on which configs make sense to be set by users. Ideally, we'd do that instead of being reactive.
Why are these in a separate block when compared to `offsets-for-times-supported`, etc.
Hmm.. I was not aware of this, and could you point me to the un-statisfying behavior that user complain about? Would like to make sure if this is really an issue to fix before merging. For this PR, I'd suggest we do not include it so that it can be merged.
nit: `could not create task {} due to {}. Will retry.`
This can probably be left at `debug`. Before it was very spammy because we didn't check if the partition was already inside `newPartitionsInTransaction`.
I was debating the same thing. Won't `NetworkClient` keep the node under the `CONNECTING` state though? It seems like either approach involves a change in the contract that could affect users who are not expecting it. It's an internal class though, so we just need to make sure that the affected Kafka code is updated (if necessary). It would be nice to include a test for this so that we can verify that things truly work under this scenario.
The channel is not being passed to `log.debug`.
Yeah, I can't think of a strong reason for either option, so I guess we can leave it here.
It would be better to only wrap the call to `.extract` -- otherwise, it's unclear where the exception is thrown. ``` public void shouldThrowStreamsException() { final TimestampExtractor extractor = new UsePreviousTimeOnInvalidTimestamp(); final ConsumerRecord record = new ConsumerRecord<>("anyTopic", 0, 0, null, null); try { extractor.extract(record, -1); fail("should have thrown StreamsException"); } catch (final StreamException expected) { } } ```
`counts` is not used
Still don't need these 2 lines.
I just find it odd to mutate an array that is a parameter to a method. Besides, using an `ArrayList` is more straightforward than using an array and maintaining an index ;-P
Doesn't look like next 2 lines are needed as already been set to 0 a few lines above.
Looks like all of these fields can be package private
Do we really want to do this? Might it be better to have a config for this? Or just run it with a fixed number of threads
nit: remove the `<` and `>` here and next line. Just to keep it consistent with everywhere else we do this
nit: can remove type arguments
The `CachingKeyValueStore` doesn't throw an NPE here, rather `org.apache.kafka.common.utils.Bytes.LexicographicByteArrayComparator.compare` does. We probably should add `Objects.requireNonNull(...)` to `CachingKeyValueStore#put(..)` etc
this should be three test IMHO
`rebalancing()` should never throw an `InvalidStateStoreException` as it is just constructing the `CompositeReadOnlyKeyValueStore` wrapper. The underlying stores should not be accessed until `get`, `range`, or `all` are called. So, i think this is safe to leave it as it is
Hmmm... Not sure. The "it might change in the future" argument is valuable. What do others think? Splitting seems to be the save option. \cc @enothereska @bbejeck @dguy @guozhangwang
Can `rebalancing()` throw a `InvalidStateStoreException` ? If yes, we need to split this an apply try-catch-fail pattern instead of using `@expected`
`theStore.all()` should be outside -- chaining is not good for this test.
This should be outside the try block
This test doesn't seem to belong here. The test class is `InMemoryKeyValyLoggedStoreTest`, yet the test is `shouldCreatePersistentStore` If anything this should be moved to `StoresTest`, but maybe it is already covered
nit: can remove the type params from this line and the next
This should be three tests.
We don't throw NPE anywhere in the `RocksDBStore`, so that means these are coming from `RocksDB` itself. It might be better for us to do `Objects.requireNonNull(...)` in the relevant methods
nit: here and the next line remove the `<` and `>` to be consistent
nit: don't need the type params on the next three lines
perhaps better to assert that `transactionManager.hasAbortableError` is false.
Probably need one like this but where we call `done` with an exception.
The log level is a tricky one. In the producer, messages for errors are all at debug level (for instance when we transition to error state in the transaction manager). So having this higher than debug may not add much value.
`Note when the windowed serde class is used, one needs...`
Ack. Re: ordering, I tried to do that but there is illegal forward referencing, plus I do want to group the configs together than putting them after the default timestamp extractor..
nit: final and the two below
nit: `if (max = value) else (max = Math.max)`.
Could we print both input and output partition lists, and also print as `Error found: input is ... output is ...`.
We should check that `inputRecords.hasNext` is false after the for loop, or any manners to make sure that pairing lists have the same size.
I think this would never happen now since the passed in `recordPerTopicPerPartition` is always initialized.
nit: `if (sum = value) else (sum += value)`
This check `records.size() < offset` seems a bit sketchy to me. Basically we are assuming that the input topic `data`'s starting offset is always 0, and there is no "holes" in the topic partitions so that the `offset` indicates the number of records we can read from the input topic. Maybe a more robust way to do that would be 1) read the input topics `data` and optionally `repartition` based on `withRepartitioning`, stop when the current record's offset is equal to or larger than the committed offset, and remember the number of records; 2) read the output topics (again optionally `repartition`) from the beginning to the end (use `seekTo`), and check that the number of records are the same as the number of records read from the input. Then we do not need to truncate, and also in verification we do not need to check list size again since they are already checked here.
Nit `{@code StateRestorerListener}`
nit: parameter descriptions are no sentences, thus no `.` at the end (on many other places, too). If we say they are sentences, they it should start with upper case `[T]he TopicPartition`
Ditto: newline after keywords are generally not recommended.
nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
nit: rename this function to `restoreStarted` to be consistent with other names. Such will help other code readers to understand these functions are for the same code granularity and semantics.
`final` and there is an extra space
nit: extra line
nit: `final` + next line and might as well do the previous while you are at it ;-)
could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok
nit: my preference is to mark all method params as `final`
Does this need to be in `o.a.k.streams.state` or this package? I'm just wondering..
Better state "when calling `setState...` in \@code KafkaStreams, the passed instance is expected to be stateless since.." Because not everyone understand what does "... for reporting all state store recovery.." means, stating from the API point of view would be easier to understand. Ditto elsewhere.
This should probably default to `NoOpStateRestoreListener` otherwise i think it is going `NullPointerException` if the user doesn't add a listener
I think we should probably do a null check here and throw. Setting the listener to null doesn't seem valid to me
Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll`
Pass this in as a ctor param rather than constructing it? The `stateRestoreCallback` is only used to create the `CompositeRestoreListener`
nit: keep fields with the same access level together
I'm thinking that we can simply this logic a bit: 1) in line 124 above, when `needsRestoring.put(topicPartition, restorer);` call `restorer.restoreStarted`. 2) then we can remove the `restoreStarted` boolean in `StoreRestorer` and also the line here.
This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about: ``` restoreRecords = new list.. nextPosition = -1; for (...) { if (restorer.hasCompleted) { nextPosition = record.offset(); break; } else { restoreRecords.add(...); } } if (nextPosition == -1) nextPosition = consumer.position(restorer.partition()); if (!restoreRecords.isEmpty()){ restorer.restore(restoreRecords); restorer.restoreBatchCompleted(currentPosition, records.size()); } return nextPosition; ```
I do not think we need this variable at all, since as mentioned above when initiating it will always be true, and that is the only place this variable is ever read.
we do not need to set `open = true` here again.
Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
We could do a small KIP and move the classes (preserving the old ones as deprecated). Overall, I don't have a strong opinion.
Nit: `.` missing at end of sentence
Nit: remove unnecessary blank.
If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes.
We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
For `reportingStoreListener`, better rename it to `globalStoreListener` as it is the instance-level listener, but it is not necessarily used for reporting only.
This makes me think (in a more general sense), that what would be the upgrade path for people to switch from at-least-once to exactly-once? For example in this case all the metrics would be broken as we are not having new clients. I looks to me that operation-wise it would be some burden on users to switch on / off this knob. cc @sriramsub @enothereska @dguy @bbejeck Anyways, this may be a more general discussion that we can have separately.
By the way, I wonder if we should just say it should be idempotent? Seems redundant to mention KafkaProducer.
This and `MockKafkaLog4jAppender` seems unrelated to the thread change.
Also, maybe we should assert that `numExceptionReceivedInCallback.get() > 0` if we expect at least one to fail (in theory, if `numSentRecords == 100`, there would be no exceptionReceivedInCallback).
Technically, this is `numDrainedRecords`.
Maybe we can have a numRecords variable for the `100`.
It seems unrelated to the other changes, so maybe revert it.
How about changing this to be only stoppable by ctrl-C? We are changing the rest of the examples as well in a manner to improve our quick start: https://github.com/apache/kafka/pull/3515
We need to specify what are the input message format needed for the value string: it seems needed to be an integer. Also stating the full string of the cmd line tool would help.
We should explain why the key ("temp") is hard-coded here.
Nit: `.` full stop missing.
Nit: `.` full stop missing.
We can call `selectKey()` instead which is like a syntax-sugar for such cases.
I'd suggest to replace `5000` with `TimeUnit.SECONDS.toMillis(5)`. This is better than magic numbers.
Do we need to mention something about producer and broker settings required to make this true? For the producer case, we can probably say that we do it automatically if the settings haven't been overridden.
Reading again, maybe you mean that an exception will be thrown back to the user although it may not be from the method one would necessarily expect. For example, `send` won't throw an exception due to a failed batch, but `commitTransaction` will. Instead of explaining in detail here, we should probably refer to the actual methods (which need to be updated as well).
We should also mention somewhere that we do not support concurrent transactions.
I think some high-level description of the purpose of `transactional.id` would be useful. We basically use it to tie two sessions together. We should also mention that it is unique to a producer "instance."
Should we qualify that "exactly once" for idempotent producer relies on the user enabling maximum retries and not attempting their own retry logic. Also, there is still the case of expiration in record accumulator. Finally, I'm not sure it's worth mentioning that the guarantees are only provided for the lifetime of the producer.
Maybe "to specify callbacks for producer.send() or to call .get() on the returned Future:..."
We can use `<>` in the right-hand side. Also, we can just pass the serializers in the constructor to make the example simpler.
I think we can probably remove "by design".
Use diamond (`<>`).
Nit: space missing after `for`.
Nit: "For instance, the transactional APIs require brokers with version 0.11.0 or newer."
Hmm, it seems like the `log.isTraceEnabled()` checks are not useful in some of the cases at least. If you pass up to 2 parameters, there is no benefit. See the underlying code: ```java if (isTraceEnabled()) { FormattingTuple ft = MessageFormatter.format(format, arg1, arg2); logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable()); } ``` For more than 2 parameters (it would be nice if slf4j would have overloads for more parameters), there is an array allocation, which is generally pretty cheap as well.
This line addresses my concerns. Thanks @guozhangwang.
`partitionAssignor.activeTasks()` -> `partitionAssignor.standbyTasks()`
`INFO` seems rather chatty to me. `DEBUG` or even `TRACE` might be better.
Maybe we should move "This is also effectively a cap on ... which may be different from this." to after "This setting will limit ... sending huge requests.". It seems like the latter describes the purpose of the setting while the former is an additional implication.
Maybe we should update `MAX_PARTITION_FETCH_BYTES_DOC` as well.
I tweaked this a little before merging.
Yeah, I have no doubt the performance is better. It's just that it seems like a lot of excess traffic and is going to be amplified by the number of transactional producers. It may be fine in the common case if the write markers are pretty quick, but if there is any kind of delay, then I'd be concerned about the brokers being overwhelmed with these requests (though maybe it's not as bad with request throttling). I'd rather err on the safe side for now since users can manually adjust the backoff. For the 0.11.0.1 release, we can provide a better solution. Most users will probably hold off until then anyway.
We should probably assume that people won't be using request throttling immediately since it was just introduced. But the same can probably be said for transactions.
`completeing` -> `completing` We should also probably mention that this is a temporary solution. And we should file a JIRA for the proper solution.
Also, 5ms seems a bit extreme. Maybe this could be 20ms or so and we could use the minimum of this and the configured retry backoff so that users can adjust it lower if they need to.
why `suspendImpl`? That is a pretty horrible name for a method. Similarly `commitImpl`
nit: ... suspended task is not reassigned
We added 1 line to this right? I don't know why the diff shows such a large change...Actually nevermind, I see the rest of the code now.
The names are terrible. You don't suspendImpl something, you suspend it. You don't commitImpl, you commit.
I considered this approach as well, but I think we can just drastically simplify this. I'm not sure there was originally a great reason for checking the base config, then connector config. I think this was an artifact of the two parts being split up originally (before we even had enrichment for transformations). We still need 2 parts since the connector may do additional validation, but we should only need to get everything we need together for the framework-level validation into one ConfigDef and then `validateBasicConnectorConfig` on that. I think a better approach might be to just make it easy to get the `SourceConnectorConfig.configDef()` and `SinkConnectorConfig.configDef()`, i.e. just make those public. Then you never need to instantiate the ConnectorConfig directly here. Instead, you use `ConnectorConfig.enrich()` on the `Source/SinkConnectorConfig.configDef()` as the baseConfigDef with the `requireFullConfig` parameter as `false`. In this way, this config never needs to instantiate the `ConnectorConfig` classes and you can leave their constructors as is. I think this has some other nice benefits, including that we don't *have* to skip the connector-specific validation even if there are errors during the basic validation. I'm not sure we want to make that change, but it would mean that even if you have a validation error (e.g. you are missing a connector name), you can still get connector-specific fields back in the result, which seems like a good thing to me since you'd still want to know about those fields as long as you had entered the `connector.class` correctly.
I don't think you want to get rid of the `validateBasicConnectorConfig` call. The default just calls validate, but in `DistributedHerder` it also validates there won't be a conflict between the worker and consumer group for sink connectors.
I don't think this is safe to remove since it allows the connector to perform validation beyond what you get from the `ConfigDef`. This method is used both for the validation endpoint and for `putConnectorConfig` where I think you want any exceptions thrown by the connector's validation.
Could we let `writeHeader.writeHeader` take the `ByteBufferOuputStream` instead of the `ByteBuffer` as the first parameter to avoid exposing this private function? It seems to be only used here and in the MemoryRecordsBuilder, whose caller has the `bufferStream` at hand as well.
I think it is a bad idea to let callers concern about calling `maybeExpandBuffer`; previously the caller is abstracted from this away.
Maybe we should rename this to `shouldRetainRecord`.
You could do `Assert.fail(...)` here rather than tracking it in a boolean etc
We don't really care about the stack trace here or below, just the message. We know the `StreamsException` is generated from this class and the message is enough to identify the problem. Further, these are done in a retry loop so can become quite verbose if failed multiple times
I'd go for being consistent with the other logging statements
One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.
Do we need to check if it is null here? I think it is probably ok if it doesn't throw any exceptions? Obviously it would be better if we could check that `loginManger.release()` was only called on the first invocation, but i appreciate that involves further refactoring
this is not needed as every test method gets a new instance of the class
the `null` is redundant.
Nit: can be `final`
Nit: Please use `{ }` (even for one line blocks)
Nit: `dir` only used once -- can be removed
Nit: can be `final`
Nit: can be `final`
NIt: fix indention -- either move `dir` to next line, or align other parameters with `dir` starting column.
Nit: both parameters can be `final`
Nit: can be `final`
Nit: I think `"not-null"` might be confusion. I think a better naming would be `"null-encoding-that-is-not-just-'null'"`
Nit: Method name and logic does not align 100%
I'm not sure about changing these fields from `private`. It looks like it just for testing, in which case i think we should find a different way to test.
`if (ignoreWhenShuttingDownOrNotRunning && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING))`
nit: extra blank lines
I think it is a timing issue: when all the threads of an instance start at the same time, there could be a short period of time when the first rebalance get triggered, so the transition is `created -> running -> rebalancing -> running ..`; but if some threads start later than other, then the instance may be not in the `running` state before coming to the `rebalancing` state, so the transition is `created -> rebalancing -> running .. `.
On second thoughts, could we remove the boolean param if we did something like: ``` if (newState != State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING && (state == State.PENDING_SHUTDOWN || state == State.NOT_RUNNING) ```
Ditto as below, let's think through how we want to set the state of the instance if the following can ever happen: some of the threads are shutting down or is already shut down, while others are continue running. If yes how we want to set the state.
For transition to `NOT_RUNNING`: the instance will only shutdown if the user uncaught exception handler decides to shutdown the whole instance, by calling `close()`, in this case it will still go through the `PENDING_SHUTDOWN` transition first? For `REBALANCE -> REBALANCE`, this is related to the thread-level `partition revoked -> partition revoked`, which I'm still wondering if we can avoid. Let's sync a bit on that.
The method name is confusing as it doesn't check if the thread is dead.
I don't think this will ever be true, i.e., in `start` we set the state to `RUNNING` and then we call `globalStreamThread.start()`. So the listener will be invoked with `RUNNING` while the instance is already in the `RUNNING` state. The `StreamThread`s aren't started until after `globalStreamThread.start()` returns.
I'd probably pass this in via the ctor. `setStateListener` is always immediately invoked after construction so might as well just add the param to the ctor and do away with this method
Do we really need 10 threads? Seems like 2 would probably be enough
Maybe 2 tests? `shouldTransitionToRunningOnStart` `shouldTransitionToDeadOnClose`
Nit: the method name an logic does not align 100%
There is only 1 `GlobalStreamThread`, so this field could be `GlobalStreamThread.State`, i.e., we don't need a map
I don't think we need this check. It is null from the beginning as there are no global stores associated. Also, as i said above, i think the field should be private
Could describe how the following transition can happen? 1) `rebalancing` -> `rebalancing` 2) `rebalancing` -> `not running` 3) `running` -> `not running`
I'm still wondering, when the instance state has already been in `not running`, how could the `setState` be still called? Here's my reasoning: 1) `NOT_RUNNING` is set only after all threads have been joined; 2) when a thread has been joined, its state has already been set to `dead`; 3) so after instance's state has been set to `NOT_RUNNING`, `setState` should never been called anymore.
As above, not sure we should make the `threads` field accessible outside of the class. Trying to think if there is another way to test this...
Again, could you describe: 1) Running -> Running 2) Partition Revoked -> Partition Revoked 3) Partition Revoked -> Dead 4) Assigning Partitions -> Dead
@guozhangwang yes that seems correct. It would seem to be a bug if `setState` is called when were are in `NOT_RUNNING` state
Ditto above: This is the only case where we pass in the parameter as `false`, but I think this check is not necessary as long as the state is guaranteed to be in `pending shutdown`.
Be aware that `NetworkClient` will take the min of this and the request timeout.
A spinning loop while the connection is being established doesn't seem right to me. If you're using TLS, for example, it will have to do the handshake before the connection is ready.
Is this actually needed? We didn't use it in the new test case.
It would be better if we only we only used the long backoff for the new test case.
@dguy If we need to access an inner function for lots of unit tests it usually indicates that our class design patterns are not good since unit test should be testing a class's out-facing behavior only; in other words cases that you need to trigger an inner function in unit tests should be rare, and I think reflection is fine as long as it is rarely used.
For standby tasks, I think cleanup the state directory is fine; adding it into the rebalance protocol needs bump up the metadata serialization version for upgrade and hence more complicated.
I think you are correct, we probably could carry on with the rebalance. We'd need to keep track of any active tasks that fail during suspension and remove them from the `prevActiveTasks` set that is updated in `removeStreamsTasks` For standby tasks we'd need to remove the state directory as that is what is used to determine if a thread has any cachedTasks. Alternatively we could change it such that we keep track of the previous standby tasks in a similar way to how we do active tasks.
I think it is less of an issue to be a separate JIRA, but either is fine to me.
IMO, using reflection should be a last resort. It is pretty horrible and makes the tests harder to comprehend. I'm not a big fan of making methods visible just for testing, either, but I prefer this to having hacky test code using reflection. What would be better is if there was an easy way of testing this without either of the approaches.... That would require a fair amount of refactoring and smaller classes.
Nit: add `final` to both parameters -- please follow a "one parameter per line" formatting.
Nit: `final` and formatting
I think we can do it in a follow-up PR after merging this one.
With this approach, it seems we could drop this case? There is also the insertion of the null for the UNSUPPORTED_FOR_MESSAGE_FORMAT case in `handleListOffsetResponse` that we probably can drop.
Nit: add `final`
Possibly my favorite aspect of this PR is that you have reminded me that `SchemaBuilderException` is even a thing that exists...
May be better to use a limited size rather than `Integer.MAX_VALUE`. That would make it easier to test the limit.
Are you planning to add this? It should be straightforward once you set a limit on the maximum size.
May be worth adding an error message for `aasertTrue` (in all the places where assertTrue is used).
It will be better to use `MockTime` rather than `SystemTime`. That will make it easier to test timeouts
It will good to clear the requests and test when empty as well.
Drop "The .. the .. " -> "Topics consumer is subscribing to" or "Consumer's topic subscription" Same for rest..
Some fields might be safe to just shallow copy, but I think a number of these need to be deep copied to avoid accidentally modifying the original schema. I think `parameters and `defaultValue` at a minimum need to change. Schemas seem like they should be fine since they'd just be fully replaced, not modified, anyway.
This is going to complain in checkstyle because of missing spaces around the `if` and `!=`
Ideally these should be final and set in the constructor.
For the SSL case, stagedReceives can be less than the max. OK to add an assert like the following? ```java assertTrue("stagedReceives '" + stagedReceives + "' is greater than max expected '" + maxStagedReceives + "'", stagedReceives <= maxStagedReceives); ```
Perhaps we could just verify that the accumulated completedReceives equals to maxStagedReceives.
OK, added that.
Nit: seems like we don't need the parenthesis grouping stagedReceives and completdReceives.
Should the disconnection happen in the poll immediately after completedReceives is non empty? Or is that not guaranteed? If it is, it seems like we it would be clearer to perhaps break from the loop once the completed receives is non empty.
We no longer include `addToCompletedReceives` in this measurement, but that seems OK.
Sensor names don't appear in JMX.
Yes, the sensors are created in `Sender/Fetcher` to avoid knowledge of the different names to the network layer. Recording is done in the network layer since `Sender/Fetcher` don't see all responses (now that any response may be throttled) and to use common logic.
Hmm.. The function is aimed to be a public API in `Topology` in KIP-120 (which I think will be renamed from the current `TopologyBuilder`?), but here it is only used as the function of the `InternalTopologyBuilder` which seems incorrect.
Thanks for the patch. I guess, we can be a little bit more precise here. It's a prefix for the client id of the internal clients. Might also be good to add the pattern that is used for the id used for the internal clients.
Thx! It's ok. It's just "round about" to keep Github diffs readable.
What about: > An ID prefix string used for the client IDs of internal consumer, producer and restore-consumer, with pattern '<client.id>-StreamThread-< threadSequenceNumber >-<consumer|producer|restore-consumer>'."; Cf. https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L454 and https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L664
Maybe this line and the one below could be moved into `start_jmx_tool`? If we go that far, we could do the locking in there as well.
Hmm, is this required? Normally `render` should automatically have access to all fields of the object it's being called on. I'm not sure why the `security_config` one is there either.
I think that's the optimal compromise. Keep `null` since it's a keyword and avoid starting a sentence with it. *Any* seems to work fine here.
I figured. I think in this case capitalized is better, or simply start the sentence with "Any " to avoid the decision. 
We should not expect an exception here but insert a `fail(...)` after `config.keySerde()` within try-catch-block. Same for all other test.
Nit: Move this line before the try-catch-block. The try-catch-block should only contain the stuff we want to test.
nit `CREATE` -> `CREATED`.
We should not expect an exception here but add a `fail(...)` after `streams.setUncaughtExceptionHandler(null);` -- and thus, not rethrow within catch.
nit: We should probably add the same instruction to the message in the `SerializationException` case as well.
Did you have a specific use case in mind where this was possible? Typically deserializers are deterministic.
nit: we're missing a period and space following the partition. I can fix this when merging.
Intuitively, I would expect `cachedRecordFetchException` to be set to null on the next line.
I see. Wouldn't it be better to fail fast if we introduce a similar bug in the future instead of logging a warning? That way, we'll hopefully trigger a test failure and fix it.
Nit: might be worth adding a simple assertion on the result just to make sure.
Makes sense, thanks for the explanation.
Could we use `TestUtils.waitForCondition`? That will time out if it takes too long for the condition to become true.
Great catch! Do we have a test for this bug? We should include this fix in 0.11.0.1.
Could you create a JIRA to keep track of this task: after java7 is dropped, we can add them as static functions for the `StreamsMetrics` interface itself (this is only supported in java8), and then also fix `ProcessorNode` and `StreamTask` to get rid of hard cast of `impl` classes.
Ditto here, can be moved into the StreamsMetrics interface as part of the follow-up JIRA.
This is also not introduced in this PR, but: inside `close()` the right ordering should be call `closeState` first then `context.close()` since the former will access the latter as well. Currently it does not introduce any issues since `context.close()` only shuts down the metrics, but moving forward we may modify it to make the later access failed.
Can we just call the overloads with two parameters here? Ditto above.
Do we need to add `@SuppressWarnings("deprecation")` here, too? And elsewhere in this class
I think if we deprecate a class we do not need to deprecate all its member functions, but we need to check any callers that may pass in this class object or return this class object as deprecated. Reference: https://stackoverflow.com/questions/15908887/how-to-annotate-deprecation-of-a-class-in-java
This is not introduced by this PR: the name has a typo: through => throw
And why is this test deprecated as well? More generally it seems the `context#getStateStore` function was being deprecated but it was not explained in the KIP wiki.
This field can be final as well.
We also need to explain a bit why we add a type converter at this layer of the store hierarchy.
remove this line
remove this line
we need to remove this line, too. It is taken car of in `mapValues(ValueMapperWithKey...)`
This should be new `ValueMapperWithKey`
"given a read-only key"
"with a read-only key"
nit: `final` I know it wasn't before, but let's stick to making params as final
You can remove the type params from `KStreamMapValues`
The next 2 methods aren't used in this PR. I guess they are used in one of the others...
nit: group these imports with the other `ValueJoiner` etc imports
i think we should just stick with `joiner` for the name of this param. here and elsewhere
nit: align params
remove this line
nit: this change doesn't look intentional
with a read only key
super nit: move method params up one line to start after `join(`
nit: args on separate line
typo: Woth -> With
super nit: ditto from above
Was this intentional? `VALUE_SERDE_CLASS_CONFIG` is deprecated.
We need to think about how we can avoid this. The package structure appears to be working against us.
We can just use the class name itself.
We can call the static function of KStreamImpl directly and get rid of the additional function in `InternalStreamsBuilder`.
Do we need to reference the whole path here, since we already import the class above? Ditto elsewhere.
nit: typo in description
Nit: add `{}` to block
We should not exit hard here, but return a appropriate error code if necessary, and propagate it back. Only `main()` should use `exit` to return error code to OS. Using `exit() `makes it hard to test code.
@guozhangwang is this not true already? In `suspendTasksAndState` we only throw an exception at the very end of the method. Also, any tasks that failed to be suspended are closed with `task.close(false)`
I think this is already true too? in `performOnStreamTasks` if a `ProducerFencedException` is caught we call `closeZombieTask`
we would have already closed the task with `task.close(false)` at this point
nit: make the `mapper` final
nit: add final to parameters
same as above for parameters
nit: `final` params
I don't think we need this test as the previous tests already prove that the data is deerialized or not. So this is really just testing the same things
We should make this test be fore `<byte[], byte[]>` as then we have covered both key and value deserialization.
Just realised we don't need this anymore.
Maybe worth a mention that only the legacy `ControlledShutdown` is missing the client id field.
There are some request header tests in `RequestResponseTest`, we should move them here so that we can find them all in one place.
We should probably check that `clientId` is still null.
Yes, I had misread the code originally.
Nit. use `{ }` for all code blocks
Not introduced by this PR, but please fix indention.
You can make this `package-private`
Can we use `assertThat(node.name, equalTo("source1")` or `assertEquals("source1", node.name)` instead of `assertTrue` for assertions like this? Elsewhere in this test, too
nit: parameter/line formatting
Exactly. A sink, can only have a single topic, and this it's type is `String` -- for sources, it can be multiple and thus type is `Collections<String>` (or `Pattern`). And if you call `Collection#toString()` it will as `[]` -- but not for plain `String` type.
Current code doesn't follow this. It's of course a matter of taste (as always). I think it makes sense as it makes diffs easier to read on Github if a parameter gets added/removed. Thus, going forward, I would prefer this formatting (and we should reformat "old" code incrementally -- similar to `final`). Btw: I added this rule to the new coding guidelines on the Kafka webpage. If we don't want the rule, we should remove it. Also, the guideline says 120 chars at max length. I am fine with 100, too, if there is a strong opinion on this.
No we don't have that rule. Personally i think it is fine as long as it fits on a single line, i.e., less than 100 characters.
From String#compareTo() ``` while (k < lim) { char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) { return c1 - c2; ``` Should the size comparison follow the same ordering ? i.e. size1 - size2
It does not always add "1" -- it adds `node.size` -- but maybe a fix would be to remove `delta` parameter and only add `1` -- as `updateSize` is recursive this sound correct. Right now, you would double count: `A(1) -> B(1) -> C(1)` all initialized with `1`. First update `C` adds `1` to `B` and `A` resulting in `A(2) -> B(2) -> C(1)`. Next, `B` is updated, adding `2 to `A` resulting in `A(4) -> B(2) -> C(1)` what is not correct (but maybe sufficient for ordering anyway -- but not sure.)
I think for calling methods single line is fine. But for defining method, we should always go with one parameter per line.
I am not sure, if this will do the update correctly. There is no ordering guarantee for the outer loop; IMHO, this could lead to double counting? Maybe it's not an issue though, as the counts are never exposed to the users and only leveraged internally to sort nodes (ie, as long as the numbers are larger/smaller in relation to each other, the final sort is the same). But I am not sure.
Ack. Got it :)
@becketqin is right-- you should handle this case. Perhaps the server sent back bad data. The way to handle it is not to throw an exception, but to complete the relevant future(s) with an error. There are a few other cases where we handle bad server data by completing a future with failure in AdminClient.
I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED. What if we need to return different error codes later? We should have an error code per log dir response.
nit: the 'else' can be omitted.
If we do not expect this to happen. Shouldn't we throwI IllegalStateException? In this case, if the broker returned a replica that is not in the request, the broker may have somehow misplaced a replica. We should probably alert in this case.
nit: i don't think `@note`
`volatile`? It is potentially being accessed by different threads and the synchronization is on different objects
Similar here. I think we should wrap any logic for changing the `state` into `setState`. Currently they are spread in three functions.
I think the lock grabbing hierarchy of instance `stateLock` and thread `stateLock` is still vulnerable to deadlocks. For example: thread 1: `StreamThread.setState` --> grab `stateLock` of thread --> `stateListener.onChange` --> `checkAllThreadsDeadAndSetError` --> trying to grab `stateLock` of instance thread 2: `KafkaStreams.setState` --> grab `stateLock` of instance -> user-specified `stateListener.onChange` --> user calls `KafkaStreams.close` inside that callback --> wait for all the stream-thread to shutdown, but thread 1 is blocked on grabbing the instance `stateLock`.
Will that ever happen, though? In `maybeSetRunning` we only call `setState(RUNNING)` if all other threads are in the `RUNNING` state. So i guess it is more to stop moving to `REBALANCING` or `ERROR`
Thanks for bringing this up @dguy @enothereska , thinking about this more I feel it is Okay to state that users should not call any public APIs of the `KafkaStreams` object inside this callback, if they do then undefined behavior.
Did we mean to swap `REBALANCING` and `RUNNING` around? If people were depending on the `ordinal` then this will break them
nit: in unit test we may want to avoid calling `create` because it will construct a lot of unnecessary modules like task manager / created / clients etc, we should consider mocking all these in `StreamThreadTest`.
nit: maybe we can separate AbstractTaskCreator and its two impls into separate classes once we are finalizing the refactoring in a follow-up PR (this PR can stay as is to keep it from exploding LOC)
This log entry could be misleading, that even if there is an exception happened and hence no task created, it will still print as `created ...`; and in practice I have once encountered this issue before which affected the trouble shooting process, I think we should try to piggy-back fix it.
Can we pass in the `TaskIdToPartitionsProvider` interface not the `StreamPartitionAssignor` here? If yes we can rename the function as well.
Is the `try-catch` still needed here? And is it possible that some `Exception` be thrown from `restoreCallback.restoreAll` below? If yes we need to think if we want to handle it here, or in higher-levels of the call trace.
I think the root cause of the hanging is that, the `list offset` response returns empty indicating the broker did not know about this topic, and the client will hence retry forever. So what I suggest is that just making the metadata refresh once in `position()`. As for the flaky unit tests @baluchicken , we can use waitForConditions in those test rather than block waiting until it succeeds.
Whether it's a new config or a new API, we'll still need a KIP. If we go that far, we'll want to consider the other methods which probably suffer the same problem (e.g. `seekToBeginning`). A couple options that we have thrown around are the following: 1. Introduce a `max.block.ms` config like the producer has. 2. Overload the methods with timeouts. 3. Use the request timeout. Not sure the best way to go here to be honest. Maybe for `position()`, we should just raise a `UnknownTopicException` instead of retrying.
The issue is that local cache may not contain this topic metadata yet or not up-to-date, and that's why we may want to send an `MetadataRequest` in these two function calls.
Actually when I was discussing with @huxihx on the JIRA itself what I was thinking is the following: 1. The call traces of `Fetcher#resetOffsets` which use `INT_MAX` are from two public APIs: ``` Fetcher#resetOffsets -> Fetcher#updateFetchPositions / Fetcher#resetOffsetsIfNeeded -> KafkaConsumer#updateFetchPositions -> KafkaConsumer#poll(timeout) / KafkaConsumer#position() ``` So we should pass in a `remaining` in the internal call from `KafkaConsumer#updateFetchPositions` all the way down to `Fetcher#resetOffsets` to replace `INT_MAX`; As for the public APIs, `poll` already have a timeout, we just need to let its internal `pollOnce` to pass in the updated `remaining` to `updateFetchPositions`, for `position` since it is defined as a non-blocking call, we should pass `0` to `updateFetchPositions` and if the call fails to return the offset in a single trial, it means no offset is cached locally and the remote offset request failes as well, we can throw `InvalidOffsetException` directly. By doing this we do not need a KIP. Admittedly it may not be optimal to treat the non-blocking `position()` call like this, but this is to adherent with its semantics; if we do want to change its public behavior then I'd prefer option 1) from @hachikuji to modify the semantics of "non-blocking" calls of consumer as we did for producer.
Yes I think after we update metadata, we should do a sanity check on the number of partitions. If we have stale information (i.e. the topic's state prior to deletion), then the partition will still exist and we can retry. But once we have updated state, we will see that the topic has a fewer number of partitions and we can raise an exception. Then how we handle it depends on the context.
API changes will call for a KIP I'm afraid. We'll also want to ensure that we preserve the old method for compatibility.
I was referring not so much to the `toString`, but specifically to its use as the logging prefix. In the producer `TransactionManager`, we use `[TransactionalId Foo]` for example. I don't feel too strongly about it, so we can leave it as is if you prefer.
Hmm, are security configs really relevant here if we don't do anything with ACLs? Seems like a lot of parameterizations here and we already get coverage from the `test_file_source_and_sink`? In general we've started to be more careful about a ton of parameterizations and covering more within each test since the setup/teardown costs can be quite substantial.
If we're just testing broker compatibility I don't think we even need this part of the test.
same here as what i said below. You can use a `assertThat`
nit: make the keys a `Set<String>` and then do `assertThat(keys, equalTo(Utils.mkSet("2","3")`
We do not need id as it is included in the prefix already.
Could we make the following function signatures a bit more consistent: ``` allAssignedTaskIds, suspended, restoring, running, previousTasks, runningTaskIds, allInitializedTasks ``` E.g. `Collection<T> XXTasks()` and then extract taskId / etc from the callers if necessary.
Actually let me put in this way: `restoredPartitions` could just be local to this function? It's only usage outside is in `clear` so it seems we can keep it local or just use `restored` directly and remove it from `clear`.
Should we consider including the error message of `e` in to the exception as well? also nit: capitalized `Fatal`.
Was trying to save one short lived object, but maybe I was too paranoid.
The problem is that `previousTasks()` is called in a bunch of places other than in `onPartitionsRevoked`. For example: 1. in the finally block of `onPartitionAssigned`. At this time the all the suspended tasks are logically closed and removed. 2. in `streamThread.prevActiveTasks()` by the `StreamsPartitionAssignor`. At this time `onPartitionRevoked` is called so that suspended tasks are constructed. 3. in `streamThread.shutdownTasksAndState()`. At this time the suspended tasks should logically be empty. I think the reason that it works now is because `suspend()` returns the copy of the values so iterator.remove does not actually moves it. In other words `suspended` list is never cleared except in `maybeResumeSuspendedTask` until the thread is shutting down. So it sounds like this list may increase indefinitely across rebalances? Ideally we should have empty suspended list at the end of each `onPartitionAssigned` and do not rely on it for `previous tasks`.
In the task constructor we already created a bunch of modules, like the metrics and the producer object. We need to make sure these modules still get cleared even when the task was not initialized.
Okay, could we have two signatures then? ``` Collection<T> XXTasks(); Collection<TaskId> XXTaskIds(); ```
Do we need to clear it here? I feel it is unnecessary as it will be cleared in `reset` anyways.
`restoringTaskIds` never called.
nit: maybe set 5 to a variable `numRetry` to make it more clear
Wild thought: should we let `unlock` return a boolean indicating if the unlock is executed, and assert `unlock` here instead of line 317 below? Maybe can be done in another PR.
I think topic should also be included in comparison. Even though the current usage can work without considering topic, given that it is a public API and can be used in a more general sense, it is better to be strict and conform to the specification.
As we mentioned in the ticket, the only test case that requires the underlying `metrics` object is `testMetrics`. For this purpose we should just rewrite this test function, to not reuse the class field `task` which is relying on the `MockStreamsMetrics`, but create another task that does pass in a real `StreamsMetricsImpl`.
Similarly as in `StreamTaskTest`, we should just use a different `cache` object in the `testMetrics` function than the class field `cache` which gets a real `StreamsMetricsImpl` directly.
Similarly for `ProcessorNodeTest`: the only difference is that for that test case we can introduce another overloaded constructor for `MockProcessorContext` which takes a `StreamsMetrics` object, and then pass a real `StreamsMetricsImpl` only for that `testMetrics` class while for all others just use the other constructor which always use a `MockStreamsMetrics` object.
We do not recommend using wildcards in imports.
nit: reorder the imports alphabetically
nit: debug statement differs from other clients
Sure, I'm just saying that `msg.value()` is an array, which does not have a useful `toString` anyway.
Is this log line needed? Seems like we get all this info in `onCommitCompleted`.
Up to you I guess. No need to expand the scope even further for a tiny nit.
Did we want to use `valueAndSchema.value()` for the last parameter? Same for the message below.
Yeah, that might be better.
nit (sorry): would be nice to be consistent with capitalization of the first letter.
Right. The `onPartitionsRevoked` callback always occurs prior to the rebalance and `onPartitionsAssigned` afterwards.
nit: could we replace "generation" with "sequence"? The term "generation" is already used by the consumer which makes this message a bit confusing. Same for the other log messages below.
No need to fix this here since it would be risky to change, but don't you think it's a little odd that we increment the sequence number even if nothing has changed? I wonder if it would be more intuitive to bump the sequence only prior to the call to `doCommit` below.
I wonder if this message and the one in `doCommitSync` is overkill. Maybe we could change the first log message in `doCommit` to include whether it is async or sync. For example? ```java boolean syncCommit = closing; log.info("{} Committing {} offsets: {}", this, isSyncCommit? "sync" : "async", offsets); ```
I think we're missing a `{}`. Same in `doCommitAsync`.
Maybe we can combine this log message with the initialization one below? The `subscribe` API does not actually do any IO or anything, so it doesn't seem worth logging separately, though it does seem useful to know what topics are being subscribed to.
as above. Name of the test can be more descriptive. `shouldRequireBrokerVersion0101OrHigherWhenEosDisabled`
The test should describe what it is doing, i.e., `shouldThrowStreamsExceptionWhenBrokerCompatibilityResponseInconsisent`
nit: I don't think we should bother checking the string messages as it is quite brittle. The important thing is that the exception is thrown
again, naming of the test
Looked into it further and the test does work. Not sure why it didn't work, but it is reliably working now
If we decide to not turn on bulk loading, then we should be able to not close and restart again after restoring is done.
We can merge these two functions into one since the `hasSstFiles()` is the only caller of `hasSstFiles(File)`.
This is interesting, so the new method introduces ambiguity in this case. Doesn't seem too bad since `null` is not a valid value though. cc @hachikuji
Yeah, seems fine. Either way, the result is an `IllegalArgumentException`.
@adyach having the parsing here could be a good idea but exiting from the application maybe not. This should be the base class for other command options classes but changing the application flow could be misleading here (i.e. Exit.exit(1)).
@adyach I have noticed that this "hidden" logic is what already happens today for the `ConsoleConsumer` tool for example (in the related `ConsumerConfig` class) so I think that we can live with that. FYI as part of the refactoring for #3453 (for having it more testable), I decided to use that PR for introducing the `CommandOptions` class even getting your good ideas here. In this case when the #3453 will be closed we should have a first version of some of the common components we need for tools refactoring.
groupId is still here
extra groupId argument
placeholder may not be required for exception
This is going to call punctuate on every node - right? Which is not actually correct, I think something like @guozhangwang suggested might be a better approach.
Should we have a variable for the data directory? It seems like we reference it in many places.
For my benefit: what is the reason for removing this? I also don't understand the related change in this file.
We should probably use jcmd in the kafka shell scripts as well. In a separate PR, of course.
Do we need to do this `close` and `open` here? We do it also on lines 283 & 286
Do we need the if/else? Since this is a unit test, it seems OK to just assert that the first element is the rate and the second is the total.
Should probably be `createIoThreadRatio` since we mention `I/O thread` in the messages.
The other constructor calls the parameter `sampledStat`. We should be consistent.
Nit: should we call this `rateUnit`? Same for the other constructor.
Are you sure this is correct? It's too different packages and thus the full package name in required IMHO.
Ah. Great catch!
Btw, we should take the chance and make `version` and `commitId` final. Something like: ```java private static final String version; private static final String commitId; static { Properties props = new Properties(); try (InputStream resourceStream = AppInfoParser.class.getResourceAsStream("/kafka/kafka-version.properties")) { props.load(resourceStream); } catch (Exception e) { log.warn("Error while loading kafka-version.properties :" + e.getMessage()); } version = props.getProperty("version", "unknown").trim(); commitId = props.getProperty("commitId", "unknown").trim(); } ```
We should close the producer.
Hmm, there's no reason to log this. Also, the check should be outside the `try`. Something like the following would do: ```java Objects.requireNonNull(topic, "topic cannot be null") ``` I'd use a similar pattern in `Metadata.add`.
`Metadata.add` should also not allow a null `topic` either. Also, we provide an error message if a user provides a null `topic` here.
I may be wrong, but my understanding was that 400 and above are errors for which error response is returned in `connection.getErrorStream`, while the code is currently reading response from the error stream for >= 300.
As above: `Arrays.<Fault>asList(FAULT_D, FAULT_B, FAULT_C, FAULT_A)`
While we're at it, we may as well use another `{}` substitution for the parameter and remove the unneeded `toString()`.
I know you just moved these lines around, but while you're doing that it probably would be worthwhile to combine these 2 statements into one. If the log is busy, these might not appear next to each other.
I think we might want to avoid holding the lock when invoking `backingStore.set` if it's not needed. The problem is that the lock is also needed in the callback. This can lead to deadlock if the backing store implementation uses its own lock. For example, if this lock is A and the backing store uses lock B, then this method will result in A-B lock order. But the asynchronous callback invocation might result in B-A. I _think_ this is not a problem for any of the backing store implementations we currently have, but we may as well be a little defensive about future implementations.
This should not be static, but it can be final.
It would be more concise to just store the config into a `transactionalId` variable and do a null check here.
Let's be consistent and just use string concatenation for both fields.
Let's use `KafkaProducer.class` instead of `getClass()`. The logger is not exposed to sub-classes, so the context should be this class.
Since we're passing the same arguments most of time, maybe we should create a factory method for the accumulator.
nit: unneeded newlines
nit: unneeded newline
You can either make them non-static or pass `Logger` as a parameter. Makes no difference to me, but `log` won't work as a static field when you have multiple instances.
Why not just use `new LogContext()`? Then we don't need the null check below.
Should Builder pattern be used for the Sender ? That way the code is more readable when new parameter is added.
Include the exception in the log so there is an indication of what went wrong? Looks like it was previously included.
Maybe we should simply pass the `ProducerRecord` in the constructor? We could then also just use the `ProducerRecord.toString` in the error so that we don't have similar issues in the future.
This should probably just return a boolean
Is it intentional that there's a space before the colon? Also, is the stacktrace useful here? Or do we just want to print the error message. I haven't looked in detail, so a genuine question.
This was originally to try and prevent a dead-lock, i.e, the `UncaughtExceptionHandler` is triggered by Thread-1. The user calls close (still on `Thread-1`). Thread-1 join will never return as we are executing on `Thread-1`, but we already know it isn't running. I think we still need this check
I'm not sure returning `true` is valid. We don't actually know if all the threads have shutdown. Though, i'm not entirely sure what to do about it. Perhaps we need to extract the shutdown Thread as a field and then we can check if it is still running. If it isn't running then we can return true, otherwise we should try and join on the thread with the provided timeout
Hmm, it was moved outside of the `stateLock` for this reason: https://github.com/apache/kafka/pull/3622#discussion_r131438053
Nit: I think the state transition should be done before the INFO log.
Why don't we need this check anymore? It's still done for `globalThread`.
I was referring to the @throws clauses at the bottom where neither seems to have been added. Seems like both are possible.
Oh you mean `UnsupportedForMessageFormatException`? That doesn't seem to be added.
It seems to be added on line 703.
You're right, we should not add the @throws lines then.
super nit: not part of this PR proper, but maybe set `RuntimeException` returned from the `suspendTask()` call to variable then use it in the `assert` statement? Makes things a little easier to understand.
Since we have three threads for this test, there can be multiple rebalances before the streams instance stabilize and start processing...
shoulc => should
`self.partitions` not updated? It will be good to fail the partitioned producer/consumer test if the partitions were not created properly by the test. Otherwise it is hard to know what was tested.
I think this `LogContext` should be passed in? It should be the same as created for `StreamThread`
looks like place holder is missing for the last argument in the earlier code itself.
missing braces for `restored`
e.getMessage will be more accurate.
nit: no need newline of 104 below.
Hmm, if there is any exception from here, we probably want to bubble it up to the caller. For example, in SocketServer.processNewResponses(), if send() hits any exception, we want to call updateRequestMetrics(() and avoid updating inflightResponses, and move on to the next response in the queue in the same loop.
We don't need to log this on every request. Perhaps in ApiVersions.update(), we can log in debug level of any request in nodeApiVersions that's older than the version the client has. This way, this is only logged every time a client connects to the broker.
This should say `AdminClient`, not `Consumer`.
Maybe we should say `initialized` instead of `created` for consistency with the consumer.
Maybe we can say "Kafka admin client has been closed" for consistency with the consumer. When grepping, it's easier to if things are consistent across clients.
Let's capitalize the log statements for consistency. There are a few of these.
nit: if you use a local variable for `lastSequence`, then you can also use it in the log message below.
typo: drop "the" before "whether"
Discussed offline, but I guess the one case where it is not safe to adjust sequence numbers is when the number of retries are exhausted.
My understanding is that we reset the producerId if we find one partition for which we have expired all in-flight requests. My question is how this affects the in-flight requests of other partitions? If one of them happens to need retry, wouldn't we hit this case? But since the sequence number isn't changing, I think it should be safe to continue retrying with the old producerId. Alternatively, maybe we should drain all partitions before resetting the producerId. That's a bit heavy-handed, but hopefully this case is rare in practice.
This method tests multiple things at once and thus should be split into multiple methods. Also, use self describing names for the methods. For example for the first test: `shouldWrapUserSerdeFromConfig()` or similar.
super nit: `byteValues` on 164 and `windowed` on 168 should be final as well
ditto for the rest of the test
Makes sense. There is still some cleanup to do.... `windowedDeserializer` is not use anymore in the test but only `windowedDeserializer1` -- also, setting up `props` has `put` and later `remove` that can both be removed... When cleaning up: consider what is *required* to set up the test, and what is not. Remove everything that is not required.
Nit: add newline
you don't use `windowedDeserializer ` or `inner` below -- can be removed IMHO
`null` check is redundant as `null instanceof StringDeserializer` will return false anyway.
There is not reason to specify this twice. If you want to test that it work for both parameter names, it should be two different test, each testing one parameter name
nit: "Start restoring..."
... should not be...
can we just return here to make it clear that we are baling out? We then don't need the further `if(!initializable.isEmpty())` checks below
`endOffsets.get(topicPartition)` can be replaced by `offset`
Is this actually true? I don't see where we do partition by partition anymore
nit: make final
nit: make the parameters final, and if you could put them on separate lines, similar to the `process` method on 327. Ditto for `assertNextOutputRecord` on lines 330 and 334 above.
@sebigavril I know that 2 of the `assertNextOutputRecord` methods were already there, but if there is some way for us to reduce the duplicate logic that would be great. Unfortunately, I don't have any good ideas ATM.
super nit: I know this pre-existed, but IMHO line 77 a little tough to read what about ``` innerStateSerde = getStateSerdes(context.applicationId(), bytesStore.name()); .... private StateSerdes<Bytes, byte[]> getInnerStateSerdes(String appId, String storeName) { return WindowStoreUtils.getInnerStateSerde(ProcessorStateManager.storeChangelogTopic(appId, storeName)); }
Hmm.. I guess that makes sense. The pattern looks kind of funky, but I can't think of an obvious alternative. By the way, is it intentional that we don't reset `now` after the call to `await`? If so, it might be better to rename that variable to `startMs` or something like that to emphasize that it doesn't actually reflect the current time after calling `await`.
Yes. But if we add some more parameters later on, it would simplify the diff. But it's also ok to keep as it.
Nit: please use single parameter per line formatting
On the broker-side this is not fatal, but typically caused by a mis-configured client. For clients, it is typically fatal, but could sometimes just be a clock-mismatch where a retry could succeed.
Are all `SSLExceptions` fatal? Also, we should probably update the log message.
Please add description for the parameters
Yes, I think it's worthwhile to check the result of `Connector.config()` just in case `Connector.validate()` is overridden and the default check there is no longer used.
This definitely doesn't cover the full space of errors that are possible here -- `asSubclass` could throw a `ClassCastException`, `newInstance` could also throw `SecurityException`. I think the `catch` was broad because this ensures that except for extreme cases like other `Throwables` or `Errors` we get everything converted to `KafkaExceptions`.
I was referring to the call to `connector.validate` two lines above. That is where the other null check in this patch in `Connector` would be applied, unless the user has overridden `validate`.
What do you mean checking again? This is just checking the `ConfigDef` returned by the connector (which was only called on the previous line) and this method is called from places that wouldn't have already validated it.
nit: extra space between `printed` and `options`
Ack. Thanks for clarification.
This does not seem to be backward compatible as we ignore `keySerde` and `valSerde` now...
Nit `.` at the end
nit: `null` -> {@code null}`
same as above, missing . at end
nit: single parameter per line
nit: both lines missing . at end
nit: single parameter per line
nit: missing . at end
nit: to get the formatting with a class, put into a single line as below.
My preference is to reduce the number of parameters where we can, especially for consistency WRT KIP-182, but I don't have a strong opinion, so I'd be okay if we left it as well.
nit: `true` should go to new line.
I am wondering, if we should to even more refactoring an pass in `Serialized` directly here to reduce the number of parameters. Not sure though if the win out weights the refactoring effort. Same for the other PRs btw. Any thoughts? \cc @guozhangwang @bbejeck
`keySerde` -> `valueSerde`
nit: missing . at end
nit: missing . at end
I think we can introduce a ``` boolean setStateFromCreated() ``` in which we grab the state lock, and then check that state is `CREATED`, and then call `setState(RUNNING)` assuming that `sychronized` is reentrant.
nit: parameter/line formatting
`{@code Joined}` `[o]ptional para[meter]s`
new line for `MockValueJoiner`
`{@code null}` -- same below
line/sentence formatting `{@code null}`.
For this function, logically it reads the field `state` multiple times, while depending on the compilation it may be just read once and cached in register, but we cannot guarantee that. So I'm not sure if it is safe to remove the `synchronized` keyword.
There is only one value returned. This method can be declared as void.
Hmm.. I don't think it would necessarily require a KIP to check for negative offsets on the broker. I guess it depends what error code we return. In any case, we'd probably still want the validation on the client side.
You mean if the offset is out of range? I'm not sure we have a good way to check this at the moment. It can't be done on the coordinator because we don't know what the valid offsets are for each topic partition, so that leaves the client where the check may end up stale anyway. By the way, there are a couple `commitSync` overloads that may need to be updated as well.
What is the offset commit is positive and invalid? cc @hachikuji
I'm happy for this to be merged if @hachikuji is happy fwiw.
Hi, may I ask why do you do `@link` instead of `@see` annotations? :)
Since we're saying 0.11.0.0 or higher elsewhere, it should be 1.0.0 or higher here, for consistency.
nit: Could we also assert that the file names have been renamed to the new style as well? @dguy
This overload can be removed, too IMHO
nit: `topic name` (no plural) -- remove `must contain...` -- add `cannot be {@code null}`
above (some some more times below)
nit: `{@code null}` (same below)
remove this overload
There may be others which belong here like `ssl.client.auth`.
@rajinisivaram, hmm, I'd rather us specify the details or link to a config that specifies them. With security, people often struggle so the more information we can provide, the better.
methods => `mechanisms`
For SSL authentication, the principal is the distinguished name from the client certificate (this is significant since even custom principal builders will probably derive principal from client certificate, but rather than DN, use specificfields like common name). To be accurate, SSL default needs to cover different cases: 1. `ssl.client.auth=required` or (`ssl.client.auth=requested` and client provides certificate) => principal is the distinguished name from the certificate 2. `ssl.client.auth=none` or (`ssl.client.auth=requested` and client does not provide certificate) => principal is `ANONYMOUS`
`selector` -> `serialized`
We typically use `(` instead of `{` for `toString` (although there is some inconsistency.
I was not sure if `MaterializedInternal` constructor can be public if `Materialized` constructor is protected. But as it can be, we don't need a static method (that would have been a workaround if `MaterializedInternal` constructor would have been protected, too)
Why does it have the same issue if added to `MaterializedInternal`? ``` class MaterializedInternal extends Materialized { protected MaterializedInternal(final Materialized m) { super(m); // } public static MaterializedInternal fromMaterialized(final Materialized m) { return new MaterializedInternal(m); } } ```
I see. `MaterializedInternals` must be `public` and cannot enlarge the scope if `Materialized` constructor is `protected`... What about adding a public static method `MaterializedInternals#fromMaterialized(Materialized)` that calls the copy constructor? This way, we could make it protected IMHO.
nit: blank missing :P
Should this be protected? Compare the discussion in https://github.com/apache/kafka/pull/3807#discussion_r137603218
Why is this changing? `infos` are not used below.
nit: A latest code style suggestion (which is not currently strictly enforced in AK Connect code) suggests to put every argument in its own line in the presence of multiline calls. You might want to apply this pattern in new changes, even if the checkstyle is not enforced right now.
I'd say calls as well. It's very common to apply similar style to both. In any case, with this still being a nitpick, I think it makes sense to have 1) all args in one line (not flexible) 2) args split in exactly as many lines are needed by the width - which leaves several args per line and 3) have 1 arg per line. There's not too much uniformity in AK right now, but wanted to let you know of a trend.
Ok, it looks better now. Let's leave it this way, with two lines.
Same issue here wrt connector name vs type. We probably need some chain like `connectorType(connectorClass(map))`.
This is a fairly complicated line, so I'd recommend pulling out the connector class name as a variable assignment just before line 433. And, these 3 lines are calling `configState.connectorConfig(connName)` multiple times, so that should probably be pulled out to a local variable as well.
The `connector` parameter is actually the _connector name_, not the connector class name. And as mentioned above, the `connectorType(String)` method expects the class name. So this needs to be addressed, likely by getting the classname from the configuration. The fact that this is being done in at least 3 places seems like something can be made simpler.
null handling here again -- should probably be an exception if we can't find the info we need, which then converts at some point into a RestException
Except that it would allow us to centralize the logic and simplify the code. That seems beneficial.
The worker only maintains the state of the connectors that it is executing. A specific connector will only be running on one worker. The other workers will not have any state for the connector. So we will only be able to determine the connector type on the worker which is executing it.
Same thing here as above: probably need to use `worker.getConnectorType(className)` here.
And here, also: `worker.getConnectorType(className)`.
I'm afraid I might have led you down a rabbit hole. The `Worker.isSinkConnector(String name)` uses the Plugin mechanism to properly load the class with the correct classloader. My recent request to move that logic into the WorkerConnector would simplify the logic, exception in a case like this where we don't have a WorkerConnector instance. So perhaps the best thing is to use the new `worker.getConnectorType(className)` that basically just uses the `worker.isSinkConnector(className)`. Apologies for causing you extra work. It would have been nice for my suggestion to work out, but its the case where that isn't known that is causing the difficulties. Using the worker should always work (assuming the connector is installed to begin with).
Yes, you'd need to find the name of the `Connector` implementation class for a given connector name. If we can't find that because we don't have the configuration, then we might just have to return null.
I am not sure. I would prefer to keep the current paradigm in which the worker only tracks the running connectors, but all the classloader logic makes it a little tricky to load the class from another context (I am not as familiar with this code). Maybe another option is to add the type to the configuration directly on creation since we already load the class in order to validate configuration and we already do some other config enrichment. cc @ewencp In case you have any thoughts
All nodes in the cluster have access to the global connector configuration and can respond to REST apis, but the `Worker` class only tracks the connectors being executed on that particular node. To find the type of a connector which is not executing on that node, we probably have to pull the classname out of the config using `ConnectorConfig.CONNECTOR_CLASS_CONFIG`.
This code is also getting the connector class name twice. Probably be better to do that once before calling the `onCompletion(...)` method.
Maybe we can use `setIfExists`.
This is not thread-safe. We should return the local variable instead of the field. Also, it would probably be safer to make the static field volatile.
Maybe we should say `a partition (which consists of log segments) can grow to...`
I am not sure about the additional sentence. It's not quite accurate since it depends on the size of the active log segment.
This should be static.
Nit: add `Cannot be {@code null}.` -- Guess also somewhere else
var `initializer` seems to be redundant.
I meant for `aggregate(Initializer, Aggregator)`, i.e. without having `Serde` also and use defaults.
nit: may worth explain how `queryableStoreName` can be find from `materialized` below.
Nit: add `Cannot be {@code null}.` (maybe somewhere else, too)
nit: add `@Override`
It is to avoid unnecessary connections being made to update metadata e.g. from `Sender` thread which will continue to retry if `needUpdate` is set. The flag will be set again if the application performs another operation (e.g. another send), but we are avoiding automatic retry.
Curious what the reason for this is.
@rajinisivaram Yeah, I think we are handling the common (initialization) cases in this patch, just a bit discomforting to leave some of the other cases a little nebulously handled. In this particular case, it doesn't really seem worth trying to prevent additional metadata retries since the NetworkClient will still enforce a backoff and we expect the user to just close the client anyway. I would probably leave `needUpdate` untouched as we do currently on failure.
@hachikuji OK, I am fine with that. @vahidhashemian Can you remove the `needUpdate` change here? Thanks.
Might not a big problem, but I wonder if we should check for the authentication exception before the `wait` as well? It is possible that `awaitUpdate` returns before the authentication failure happens. A subsequent call may then begin with the `authenticationException` not null which would cause a needless `wait`. I think an easy solution is to move this line up to the beginning of the `while` block.
Hmm.. In the consumer, we only fetch metadata if we know we need to, and that is sometimes conveyed only through the `needUpdate` flag (as in the case I mentioned). One additional note: this flag alone is not sufficient to prevent metadata retries if the metadata max age has expired (which will be the case on initialization).
Would this be better as an `nc -z` test? Grepping logs was always kind of a half-assed solution, and the real test we care about seems to be whether anything is actually listening on the port. We have this now for at least jmx (thanks to yours truly) and ZK (thanks to @kkonstantine) and I'd like to continue the trend elsewhere as it is far less brittle than grepping logs.
Wouldn't it be better to do this right after the last `pollSelectionKeys`? We can even just do `readyKeys.clear()` then.
If I got this correctly, we have 3 overloads (topic, topic+consumed, topic+ consumed+materialized). Does a 4th with topic+materialized not make sense? Btw: with consumer+materialized, users can specify key/value serde twice (ie, if default serde must be overwritten, they need to do it twice? -- maybe we can improve this?) If double specification is required, we should have a check that both key/value serdes are the same? Applies to `#globalKTable()`, too.
Also a quick question: if `Consumed` does not specify the same serde as `Materialized`, should we just use different serdes then? I'm asking this mainly because today we will do a deser reading from Kafka and then a ser writing to state store, and maybe we can avoid this deser/ser together as an optimization. But if we allow different serdes here we cannot do that.
4th overload makes sense to me as well. I like the idea of placing the serdes in `Consumed` into `Materialized`, but I'm trying to think would there ever be a case where they need to be different? I can't ATM, so I it's a yes for me.
I think in DSL, users may still wants to access a global state store in `process/transform`, that was the motivation for adding this API. Personally I'd vote for option 2) above, to keep the APIs succinct without semi-duplicated calls. But seems you all prefer option 1) in terms of user convenience, so I'm fine with that as well.
I think people want to use a global store in DSL, too. And forcing people to call `build()` to add one, is not a good idea IMHO. (cf option (2)). Also, if you argue like this, we could remove `addStore`, too, because people can also add the store via `builder.builder().addStore()` -- however, in KIP-120 discussion, it was explicitly requested to add both methods to `StreamsBuilder` to avoid this pattern. Also note, people who are new and want to use a stateful process() will always ask: how can I add a store? There is not API on `StreamsBuilder`.
For option 2 seems a little awkward IMHO, I second what @mjsax says.
Should be: ``` Thread.currentThread().interrupt(); ```
I think we do not need to reset here since we are throwing the `InterruptException` again, which will set the flag as well.
Apparently the my understanding of `TreeSet` is not accurate. It uses the comparator to decide whether the entries are the same or not. We can use a TreeMap<Long, Set<ProducerBatch>> then. We may also want to bucket the timestamp a little bit, say one second to avoid huge amount of Sets created for each ms in the `TreeMap`.
It seems we don't need the `deliveryTimeoutMs` in the sender. It is only used as an argument passed to the accumulator. But the accumulator already has the config.
We usually just use `earliestDeliveryTimeout` in Kafka.
The check 'if (deliveryTimeoutMs <= (now - this.createdMs))' inside maybeExpire() would be true. Looks like another method can be created inside ProducerBatch which expires the batch.
in not -> is not
This variable can be dropped.
I was thinking about this too. Using millisecond as unit for Map key is not prudent. After the switch to second as unit, we may need to check the two adjacent buckets keyed by ts-1 (sec) and ts+1 (sec).
This test has nothing to do with linger.ms anymore...
Hmm.. Might not be too important, but it doesn't seem necessary to include the retry backoff in this check. If the user sets retries=0, then the backoff shouldn't matter.
It seems we may release the memory for the expired batches before the response is returned. This means the underneath ByteBuffer is still referred by the ProducerBatch instance in the inFlightRequests. I am not sure if this would cause any problem, but it seems a little dangerous.
The while loop may break if the request size has reached. So there is no guarantee that it will iterate over all the partitions. One alternative is to find the nextBatchExpiryTimeMs in the expireBatches.
Similar to above we should rename this.
They can't construct a kafka producer with the changes made in this PR.
Assuming `nFlightBatches` is a TreeSet suggested above, this code can be simplified to: ``` while (!inFlightBatches.isEmpty() && inFlightBatches.first().maybeExpire(deliveryTimeoutMs, now)) { expiredBatches.add(inFlightBatches.pollFirst()); } ```
`tp` is not used anymore.
No longer used.
This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
This would be just `inFlightBatches.add(batch)`
It seems better to say Producer.send() instead of send.
We are passing `now` everywhere else. Maybe we can just keep the argument name the same.
deliveryTimeoutMs should be mentioned
It is probably cleaner to have an explicit `EXPIRED` state.
isFull is no longer used.
The original reason we have this optimization is because we used to have a big sorted data structure. So avoiding inserting elements to it makes sense. Given that now the batch order in the RecordAccumulator is already guaranteed. It seems we can just put all the drained batches to the inFlightBatches queue, which is simpler.
Understand. That part can be refactored - goal is to reduce unnecessary comparison.
Still not used.
If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all.
I don't see bucketing
@sutambe I also think we should mention the scenario that a Record is added to a batch that is about to expire.
To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization.
We should change the test name to something like testBatchExpiration. and the test below to testBatchExpirationAfterReenqueue.
The typo is still there.
Do you mean it should NOT be included...
typo in the test name.
Personally I still think a clear EXPIRED state would be clearer. We can let batch.done() method take a FinalState argument instead of inferring the state from the exception.
request1 and request 2 are not used.
The logic for updating this field seems to assume that the batch at the front of the deque will always be the next to expire, but I'm not sure that is true in the case of retries.
After we reset `earliestDeliveryTimeoutMs`, it seems that we do not take into account the expiration times of in-flight batches.
Checking my understanding. With this change, it should no longer be possible to expire a batch before linger.ms has completed and the batch has been closed. If so, do we still need the logic to abort appends on expiration? (It might be safer to have it anyway, just checking if it is still needed for correctness)
We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time.
Shouldn't we also be removing the batches from the inflight set when the batch is completed (failed or successfully)? I might be missing something, but I don't see that code here.
As far as I can tell, it shouldn't be possible to abort a batch after it has been completed. Is this correct? If so, I think it might be better to continue to raise `IllegalStateException`. It's preferable to keep the allowable state transitions as narrowly defined as possible since it ensures faster failure for unexpected paths.
This is still not used
Still not used
I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
createTime -> creationTime
nit: 'else' can be dropped
This method either throw exception or return true, which indicates there is no need to have a return value.
The method never returns false. We can keep it as void if so.
We may need to call sender.run() one more time to ensure the message is not reenqueued. The reqenqueued message won't be sent out again in the same sender.run().
The map is not used.
The map is not used.
Got it. Good catch. Yes, we should remove the completed batch.
In line 126 above, maybe we can just call `e.toString` which will include the message string if it contains? otherwise LGTM
to me it seems like we can't possibly know what the constraints of all reporters would be and they don't provide an interface for validation, so it should be up to them to figure out how to substitute. but i've also asked some other folks to maybe chime in here who may have better context on how we've handled this elsewhere.
We did the dot to _ conversion for Yammer metric mostly because reporters like Graphite typically use dot to represent hierarchy and quite a few people are using the existing Graphite reporter that may be confused with dot. Since Kafka metric is new, we could just let individual reporter deal with this issue, instead of changing the metric name directly.
you can just do the conversion to unmodifiable map one time in the constructor. it looks like at the moment this is only accessed in tests anyway.
Oh, I just noticed. Then `synchronized` is not needed anymore.
It was removed from the other versions of `group` but not from here.
I think key-pairs of strings as list is brittle. But we inherited that.
Why would `workerId` ever be `null`? And does having the `CONNECT_WORKER_ID_SEQUENCE` really help since all your workers would just have ID = 1? If this is just for tests, seems better to just require the ID to be passed in since we effectively require it for `Worker` and everything else.
might want to rename `workerId` so it doesn't shadow the member field. something like `workerIdOpt` could work
ah, right. nah, that's fine. just when reviewing I had the thought that if we guaranteed non-`null`/non-empty in the constructor, this wouldn't be necessary. i realized that it was actually intentional, but easy to miss when reviewing here and not getting the same highlighting as an IDE
I know the naming thing has bit us in the past, is this same approach used elsewhere and/or how was it decided on? Specifically, metric name constraints really shouldn't be JMX specific if that is the case here, despite the fact that the metrics is so obviously JMX-inspired. I can easily find https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala#L46 but nothing else. Have we not had the same problems because metrics w/ topic names in them already have constraints on the naming? If I am remembering correctly, I think maybe both @gwenshap and @junrao were involved in some discussions, I think `-` vs `_` was a problem at some point? Maybe one of them could chime in here.
nit: Seems that this is not the right place for this method. It's now between static and non-static field members. It's used in the constructor, but still, it should probably be placed lower in the class.
Allow me to begin by saying that I admit and like the simplicity of this pattern. Still, if this method is frequently used, and if the `gets` >> `creates` (are expected to be way more), then an alternative would be: ```java private final ConcurrentMap<String, MetricGroup> groupsByName = new ConcurrentHashMap<>(); ... public MetricGroup group(String groupName, boolean includeWorkerId, String... tagKeyValues) { MetricGroup group = groupsByName.get(groupName); if (group == null) { Map<String, String> tags = tags(includeWorkerId ? workerId : null, tagKeyValues); group = new MetricGroup(groupName, tags); groupsByName.putIfAbsent(groupName, group); } return group; } ``` which is almost identical in terms of code. This pattern might waste a few objects on initialization, but otherwise it relieves the map from global synchronization on `gets`. If both `gets` and `creates` are expected to be just a few it doesn't worth it probably. Can't say because the use of `group` in this PR is not demonstrated.
bit-nitpick :) ``` // modulo 2 operation if ((keyValue.length & 1) != 0) ``` most definitely optimized out by JIT because the divisor is known (`2`). Is it too hard to read? Sometimes it's good to train ppl who read the code by example. Can't be more nitpicking than that and if you want to keep uniformity with other versions of `getTags` elsewhere never mind. Not 100% sure either, leaving here to ask what you think in general for such optimizations that are JIT optimizable too.
Do we want a `ConnectException` here instead? Not sure.
empty line needed
Yeah, there's some didactic aspect to a few lines that are just a bit harder to read of course. (for instance if it was `var` instead of `2` things would be different). But I was on the edge too. Fine with leaving it.
True there's nothing concrete. Google Java Style Guide mentions a logical order but nothing specific. There are some old Sun conventions: http://www.oracle.com/technetwork/java/codeconventions-141855.html and then, if you look at Intelij's rules, under Code Style -> Java -> Arrangement you get a feel of some other conventions. But beyond guidelines and given the absence of clear consensus my point is that, besides static initializer blocks, I'd expect member fields, constructors, methods in that rough order.
Sure, I was referring to future coding errors that will be revealed only during runtime. The current use is fine. I'm fine with NPE.
We should probably include the full package name `org.apache.kafka.common.metrics.MetricsReporter`.
nit: `If subscribe is called previously with pattern, or assign is called previously.` This is to make the explanation more concrete. Ditto below.
nit: an aync
Hmm.. should we enforce `partitions` to be not null actually? @hachikuji
This is a good catch.
nit: `<` is unnecessary.
"... is called previously... " without a subsequent call to `unsubscribe()`? Same below.
Huh, weird. Didn't realize we implemented this behavior. Seems like a better way would have been to have a no-arg `seekToBeginning()`. I think I'm with @guozhangwang. Maybe we just raise an exception on null? This matches current behavior.
We probably have to keep the `size() == 0` behavior for compatibility.
This seems redundant.
Do we want to add a couple extra words ` which returns a WindowedKStream enabling count, reduce and aggregate operations` or something along those lines? The same goes for the other deprecated aggregation actions.
`use {@link #toStream()} followed by {@link KStream#to(String)} and {@link StreamsBuilder#table(String)} to read back as a {@code KTable}` ?? same below
Personally, yes, I prefer one call, but I leave it up to you.
Seems like "topic won't be created" is specific to one of the requests.
if we keep ending up with this pattern, it might be clearer to create a `Listener` implementation that delegates to a list of listeners instead of chaining them manually this way
Relatedly, I think there might be some sort of checks in unit tests in maybe the producer or consumer that validate metrics are unregistered, might be able to use a similar approach here.
this state is missing from the KIP, it should be added
paused -> running
I don't see sensors being removed, but for tasks we'd definitely want to remove them when the task gets reassigned to another worker. I haven't thought it through, but this might be the right place to be doing that rather than marking it unassigned.
`error` is unused
we just generally try to avoid the mess if the code isn't ready to be committed yet. it can always be held onto in a branch, wip pr, etc
We missed it in the first round probably because those metrics only had 1 tag, but the `tags` method should use something like a `LinkedHashMap`. Without this, the tags can get jumbled. I noticed this because jconsole's hierarchy was listing the task ID above the connector name.
I think this should ensure it preserves the order of the tags.
I mean it's odd to submit an empty job to the executor in order to verify progress. Why not call `get` on the close future itself.
I dont remember why I did that. Looks like `future.get` would be better.
to be differentiate from the previous error log: `base state directory ...`
Nit: fix indention (`final` keyword should start at same indent)
This needs to be there otherwise the test won't run
this could be set to: `this.repartitionRequired || streamImpl.repartitionRequired`
could move line 368 above this and then declare this as: `String [] parentNames = {this.name, streamImpl.name}`
nit: we usually do not use unnecessary numbers as part of the parameter; rename to `streamImpl` instead.
We should check `stream` parameter is not null here.
nit: unnecessary extra lines: one empty line is good enough.
Nit: remove unnecessary `this`.
Hmm, doesn't seem like this is correct. >version ['1', '0', '0-SNAPSHOT'] major_minor ['1', '0'] Extracting ['tar', 'xf', '/Users/ijuma/src/kafka/core/build/distributions/kafka_2.11-1.0.0-SNAPSHOT-site-docs.tgz', '--strip-components', '1'] Traceback (most recent call last): File "./release.py", line 235, in <module> command_stage_docs() File "./release.py", line 227, in command_stage_docs cmd('Extracting ', 'tar xf %s --strip-components 1' % docs_tar, cwd=os.path.join(kafka_site_repo_path, docs_version(version))) File "./release.py", line 108, in cmd output = subprocess.check_output(cmd, *args, stderr=subprocess.STDOUT, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 566, in check_output process = Popen(stdout=PIPE, *popenargs, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 710, in __init__ errread, errwrite) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 1335, in _execute_child raise child_exception OSError: [Errno 2] No such file or directory: '/Users/ijuma/src/kafka/../kafka-site/10'
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Maybe we should remove the `Collections.synchronizedList` wrapper(s).
Nit: add `final`
final Why do we need to catch an fail here? We can just let the exception bubble out to fail the test
Looks ok to me.
This is also the default, I think.
This is the default.
We can pass the serializers in the constructor and it's a bit more concise.
Nit: long line.
@hachikuji, if you have a chance, have a look and see if this is OK.
Should we add a method that is like this one, but returns the exception instead? It seems a bit convoluted to throw a stored exception just to catch it. Not sure if that helps other cases, but at least here, it seems to make the code cleaner.
Why are we calling `handleFailure` directly instead of `fail`? If we used `fail` in both places, we could extract a method and call it twice in succession, once for `newCalls` and once for `callsToSend`.
The serialization package is open to everyone (it's public API and at the lowest layer). So I don't think we should worry about that. It's not like we're avoiding a dependency here, we are just hiding it via a string based config (that still requires the default constructor to be present).
yeah, other stuff from `RuntimeMXBean` was just a suggestion, obviously i wouldn't limit to that. classpath was the main one i saw that is useful. i don't think that it's too confusing since it is also alongside a bunch of other general system info. also, we still support the classpath approach, we just won't fix conflicts. we've definitely had cases on the mailing list that ended up being classpath issues that we probably could have spotted the likely root cause more quickly if we had had access to the classpath info.
Should we add some more stuff to round this out (and make use of all the support for reporting more than one value...), e.g. some other `RuntimeMXBean` info? For example, classpath info seems like it'd be useful (probably more so before `plugin.path`, but still probably handy from time to time).
I'm fine either way, the main difference is that everything listed so far is global and not really specific to Connect (and can therefore be processed and logged immediately upon startup) whereas getting `plugin.path` requires doing some argument parsing and would be logged later on by the normal argument parsing `logAll` call.
I'd suggest moving this static method after the non-static methods.
Maybe this should be trace level. I can imagine it being very spammy when you have a lot of partitions. Also, we usually capitalize the first word.
Do we really want to always set like this? What's if a user want to provide a custom Long-Serde that just works fine? Maybe we should only overwrite if value serde is `null` ? It's just a thought -- not sure about it.
Is this really worth it? It seems like a `forMagic` without a transactional id gives you most of the benefit.
I think you should create an interface rather than passing in the `AssignedTasks` class, it seems you only need a single method. The contract should be on an interface rather than the class.
nit: "another thread wrote to ..."
why this? You already have the position from above
By doing so we do not need to change the current APIs at all.
Instead of passing the whole AssignedTasks object and hence introduce this penetrated dependency, could we instead throw the MigratedException as is, and only add the task information in `updateNewAndRestoringTasks` and re-throw? Then the exception's message function would depend on whether or not the task field is null.
really? we have the position already from either the last record offset or from the previous call to `consumer.position(...)`
Could we check that `close()` was not called, i.e. no exceptions get thrown in easy mock then? Ditto below.
Don't think we need to change it from the for each loop. We don't need to remove the closed tasks as they are immediately cleared after the call to this method
IMHO the `close` method is little easier to follow by putting `if(clean)...{ }` block in a private method, possible name `closeIfClean(clean, task, t)`
yeah, i was thinking we could inline this method into `close`, too
Yeah, it's a good question. `IllegalArgumentException` doesn't feel quite right. Another option would be `IllegalStateException`. Also, we should probably mention the configuration property in the exception message.
The message doesn't seem to match the condition above.
from the KIP, I think this is supposed to be `source-task-metrics`
Since these exception messages are unrelated to the bug you're fixing, it would be better to leave the inconsistent punctuation for another PR.
"incompatible with the *broker's* configuration" would be a bit clearer, I think.
This test case seems like overkill since we're already checking the `beginningOffsets` path in the `KafkaConsumer` tests.
I think this test condition is unnecessary. The `poll` in the line above completes only when the future is done. Same below.
nit: `if check_order:` One of these above also.
Did you consider writing the doc strings here instead of inside the request objects. Might be more convenient to find that way and it matches the same pattern that we use for `Errors` (for example).
No strong preference, but I think it's fine if the high-level description of the API is decoupled from the request/response objects, which are mostly just concerned with schemas. The location in `ApiKeys` makes the descriptions a bit easier to find.
It should be something like "Sent by an (admin) client to get data about consumers groups managed by a broker. To get a list of all consumers groups in the cluster, it needs to be sent to all brokers."
It should be something like "Sent by an (admin) client to get data about a specific consumers group like main information about members in such group."
Just a minor nit. Using "(admin)" instead of "admin" as you have already done for the others admin client related messages.
I think similar to produce and fetch, LIST_OFFSETS should be sent to the leader of the partitions
It's sent by the (admin) client to the leader for the topics partitions included in the request
Do we need this? The test doesn't actually use or test it so it seems irrelevant
Hmm, good question. I may have actually been wrong about which values should be involved. I think @gwenshap and I had a long discussion about this awhile ago too and there are many ways you could define lag. I think the real problem here is that we may not be exposing enough information from the consumer to compute what I would really think of as lag -- FetchRequests include high watermark info so you know how many records are in the log but not yet returned to you, and the consumer creates metrics based on that. But we don't have access to that info. A connector that commits on every message would look like it has 0 lag, but it could be very far behind in the topic.
Ok, sorry, I'm thinking more about this now with review, and I guess this will always just be either 0 or 1 batch of messages since the processing -> put() will be synchronous for each batch collected from the consumer. So I guess maybe the committed - consumed makes sense as it is the total still thought to be somewhere in flight (or more accurately, not yet known to be guaranteed delivered into the destination) does actually work. I think, as you mentioned, lag is just confusing there because you could be completely done processing, the data could be in the destination, and we may just not yet have gotten to a periodic commit yet. I mainly would worry about that since connect defaults don't commit all that frequently and it is hard to say what it means if, e.g., the HDFS connector returns a large "lag" since it *needs* large "lag" to write large files. :( sorry, i think this might need some more thought
right, i may also be playing fast and loose w/ terminology in my responses :) But I think we're on the same page as to the meaning of the different options. My concern with the HDFS uncommitted case is that compared to consumer lag where, at least roughly, you could pick a number and use it across all topics for alerting, this doesn't have that property. The value you would alert on for HDFS is completely different from the value you'd alert on for ES. In this way "lag" definitely seems inaccurate terminology -- the connector isn't really "behind", it just intentionally has uncommitted data. I see a few options: * Include both! Something like "processed-lag" and "committed-lag". * Don't use "lag" when referring to uncommitted data. It could just be something like "uncommitted-count" (which then opens the question of doing it per partition, max across partitions, or sum over all partitions). And of course these aren't necessarily mutually exclusive. The potential drawbacks I see with the first point is that maybe the processed lag just isn't that interesting since it is bounded by the # of messages that fit in a single fetch request anyway, and introducing both may lead to confusion if we're not super clear in the docs about which is more important to monitor. In particular, now that I've thought through it more I'm struggling to find a problem I could discover / diagnose based on the processed lag. The drawback to the second is that we're diverging from the terminology used elsewhere. But maybe that's fine since, as we're discovering through this conversation, they are in fact different things. You could always re-raise this in the KIP discussion thread to get additional feedback. Also, might be worth looking at whether streams has any relevant metrics and how they handled this. In some ways they have the same problem -- they have the actual consumer lag, but also the time for a message to make it through the topology and all downstream messages to be acked. At best I would guess they could track lineage within a subtopology and provide info based on that, but I am guessing those metrics may simply not be there due to the more complex processing graph that they have.
should we just do this per partition? if there were any negative values, this could still give incorrect information.
sorry, that was unclear. i meant put the `Math.max` inside the loop and check `diff` instead of the aggregated `activeRecords`. you shouldn't get negative numbers, but if a connector does something wonky with the offset commits, it could happen that committed offsets are ahead of consumed offsets.
Not sure this is what people will generally mean by lag -- while the committed offset matters, normally if the consumer is in the process requesting the lag I think it'd mean the FetchRequest lag, i.e. how far behind *processing* the records is the consumer in comparison to what the broker indicates is the most recent offset. in other words, I think i'd update this at the end of each `put()` and change from `committedOffsets` to `processedOffsets`.
same question as other pr -- this is `sink-task-metrics` instead of `sink-tasks-metrics` in the KIP
I thought the timestamp would uniquely define the segment in which that key is stored.
Here we need to do: `final Agg oldValue == newValue == null || sendOldValues ? fetchPervious(..) : null;` This is because `SessionWindows` have a dynamic time range, the the start is always fixed. So we need to send deletes for the previous smaller window when a window is merged, i.e, a simple count: a@0 -> SessionKey(key=a start=0, end=0), 1 a@5 -> SessionKey(key=a start=0, end=0), null (delete this as it is merged) SessionKey(key=a start=0, end=5), 2 (this is the new merged session)
@xvrl there is no `get` on `WindowStore`. We could add one and it would work in scenarios where we don't have duplicates, i.e., the key for a WindowStore is (recordkey, timestamp, sequenceNumber) - if the store doesn't have duplicates the sequence number is always 0. If the store does have duplicates then we don't know what the sequence number is. Without a KIP to add a `get()` to `WindowStore`, the only thing we could do is add a bit of a hack to see if the inner most store is a `RocksDBSegmentedBytesStore` and then we could call `get(..)` on that. If it isn't, then we'd still need to call `fetch`. For the DSL this would work as the only time we have duplicates in the `WindowStore` is for joins and we disable caching for those so it skips this code path. However, for the PAPI, we would need to always disable caching if duplicates are set. Which we probably should do anyway as it won't work as is.
Thanks for the explanation @dguy, very helpful to understand where caching and sequence numbers come into play. It might be worthwhile to put this in a JIRA somewhere. I do think it would be a useful optimization to have eventually, as fetches have some setup / teardown overhead.
There is no `get()` for a `SessionStore` The key in the session store is a combination of the record key, start and end time. We only know the start time for the previous key so we need to find the previous session with the correct start time.
nit: remove blank lines.
Perhaps we should not change the return type here unless we decide to make the more extensive `LinkedHashSet` change across all APIs (and corresponding KIP).
Doing this would also make the compile complain about all instances that instantiate MetricNameTemplates with a non-order-preserving Set, which I think is what we want. As it currently is implemented in the PR, I don't think the compile actually catches anything.
Kinda, but it had a TreeMap before, so it used the "natural ordering" of the Strings which means... something. Anyway, are we okay with the fact that the ordering may change according to the person who built the docs? The kafka.apache.org webpage will then show whatever order the site-builder's JVM used? Or is it Jenkins? We don't really have any other choice, other than making everyone use LinkedHashSets, right? And we decided we don't want that.
The sentence "Note that the order of the tags..." is not applicable because there is no set in this case. It's just the order of the varargs.
fyi, the pr for non-double stats got merged, so we could switch this back to the original design / fill in the missing string metric
How about "if the proposed replica assignment is invalid. For example if some of the partitions have different number of replicas or a duplicate replica assignment was found"
Might want to emphasize "the policy configured on the broker"
We don't use LeaderNotAvailableException in listTopics
We don't use UnknownTopicOrPartitionException in listTopics
not used here (InvalidTopicException is used instead)
We don't use ReplicaNotAvailableException in listTopics
We don't use TopicAuthorizationException in listTopics
`InvalidTopicException` indicates that the topic name itself is invalid, not that it collides with another topic name
Should be larger
This is only one reason why the request might be invalid. Probably best to just say "if the request was invalid" -- the specific exception text explains more about why it was invalid.
It might be better to be more specific here. "if no authorizer is configured on the broker"
Can't we pass a `null` and add a check with `transitionToRunning()` -- it's not a critical code path...
readyPartitions is already created. It seems we don't need the extra Set created inside readyPartitions() - readyPartitions can be passed to readyPartitions()
Please add @return
Style nit for "note" and whitespace between `Bytes` and `byte[]`: > (note, state stores always have key/value types {@code <Bytes,byte[]>} should be > (note: state stores always have key/value types {@code <Bytes, byte[]>}
Typo: > or use the default RocksDB backend[] by providing ...
> For high-level DSL, should be > When using the high-level DSL, ...
> For low-level Processor API, should be > When using the Processor API, ... (IMHO we should also stop saying "low-level" PAPI. It's simply a different API.)
Typo: "you can create [a] windowed ..."
Okay :) I guess it is personal taste then, I'm going to merge as is.
Consistency: Here we say > with [a] custom "auto.offset.reset" strategy and below we say > Enum used to define auto offset reset policy when creating {@link KStream} or {@link KTable}. The AK docs say neither "strategy" or "policy", it's just a config like any other. Hence I'd suggest to change the above consistently to sth like: > with [a] custom {@code auto.offset.reset} configuration and > Sets the {@code auto.offset.reset} configuration when creating {@link KStream} or {@link KTable}.
Typos: > You can read [a] topic as `KStream` with [a] custom timestamp extractor ...
Typo: > Similar[ly], you can ... with [a] custom ...
Suggestion: We already say "Kafka Streams API", "DSL", and "Processor API". That's already a lot of "APIs" even though it's all about the same thing. I wouldn't label interactive queries to be yet another API. I'd just say: > ... to access the content via interactive queries:
Suggestion: I am not sure which terminology we use elsewhere, but "changelogConfig" may suggest to the average reader that there's a separate "changelog" concept that they may have never heard of. It's simply a topic configuration. I'd suggest to rename `changelogConfig` to `changelogTopicConfig` or the shorter `topicConfig`.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
Why "queryable-store-name"? IIRC we don't say "queryable store" anywhere else in the docs -- we use the term "interactive queries", if anything.
That is correct: ``` {@link org.apache.kafka.streams.kstream.KTable KTable} ``` will show on java docs as `org.apache.kafka.streams.kstream.KTable`, while the above will show as `KTable` whose ref links to `org.apache.kafka.streams.kstream.KTable` still.
Hmm, this seems different than the KIP and seems to provide less information, e.g. rebalancing never shows up. I figured this would either reuse the enum from AbstractCoordinator or put a new one in AbstractHerder that the herders then update. Despite the class name matching, I think this should really refer to the worker process rather than the state of the `Worker` class. The class is really just a container for connectors/tasks so it doesn't have enough information to understand the state of the whole process.
That was what I meant.
Consider moving this line into the block above
Worth stating that the default is `-1` like you did in the PR.
I guess another way would be to let this configuration be a delta which is added to the the current time. That way we wouldn't have a lot of messages with the same timestamp, which might be a little uncommon in practice.
Ah, ok. That works.
What do you think about logging this as a debug message? Basically, grab the class that's returned (after being loaded) and output a message saying the class was loaded.
what threw me off with the original message is that it gave the impression that it had just finished restoring those partitions, giving the impression it kept restoring them over and over again. In reality this log message just reports that restoration is still in progress and that so far we have completed the given set of partitions, but it might log this message at arbitrary times during the restoration process, not only when a partition was completed.
The wording here is a bit confusing. It makes it sound like we are restoring partitions that have already been completed
maybe "Restoration completed for partitions:"
For completeness I would add ``` The action may be invoked by the thread that calls {@code whenComplete} or it may be invoked by the thread that completes the future. ```
and also add it to thenApply while we're at it
For this to be useful to me in my work on KIP-183 I also need `addWaiter()` to be made `public` on `KafkaFuture`.
I think at the moment, we should never get here, so `IllegalStateException` is fine.
We can also get here if handshake has already failed (state == `State.HANDSHAKE_FAILED`) and there are still bytes to be flushed in `netWriteBuffer`.
firstException wouldn't be null at this point - see call on line 510 It seems call on line 510 can be removed
That is a good catch @tedyu . Thanks for that. cc @mjsax for confirming.
nit: in _the_ accumulator
Why are we not just checking the `sizeInBytes` ? Cache.size returns the number of items in the cache and is unrelated
Do we need this check here? Seems like we'll never hit this condition since we check after incrementing the size in the loop above and return if an overflow occurred.
Maybe we could restore this and do this in the constructor: ``` this.maxPollTimeoutMs = Math.min(maxPollTimeoutMs, MAX_POLL_TIMEOUT_MS) ``` That removes a bit of the awkwardness of using `5000` in `KafkaConsumer` without explanation and it ensures that there is no behavior change for the other usages of `ConsumerNetworkClient`.
No, what I was suggesting is to add synchronization to the methods inside `Heartbeat` itself. For example: ``` public synchronized long timeToNextHeartbeat(long now); ```
This is fine for everything past 1.0.0, but if we do want to make things releasable using this script on trunk (similar to how people use the merge script probably), then we'd want to maintain support for the 4-component version and validate ones starting with 0 vs >= 1. I'm fine going either way (not maintaining that for simplicity and bug fixes on older branches will need to use the older release script, or just adding in a bit more logic here).
Hmm, I don't think this is right. We still need to build Scala 2.12 separately since it requires Java 8.
That's fine, might just need to be made clear on the release instructions wiki since it is different than how, e.g., the merge script is used.
I was thinking we'd just use the release script from the older branches for older releases.
Maybe consider JUnit Parameters here, but fine as is. EDIT: Thinking some more about this, I'd leave it as is.
Nit: `streamsBuilder` -> `builder`
Ack. Missed that part...
Maybe add `@see Punctuator`
AFAIK this should really be the stream time when the operation is being called when running in `{@link PunctuationType#STREAM_TIME}` and the current system time when the operation called when running `{@link PunctuationType#WALL_CLOCK_TIME}`
Maybe add `@see Cancellable`
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Not sure we need the while loop since it waits for a day at least.
typo `direcctly` -> directly
typo `operaate` -> `operate`
semicolon not needed
100ms? Perhaps it was useful to have the constant since it at least made the unit clear.
I see a unit test failure because these parameters don't match the constructor of `BasicPlatform`.
Why is `completed` count being tracked? It doesn't seem to be used anywhere.
`log` not used
Should change to `POST` to match Java code changes.
I was thinking: ``` if (tries > 0) Thread.sleep(tries > 1 ? 10 : 2); ```
Yes, that is fine.
final here as well
nit: final for parameter here
nit: make parameter `final`. Same for other test methods.
I think it would be better if the users don't have to look in the cause.
I am not sure about this. If you are sure that's the case, we can remove the handler.
Shouldn't we sync broker mock time ? IMHO, in a test, there should be only one mock time object that is used globally. With this change, we get an internal broker time and a external test mock time and both are decoupled.
This seems to overlap with #4095 -- should not be part of this PR IMHO.
This needs to go outside of the `try` because the call to `release()` is in the `finally`. If we're going to do this, I would suggest moving the validation checks outside of the `try` as well.
acquireAndEnsureOpen can be moved here
I think most (all?) of these assertions are very obvious and the long text is just repeating the code. Does the text add value of is it just noise? Also, the lines should not be longer than the GitHub review window.
If you use the `assertEquals` that takes a `double`, you can pass a `delta` value, which makes the code a lot more concise.
Add license header
Seems like this should be a `long`.
This isn't used in this class. Probably should move to `AssignedStreamsTasks`
Seems that this class is a bit redundant, i.e, we could just construct an `AssignedTasks` with the `logContext` and `"standby task"`
Why do we need to do this? `log` is private and is not used
Looked into this part as well. I think extracting `initializeNewTasks` for active tasks and standby tasks is a bit overkill than just letting `StandbyTask.initialize()` return true.
null means "return me every topic you know". The empty list means no topics. (This changed in a previous AK version)
Please, let's not. The other functions in AdminClient do not rely on metadata caching-- they use the latest metadata that is available. Deleting records shouldn't be a common operation. If it is, we can have a metadata cache with a configurable expiration time. I think it's also really bad to set an exception based on possibly stale information. You give the user no way out if the cache is stale (besides creating an entirely new admin client object, I suppose).
With an empty list it means "return me every topic you know".
I don't think this will work, will it? You should specify the topics that you want metadata about. If you specify an empty list, no topic info will be returned.
I don't think that will work, because the metadata object that is maintained in AdminClientRunnable does not fetch information about any topics. In general the way the Metadata object works is a very poor fit for AdminClient-- it was designed for producers and consumers, where you have long-running subscriptions and so forth. It is much simpler and better just to make the metadata call here-- we can always optimize this later with a cache if needed.
Ditto here. I think we should consider getting rid of the metadata request and exposing any exceptions from this request to the user's expected delete record response, instead we just rely on the whatever the current metadata (up-to-date or not) and if there is no leader known we set the future exception immediately.
Hmm.. we already have a `metadata` object that is keeping updated by the `AdminClientRunnable`, can we just call `metadata.fetch()` to get the current cluster information? Then in line 1918 if we do not have the current leader we can still return `LEADER_NOT_AVAILABLE` to let the caller retry as it is a retryable error code.
Actually, nvm. Just to clarify: `leaderFor` may return null either 1) the metadata cluster does not have this topic partition at all, or 2) the topic partition info exist, but its `leader` is null. For case 2) we should already have an error code and checked in line 1911 above already. But case 1) may still exist, for example, if the topic exist but with 4 partitions only and you are requesting to delete on that topic's partition 5.
Since we are sending a metadata request with specific topics instead of "asking for all topics", when `node != null` we will always see a `Errors.LEADER_NOT_AVAILABLE` on the per-partition error field, so this check should already be covered in line 1911 above.
nit: ditto here, `cluster()` will reconstruct a new object on each call.
Thanks for the catch!
As per our offline discussion, we'll leave it like this for now
If we throw InvalidTopicException directly, be change public API (possible exception are part of the API) and thus, this would require a KIP.
You need to insert a `fail("Should have thrown IllegalArgumentException")` to make sure that the test fails if no exception is thrown (same below for all the other tests with this pattern).
I think @omkreddy is just suggesting to create a helper to avoid the code duplication. Seems like a good idea to me.
Just throwing on the first is probably fine. Alternatively, if you want to list them all, I'd suggest iterating through them and collecting them into a collection rather than using suppressed exceptions.
Actually it's not exactly 3X v.s. X. And here is the difference: Assuming the broker is down, then without this PR the producer would first use `request.timeout` to throw the exception for records in its accumulated queue, and then gets caught here and retry sending, and upon retries it will wait up to `max.block.ms` since queue is full and then throw the TimeoutException again, up to three times. So the total time it can endure broker to be down is `request.timeout + 3 * max.block.ms` And without this PR it would be `request.timeout`. Note that the issue itself will only happen if we do not yet know the destination leader of the partition when broker is down, so its likelihood-to-hit is not like 100%.
@mjsax That is right, the `TimeoutException` from `Sender#failBatch()` is returned in the callback's exception, which will only be thrown in the next call. And `retries` will not help here. So it is really `max.block.ms` v.s. `3 * max.block.ms`. Currently this config value's default is 60 secs and Streams does not override it. So the effect is that if we do hit the issue that KIP-91's solving, it is a resilience of 60 seconds v.s. 180 seconds.
Hmm.. I'm wondering how did we succeed in this test case, since in the above code `send()` call is only captured with `TimeoutException`? Note that we only set the KafkaException in the callback while here we throw exception directly. And in fact, you changed the expected exception from StreamsException to KafkaException in line 128 above.
Lines 45 and 46 should use the static constants in this class for the name of the topics: `TOPICS_REGEX_CONFIG ` rather than `SinkTask.TOPICS_REGEX_CONFIG`, and `TOPICS_CONFIG` rather than `SinkTask. TOPICS_CONFIG`.
This could be `ConfigException` instead. With the `ConnectException` hierarchy we get a little bit stuck because we can't have `ConnectException` be the base of all our exceptions and not redefine some. There are some from core Kafka that are fine if triggered from within the framework, and `ConfigException` falls under that category -- the biggest value aside from standardization that asking for `ConnectException`/`RetriableException` from connectors/tasks is that it is a signal for how to handle the exception, but the runtime code doesn't require that everything inherits from those (and in fact we handle other exceptions from connectors as well, we just would prefer that the connector think about how it wants the framework to handle possible errors).
It's fine to remove it. Nothing in `connect-runtime` is public, supported API. Only `connect-api` is public and it is expected that changes to APIs marked as public in Java will be made since you need to mark them public to use them from other packages.
nit: As key and value is accessed, we might want to iterate throw the `.entrySet` instead of `keySet`
nit: add `final`
Should we guard against NPE? (same blow)
nit: add `final`
We want to get `endOffsets()` and `beginningOffsets` for the same set of partitions. A single request cannot get both at once AFAIK. Also, the reset tool is not considered to be on the "hot code path" -- thus, we don't need to worry about performance too much and apply (unnecessary?) micro optimizations. Just my two cents here.
This could throw a NPE. I think we should guard against this, and throw `ParseException` if NPE happens `ts.split("T")` is redundant and should be extracted into a variable.
These two calls boils down to ``` Fetcher#retrieveOffsetsByTimes(Map<TopicPartition, Long> timestampsToSearch ...) ``` I wonder if they can be combined (to save roundtrips).
Should this call be governed by !dryRun
nit `Production exception handler` -> `{@code ProductionExceptionHandler}`
I asked because Connect doesn't make any guarantees about thread safety of that class in the first place (within the framework, everything is sufficiently locked, though people may be implicitly making assumptions in connectors since it was intended to be immutable anyway). We're making micro-optimizations here without benchmarks, so we really do need to take care. I can think of cases we're definitely increasing overhead here (e.g. any transformation that filters out data would probably mean schema hash is never computed whereas now it is). I can't think of any cases in the connectors we've built where you would end up double computing (they pretty much all do any sort of schema hashing in a single thread because the whole point of tasks in connect is to handle parallelism at the fmwk level automatically for you, and any threading usually is internal to the library handling the connection to the other data storage system; the caching in converters would also generally be safe except for static schemas, and in that case the overhead is not worth worrying about and I think truly static schemas in connectors are pretty rare anyway).
Are these 3 lines an artifact of code reformatting? They don't seem to correspond to changes other than the reordering itself. The previous order matches the variable declaration order, so maybe we want to leave this as-is.
Are you implying cache efficiency? I'd imagine the order to matter if one of the fields had more chances to return `false` than the others. But I don't think it's worth thinking about this here. Feel free to leave this as you have it. When I saw it, I just thought your editor performed the reordering.
@kkonstantine I think the point is to avoid walking nested data structures that would require pointer chasing, so essentially yes it is a cache efficiency thing. If we really wanted to optimize this we would want to make sure everything that can be inlined in the class is checked first, i.e. `optional`, `type`, and `version`, before other simple fields like `name` and `doc`.
Might be overkill if this is the only use case, but we could also add a composite validator.
Ha, you are right. Glossed right over this.
No need to add bracket for single line statement.
leadMetricAddedPartitions is no longer used.
Is the support for multiple pids only for clean up? Since the code checks for a fixed control file, I was thinking we expect only one process at a time.
@alexjg The code is actually looping in this line, before the `waitForCondition` call is actually triggered, I think that is why you are seeing the indefinite hanging.
Can you add a few more characters, like space, tab, `%`, `+`, etc.? Some of those are used in the URL encoding, so including them would help ensure we're properly encoding and decoding.
you are right :P
I'd prefer to pass in the two config params here rather than the actual `StreamsConfig` we don't need the entire config.
nit: use `{}` instead of string concat for `retries`
`null` is redundant
I'd prefer this was left as it was. The `StateDirectory` doesn't need a `StreamsConfig`.
`... retry attempts due to timeout. The broker may be transiently unavailable at the moment. ..` Ditto above.
It may be a bit tricky as they use different string concatenation methods.
identical strings in log and exception
I wonder if it is worth refactoring this to remove duplication, i.e, add a Functional interface, then implement it three times to just perform the op on `globalStateRestoreListener` then have a method sth like: ``` performOnGlobalListener(GlobalListenerAction action, String storeName, TopicPartition partition) { if (globalStateRestoreListener != null) { try{ action.perform(); } catch (final Exception e) { /// streams exception stuff } } } ``` Other methods delegate to this new method. The guard and error handling are encapsulated in one place. When we eventually get to java 8 we can just do a lambda call
Do we really want to log and throw? I beleive this will get logged again where it is caught, so it seems a bit odd to log it here. Also just realised this hasn't changed in this PR, but still...
There is similar code forming StreamsException. Consider refactoring to reduce code dup (can be done in another issue).
Nit: `throw new IllegalStateException("Stream-client " + clientId + ": Unexpected state transition from " + oldState + " to " + newState);` Capitalize `S` and add `:` (same blow)
Nit: single parameter per line formatting (same below)
Nit: remove `this`
Nit: This is only used once. Remove variable.
This is still missing in the KIP wiki page.
typo `fro` -> `for` Nit: `{@link org.apache.kafka.clients.admin.AdminClient}` -> `{@link org.apache.kafka.clients.admin.AdminClient AdminClient}`
When are we going to drop support for 0.8.x? :)
Do we need to do this for so many stores? i think 2 would be sufficient as the test becomes quite noisy and it is difficult to see the intent
ok - leave it as is i guess
`KafkaAdminClient#prettyPrintException` includes the class of the throwable in the string, which this method does not. `KafkaAdminClient#prettyPrintException` is also intended to generate a short (one line) description, not dive into the stack traces and causes. I'm sure there's some unification we could do at some point, though, if we had more utility methods for dealing with exception text
I don't think this is the right approach. Even if the root cause is important, we still want to know that we experienced a disconnect. If I'm reading it right, the current patch does not show this. In general, we can't really make blanket statements like "the root cause is always the most useful thing" It depends on how the code is using the exceptions. Also, we shouldn't change what any of the exception output is without making a detailed examination of the before and after log output, and making sure that we like the "after" output better.
It will be worth mentioning that it includes the root cause since this is in `Utils`.
Yeah, I was wondering if it would make sense to pass the request context so that at least you could add the URL path (and maybe params) to the message. One additional nit by the way: when DEBUG is enabled, we'll see this message twice because of the log line above. We could print this only if debug level is not enabled or something.
`encodingValue` can still be null right, as `configs.get(..)` could still return a null.
Fixed when merging the PR.
Ah. Important detail. Thanks for clarification. Than we should be good.
Nit: double space
Hmmm... This make we wonder if the PR is correct. For in-memory stores, I think we should not write a checkpoint file -- assume an in-memory `StandbyTask` crashed and later recovers on the same host. For this case, we need to make sure that we read the whole changelog topic. A checkpoint would indicate that a part of the changelog is already restored what is not the case there.
I think we do not need to back off here, since the request will be parked in the queue anyways during retries.
Ditto above, if we do not expect this to ever happen, we do not need to `Thread.currentThread().interrupt();` and instead just log error and throw as IllegalState with the error message indicating this should never happen.
Case 1): yes, the queued requests will eventually be timed out with the exception set. I'm thinking the loop would cover it as it would catch `TimeoutException` and retry.
Should we add it to `createTopicNames` also? Otherwise we will retry and fail again.
I think there are two slight different cases that we are discussing here :) First case is when the broker is unavailable, we do not yet send the request out even since we do not know who to send to with empty metadata, hence this request will sit in the admin client's queue until the broker comes back and the metadata gets refreshed; Second case is after the request is sent, broker crashed, and even after it resumes the request is lost and admin client is doomed to throw timeout exception still (note if it is a broker soft failure like GC the broker can still send response back in time). With a longer timeout the first case can be remedied, but not the second case. And I'd not expect `AdminClient` improve on this end before the next release. So maybe we should add a retry loop wrapping the `numPartitions` and `createTopics` call still.
Ditto here for different exception types.
Isn't this more likely to happen in practice? Do we want to produce this as WARN? I felt making INFO or even DEBUG is better.
Should we finer-handling different error cases here? ``` /** * Possible error codes: * * REQUEST_TIMED_OUT(7) * INVALID_TOPIC_EXCEPTION(17) * CLUSTER_AUTHORIZATION_FAILED(31) * TOPIC_ALREADY_EXISTS(36) * INVALID_PARTITIONS(37) * INVALID_REPLICATION_FACTOR(38) * INVALID_REPLICA_ASSIGNMENT(39) * INVALID_CONFIG(40) * NOT_CONTROLLER(41) * INVALID_REQUEST(42) */ ```
Thanks for the follow-up.
Although we are using the same default of `retries = 5` and `retry backoff = 100ms` now, there is a subtle difference that in the old code, we throw `TimeoutException` and handles it outside the call with retries, while in the `AdminClient` timeouts are not retried but failed directly. So we are effectively less resilient to broker unavailability. I synced with @cmccabe offline and I'm thinking maybe we can have a longer default request timeout value for admin configs for now using the prefix, and in the future we may have improved Admin Client generally to provide different timeout values for client / broker.
Are the sizes not configurable? The constants are too hidden here, it may be better to declare them as a static at the start of the class if not configurable.
This is assuming that `totalTopics` is always a multiple of `MAX_BATCH_SIZE. Is that always true? Perhaps it is better not to make that assumption in any case.
@cmccabe Since `flush` blocks until all records are sent, wouldn't it be better to compute the delay time after flush completes? Ideally, an async flush would be better to avoid more delay than required, but that would need another thread. Alternative is not to flush at all, since only the records in the last incomplete batch would be delayed when `linger.ms > 0`.
Shouldn't this be a config exception? It is not really invalid partitions.
sorry, a bit late to the party, but if Kafka Connect can't create the topic but can still read / write to it, it should also have the describe rights. In which case, we can check if the topic exist using a describe? I feel that right now this might be introducing a bug. Say the Kafka Connect isn't authorized to create a topic and the topic doesn't exist, then it will still go on with the execution
nit: looks like we're missing a space after the comma. It was a problem in the original as well.
Looks like these changes were not needed.
There might be a few cases where we used `Connector` when creating the mock if it didn't previously matter to the test. e.g. https://github.com/apache/kafka/blob/trunk/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java#L355 is one case I can find. https://github.com/apache/kafka/blob/trunk/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java#L635 might be another case, where we used Connector since I think both source and sink tests might use that same code. I guess the `Class.forName` is probably ok since we've swapped in the correct `ClassLoader` at this point, though not really ideal as we're doing more lookups when we already have the instance of the connector. I think it would be better to fix the tests (though I'm not sure off the top of my head how many changes this will require...)
Minor: should be "Throw an exception *if* ..."
not critical since it's not a ton of logic, but since this logic is repeated, it might be better to turn it into a utility method on `SinkConnectorConfig` and use it both in that class's validate method and here.
Should we still continue deleting the global state directory if the local task directories cleanup failed? I'm thinking if the local stores deletion fails while global store succeeded, we will throw an exception, and user may retry in which case `globalStateDir()` will be called and trying to re-create the folder and delete it.
Throwing `IllegalStateException` is served as the purpose that "this should never happen, and if it does it is a bug and hence it is ok to fail and stop the world".
Updated this when merging.
I think we may not need this, just the other method, but it would be good to verify. Similar for other classes.
Rather than adding a subclass of `SegmentIterator` we could use a functional interface to get the next iterator. The `hasNext` method in this is identical to the one in `SegmentIterator` expect for one line.
nit: unneeded newline
Should we include the config source instead for compatibility? Also, just double-checking that it is intentional to leave the synonyms out of `equals`.
I'm not sure it actually matters since users are unlikely to construct this object manually, but it seems like we should use `ConfigSource.DEFAULT_CONFIG` if `isDefault` is true and `ConfigSource.UNKNOWN` otherwise. Then `isDefault()` will continue to work with this constructor.
nit: we have tended to use an `int` for the version in other cases to avoid the annoying typecasts in the caller.
I think you need `equals/hashCode` since `synonyms` is now included in the same methods for `ConfigEntry`.
nit: weird alignment
Sorry, I have mistaken about the print null -- will add a check for this.
@ewencp Thanks so much for your suggestion! I see your point now. I am making the fixes as we speak.
is `connName` correct here? `getConnector` takes a `connType` parameter. it looks like it already throws an exception if it can't find the class. i want to make sure we get these correct because there has definitely been some confusion in the past and `maskCredentials` definitely looks like it is supposed to take the connector name whereas `getConnector` definitely takes the connector type (i.e. class name). You might need an extra hop through `AbstractHerder.config(String connName)` to get to the actual config, from which you can extract the connector class name.
Can this be made polymorphic instead of actually checking the `PASSWORD` type here? e.g., could we simply always do `newConfig.put(key, entry.getValue().toString())` since `Password.toString()` always hides the value (in contrast to the `value()` method? I don't see a use atm, but keeping this generalizable seems potentially valuable. wouldn't block on this, but it doesn't seem great to have to bake details of the config type into code in the `AbstractHerder` that otherwise doesn't care about it.
Oops, that was actually my fault.
I think we should clear `immediatelyConnectedKeys` at the end of each `poll()` as well.
A few notes: * It is possible for `schema` to be null * `struct` should never be null, since all of the existing/current `LogicalTypeConverter` implementations in `TO_JSON_LOGICAL_CONVERTERS` always return a non-null value.  `struct.schema()` should never be null, since it's not possible to create a `Struct` without a `Schema`. So, this change should be good and won't result in an NPE as long as the above conditions are true.
This is ok in terms of the test, but `LOCAL_CLOSE` is the state used for client-side disconnection. For server-side disconnections, the state would be whatever the client state was at the time the disconnection was detected. Could just use `ChannelState.READY`.
The default seems to be null, not empty.
`closing` should be set in `close()` rather than `closeConnections` - i.e. set in when closing server socket, not when closing client connections.
I just thought about this. I think `endOffset` should be actual endOffset, ie, `11` for this test -- we pass in the `offsetLimit` as 5 in `StateRestorer` below.
with 10 messages and a commit marker at 5, we need a second commit marker at 11: `0...4,C,6,...11,C` thus, endOffset should be 12. Having say this, I think a simpler setup `0...,9,C` and endOffset `11` would be sufficient for this test case.
as above. endOffset should be `12` and passed offset limit in next line should be 6.
We are cycling forth and back here... This change indicate that our logic is still not correct -- and looking at the code it makes sense: Here https://github.com/apache/kafka/pull/4300/files#diff-46ed6d177221c8778965ecb1b6657be3R279 we set `nextPosition = record.offset()` what is `5` in this test case and equals to `endOffset` -- but we should return `10` here (and than this test would fail without the change). We are still mixing up the two cases using a regular changelog topic and the source topic as changelog...
This should still fire... If this does not throw, it indicates that our check for the regular restore case with a dedicated internal changelog topic is broken.
we remove `try-catch-fail` here -- if there is an exception the test won't pass anyway. (same below)
nit: maybe just move into the line above? same above and below.
should we not just add 10 records in a row (offsets `0...9`) and simulate commit marker at offset 10 via setting endOffset to 11 ? This setup would indicate, that the record at offset 10 is either a non-transactional message (we could have mixed writes) or it's in pending state (neither committed not aborted) and thus endOffset could not be 11L as "last stable offset" is 9 or 10 and endoffset must be smaller than "last stable offset")
shouldn't endOffset be smaller here (or is test name incorrect)? I think a good setup would be `0,...4,CM,6,...11` and endOffset = 6.
The consistency point is true IIRC -- for `streamTimePunctuationQueue.schedule(schedule);` we also punctuate immediately -- there is hard to change as we would need to consider the timestamp of the very first record we process. I am just thinking, if we should keep the current behavior and put the burden on the user to ignore the very first punctuation call? We had a similar ticket in the passed already, but no conclusion how to proceed: https://issues.apache.org/jira/browse/KAFKA-6092
> I ignore 1 out of every 2 punctuations Uh. That's kinda painful... I think we need to discuss this in more details, ie, what semantics we want to provide. \cc @bbejeck @dguy @guozhangwang @miguno
TBH I don't remember right now any reason for this. Probably just kept it consistent with stream time punctuation. Right now, I can't think of any reason why punctuating immediately would be desirable though.
can we change the test, to include a "pass" over the next schedule? atm, "stream-time == next-punctuation-time" but we should cover "stream-time > next-punctuation-time" (with jumping over a whole schedule)
I changed the type to boolean here (and on line 89) - compilation passed. This would avoid unboxing.
Seems the `fastTimeout` should be thread-safe since it will be accessed by both RequestSendThread and controller event thread. Perhaps `volatile` is a least requirement for memory visibility.
No need to apologize. :)
@mjsax @bbejeck I renamed the method. Apologize for my poor method naming skills.
This test just needs a rename: it calls `shouldNotAllowToResetWhileStreamsIsRunning()` and should have the same name. There are two separate test below: one for invalid input topic and one for invalid intermediate topic. (or did you mean something else, @bbejeck )
maybe a nit: I understand what you are doing here, but IMHO there should be two separate tests.
@ssanthalingam thanks for updating, no apologies necessary!
Nit: `containsMbean` for consistency with `KafkaMbean`.
We can remove only if mbean.metrics.isEmpty. This line should be added in metricRemoval() method after unregister call.
We can compute this once and pass it to `removeAttribute`.
No, I mean something like: ```java String mBeanName = getMBeanName(prefix, metricName); KafkaMbean mbean = removeAttribute(metric, mBeanName); ... ```
looks like this is not passed to Metrics object. we can use reporter instance at below line.
Let's also verify that `containsMBean` returns `true` before the removal. Also, it wouldn't hurt to check that the second `removeMetric` call removes it from `JmxReporter`.
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final` to both parameters (same below)
as mentioned above: remove this overlaod
either move `this` to next line, or fix indention.
Because `ValueTransformerWithKeySupplier` is a public interface, we should try to find a solution that does not add a deprecated method to this new interface. If my proposal doesn't work, I am sure there is another solution (using sub-classing etc) to do some internal re-directs to make it work.
nit: add `final`
This should return `-1` as old `BadValueTransformer`
nit: add `final`
I think it is probably worth adding as even if it is deprecated it is still supported
Thanks for the explanation. I checked the logback implementation and it does indeed use the argument array. Maybe it's not such a big deal since we have the log level guard.
Maybe a cleaner approach is to provide two separate `Logger` implementations.
Nit: add `final`
I personally support Damian idea here. Don't think it's overkill but actually more elegant OO code. If you feel strong to keep as it, also fine with me though.
Yes. We have the same issue with `AdminClientConfig` and `retries` -- thus, we instantiate a `AdminClientConfig` got get the default out of it. Can we do the same thing here and instantiate a `ProducerConfig` object? I now it's not very nice code, but still better than hardcoding the value.
Thx. Got it.
I don't think it is overkill either, it is just good OO design. Anyway, i'm not going to block this PR because of it.
Do we need to pass in all admin client configs? Actually, we only need retries. Also, I am not sure if we would set the correct default here. If nothing is specified, we would use `StreamsConfig` retry as default, but we actually should set `AdminClientConfig` default -- atm, both might be the same so it doesn't matter too much. Just is doesn't seems to be "correct" (in a very strong sense).
should we apply a try-catch patter instead of annotation? It's not a single line test? (same below)
Sound like it uses `Properties` class (while it doesn't) -- but maybe I am overthinking this... Should be fine.
Compiler still gives some warning here, as you are erasing here the java generic type. Adding the `<>` infects the rest of your code with the `? super T` you have to add at the correct places.
So in my updated PR I change this line to line up with CompletableFuture.
Fixed this in my latest update of #4033.
Ah I see, thanks!
might be true now, probably not true long term. also probably depends on where this is used - in a transformation for a source connector, it's likely for the foreseeable future that the headers are empty; for a sink connector, anywhere people have started using headers it is very unlikely they are empty. the optimization is fine, i just watch for these things as they complicate the code and if they appear in the first version of code, usually aren't backed up by real data suggesting they are valuable.
Infinite loop doesn't seem to make sense. Should this be re-written as? ```java protected Header makeNext() { while (original.hasNext()) { Header header = original.next(); if (!header.key().equals(key)) { continue; } return header; } return this.allDone(); } ```
I get the intention for lazy initialization. Asked generally since I wanted to be 100% sure about the expectations w.r.t. concurrency. I think it's best to keep `volatile` since the field is not final and might be accessed in sequence by different threads, but `synchronized` can be skipped if there's no concurrent execution by different threads.
this should be declared volatile (considering the double-checked locking below)
Got it. May be we could have something like SchemaAndValueUtils to include such utils for it. Just didn't feel great seeing this method in the ConnectHeaders class.
may be use Objects.requireNonNull
retainLatest() and this method have a lot in common. We could potentially refactor it, but not too concerned if its left as-is.
probably not required to do a special logic for ConnectHeaders. The equals check using iterator below should probably be suffice.
Maybe use Objects.requireNonNull
Exception message doesn't look right (the word "list").
the norm (ideally) for AK is to only use `this.` as needed to handle masking. i'm sure there are exceptions, but almost universally the use case is for constructors to avoid awkward parameter naming in order to avoid conflicting with a convenient member variable name.
super-nit: this should say `Check that this is a Decimal`, not `date`. repeated below for time and timestamp, but slightly less egregiously :)
i think leaving as is should be fine atm, and tbh at least they are both close enough together to be easily modified together. if we think this is useful enough, i'd file a jira dependent on the jdk8 update so we can follow up.
nit: should be `ConnectHeaders`. *probably* would be easy to figure out, but better to just get it right :)
Just a reminder, `ByteBuffer` again :)
style nit: if the entire body is surrounded in a conditional, it's usually more readable to just check the negation and return, then reduce indentation with the rest of the body. no real need to fix here, just a style thing to watch out for moving forward.
I've seen this a few places -- `SchemaAndValue` already has `SchemaAndValue.NULL` field which does the same thing -- no need to repeat a bunch of times in a bunch of classes.
Avoid concatenation? This should still log the exception with stack trace: `log.warn("{} caught an exception: ", what, e)`
I think `nanoseconds` should be `milliseconds`. It's a little more idiomatic to do it like this: ```java time.sleep(autoCommitIntervalMs); coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); ```
This test case passes without the fix. It doesn't look like it even goes through the auto-commit path.
This looks unintentional.
We intentionally disable auto-commit so that it does not interfere with other test cases. If we are testing auto-commit behavior in a test, we just enable it locally for that test case (see `testAutoCommitDynamicAssignment` for example). Let's continue to follow this pattern.
The name should mention the fact that this case covers auto-commit.
This is not actually needed.
Note that we are effectively only retrying 4 times with this logic, since if we happen to successfully send out all the record on the fifth retry (i.e. `needRetry` is not empty) we are still going to throw error here. So more strictly we should check ` --remainingRetries == 0 && !needRetry.isEmpty()`.
Your understanding is correct @mjsax .
IMHO we should consider changing to ` @Parameterized.Parameters(name = "caching enabled = {0}")` which prints the whether caching is enabled or not vs. just the index of the parameter.
this will never be called if one of the assertions fails
nit: java style
nit: add `final`
This check should go into `GlobalKTableImpl#queryableStoreName()`
I am just wondering: it seems that we don't have a check in place that `storeBuilder.name()` does not return `null` -- this would be bad and should not be allowed. Also, we should never put `null` as name, but generate a name. This check `isQueryable()` should be done in `GlobalKTableImpl#queryableStoreName()`.
Regarding 1) and 2), I think it is not needed, and hence we can use the same `streams` object. 3) fair enough. 4) sounds good, keep it as is then.
We could change other callers of `isGlobalSource` and use the index instead? E.g. `describeGlobalStores()`, and then this `isGlobalSource` itself could be removed.
nit: fix indention: should only be 4 spaces (same below)
nit: add `{ }` to then block (we always use `{}` for all blocks.)
nit: move `"topic2"` to next line and fix indention (should only indent by 4 spaced) -- this will reduce the line length and make code better readable.
I'm wondering if we can avoid injecting the thread logic in the `setState` function. One idea is to not allow the transition from `CREATED` to `PENDING_SHUTDOWN`, and hence the `setState(PENDING_SHUTDOWN)` in `KafkaStreams#shutdown()` will return false (note all other states except `DEAD` can transit to `PENDING_SHUTDOWN`), and then we can depend on the return value i.e. in `shutdown()`: ``` if (!setState(PENDING_SHUTDOWN) && state() == CREATED)) { completeShutdown(true); } ```
nit: add `final` and line too long
nit: fix indention -- either move `mockTime` one line down or indent other parameters to align with `mockTime`
nit: add `final`
Consider naming the topic "topic2" since there are only two topics in the test
nit: move `table1` to next line, or fix indention below to align with `table1`
why do we put "unknownTopic" here? Should we subscribe to "topic1" and "topic3"? Or we can actually pass in an empty list? I guess this is copied from `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks` -- there we need to pass in `unknownTopic` as this topic does not exist in cluster metadata.
as above. (please fix in `shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks`, too)
We actually don't need to name the store. This could be `.count()` plus updating the name for the repartitioning and changelog topic.
Sorry for the forth and back -- for `assertThat` you original code was correct and expected argument is second one... (it different for `assertEquals` -- my bad(!)).
Nit: `assertThat` take expected result as first parameter IIRC (otherwise error message on failing test is "reversed")
Thanks for the cleanup! Really appreciated!
as above: flip arguments
I understand your point -- however, naming must be deterministic and should not change from release to release (otherwise, upgrading hard harder and we need to mention it in the upgrade docs). Thus, if we use internal names in tests, we have some implicit testing that naming does not change. That's why I prefer to not name the operator, too, if not required.
nit: could be useful to log the type of exception in the assertion message.
You don't want to pay the cost for the computation of the hashCode if it's never used. I still think it would be better to make the hashCode very cheap by just using the `id`.
nit `java.util.` can be removed
It's intentional to avoid build warnings about importing deprecated classes.
nit: `java.util.` can be removed (note, we only specify the whole package for `Topology`, because otherwise we would need to add an `import` statement and get a warning about "unused imports" and a failing build.
nit: add `final` and put single parameter per line
I think it would be more intuitive if we would reorder parameters to "topic, pattern, topic, pattern".
I don't think we really need to include the exception in the message since we'll get the full trace anyway.
Seems like `handleCompletedMetadataResponse` also needs to be updated.
Thanks for your reply. I guess this may be true in some cases, but at least for e.g. `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec` it works even if the topic contains dots.
i don't think this is what you want here. `broker_ids` is evaluated once. the `wait_until` implies you want to wait until the `broker_ids` appear in the the log files for the broker. but that doesn't seem like the target you want.
i don't think there's a good way to do this directly with `monitor.wait_until` because it was written originally to look for a fixed pattern (we've mostly used it to monitor startup where we know a specific message gets logged once the service is actually serving requests). you could either look for the fixed pattern (e.g. if this message only gets logged once and before the broker ids it has a known msg) or do a `wait_until` yourself. `wait_until` takes a function, so you can just wrap up the check in a local function that returns a boolean and pass that into `wait_until`.
no worries. fyi, you still can, you just define a local function that has your check logic. you can `def foo()` anywhere.
definition of `cmd` seems weirdly separated from execution here. not critical, but moving it into the `with` makes things clearer.
this looks cleaner as `"[" + ", ".join(self.idx(node) for node in self.nodes)` + "]"
normally in the tests we use `wait_until` instead of manually doing retries, and it generally makes the entire thing a one-liner. it's also generally better since you can just set time-based termination conditions instead of coupling # of retries with the timeout.
This computation of `broker_ids` can be simpler with a set comprehension like `{node for node in self.nodes if self.is_registered(node)}`
Ack, thanks for clarifying. FWIW I ran 100x on two separate occasions and all passed. Thanks for the patch, LGTM.
That makes sense, I think keeping it as-is is better.
Another way to write this, that reduces a couple lines of code would be: ```java if (allTopics.remove(topicName) == null) { future.completeExceptionally(new UnknownTopicOrPartitionException(String.format("Topic %s does not exist.", topicName))); } else { future.complete(null); } deleteTopicsResult.put(topicName, future); ```
nit: remove empty line
This needs to be updated with the new constructor that accepts two arguments.
requires new constructor. Same below in other places
`!=` feels a bit strange as opposed to `<` for a classic `for` loop's condition
Do we need to lock here? I think the lock has already been taken out from the callers of this method
validateStoreOpen() can be outside of lock block.
`validateStoreOpen()` can be outside of lock block.
Hmm.. It's unclear to me why `getTGT` and `reLogin` are synchronized in the first place. As far as I can tell, they are only used by the refresh thread.
nit: maybe we may as well make a private method, say `ensureNotExpired` or something like that.
nit: the timeout is a little obscured below. I'd suggest storing the timeout itself in a nicely named variable and using `time.sleep()` instead of using the auto-tick capability of `MockTime`.
If we call this twice and build two list, we cannot guarantee load balanced assignment across threads. Assume you have two thread and 3 active and 3 standby tasks. We would end up with: ``` t1: a0, a2, s0, s2 t2: a1, s1 ``` However, after assigning all active tasks, we should not start to assign standby tasks on the first thread but on the "next" thread: ``` t1: a0, a2, s1 t2: a1, s0, s2 ``` We can achieve this, by building a single list of active and standby tasks IMHO. Or do you think that this kind of load balancing is secondary? \cc @guozhangwang @dguy
If `taskId == null` we should call `break` to terminate instead of finish the whole loop.
Nit: move into the `if` block where it is used. (also add `final`)
nit: final int `activeTaskAssignmentLength`.
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final` (same line below)
In the above line 102 calling `initialize()`, both `initializeStateStores()` which registers the stores for restoration and the `initTopology()`. And even if it returns false these functions are already called, so I'm not sure if it will work.. I'd suggest doing the following: 1. having the `AssignedTasks<T extends AbstractTask>`. 2. let `initializeStateStores` returns a boolean just like `initialize`, then in the line 103 above, call `AbstractTask#initializeStateStores` instead. 3. remove `initializeStateStores` from `initialize` of stream task and standby task.
Hmm, is this correct? `restoring` contains those tasks that are still restoring not not finished. Should we just call `initializeTopology()` in the `transitionToRunning` function? (This way we would not need to add the line 106 above).
Hmm, when is prevMetadata ever set to INCREMENTAL? From the code, it seems that prevMetadata is only set to response.metadata(), which always starts with FULL.
Note that `Hashtable` uses 11 (a prime number) as the default.
I think the optimization of using array indices instead of pointers is a bit questionable without some benchmarks. Heaps larger than 32 GB are rarely (or never) used in Kafka. And having to go via the array has some cost as well.
I am wondering if this can be lowered to `DEBUG` since it is handled internally.
Since topicPartition doesn't exist in next if we get here, there is no need to remove it.
I am wondering why this is not an assertion. Would the broker ever be expected to return only a subset of the partitions in a full fetch request? To be honest, I think it would be fine to skip these checks and just assume the broker gives us the right thing.
I'm not sure this is a good idea. If we're unlucky, the partition we're interested in may not be listed. Since this is an exceptional case anyway, I would suggest using the more verbose message.
Same as above. It will probably be particularly useful for incremental fetches to have the partitions explicitly in the log message.
Should this be retriable? Same question for `FetchSessionIdNotFoundException`.
nit: These are the same descriptions as above. How about creating a static `Field` instances or at least extracting the message.
This is the same code as in `KTableFilter` -- we should refactor and share code.
Why not something like: ``` final List<String> storeNames = Arrays.asList(parent1.valueGetterSupplier().storeNames()); storeNames.addAll(Arrays.asList(parent2.valueGetterSupplier().storeNames())); return storeNames.toArray(new String[storeNames.size()]); ``` ? I don't think it is on the critical path so performance shouldn't be an issue
nit: don't need `<K, V>` can just be `<>`
I understand that. I am just wondering, why we create a `ArrayList` here in stead of a plain array: ``` final String[] stores = new String[storeNames1.length + storeNames2.length]; int i = 0; for (int j = 0; j < storeNames1.length; ++j, ++i) { stores[i] = storeNames1[j] } for (int j = 0; j < storeNames2.length; ++j, ++i) { stores[i] = storeNames2[j] } return stores; ```
Damian suggest is actually different as it avoid the loops... That would also be fine. But using ArrayList plus for-loops is a mess IMHO.
the drawback of renaming here is that the name `start_cmd` is used with `start`/`start_node` pretty standardly across the rest of this code base
these should probably be defined as class constants so users can specify the startup modes without having to use literals. this helps if you have an IDE indexing stuff, and lets you prefix in ways that clarify the meaning, e.g. `STARTUP_MODE_LISTEN = 'LISTEN'`. kind of unfortunate we haven't migrated to py3 yet, 3.4 added real enums which would be nicer here.
isn't 8083 basically a constant in this class anyway? what else would we check for? seems like if we don't want it to be a constant it should be a member variable and not passed in here? other things like `_base_url()` wouldn't work if you changed it only in the call to this method.
ack. would probably be good to verify the minimal diff to rename back after we're confident of the current patch (and might reveal more info), but this isn't all that big a deal if it changes, just makes it a bit inconsistent.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
These methods look like they are identical to those in the previous test class above
nit: add `final`
I am must wondering, if this should go into it's own test class `KStreamGlobalKTableJoinTest.java` ? Similar below.
Definitely don't add an abstract class! Let's leave it as is for now, then.
Not part of this JIRA, but should be rewrite this (and the existing) test to a proper unit test (-> `KStreamKTableJoinProcessorTest`)? If you don't want to rewrite the test it's ok, and we create a JIRA for it.
Are these methods the sam as in the class above? If so consider refactoring so we only have them once
nit: add `final` -- applies to all other vars below too.
i think this would be better off as a test rather than in `setUp`
nit: "classes that find in its urls" -> "classes that it finds in its urls"
Does the heavy-handed locking with `synchronized` rather than implementing the multithreaded loading with finer-grained locking have some motivation here? It's protected API, but URLClassLoader already has some fine-grained locking via ClassLoader's `getClassLoadingLock(name)`. I'm mainly curious because this approach is actually different from what is recommended in the "Recommendations for Multithreaded Custom Class Loaders" of the doc you linked. It seems like it should work, but disables multithreaded gains added in JDK7. The implementation used in `ClassLoader` seems to just allocate a lock object per `name` which is pretty easy to just copy even if we end up using different lock objects here than in `ClassLoader`.
is the `X` just a placeholder? we can just fill in `1.1.0` and update if this slips from the release
Same question here as earlier about the `Locale`
nit: `anf` -> `and`
i know it was motivated by findbugs, but this was probably a good refactoring anyway :) `RestServer.httpRequest` to *make* an http request has always been a bit awkward
nit: I don't spot any, but safer to avoid typos by just having constants for these
Is there a `Locale` that would work for lowercasing these that would work and not vary by the JVM config? I think based on the restricted characters, just using something like english should be ok.
Could also use `Collections.singletonList`, which would also make this immutable
is there a reason for using protected here? seems like it could be package private or private
This is an internal class -- no need to mark as deprecated.
This line fails in the build and needs a fix.
Nit: add empty line
we should not copy the code from the old `addGlobalStore()` but rather call the old `addGlobalStore()` passing the generated names.
While we're here, may also want to mention as a second paragraph (?): > The task will be {@link #stop() stopped} on a separate thread, and when that happens this method is expected to unblock, quickly finish up any remaining processing, and return.
I think, we should use three different values to make sure that the different prefixes overwrite the configs for the corresponding clients. Looking into the test below, they seem to be redundant with this one? We can also remove this test and keep the other three (that would avoid redundancy, too)
I think, we need also remove AdminClient configs, too.
Why do we need this? For custom configs, we don't enforce any restrictions.
as above: we need to remove adminPrefix configs
add check for restore-consumer and admitclient
This seems to be redundant from above (or already the merged test...) -- restore-consumer and adminclient would be missing though.
as above -- add check for two missing clients
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
could we turn this into a loop? i.e., sth-like? ``` List<KeyValue> records = Arrays.asList(...); for(KeyValue record : records) { mockTime.sleep(10); IntegrationTestUtils.... } ```
should we wrap the processing etc in `try{..}finally{..}` so we ensure that we do `streams.close()`? Also maybe close with a timeout? Same elsewhere
I would expect this to be `UTF-8` with a dash. That's the format in https://docs.oracle.com/javase/7/docs/api/java/nio/charset/Charset.html
I think we probably should clean these up so they are more useful for users, but won't block merging this on that. I was thinking something more along the lines of: ``` UUIDDeserializer deserializes UUIDs in standard 36-byte hexidecimal string representation. ``` then followed by the encoding details, i.e. the stuff a user wants to know. Many users won't even think about the fact that what's going into the deserializer (or out of the serializer) is actually a `byte[]`.
no probs - leave as is then
We could perhaps add a `leaderId` in `PartitionMetadata`
Since shouldBeSinkNode.get(0) may not be SinkNode, consider renaming the variable
If storeToChangelogTopic.get(storeName) is not null, assert that stateStoreToChangelogTopicOnlyForRestoring doesn't contain storeName.
I think one call of storeToChangelogTopic.keySet().removeAll() outside the loop should be equivalent to what you have now.
With fewer (outer) loop iterations, performance should be better.
The logic is different for ListGroups. We have to send a separate request to every broker in the cluster and then aggregate the results.
We do not have a unit test for delete group yet.
Like DescribeGroups, we need to find the coordinator for the group to send the OffsetFetch request to.
I'd vote for doing the flattening internally than expose the node information in the results, as this is supposed to be internal implementation details that is better not leaking out. The Scala API returning `Map[Node, List[GroupOverview]]` was not a well designed one in my hind-sight.
Ditto as above, we could use any node to find coordinator.
Why we need to ask controller for the coordinator? Should we just ask any node? I.e. `LeastLoadedNodeProvider`. cc @cmccabe
I'd suggest flatten the map to abstract away which nodes contains which consumer groups as they are supposed to be internal information, we have the freedom to change those internal impl whenever we want. Once we expose such a public API it will be partially public information and hence hard to change.
This part is not correct: we are passing clientId as `consumerId`, and memberId as `clientId` to the MemberDescription. We should just use the same string names, `clientId` and `memberId` in `MemberDescription` and pass in to constructors accordingly.
Unfortunately, the consumer groups are not aggregated in the same way that topic metadata is. To get all the groups in the cluster, you have to send the ListGroups request to all nodes.
The DescribeGroup API has to be sent to the group coordinator, which is potentially a different node for each group. You use the FindCoordinator API in order to lookup the coordinator for a given group. The logic should be something like this: 1. For each group in the request, send a FindCoordinator request to any node in the cluster. 2. Group the results by coordinator id. 3. Send DescribeGroups to each coordinator from 2. Ideally, we should also handle retries correctly. It could happen that the coordinator moves to another node by the time we send DescribeGroups. In this case, the error code will be NOT_COORDINATOR. We should handle this by looking up the coordinator again.
This is not correct. It's blocking, which turns this into a blocking API rather than a non-blocking one.
As I suggested before, to not expose the node information, we should remove this function.
We can still follow the original pattern of calling a `listNodes` to any broker, get the result, and inside the response handling logic based on the result sending a `listConsumerGroups` to each node, and then group and flatten the result so we hide the node information of each sub-map it is belonging to.
That is a good point. I think adding a boolean flag in addition to the `future` result indicating if it may be incomplete is better than exposing the node map. I.e. we could have two fields inside `ListConsumerGroupsResult`, a `boolean` and a `KafkaFuture<Collection<ConsumerGroupListing>>` (I think we do not need to have nested KafkaFutures so I did not have that inside `Collection`, but correct me if I overlook anything).
For `all()` function, its returned type should be `KafkaFuture<Void>`; ditto for other two Results as well.
Where is this function used? I'd suggest we only keep one function, i.e. ``` public Map<TopicPartition, KafkaFuture< ConsumerGroupDescription >> DescribeConsumerGroupsResult#values() ```
I'd suggest only keep `partitionsToOffsetAndMetadata` here.
I think we can just have one function between `values` and `groups` here. I'd suggest we use ``` public Map<TopicPartition, KafkaFuture<Void>> deletedGroups() ```
@rondagostino Thanks for the PR. Couldn't we just use substring until the first "=" ? Something like this would be easier to read? ``` int index = urlSafeBase64EncodedUUID.indexOf('='); return index > 0 ? urlSafeBase64EncodedUUID.substring(0, index) : urlSafeBase64EncodedUUID; ```
Although unlikely, but maybe it is safer to only decrement only when pollLast() did not return null.
Same here, it is probably safer to only decrement counter when the inFlightRequest is not null.
Good point. In that case, it seems we don't need to do the null check here. Sorry for the back and forth.
Yeah, I don't feel too strongly in this case. The advantage generally is that the scope of the test case is clearer which makes failures easier to investigate.
nit: `Arrays.asList` a bit more concise.
May as well add `topicName` to the message so that the user knows which case they've hit? Same for the others.
Nit: fix idention
Yes this sounds good. IN case you want to run it locally, feel free to read the README.md file, which includes cmd for running a single test.
Nit: can you clean up the code further and add `{ }` (we prefer to use `{}` for all blocks. Thanks.
Might be nice to have this literal be a constant, since it's referenced in two places.
Sounds good to me. I am also fine in changing from `update` to `updated` -- hope @joel-hamill can give some advice what reads better.
To me this would make more sense: ` * For each {@code KTable} update, the filter is evaluated based on the current update record and then an update record is produced for the result {@code KTable}.`
@hachikuji Yes, that sounds good to me. It should reduce heap usage when there are lots of requests requiring down-conversions while still avoiding blocking the network thread.
This should be `CLIENT_DNS_LOOKUP_CONFIG` since other configs have that suffix.
This line is too long. We need the linebreaks back here
I don't think we want star imports.
I think these are left from your merge. You need to fix and pick which line during the merge. This will break the compilation
Add definitions for `WorkerConfig#CLIENT_DNS_LOOKUP_CONFIG` and make this just `CLIENT_DNS_LOOKUP_CONFIG`.
remove unnecessary newline
remove unnecessary newline
You've added a few empty lines in this file. We should remove these
Yes but you've redefined it in this class (https://github.com/apache/kafka/pull/4485/files#diff-48c2761c8e3ea24263f9cd1b020363e7R56). So we either use the redefined field (and remove `CommonClientConfigs.`) or get rid of the redefined and use the `CommonClientConfigs` field.
We should keep the definition in `ProducerConfig` so users can easiy see what configs are available for producer (and same for others). We should use `.define(CLIENT_DNS_LOOKUP_CONFIG` here to be consistent with `.define(BOOTSTRAP_SERVERS_CONFIG` etc. To clarify, we want to retain line 56 as-is: ``` public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG; ``` We want to remove `CommonClientConfigs.` only from line 245.
nit: add `final`
nit: add `final`
nit: add `final`
Typo. `my.custom.config` -> `my.custom.config2`
nit: `, and` (missing comma)
fair enough - we still don't need the assertion though as it is already true as of line 69
+1 - this line and the next shouldn't be needed
nit: add `final` -- same next line
nit: make parameter final
If `useContextSerdes` is false, we should use `Serdes.serdeFrom(keyClass)` and `Serdes.serdeFrom(valueClass)` since the context may not set the serdes.
EDIT: actually, I think it does not matter since the context serdes are always the same as the passed in class types. So we can just remove this in a follow-up tech debt cleanup PR.
I referred to the value serializer.
Split into two test? Or rename accordingly.`testDelete` is not a good name IMHO.
The iterator should return exactly one record. This, we should add an `Assert.assertFalse(it.hasNext());` after the `if`
Schema? It's Field class...
Another way to put this is that maybe we should make sure our built-in converters can handle calls to both `configure(Map, boolean isKey)` followed by `configure(Map)` with the `TYPE_CONFIG`. then, only things that are `Configurable` see the second one. old implementations wouldn't see it as they would not have implemented `Configurable` (except in unusual circumstances). New implementations could be warned by docs in `HeaderConverer` that they should take care to handle that sequence of calls.
This still doesn't seem correct. We're only invoking configuration if the plugin implements `Configurable` afaict. This does not work for `Converter`s implemented against the new API and assuming the "forward" configuration. We *must* always invoke the "old" `configure(Map, boolean)`, and only invoke the `Configurable` version as a backup. Possibly it would make sense to indicate on the `HeaderConverter` that the `Configurable` methods should be idempotent if we need to be able to implement both. Not sure if we can test this easily with unit tests, but I think we might want a plain old `Converter` (that does not implement `HeaderConverter`) in tests to validate compatibility... but it's possible we'd need either integration or system tests to properly validate.
The code isn't huge, so by no means a blocker, but @kkonstantine pointed out that the entire block before configuration (and most of config modulo the conditional) is identical between header and "normal" converters. The main difference in the header blocks are just the class referenced (`Converter.class` vs `HeaderConverter.class`). Consolidation would be nice if it's easy to do, but at the same time I'd rather get a fix to the immediate problem in, so definitely wouldn't block on saving 10 lines of duplicated code.
by no means a blocker (especially given public interfaces relying on this same problem), but i would not be opposed to making internal APIs not rely on these booleans -- clear enums, with properly handled default cases (presumably throwing exceptions) would be better. we tend to let things like this leak into internal (and subsequently external) APIs, but I think it is probably always a mistake -- unclear from the caller standpoint and not extensible.
Is this actually possible? shouldn't a `CLASS` config fail to even parse if it doesn't have a valid value? (which also raises an interesting question re: parsing and ensuring the parsing context is always the right one.)
Shouldn't this be in the contract of `Utils.newInstance` to not return some other class that doesn't match? I think this is pulled from `AbstractConfig` which makes sense for consistency, but I don't get why `Utils.newInstance` would ever return a value with an invalid type.
Why do we need this? in L147 we wait until the output is received anyway.
nit: Shutting down -> Complete shutdown .. Also in the verification we can change accordingly.
Yes. Question is, if we can verify that all 3 streams instances are running and waiting to brokers to go online instead of plain sleep.
Maybe we can close the first group here and verify that the sensors/metrics are no longer registered? A similar check for the sink would be good.
It would be difficult to give meaningful enum names in short time. Using two boolean parameters seems to be better for the moment. If there is ever need for 3rd boolean, we can refactor the code.
The need for this check is a bit unfortunate since it makes the api a bit unsafe (kind of tough to tell at a glance that all current uses are safe, though I think they are). Since we have only three valid options, I was wondering if we could replace the two booleans with an enum representing the disconnect state or something. Not too big of a deal since it's internal, but might be worth considering.
@hachikuji Thanks for the review. Updated to use an enum. Can you do another quick review? If it looks ok, I will merge and backport when the tests complete.
Since only two booleans affect decision making, I think reducing the number of enum to 3 would be more readable (there is no true false combination).
@hachikuji @tedyu @ijuma OK, I am going to revert and commit for now. We could improve on it later if we make updates to the code again.
I think the enum approach was better. I'd go with `DISCARD`, `GRACEFUL` and `NOTIFY_ONLY`.
Yeah, no rush.
@parafiend Looking at this again, adding to `failedSends` in `poll` can result in multiple disconnect notiifcations for a channel (we have to guarantee exactly one). `failedSends` are processed on the following `poll()`. But for the write exception here, we process `close()` in the catch block and the channel is added to `disconnected` list. In the next poll, when `failedSends` are processed, the channel will be added again to `disconnected` list. It would be better to set a flag rather than update `failedSends` and change the close in the catch block to: ``` close(channel, !sendFailed); ``` This will close the channel in the current poll without waiting to process any outstanding requests.
Maybe this is the root cause: a few lines above: ``` private final RocksDBStore rocksDBStore; ``` change to ``` private final RocksDBStore<K, V> rocksDBStore; ```
Wearing my performance paranoid hat here: unlike the StreamsBuilder's `Materialized / Consumed / etc` constructs which is only used once, this is actually on the very critical path of the processing of each record, and on average the forward call itself can be triggered many times per record. So I'm a bit concerning about the new `To` object per call to the timestamp setters here. Could we do some benchmarking say with the simple benchmark code and watch the GC metrics to analyze its impact? If it shows non-neglectable overhead I'd suggest we re-consider its API to, for example, adding overloaded `forward(k, v, timestamp)` instead.
If users specify the wrong processor node it will cause NPE, which is a bit hard to reason. Better check null on `child` and throw an informative error message before calling forward.
looks like we could extract: ``` setCurrentNode(child); child.process(key, value); ``` into a method and use in all three places
Is this change actually needed? The intent currently is to let `BoundField` be constructed only within the scope of a given Schema and to use `Schema.get` in order to get an instance. We do not expect a `BoundField` constructed in the context of one schema to be used for another schema. Note also that the includes test case does not cover this change, so we are probably missing another test case if we think this fix is important.
Can we remove the unused constructor now? Also, I think I mistakenly updated the hashcode method. I can submit a minor PR.
Yes, it was intentional. During authorization checks, we compare incoming session principal with principals stored on acl cache/store. tokenAuthenticated field has no meaning for stored acls/principals. So we are ignoring tokenAuthenticated while comparing principals. Another option is to move tokenAuthenticated field to Session/RequestContext objects. This may require adding new tokenAuthenticated() method to Authenticator interface. Let me know if there are any other ideas.
The type in Collections.<String, String> is unnecessary.
What does the final "\n{}" do? It prints out the migratedTask (using toString() I assume)? What does a task.toString() look like? In other words, I'm trying to understand what this log line will look like.
Cool, that's helpful. Thanks.
do we need the `throws InterruptedException`? doesn't seem to be thrown anywhere
this test doesn't seem to throw `InterruptedException` as well
nit: final and also below
`"count-one"` -> `storeName1`
I think the only issue is that this message in particular might get a little spammy when we are discovering a new coordinator since it can take a little time for the cluster to converge. Seems fine to increase verbosity for the other errors though. By the way, it looks like we're missing the word "failed" in the message
nit: break line (way too long)
as above: avoid `/` Update to ``` log.warn("Unable to read '{}{}{}'. Using default inputValues list", "resources", File.seperator, fileName); ```
Just some more nits. Can you add `final` wherever possible: `ClassLoader`, `String filename`, `BufferedReader`, `for(final String...)`, `Exception`
Add a log saying that internal strings are used since inputValues.txt is absent.
About 5x to 10x the size of what the array holds.
Use `File.separator` instead of `/`
Should this be `num_lines=3` (cf. L116 and L126)
Should this be `num_lines=3` for all three (cf. L140-L142)
Should this be `num_lines=2` (cf. L120)
Should this be `num_lines=3` (cf. L115 and L135)
Should this be `num_lines=2` (cf. L121)
nit: remove empty line
`2` -> `entries.size() - 1`
Can we change `subject` to `rockDdStore` -- it's a weird name IMHO.
nit: remove empty line
Hmm.. `first` and `last` of RocksDBStore are not used anywhere, I think it is a leftover of the code clean up introduced in KAFKA-4499 (cc @RichardYuSTUG ). Maybe we can just remove these two functions from RocksDBStore.
Could we put this test to `AbstractKeyValueStoreTest` so that all types of kv stores will test it? Ditto below.
It was a good idea to remove the `processOutstanding` field from `CloseMode`, but I think this would be a little clearer if we kept the `notifyDisconnect` field.
nit: double space
We definitely need `@Override` annotation to make clear that we change how `scan(url)` will function when is called by `scan()` on the base class.
`final` is not required here.
Let me clarify what I meant. In `TransactionManager.initializeTransactions`, we return a `TransactionalRequestResult`, which we wait on from `initTransactions()`. What I am suggesting is that we could cache the instance of `TransactionalRequestResult` inside `TransactionManager`; if `initTransactions()` times out and is invoked again, we can just continue waiting on the same result object. So it does not change the API.
Yes, I am suggesting that we allow the user to retry after a timeout. The simplest way to do so is to cache the result object so that we do not send another InitProducerId request. Instead, we should just continue waiting on the one that we already sent.
This should probably say "Timeout expired while initializing transactional state ..."
I don't think this this is what we want. We're not using an `Error` anywhere else in the producer. I'd suggest we just throw `TimeoutException`, but it is a `RetriableException`, which would be misleading if we do not allow retrying. We could either introduce a `FatalTimeoutException`, or we could try to make this API safe to retry. For example, to implement the latter, we could cache the result object so that on retry, we continue waiting for it.
We should probably use the producer's max block time instead of the request timeout.
We should also add `assertThat(windowStore.fetch("a", 0), equalTo(null));` or similar to make sure `a` is only returned for the correct timestamp.
The code is correct, but confusing to read for me as a human...
Ack. Was not sure if it's an intentional change. Thanks for clarifying that it's an actual fix.
as above update test name
old code sets `retainDuplicates=true` but new code sets `retainDuplicates=false`
add missing generics to return type and remove "unchecked" suppression? nit: add `final` to parameters
nit: add `final`
nice cleanup -- the original code was quite clumsy...
nit: add `final`
should be `apply(oldAgg, value);`
nit: add `final` -> `for (final Map.Entry...`
nit: add `final`
Hmm, but if the value is null, we won't hit those points: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java#L638. With this patch, both of these tests fail: ```java @Test(expected = ConfigException.class) public void testEmptyTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, ""); connector.start(sourceProperties); } @Test(expected = ConfigException.class) public void testNullTopicNotAllowed() { sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, null); connector.start(sourceProperties); } ```
I think using a string and the `NonEmptyString` validator would be a little clearer. That would allow us to skip any additional checks in `start`.
Ack, thanks for the explanation.
The reason I ask is that I was expecting this would be implemented with a `ConfigDef` validator, but I didn't see one defined.
You could just do `values()[version]` after the appropriate bounds check at the start of the method. So that this doesn't have to change as we add versions. One downside is that `values` causes a copy. So we could save a copy as a private field instead (this is sadly a common pattern with enums).
I think you'd want to use actual listener ports, not the JMX one. The JMX one is presumably opened very early when the process starts, but we want to make sure the Kafka service is actually up, running, and ready to serve traffic. That's why the previous version was checking for a message that happens much later in the startup process.
Is `open_port` guaranteed to be called once? If not, `broker_ports` should be a `set` instead of a `list`. Alternatively, it could be an accessor that gets the full *current* list from `port_mappings`.
nit: space after `try`.
Missing a space after the comma, which is causing checkstyle to fail.
I think this is a typo that can be reverted.
I see. Than please remove the next line `driver.process` and simplify to ``` new ProcessorTopology(...) ``` (we don't need variable `driver` for this case)
we should not use Java keyword `assert` but Junit `assertXXX` methods (`assertEquals` for this test)
Nit: package prefix not needed
I think we need to insert a `fail` here to fix this test
We should improve this test by moving this line outside/before `try`
Why do we need to call `getCause()` ? Does another `StreamsException` wrap the actual expected `StreamsException` ? If yes, this should not happen IMHO.
I think, we should remove the expected class here (note, that a `StreamsException` might be thrown not just from the last statement that we want to test -- thus, using `expected` the test might pass even if the exception is thrown from a different place what would be an actual failure
We can import `StreamsException` an get rid of the package prefix. (We just did not import `TopologyBuilderException ` because it's a deprecated class and for imports we cannot suppress warnings about usage of deprecated code. Btw: you can also remove `@SuppressWarning` from this method.
I think the gap in the current tests is that we are only testing with one partition. Indeed this test case also passes without the fix. To trigger the failure, we need one more partition in the test case.
Sounds right to me.
You need to fix these other methods also. If the exception is raised, the call to `release` will lead to invalid state.
Nit: add `final`
Nit: add `final`
Nit: add `final`
nit: better log the latest supported version as well.
Ah you're right. My understanding is that to manual upgrade from version 1/2 to version 3, we set `upgrade.from` config accordingly, so first rebalance everyone use version 1/2 in subscriptionInfo and AssignmentInfo; then in second rebalance someone send subscriptionInfo with version 3, and someone send with version 1/2 (they have not bounced yet), so assignmentInfo with version 1/2 are sent back again.
could this be changed to `usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion && receivedAssignmentMetadataVersion >= 3`
Making these mutable fields non-private makes it harder to predict what can happen with them (since anything in this package and any subclass can set the field). Maybe consider keeping them private and exposing `protected` getters.
True, but I find the naming a little more clear, but we can chalk this one up to personal preference, feel free to ignore.
Here's my reasoning of the cases: 1. `receivedAssignmentMetadataVersion > supportedVersion`: this should never happen. 2. `receivedAssignmentMetadataVersion == supportedVersion`: normal case, the leader only knows up to `supportedVersion`, and hence sends this version back. 3. `receivedAssignmentMetadataVersion < supportedVersion`: if some other consumer used an even older-than-`supportedVersion`, in this case this consumer will again send the subscription with `supportedVersion`. So it seems we do not need to distinguish 2) and 3) since for either case, line 763 and line 770 will actually assign `usedSubscriptionMetadataVersion = supportedVersion` right? Or do you just want to distinguish the log entry? If that's the case I think simplifying this to: ``` if (receivedAssignmentMetadataVersion > supportedVersion) { // throw runtime exception } else { if (receivedAssignmentMetadataVersion == supportedVersion) // log normally else // log differently usedSubscriptionMetadataVersion = supportedVersion; } ```
`info.version()` could be replaced with `receivedAssignmentMetadataVersion`
nit: use "{}.x." vs. string concatenation
`receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION` should be guaranteed at the server side as always right? If that is true, I'd suggest we refactor it as: ``` if (usedSubscriptionMetadataVersion > receivedAssignmentMetadataVersion) { if (receivedAssignmentMetadataVersion < EARLIEST_PROBEABLE_VERSION) { // throw illegal state exception. } // .. below logic } ``` So that we can detect potential bugs.
nit: empty line space.
I think keeping it in nested condition is fine, even further, I think `if (receivedAssignmentMetadataVersion < 3)` should not happen since in this case the leader should have failed and not returning anything, so we can add an `else` to throw RTE to make it more strict.
Should we enforce another rebalance when this happens? Otherwise we may live in a long period where all clients can support a higher version now while we still use lower version, so some advanced features (e.g. rack aware assignment in the future) cannot be picked, until the next rebalance is triggered.
Should we still do `taskManager.setClusterMetadata(fullMetadata);` before returning? I'm not sure if it will give us any good but just bringing this up..
We should have a constant rather than using '262' directly
nit: Maybe we can consolidate this logic into a function as well? E.g. "respondSentRequest" etc.
I think this method can still block regardless of requestTimoutMs at least due to the following calls: ``` updateFetchPositions() -> coordinator.refreshCommittedOffsetsIfNeeded() -> fetchCommittedOffsets(missingFetchPositions) -> ensureCoordinatorReady() { // Using zero as current time since timeout is effectively infinite ensureCoordinatorReady(0, timeoutMs = Long.MAX_VALUE) }```
typo: **longer** than
To clarify, there are two steps in `updateFetchPositions`. The first is fetching committed offsets; the second is lookup up missing positions. The latter is asynchronous now, but the former still blocks. We'll need to fix this once the KIP is approved, but it could turn out to be a little tricky.
@mjsax if `resume()` is called on the consumer `verify` will fail the test.
Maybe use the `addNode()` available on this class for consistency? (applies a few times in this file)
Do we need this boolean? Also seems a bit odd that `topologyOptimizer.optimize(..)` doesn't return a `Topology` - i understand it is because of existing code, but is there a way we could make it return a `Topology`
Also, same as above (and elsewhere), mixture of `final` and not `final` locals. They could all be `final` - i don't really care either way, but consistency would be good
this is for L134 - topic is not used
I actually think neither of these supressions are necessary here. You mentioned before that you'd make a pass and remove the "unchecked" supressions. While you're doing that, can you re-evaluate other supressions that are currently in the code? I think it might be a good practice to remove supressions when possible.
Can this be a local variable now? It seems it's not used across methods.
I think we can simplify this test. We only want to make sure the application directory gets deleted. Thus, we can create it manually, create the driver with corresponding config (passing in a empty Topology), call `close()` and check if the directory was deleted.
Can we make this private? Doesn't seem to used anywhere else. It has an odd return value, which is less of an issue for a private method.
Hmm, normally `IllegalArgumentException` indicates that the argument to a function is bogus, right? That's not really the case here-- the function argument was fine, but the topic wasn't set up correctly. This can probably just be a generic `RuntimeException`, since we don't have a need to make it something fancier.
I did notice that you renamed the method in a subsequent commit which covers the "name should also ideally indicate the difference" part. :)
+1 from me
I would say leave it out, I believe the change from `0.10.1` to a higher version requires a full stop and can't be done from a rolling restart.
Actually from `0.10.0` to `0.10.1` requires a full stop, from `0.10.1` to newer version does not. But I think it is still fine to leave it out.
I think you can do the zookeeper start in the `setUp` method
Don't we need to set version 0.10.1 for those initially? Otherwise, they will have trunk version
I see. That makes sense. I run into the same issue and put a workaround in #4636 -- we should clean this up, after we put proper testing for older versions into place as discussed in #4636.
Is there a case when updateQuota() can return false? Ditto for removeQuota().
Do we need quotaType? It seems that it's unrelated to metric tags.
After discussion on #4713 I think this idea should actually work. Nit: Can we rename the lock to `LOCK_FILE_NAME + " -" + taskId` though.
Another name might be `seek()`.
nit: add `final`
should we put `kafka` and `numThread` directly into `props` and reduce number of parameters here (would also simplify all the "passing through" code.
nit: add `final`
nit: add `final`
nit: add `final` to cleanup code
nit: add `final`
What about: `"Represents a signed integer between 0 and 2<sup>32</sup>-1 inclusive. "` This is a nit though, ignore and discard as necessary.
even clearer: "Represents a signed integer"
nit: "Null value is encoded..." -> "A null value is encoded..."
"encoded on two bytes using network byte order" -> "encoded using two bytes in network byte order"? Similar for the other integer types if you like the suggestion.
nit: not sure why Boolean is capitalized here, but not above.
nit: Values 0 and 1 *are* used..
nit: A null value
I'm +1 on supporting the timestamps, even if it's not commonly used now, users will often look to tests for example usage (at least I do). I'm also +1 on removing `childIndex` for the same reason, but I don't have too strong an opinion on that.
Can you point out where it would fail? Unclear to me atm.
nit: `child` -> `toChild`
nit: use static imports to get rid of `Assert.`
nit: use static imports to get rid of `Assert.`
nit: remove -- not used
explain why `Integer`, `Long` is used instead of `int`, `long`
Child-Index was deprecated recently -- should we remove it? Also, with KIP-251, we should capture output timestamps, too.
nit: break line
nit: line too long
nit: `uoe` -> `expected`
not used: can be removed
Hmm.. is this correct? If `forward(kv)` is called without childName or childIndex, it means sending to all children. So should this be `capture.childName == null || ...` ? Ditto above in line 414.
For this specific API, I suspect it is ever commonly used in PAPI, so I'm fine with not supporting it right away, also as a way to encourage users to change code sooner than later, if there's anyone.
This method is also deprecated. We should throw same exception as for `childIndex`.
nit: `This` -> `{@code MockProcessorContext}` "this" , "here" etc is bad style IMHO
nit: `{@code CapturedPunctuator} holds captured punctuators, along with their scheduling information.`
Nit: why not `private final String childName; // nullable` (would be consistent with L60)
Nit: `A method for getting...` sounds clumsy to me
Nit: `A method for...` (as above)
nit: remove `this` if not required (code style)
the broadcast was tested above already -- we should remove it -- one test for one feature
nit: remove (was tested already)
Sounds good. Would be helpful for users to understand. The exception message should explain what they need to do to avoid the exception.
Ah. Thanks. I missed the line in the constructor when a `StreamsConfig` is created -- thought there is no `StreamsConfig`. Makes sense now.
nit: `kv` -> `keyValue` (thought the whole class) -- IMHO, we should avoid abbreviations to improved code readability
nit: top of class
nit: top of class
Can we simplify the param-doc? Mabye: "The MockProcessorContext allow a `Processor` to access those config during runtime?" It it required to lost all of those with the corresponding methods? nit `{@link StreamsConfig}`
I understand why. But other contributors might not (and I might forget why in the future and want to change it...). It's not obvious from the code and thus should be explained with a commend, IMHO.
Where does it fail if we don't specify them? In KS they are required, because we pass the configs into a `StreamsConfig` -- but we don't use `StreamsConfig` (if I did not miss this) thus I think we can simplify the code here and just pass in empty `Properties`
`assertX` has expected value as first parameter -- we should switch both to avoid confusing error messages. Applied to whole class.
nit. Add `{ }` to block (we always use them). Same below.
nit: `{@code null}`
nit: no new paragraph required
I would omit this (not the test, just the sentence) and put examples into the web docs.
Oh I meant that we explicitly get first three values from percentiles
@cmccabe Looks like the log entry hasn't been updated.
I don't think this works. This branch only handles connections which are completed "immediately" in `doConnect`. This is not the common case, which is why all of the test cases in `SelectorTest` fail with this change.
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Left this out of my previous review. This needs to change as well as it will return false for two `Segment` instances with the same `id`. Also, this statement will always evaluate to false as `Long.compare(x, y)` only returns `-1`, `0`, or `1`.
need a check for null on `obj` here as well
Since equality is measured by comparing timestamps something like `return Objects.hash(timestamp);` should be used instead.
This implementation of `equals` will return false for timestamps of the same value; maybe this could be something like `return Long.compare(timestamp, otherTimestamp) == 0`
Should check `other` for `null`
Fair enough. If we think it's the right thing to do, then making the change in `Utils` seems like a good idea to me.
Admittedly, this is a random time to bring this up, but a pattern that I like for this is: ```java final long seed = providedSeed != null ? providedSeed : new Random().nextLong(); System.out.println("Seed is: " + seed); final Random rand = new Random(seed); ``` where `providedSeed` comes from an env variable, command-line arg, whatever... This way you get the same default behavior (pseudorandomly generated sequence of test values), but you also have the option to deterministically reproduce a previous pseudorandom sequence from a prior run. This can be helpful in diagnosing flaky tests. Not saying this is needed here; I just wanted to share the pattern.
I do not know how to do that.. or if it is even possible. I think adding this extra step is not too bad as in `release.py` we already wrap some other places where we need to remove `-SNAPSHOT` anyways, this is just for confirming, that in trunk maybe we'd update `release.py` as well.
`.toString()` unnecessary here are other similar logs.
Yes, `totalRequestCount` sounds clearer. We might also consider setting it back to 0 in `MockClient.reset`.
Maybe we could make this assertion precise? I think we expect the request count to be 0.
@vvcephei The log files are deleted when a processor is stopped and started (note, this does not apply if one does a `processor.restart()` but only on explicit `processor.stop()` plus `processor.start()`). Thus, first occurrence in the log is L96 and this is second one).
I'd suggest update the existing test directly in this PR.
`topics` defined here and in next test, maybe move up to `init`
The original motivation is to maintain the information of which thread was hitting this error, but the current implementation does not maintain that any more.. I'm not sure if it was lost somewhere in previous commit or it is never the case. But this is what I've in mind originally: ``` final String logPrefix = String.format("stream-thread [%s] ", threadClientId); final LogContext logContext = new LogContext(logPrefix); final Logger log = logContext.logger(LogAndContinueExceptionHandler.class); ```
Would be helpful to understand the reason as well. The problem with a mismatch is that it's harder to track down the source of a log message and it messes up line numbers when printed.
Yes they are designed to be public.
I see. There may not be a nice way to wire in the thread id from a static context. If we can't get the thread id, I'd suggest accepting the change here so that at least the message points to the right file.
nit: "cannot be run" or "cannot be executed" same in other versions of this test
nit: add `final` (we add `final` to all variables when possible -- applies multiple time -- please update all variables (inc. loop-variables)
Since we can handle the case in the restoration phase above, I think we do not need to use a separate globalNonPersistentStoresTopics here anymore. Instead, we can do the following inside this function: 1. Filter the entry of the pass-in `offsets` map if `!store.persistent() || storeToChangelogTopic.containsKey(store.name())`. 2. checkpointableOffsets.putAll(filteredOffsets); 2.a. In line 245 above, we can still only heck if `checkpoint != null`. 3. if (!filteredOffsets.isEmpty()) filteredOffsets Note that after the restoration is done, we will fill in the restored offset in line 287: ``` checkpointableOffsets.put(topicPartition, offset); ``` So after the restoration phase we should have the checkpointableOffsets map populated already.
In 1.2.0 we add an optimization to avoid writing the checkpoint file if there is nothing to write (i.e. the available offset map is empty): this is not a bug fix but just some optimization. If you have other persistent stores in your topology the checkpoint file will still be written. Here is the JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-6499
For global state stores, here is the ordering of each stage: 1) Initialization: `GlobalStreamThread.initialize()` -> `GlobalStateUpdateTask.initialize()` -> `GlobalStateManagerImpl.initialize()`, where we read the checkpoint file into `checkpointableOffsets`. 2) Restoration: In the same `GlobalStateManagerImpl.initialize()`, we call `stateStore.init()`, in which `GlobalStateManagerImpl.register()` is called, and hence `restoreState()` will read from the loaded `checkpointableOffsets`: if it contains offset seekTo(), otherwise seekToBeginning(). 3) Starting: The restoration will bootstrap the global stores up to the log end offset, and after that we will write the restored offset to `checkpointableOffsets`: i.e. we will update the map, with the new values. At this stage the non-persistent stores' offsets should be written to it as well (i.e. line 288). Then we will call `GlobalStateUpdateTask.initTopology` to create the update node and go ahead the normal execution. So here the returned `stateMgr.checkpointed()` should already contain the restored offset already, therefore we can safely call `globalConsumer.seek()` in its caller now. 4) Checkpointing: When we call checkpoint(), we should make sure that non-persistent stores are not written to the checkpoint file, and actually whether we should filter on the `checkpointableOffsets` does not affect correctness anyways since we do not use it anywhere anymore, but to be consistent with its name I think it is still better to filter out those non-checkpointing offsets. Note that the whole logic is a bit awkward as it was spin off the `ProcessorStateManager` class, and as I mentioned above we can consider consolidating them in the future.
nit: add `final`
Let's piggy-back on this PR: it should not be a big fix.
You need to pass in a `Pattern` but not a `String` here. This actually exposes a "bug" in the test-driver -- it should check if the topic name is valid -- and a pattern is not a valid topic name. Not sure if we should have a different PR for a fix or piggy back on this PR. \cc @guozhangwang @bbejeck @vvcephei
You cannot use `setupMultipleSourceTopology` to register a topic-pattern because it calls `addSource(String, String)` but to register a pattern you need to call `addSource(String, Pattern)`
Discussed offline with @ijuma. This actually isn't quite enough to fix the example he brought up earlier in all cases. For example, it still doesn't work if you construct `Config` with a `Set` for the entries. Given that we can't guarantee equality in this way generally, I'd probably suggest we just revert to using `Collections.unmodifiableCollection` like you had before.
On second thought, considering the usage of `get` below, maybe we should create a `Map` instead so that we can more easily access an entry by name.
I wonder if we should copy to a list so that we are guaranteed that the collection won't change behind our backs and so that comparison between two `Config` objects will be valid even if they are constructed using different collection types.
suggestion: `because the URL of the leader's REST interface is empty` (otherwise it should have been `due to the URL of the leader's REST interface being empty`)
nit: `,` (comma) doesn't seem required in the sentence.
I'd rather not use exception handling for branching. I think I'd prefer to address this case with an earlier check for `leaderUrl() == null` and instantiate a more informative exception to pass to `cb.onCompletion`
Although the checkstyle rules currently do not enforce curly brackets in if/else blocks that contain a single statement, because this statement here spans multiple lines, I feel it'd be best to enclose it within `{ }`, even if that's optional.
I'd prefer that we use `TimeUnit.SECONDS.toMillis(10)` and `TimeUnit.SECONDS.toMillis(1)` here. It avoids having to do any math when looking at the code.
I see. Do not bother then :) At lease we are not introduce a regression to make perf worse :)
@bbejeck That is a good question! Originally I thought it is okay to always calling `hasNext` inside `next()`, as long as we make sure `hasNext` implementation is idempotent, i.e. calling it multiple times before `next()` does not have side effect is sufficient. But by making it idempotent we could have the corner case you mentioned. For example: ``` t0: call `hasNext()` -> store is still open -> call `makeNext` -> `next` field is set. t1: store is closed. t2: call `next()` -> call `hasNext()` again ``` Without this check, at `t3` we would still return the `next` field.
nit: ` .. over the properties prefixed with {@link #CONSUMER_PREFIX} and the non-prefixed versions (read the override precedence ordering in {@link #MAIN_CONSUMER_PREFIX)..` ditto below.
We can still set `final` -- note, that `final` implies that the parameter `config` cannot be reassigned to a different `Map` -- the `Map` itself is still mutable though and thus it can be altered within `KafkaConsumer` later on. (Adding `final` is more a question of style here.)
Nit: `restore` -> `consume` ? This consumer is used for regular processing -- "restore" sounds like fault-tolerance only.
nit: `isolate ... form other client configs` -> ``` override ... for the main consumer client from the general consumer client configs. The override precedence is the following (from highest to lowest precedence): 1. main.consumer.[config-name] 2. consumer.[config-name] 3. [config-name] ``` Ditto below for other two.
Nit: it's also to distinguish from other client configs -- not just consumer configs.
typo: `consume` -> `restore`.
Actually I've been thinking .. could we move the construction of the `TaskManager` and its `taskCreators` into the constructor of `StreamThread` directly from `create` call? Then we can get the threadName from `currentThread.name()` directly and do not need to pass this parameter around any more.
We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields.
Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
I think this should be *above* the previous line? Order of output could be confusing given current phrasing. Might also want to include `isKeyConverter` in the log.
same question as above about moving this above the call to `configure`
yeah, that's just about internal code being readable and understandable, not critical to current issue.
In fact, now I am wondering if we should rename the enums to be clearer. We can follow up on that separately, but I realize these branches aren't really very clear currently.
same for tests below as well
ditto here and others below
nit: `crf` -- we avoid abbreviations because they make the code much harder to read.
nit: add `final`
nit: formatting -> should be in the line above.
nit: if unused, remove the variable instead of suppressing a warning.
We don't usually use `get` prefix for getters.
Since this integer is decremented when `tryUnmuteChannel()` is called, would it be a bit more intuitive to name it `muteRefCount()`? If so, we may want to also rename methods such as `incrementUnmuteRefCount()`, `decrementUnmuteRefCountAndGet()`, `getUnmuteRefCount()` and `incrementChannelUnmuteRefCount()`.
Ditto on the properties and the driver.
nit: should we inline these? The variable names are barely shorter than the method names.
ditto on the properties and the driver.
Ditto on the properties and the driver.
Ditto on the properties and the driver.
ditto on the properties and the driver.
`mkProperties` could compactify this code, but it's not necessary; your call.
Nice solution to this problem.
This cleanup seems a bit awkward. It assumes that tests will initialize the driver but not close it, which seems like a strange abdication of responsibility. I think it would be cleaner and clearer to get rid of the driver field entirely. Tests that need the driver already initialize it; they can declare it as a local variable as well. Then, they clearly need to close it as well. Since `TopologyTestDriver` is `AutoCloseable`, one option is to declare the driver in try-with-resources style: ```java @Test public void myTest() { try (final TopologyTestDriver driver) { // the test code } } ```
There's now a `Utils.mkProperties` method you can use (in conjunction with `mkMap`) to set these at the declaration site instead of setting them (redundantly) before every test. Then you won't need the `@Before` at all.
This is not a suggestion for change: while working on removing `KStreamBuilder` and `TopologyBuilder` I realized in some unit tests we may still need this class to access the internal topology builder. So probably we cannot remove it even after that, but we can discuss this later in the cleanup PR.
I think we ditch the before/after methods as I previously recommended.
Ditto on removing these before/after methods.
Actually, on closer reading, I think it was like this because the old driver didn't have a way to "advance" the time, just to set it, and the test logic wants to "advance" the time inside a loop at some points, so I think we should delete the variable and use `advanceTimeMs` instead.
I think this can be package-private, since your wrapper is in the same package.
ditto on removing before/after.
ditto on removing before/after.
ditto on removing before/after.
ditto on removing before/after.
ditto on removing before/after.
ditto on removing before/after.
ditto on removing before/after.
please see my earlier remark about localizing as much as possible and adding a tear-down.
Ditto on removing before/after
I haven't been around here long enough to be confident about this, but here goes... It looks like those tests are making assertions about the structure of the runtime topology that results from different DSL operations. To test that, it should be sufficient to directly call the relevant `build` operations to get a `ProcessorTopology` from a `Topology`. It doesn't seem to need the TopologyTestDriver to be involved. Nevertheless, this illustrates the tension between an internal test driver and an external one. IMO, this class should be only an external test driver, so if there's some stuff that we need for internal testing purposes, then we need to keep the internal test driver around. So I guess my suggestions are: 1. try to rewrite the those tests to avoid using a topology test driver at all 2. if that fails, just use the internal test driver instead
nit: remove empty line
Might be simpler to call `recordFactory.advanceTimeMs()` and not specify the timestamp for each record explicitly? We also leave it as is.
I think, we can just remove this line. Calling the (old) driver is not required in this test.
As above: `ProcessorTopology` is an internal class and should not be exposed to the user.
This class is part of public API -- thus, we cannot change any public interface without a KIP. I am also not sure, if we should change this class in the first place. What limitations did you find rewriting the tests? We should try to rewrite out test without any change to this class IMHO. For this particular change, we should not expose `InternalTopologyBuilder` because it's an internal class.
nit: add `final`
If it's the same for every test, you could also just create a `getProperties()` method for the tests to call when they need the properties. This way, you won't need a field to store the properties in.
Alternatively, we could ditch the `driver` field and just make it a local variable in every test. Having it as a field made more sense when it was `setUp` in each methods, but now that it's instantiated in each test, it would be simpler to limit the scope to a local variable. Each test would need to call close at the end, but that's fine with me. If anything, it demonstrates proper use of the driver.
Ack. Fine to leave as is -- was just a thought.
Doing in `@Before` is fine. But we don't need to call `new` each time. `this.props` will be a new empty `Properties` instance anyway -- we don't need the second object.
It's a +1 from me to update this test to cover the deprecated API. IMHO, as long as it's still in the code base we shouldn't remove any tests covering the old API.
Ditto on the properties and the driver.
I'd suggest creating the windows in this function based on the schema type at once, and use them everywhere in the test functions. Looking through the tests, I think three windows are good enough for all of them: `[10, 10]`, `[500, 1000]`, and `[1000, 1500]` as long as we reduce the retention time to 1000L.
nit: define `KeyValue<Windowed<String>, Long> deserialized` once before the `if - else` statement then you can call `results.add` once by moving the `results.add` to outside the `if - else`
The deserialized is actually different since we need to differentiate between `timeWindowed` and `sessionWindowed`. It is partially because our signatures for time windows and session windows are not the same.
Same as before, `new Integer[]{}' not required for `Arrays.asList`.
The statics `DEFAULT_TOPIC_PREFIX` etc. that are no longer used can be removed.
Doesn't look like this field is needed.
Perhaps an exception would be better than an empty set since this would be unintentional? Otherwise the check itself is unnecessary since the following code would do the same thing.
Unnecessary `new String[] {}`, can just use the string as varargs for `Arrays.asList`. Same pattern is used in multiple places in this class.
I guess it's tough to avoid blocking here. All of this would be considerably easier if we moved the rebalance to the background thread. Another improvement for another time.
We have three `remainingTimeAtLeastZero` functions, in AbstractCoordinator, ConsumerCoordinator and KafkaConsumer. Is it intentional? If not we could leave just one to avoid unintentional code divergence in the future. cc @vvcephei
I think the basic idea makes sense and is what I was expecting. It might not feel too elegant, but I think a simple approach is best initially. An interesting point to consider is what would happen if an offset fetch is in-flight while a rebalance is in progress. When it returns, the offsets may be stale. I am wondering if it makes sense to fence the response using the group generation. In other words, we record the generation at the time of the request and then verify when we receive the response that it matches the current group generation.
As before, the old code tried to optimize for the common case where the consumer was joined.
nit: move below the shortcut return below.
Since we don't use the context anyway, an alternative fix is to use `Processor` directly rather than `AbstractProcessor`.
Alternatively, if we don't care about the context, we could just use `Processor` directly instead of `AbstractProcessor`.
original was better
line too long
one parameter per line
one parameter per line (same below)
orignal was better
as above. (don't make lines longer)
for proper rendering, we should introduced list html markup
why do we make lines longer? harder to read now
original was better
nit: this seems to make it worse -- it's wider than what's Github shows
one parameter per line
original was better (ie. `{}`) maybe one parameter per line
`punctuate(long timestamp) {}`
original was better
original was better
original was better
original was better
not required after `<pre></pre>` block
original was better
original was better
line too long (same below)
original was better
original was better
In the other two variants, we used a literal `1` and `2` here. Not sure if it matters at all, since the way we get here is that `usedVersion == 3`.
If `latestSupportedVersion` is ever going be different, we should use that field than hardcoding it here. But personally I am not sure where is `latestSupportedVersion` ever going to be used. Although the KIP did include this in the proposed changes, it only talks about how the `SupportedVersionNumber` of `AssignmentInfo` will be used, but not `SubscriptionInfo`..
I think this and following usages around `latestSupportedVersion` are related to the upcoming version probing code. It's a little mysterious to have a "latest supported version" always equal to the "current version" in this PR in isolation, but I don' think it's actually problematic.
It seems like these two approaches are fighting for dominance in this code. Personally, I favor the "flat" design, where these methods don't call each other. It's just easier to read when each one in isolation lists all the operations it needs to do. It's also more general, since a hypothetical future version N might add some new metadata in the middle, or even drop some previous metadata and so wouldn't be able to call decodeVersion{N-1} in its implementation.
To make it more rigid: we can pass a valid end point string, and then check that the field is still `null` below.
I'm embarrassed that I missed this one.
Personally I'd prefer to have decodeTasksData in which we hard-code the logic of doing both prevTasks and standbyTasks, we do not code-share for these two task sets but we share code of constructing the set for version two and version three. I guess we cannot get both code sharing, and since it is really a nit I'm fine either way :)
Probably a tough case to hit in practice, but I can't say it's impossible.
By the way, one of the downsides to using the __consumer_offsets topic, is that it effectively makes the `listConsumerGroups` API dependent on having Describe access to this topic.
This does not need to be a map, a list is good enough since we would not call `addList` with the same groupId, similar to `addError`.
I am also wondering if we should mention something about the high watermark.
we do not have the parameter for the topic group in line 89...
The first line should be changed to `Skipping creating tasks for the topic group {} since topic {}'s metadata is not available yet; no tasks for this topic group will be assigned to any client.
If it falls into an endless loop, it means that we've discovered some other bugs. In Jenkins jobs we have a timeout so it will eventually timed out after that and people can be notified.
nit: move `process()` to next line (easier to read)
We don't want to do it this way, because, we want restore and calling `mainCosumer.poll()` to interleave -- otherwise, we might drop out of the consumer group as restore is expected to take longer than `max.poll.intervall.ms`. Hence, within this method, we should only do a single `poll(restoreCosumer, 10)` an return afterwards -- the main loop will make sure that this method will be called again to resume the restore.
This would update `updateEndOffsets` multiple time -- should we set it only once? Note, that `needsRestoring` will not be empty until the full restore is completed.
nit: don't need `result` can return ` new ConsumerRecords<>(mergedRecords)` directly
The goal of the ticket is to actually remove this check.
@ConcurrencyPractitioner thanks for updating the PR. My point from before was that we should restore each batch of records returned from each `poll()` call vs. keeping all returned records in memory and start the restore process when there no more records to fetch. Sorry if I did not make that point very clear.
Do we need to check if restore is completed for some partitions? I think, with EOS and commit markers, there is a corner case that the check below does not detect that restore is complete even if we fetched all data (but not the final commit marker). For this case, records.count() could be zero but the actual `position()` for a partitions was advanced by 1 to step over the commit marker.
we also want to remove this check
It's an interesting thought, but users may override the number of partitions for `__consumer_offsets`, so I don't think it will work. More generally, we are trying to avoid dependence in the clients on the `__consumer_offsets` topic since it ties the behavior of the client to what is more properly an implementation detail.
@hachikuji is correct. We can't do blocking operations in the admin client service thread. We certainly can't do blocking operations that wait for the service thread itself. This will deadlock. I think it's a good idea to have a coordinator node provider, but we need to build out a little more infrastructure to make it possible. I have a change which should help with that, at https://github.com/apache/kafka/pull/4295
The ideal would be to use the `CoordinatorNodeProvider` here. There is not much benefit in having it if we just invoke it inline. The problem is that the `provide()` method is called by the send thread, so we cannot have it block on an operation which itself depends on the send thread. To make it work nicely in this way, we probably need an asynchronous `NodeProvider` API which effectively lets us chain the `DescribeGroup` request on to its completion. For example, maybe something like this could work: ```java interface AsyncNodeProvider { KafkaFuture<Node> provide(); } ``` cc @cmccabe (who may have some ideas as well)
`toString` of `FileChannel` doesn't seem to be useful: ```text scala> FileChannel.open(new File("/Users/ijuma/Downloads/trace.log").toPath) res5: java.nio.channels.FileChannel = sun.nio.ch.FileChannelImpl@591e5fd4 ```
nit: add `final` (2x)
nit: remove empty line
nit: add `final`
nit: add `final`
nit `{@link org.apache.kafka.common.serialization.Serdes#Long()}` -> `{@link org.apache.kafka.common.serialization.Serdes#Long() Serdes#Long()}`
nit: final on params here and methods below.
nit: add `final` (2x)
using a builder and string concat? maybe just `append("blah").append(var).append("\n");
Is this designed to have thread-level be the parent of the cache-level? I think originally we want to have task-level be the parent of cache-level (but there is a bug for that so it may not actually be the case).
That is right, and originally we use `Metrics.metricName()` to leverage on the most common configs which is `"client-id" -> threadName`. But here you have removed it. Is that intentional? I think for thread-level we should have just one tag: `"client-id" -> threadName`, and for task-level we should have two tags: the one with thread level plus the task id, and for cache / store / processor-node we should have three tags, the two from task-level plus the record-cache-id / store-name / processor-node-name.
Why not organizing the thread-level sensors as cache-level sensors as well? I.e. `Map<String, Deque<String>> threadLevelSensors = new HashMap<>()` where the string key is just `threadName`, since we will only remove sensors for the whole thread at once.
True, will we ever want to have this ability? But the change seems fine to me.
Personally I'm still a bit leaning towards making the groupName as a field inside `TaskMetrics` same for `ThreadMetrics`, since it is quite hard to debug a typo issue in this case, and more string constants, more likely we will hit this.
Probably not, and since sensor names are only used for internal bookkeeping there should be no compatibility issues with the change.
My rationale is mainly around typos :) In the past I have some experience spending much time on troubleshooting a typo caused issue, since these issues are usually not well exposed in the exception messages, e.g. if you had a `stream-record-cach-metrics` in one of the lines it would be hard to find out..
Makes sense, we can do that later.
I'd suggest rename `ProcessorName` to `GraphName` to be consistent with the base `StreamGraphNode`, also to distinguish with the physical topology's `XXProcessorNode`. Ditto else classes.
I see. I'd suggest distinguish the process / transform with other stateful nodes, since while reviewing KIP-292 I found their semantics regarding the resulted KTable's materialization state, queryableStoreName, serde inheritance, etc are quite different.
That's fine then. Note that if it ever introduces too many LOC that is going to be thrown away shortly, we can always just add empty no-op functions which will be broken if ever called atm to save time not adding wasting code.
nit: add `final`
Seems no harm running this test case for every parameterization. I know it's redundant since we are not using either of the fields, but it feels less arbitrary.
Maybe worth a test case which ensures correct behavior with start and end = 0 as well? Another good one would be to have a single batch which is truncated in the middle.
These should be `<pre>` rather than `<code>`. The latter is more for phrases, not blocks, and loses all indentation and line breaks within a block of code. Then you can get rid of the `<br>` tags.
We shouldn't use `<br>`; instead, use a `<pre>` section around the lines.
Good point, but it could be clearer. This implementation can be used in production, but the `PropertyFileLoginModule` that also ships with this reference implementation should NOT be used in production.
Is it intentional to swallow this exception? At a minimum, we should probably log the exception at DEBUG level.
ditto on (what I think is) the impossibility of this condition being false.
This should never happen, right? maybe we just don't check it and get an NPE if somehow driver gets set to null after the setup.
I think a better test scenario is to move the logic in `close()` call, i.e. when the stream thread is being shutdown, and topology is closing, we call `processorNode.close()` in which we wait for a while and then tries to access the global store. It mimics the case where in closing the store cache is flushed and hence tries to access the global store again.
`expectedTimestamp` parameter missing (insert before `expectedHeaders` to align with method name.
flip order of `headers` and `timestamps` to match method's name
nit: missing comma `headers[,]`
`timestamp` missing (twice)
`timestamp` missing (three times)
for headers, we need to do the same as for `recordValue` ? (I think you c&p from `compareValueTimestamp` -- there it's different because timestamp is a `long` and cannot be `null`.
I would remove `<` and `>` here in replace with `"... value=" + expectedValue + "..." ` -- we used `<>` just to mark a `<k,v>` pair, but not required for value itself (cf. different of `compareValueTimestamp` and `compareKeyValueTimestamp`) missing space before `but`
maybe use "a", "b", "c" as values, as the transformer counts the number of calls to `process` (for better distinction with next test)
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
nit: remove empty line
store not used
nit: fix indention (we usually use 4 spaces, not 8)
I think we should rewrite this to make sure the NPE is thrown in `get()` but not in the constructor. ``` final KTableTransformValues kTableTransformValues = new KTableTransformValues<>(parent, new NullSupplier(), QUERYABLE_NAME); try { kTableTransformValues.get(); fail("..."); } catch (final NPE expected) {} ```
nit: remove empty line
store not used
nit: add `final`
This fail just raises an AssertionError from a thread. I'm not sure it will actually cause the test to fail.
Since the interrupt status has been cleared by using `Thread.interrupted()` and `InterruptedException` is caught here, the outer `run` call further up the call stack knows nothing about Sender thread had been interrupted and will keep running. An alternative way is not to capture `InterruptedException` and let it be thrown the outer `run`.
We usually avoid writing to standard out in test cases. A few more of these.
nit: list the members list here would help trouble shoot.
nit: add `final` (we use `final` whenever possible)
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final`
Yeah, builders are nice for the reasons you mention. However, since it wasn't discussed in KIP-222, I think keeping it package private for now might be better.
Curious why you made this change. Lists are a bit more convenient for users.
One issue with `Collection` as that `equals` is not defined. So, one should be careful about returning it in public APIs. It's OK to receive it as a parameter though.
The older client would still return false for `rebalancing()` in your example since it would not be aware of the new string. To be clear, even if we have an enum, we can still have methods that return true for multiple states. Having said that, it is annoying not to be able to display the string if the type is unknown. The ideal scenario is hard to do in Java. Something like: ```scala sealed trait State case object Foo extends State case object Bar extends State case class Unknown(value: String) extends State ```
nit: memberDescriptions might be a better name
extract groupMember.memberAssignment() and protocolType.isEmpty() to final local variables to avoid code (& method call) duplication
extracting this to a private method (with a signature like List<MemberDescription> getMemberDescriptions(List<DescribeGroupsResponse.GroupMember> members) might be better for readability
We will return an empty assignment while the group is rebalancing. Rebalances can sometimes take a little while to complete, and we have found it useful to still be able to show the members when the group is in this state.
I was thinking about this too. I think the reason we left it as a string in the protocol is so that we could change existing states or add new states without requiring a version bump. By extension, the AdminClient could just expose the string and be left agnostic to specific states. In retrospect, however, we should have just used an enum in the protocol because we needed to check explicitly for particular states inside tools and now in AdminClient. Given that, I'm inclined to use an enum here and file a jira to change the protocol to use an enum as well.
Reading the code details here, I think the `materialized` is not necessarily related to the `statefulRepartitionNodeBuilder`: it is only for materializing the resulted KTable, and hence should be only related to an `TableProcessorNode`. The fact that it is extending `RepartitionNode` which in turn extends `StatelessProcessorNode` actually points out the issue: it has a materialized `state` but is under the category of `stateless processor`.
Is it correct to set parent node name as `this.name + "GROUP_BY"`? Seems the parent node name is not set in this way. Ditto below.
I think one thing to take care is to explain / make clear when to use what mechanism: 1. We have a repartition graph node used for implicit repartitioning before aggregates and joins. 2. In some other places, e.g. here in selectKey, we do not create a repartition node but use a flag instead. It is better to elaborate why we made this design.
Not sure why we would lose the generics? Can't it be change to: ``` private <K,T> String repartitionIfRequired(final String queryableStoreName, final RepartitionNode.RepartitionNodeBuilder<K, T> repartitionNodeBuilder) ```
nit: extra space.
nit: unnecessary newline
we need to shutdown the executor service even if there is a test failure (either in a finally block or the teardown).
nit: as above; remove
+1 from me.
We should connect the state stores with this request processor instead of enforcing us to remember which processor to set while initializing. E.g. here we can do (note I removed the procNames): ``` private void doTestValueGetter(final StreamsBuilder builder, final KTableImpl<String, Integer, Integer> table2, final KTableImpl<String, Integer, Integer> table3, final String topic1) { final Topology topology = builder.build(); KTableValueGetterSupplier<String, Integer> getterSupplier2 = table2.valueGetterSupplier(); KTableValueGetterSupplier<String, Integer> getterSupplier3 = table3.valueGetterSupplier(); InternalTopologyBuilder topologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology); topologyBuilder.connectProcessorAndStateStores(table2.name, getterSupplier2.storeNames()); topologyBuilder.connectProcessorAndStateStores(table3.name, getterSupplier3.storeNames()); try (final TopologyTestDriverWrapper driver = new TopologyTestDriverWrapper(topology, props)) { KTableValueGetter<String, Integer> getter2 = getterSupplier2.get(); KTableValueGetter<String, Integer> getter3 = getterSupplier3.get(); getter2.init(driver.getProcessorContext(table2.name)); getter3.init(driver.getProcessorContext(table3.name)); // same below ``` Ditto elsewhere for initializing the getters.
Better name as "setCurrentNodeInProcessorContext"? And then in java docs mention that it returns the processor context with current node set.
I think this is because this function is only for init the getters, which requires the state stores is connected, hence accessible in `init` to the node.
remove and rename test to `shouldCreateSourceAndSinkNodesForRepartitioningTopic`
nit: move to next line ("weird" formatting)
nit: add `final`
nit: fix typo `store[s]` ;)
add `fail()` in next line to make sure we hit the timeout and throw an `AssertionError`
nit: add `final`
nit: rename `ivtwks`
Nit: rename `vts`
Use `StringBuilder`? Possibly also change the name of the method since it is not a getter.
Same as above - use StringBuilder and rename method.
Double.valueOf not required, auto-boxing can be used. Here and in the lines below.
Can remove both `Long.valueOf`
should probably remove the `get` prefix and use `optionValue` since that is the convention followed in the Kafka codebase. Lots of other occurrences like this.
Safer to synchronize on `ExpiringCredentialRefreshingLogin.class` in case this class gets sub-classed later.
Is there a case when we don't need synchronization? Passing in the class from the constructor is fine, but I think we should just synchronize all the time.
OK, let's keep it for now.
Is there a reason why you had to add this code? `clientSaslMechanism` should already have been set to inter-broker SASL mechanim by the broker for that case. We rely on that in `clientCallbackHandlerClass` for example.
nit: Capitalize `p`.
Hmm... note that when EOS is turned on, each task will has its own producer client and the producer object passed in here will be null. So I'd suggest update the following code in line 1224: when `producer == null`, try to iterate the owned tasks from the `taskManager.activeTasks().values()` and get its producer (it is private, so we may need to add a package private getter). In this way for both EOS and non-EOS we will get the producer metrics.
Update the above TODO with only admin client left.
I might suggestion: "Infer the schemas when deserializing JSON and when " + SCHEMAS_ENABLE_CONFIG + " is false and the JSON keys/values have no embedded schemas."
Nit: we always use braces around if-blocks.
Might be useful to mention the serialized representation is JSON, and to say that this only affects deserialization when `schemas.enable=false`.
nit: one space.
might as well change the field name as well
I don't fully understand why we check for a `processId` less than or greater than the ones defined above.
I could be missing something but a range of 1 to 9 state stores seems large to me for an individual task to have
One more thing -- missed this before: can't we remove `completedPartitions` and call `iterator.remove()` instead? Maybe we need to change the iterator to iterate over the `HashMap` instead of the "key-set" of the HashMap though.
I think we can remove this variable to simplify the code further.
we can also remove the entry in `updatedEndOffsets` if this is true.
@mjsax What you suggested sounds right to me.
I think we can move this into the block above line 70 to 72 and simplify: ``` if (!needsInitializing.isEmpty()) { initialize(); updatedEndOffsets.putAll(restoreConsumer.endOffsets(needsInitializing.keySet())); } ```
replace `restoringPartitions` with `needsRestoring.keySet()` to get rid of the unnecessary variable.
This change seems not needed.
Please fix this, too.
nit: camel case
as above nit: double space `to Kafka`
typo: `per reach record`
If this method is used *only* for informational purposes, maybe we should just call `toString` on the topicNameExtractor and make sure that the `StaticTopicNameExtractor` includes the topic name in its output. Better yet, we could just change the method to return the `topicNameExtractor` itself, since it really doesn't *have* a topic anymore. If it's used for any other not-strictly-informational purpose, it seems risky to return some other string, claiming it's the "topic". It seems like it could become a bug in the future...
`KeyValueMapper` need to be updated
+1 to "based on the {@code topicExtractor}"
Maybe extract lines 361-363 to a method named something like `createSinkNodeWithTopic` returning a `SinkNode` then this method body would just have single `if-else` statement. But not a big deal could stay as is.
I don't think so. If `A` connects to itself, it's still a single node: ``` topology.addSource("source",...); topology.addProcessor("name", ..., "parent"); // throws because "parent" no added yet topology.addProcessor("source", ...); // throws because "source" is already used topology.addProcessor("processor", ..., "processor"); // throws because it connects to itself --- note that internally, we would add "processor" first before setting up the connection (ie, the parent is know then setting up the connection as we just added the parent) --- however, we do have an additional check in the code to throw if people build a loop like this ```
I think we throw for all three cases.
typo `reach` should be `each` (Might be somewhere else, too. Please double check.)
Should be two tests instead? (Leave it up to you to decide -- don't insist on a change)
we're in Java8 now... I think you can do: `(key,value,context) -> { ... }`
I know this is currently only called within a synchronized block, but this method implementation requires that be the case. What do you think about making this method synchronized, removing the synchronized block around the call in `close()` (lines 141-143 above), and leaving the other synchronized blocks as-is. I know that's overkill, but it's actually no more overhead and it's safer in case somebody in the future calls `tryStop()` outside of a synchronized block.
I'm not sure this is correct. If we start paused, then initialize() and start() won't be called. I think it is likely in this case that existing connectors could break under this scenario because they wouldn't expect stop() to be called if neither of the initialize() or start() methods were called. The rest of the resources (producer, transformation chain, statusListener) should still be closed as they are already initialized for the WorkerSourceTask when passed to the constructor.
Nit: the method is `Transformation.close()`, not "stop". The log message should probably reflect that.
Nit: the method is `Producer.close()`, not "stop". The log message should probably reflect that.
The problem is that we are losing the indication that the interrupt has occurred. A caller up the stack may depend on seeing it. I think I would just let the exception be raised in all cases even if the producer is being closed.
yeah, the existing `IllegalStateException` is confusing and we should fix it.
Can you explain why we need to catch this? It's generally a bad practice to ignore interrupts, so usually we either let the exception propagate or we reset the interrupt so that the caller has a chance to observe it.
Can you elaborate? I don't see any point in the code where we would return between adding the topic and awaiting the update.
Should we chain the caught exception? We expect this to be the close exception, but it could also be a timeout or an authentication failure. Might be useful to know in some scenarios.
I think it's fine to leave this unchanged since it is only invoked at the start of the mock producer apis.
Not a big deal, but perhaps we could let `MetadataUpdater` implement `Closeable`? We can still override `close()` so that it doesn't throw an exception.
This message seems a little low level for something which will get propagated back to the user. An alternative to consider would be to let `awaitUpdate` return a boolean indicating whether the update happened or not. That would allow us to raise an exception with a producer-specific message from `send()`.
I am wondering if we should throw `KafkaException`. This is an expected state since the producer is designed to block in `send()` to await metadata and there is not really any way for a user to avoid it. To be consistent, we could also raise `KafkaException` from `RecordAccumulator` in the similar scenario.
May be the Converter, Serializer and Schema can be injected through the constructor. May be doesn't make sense for the samples(). I'm not too opinionated on the approach here since we can't use Junit 5.
nit : can use assertNull
not part of the kip any longer
+1 to rename to `windowedKTable` nit: fit formatting (either move `consumed` down one line, or indent other parameter to match indention of `consumed`)
As mentioned in the KIP discussion thread: it seem unnecessary complex for user, to specify all those value. The only parameter that is mandatory is the window size. If people call `KGroupedStream#windowBy` all other parameters also optional; they should be optional when reading a topic, too.
`windowSize` should be `Duration`
method name changes
This is not part of the KIP any longer
a logger should always be private -- if classed extend `KTableSource` the should create their own logger with the corresponding "child" class name.
nit: might be better to name it windowedKTable
the method name changed to `windowedTable` and `windowSize` parameter is missing
nit: might be better to set end to another timestamp.
@ijuma For me it seems `isJava8Compatible()` could also be removed (with all its test usages)
Would it be more intuitive to put this logic in AdminClientRunnable.makeMetadataCall.handleResponse()? This allows us to reorder calls in `callsToSend` only when the MetadataResponse has no error. And we don't have to check timestamp to know whether the metadata has been updated.
This is a good find. What happens if the metadata request itself is queued up to be sent to a node which is no longer online? Will it be stuck in `callsToSend` until it times out? I am wondering if we should check `NetworkClient.connectionFailed` after every poll() for all requests in `callsToSend` and reenqueue them as we are doing here. This is what we do in `ConsumerNetworkClient`.
Let's avoid TODOs in the code. We can point to the relevant part of the code in the JIRA.
This doc seems wrong for `retry.timeout`. Or is this really a retry limit? Either way, might want to mention the constraints (e.g., "Must be a positive number.") that are enforced by the config.
Doesn't follow AK setter convention. Any reason to maybe set this in an `initialize` method? Seems like it should be called up front, rather than repeatedly. Anything else that should be set only once? Removing unnecessary setters might help with the usability.
Remove the last sentence, since the `reset` method is private, and looks like it's handle automatically.
Is there any way to put this into methods on the `ProcessingContext`, like `toString(boolean includeRecord)`? That way, its `toString()` method could really just be `toString(false)`. Not only is it a bit more testable, the logic of this reporter is more readable and focused on writing to the log.
Do we need to prefix these with `msg.`? Also, it might be nice to control the message format when message keys and values are to be included. I know that's more code, but it could be made to be similar to the other option.
Maybe: Include in the log the Connect key, value, and other details of records that resulted in errors and failures.
Might be more useful if this explained what an "error context" is. Something like: Log to application logs the errors and the information describing where they occurred.
Nit: reword to avoid "log" being ambiguous as verb or noun: "Writes errors and their context to application logs."
Nit: will this ever be null? Would be nice to know in an the implementation if it can assume it's never null. Also, need a description of the method.
This style of setters is not AK convention.
This seems error prone to be checking for this, rather than using the ConsumerRecord's timestamp type.
AK convention is to not use `set` setters or `get` getters.
Setter methods again.
nit: in kafka we usually don't use `get`/`set` prefixes
nit: in kafka we normally don't use `get`/`set` prefixes
Yeah, that works, too, and is more align with the current code.
Right, so we should just reword Each connector gets their own dead letter queue topic to be more clear, so something like Each connectors dead letter queue is usually written to a different topic. Not super important, but Im just looking to eliminate potential usability issues.
we shouldn't enforce. un-schema'd formats like json might be fine sharing the topic. we should continue to discuss what additional metadata we add to the DLQ messages, but with the right metadata, there's really no problem sharing the same queue if someone wants that simplicity. even in decoding avro, they can still decode based on schema ID, they just need enough context to figure out what to do with it
In this case everything is quite readable since all the things we're delegating to are super short method calls, I found the code that invokes this quite readable (but of course that's subjective)
nit: not a big deal here, but for unit tests I think given the very low overhead it is better to separate out each of the cases into their own test as it can help make it more quickly obvious if issues are with a specific case or if it affects multiple cases.
Hmm, this doesn't need to block merging this, but we should think carefully about doing delay this way. The rest of Connect avoids trying to rely on Java's `interrupt` behavior because it's not really a reliable way to *actually* interrupt threads, and in a system where there are pluggable components that are allowed to block indefinitely, relying on functionality that most Java developers don't understand well probably isn't going to work all that well. It may not have actually gotten to a KIP, but there was at least some discussion on a JIRA somewhere about making connect perform interrupts in addition to the basic task `stop()` calls it already does, but it doesn't currently do this. For anything that can end up with pretty long sleep periods, we should try to make sure there's a good way of interrupting it and moving on (e.g. so rebalances wouldn't get delayed because there's a connector that's encountering errors). At a minimum, since we don't do interrupts currently, I think we wouldn't interrupt this code currently. The other approach we use elsewhere is to `wait` on a monitor so we can set a flag and interrupt with `notify` and have it bail out immediately.
Same question here about just using a static ConfigDef instead of a static method.
Pls use caps :) (see your `ErrorReporter` above).
For a nice example where caps make sense see right below, where two sentences are included.
Have you thought about rearranging the parameters so that the operation is last? It's the only parameter that might be a lambda, and having it last would be more readable.
nit: as in `position` above, `this` is not required
nit: as in `position` below, `this` is not required
nit: as in `position` below, `this` is not required
nit: as in `position` above, `this` is not required
Reading up to here, I'm convinced caps or no caps is random :)
Why "klass" in the resulting string? Just use "class".
doesn't look a great name for its behavior. perhaps something like currentContext
Look like a ProcessingContext builder method while it is not. Wouldn't it be better to keep this void
nit : missing new line
I know we've been around and around about this, and at this point, I can't remember if this was already discussed and discarded, but... I think it may make sense to just add "none" or" 2.0" as the possible values. We still aren't sure if upgrading optimizations will always be safe, so explicitly stating the optimization value you want will protect you against accidental upgrades when 2.1+ come out. Also, the explicit optimization level will protect against accidental *downgrades*, which could also be unsafe. For example, if I deploy v2.1 with "all", it'll set me up with 2.1-level optimizations. If I decide I don't like it and go down to 2.0, still with "all", Streams won't know any different, so it'll construct my topology according to 2.0 rules, potentially leading to data loss. On the other hand, if I had deployed it with "2.1" and then downgraded to 2.0, Streams could refuse to start, since it doesn't understand that config value. I think it would be really nice, in the future, to investigate ways to restructure the topology dynamically without losing data, but until we get there, it's better to err on the side of explicitness and safety.
I think "all" is fine, as "all" always means "latest version" -- if we release 2.1, we would introduce "2.0" and "all" as become a surrogate for "2.1" -- I think it's clean this way, ie, we only have a version string for older releases.
This logic is repeated in a couple of places. I'm wondering if we could change `MaterializedPeek` to take the `InternalSteamsBuilder` as an additional constructor param and have the logic inside the class, and this block of code could be replaced with `new MaterializedPeek<>(materialized, builder).maybeIncrementTopologyCount()` or something like that.
Would it be slightly simpler to use `private long nextAllowedRetryMs = 0`? In general `long` seems simpler than `Long`
Hmm... in this case not sure whether we actually need a sentinel. What we want is the earliest time after which we can send the request. And by definition this value should always exist, i.e. it won't be undefined. By default it will be 0 which means we can send request anytime. Anyway, this is an internal variable and I don't have a strong opinion. I will let you decide this :)
Ok. Then the KIP wiki needs to be updated.
Hey @junrao, It was discussed on the dev channel that we shouldn't store or pass around the '*' the end as this then requires places to validate the resource name ends in a '*' on API calls and when loading from ZK. Also, with the rebrand of this from 'wildcard-suffixed' to simply 'prefixed' then I think we can drop the '*' completely. e.g. the user would add an ACL to any topic resource the has the prefix 'foo'. Look mum, no asterisks! This also helps separate this from the current 'wildcard' support i.e. '*'.
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
You can now use Java8 if you want! ``` static { CODE_TO_VALUE = Collections.unmodifiableMap(Arrays.stream(ResourceNameType.values()) .collect(Collectors.toMap(t -> t.code, Function.identity()))); } ```
use `return CODE_TO_VALUE.getOrDefault(code, UNKNOWN)`
Each unit test should test one thing, so break this into separate tests
Each unit test should test one thing, so break this into separate tests for the different test conditions.
anti-pattern: ``` if (...) { return x; } else if (...) { return y; } else { return z; } ``` Can be simplified to: ``` if (...) { return x; } if (...) { return y; } return z; ```
'valueInZK' is poor name here - it's the name of the wildcard-suffixed pattern. 'intput' is actually a concrete resource name. `valueInZK` won't have the '*' suffix, (as discussed on the thread). - update the java doc accordingly too. Personally, I see a lot of scope for this being called in code where it shouldn't, (as it currently is). A more defensive API might be for the signature to be: ``` public static boolean matchWildcardSuffixed(Resource wildcard, Resource concrete) public static boolean matchWildcardSuffixed(ResourceFilter wildcard, Resource concrete) ``` And then to have the method throw if the wrong types of `wildcard` and `concrete` are passed in, i.e. `wildcard` should always have type `WILDCARD_SUFFIXED` and `concrete` should always to `LITERAL`
I think this function is now two dimensional, i.e. the response might need to be something like: ``` Resource type is ANY and resource name is ANY ```
nit: rename `byt` to `defaultValue`
include in equals and hashcode
nit: blank line.
This is part of the public API, so we don't know all the ways its being used. I still think we're better off output a more complete message.
It seems that this is an internal request and is never throttled.
Ah, well, then thanks for making it both legible and correct.
Thanks for cleaning this up. I still can't read the old code without taking notes...
nit: `is add` -> `is added`
Since all stores will be at least wrapped with `MeteredXXStore` we can add the error handling only at the metered store layer to be consistent. Besides, the inner `RocksDBSessionStore` would actually be `<Bytes, byte[]>` typed always, so this would not help. Same for the RocksDBWindowStorebelow.
nit: move to line above.
As above. Not sure if we need this, as the store should be wrapped with `MeteredWindowStore`.
Even without EOS and assuming KIP-211 is in place, we'd probably still to commit regularly as a pre-requisite to fix https://issues.apache.org/jira/browse/KAFKA-6502.
This is not a feedback: as we are changing the main loop of StreamThread, we may need to carefully benchmark if this change along with the main loop changes will have unexpected performance penalty: with low traffic input stream, we are effectively sending sync commit requests more frequently. cc @mjsax If it does become a problem for performance, we could consider making the commit request async, and consider a commit only completed after the commit response is returned. Of course it means more complicated logic.
Do we need `transactionInFlight` here? Previously we just considered if `eosEnabled` for committing a transaction.
For `shouldCloseProducerOnCloseWhenEosEnabled`, we could update it by adding `task.initializeTopology()` before `task.close()`. The former would call `producer.beginTransaction` which will then fix this issue.
Hmm.. `poll.ms` is not a ConsumerConfig, but a StreamsConfig, so users are not expected to prefix it with the consumer.
nit: update parameter description; still says "millis"
nit: use class `import` to avoid long name here
nit: use `ConsumerConfig .SESSION_TIMEOUT_MS_CONFIG`
Do we want to consider using a configuration for this and other spots where we have hard-coded values? With KIP-276 merged users can specify different timeouts per consumer type.
Same as above and others as well.
I'd suggest using 100ms not 10ms, since this call is in a larger while loop in `StreamThread.runOnce`, and hence I concern doing `poll(0)` would unnecessarily increase the restoration latency.
Here I'd suggest doing the opposite: `poll(0)` since it is during the normal processing, not during restoration; so we can afford to not having some time in a few iterations. Instead, we want to proceed to the next iteration to call the normal-consumer.poll sooner to not be kicked out of the group.
I think we should test for the exception -- if we change the behavior intentionally, we should remove the `fail` as well as the `try-catch` instead of allowing both behavior to pass (it's an either or form my point of view). IMHO, tests should be on an as narrow code path as possible: if we change behavior and a test fails, we are forces to reflect on the change, what is good as it guards against undesired behavior changes...
Thank you, Java8! :)
nit: `the normal-consumer.poll` -> `main consumer#poll()`
nit: `sooner` -> `as soon as possible`
Do we need to wrap with the LinkedHashMap? Could we just do `Collections.unmodifiableMap(metrics.metrics());`
I think you meant `addLatencyAndThroughputSensor`
Nit: we typically just say `partition` in these cases. Same for the other `log.debug`.
I think what you want here is `Records.LOG_OVERHEAD`.
The config name was chosen for consistency with the producer, but it is not an ideal name when considering the consumer in isolation. The consumer can block longer than this if a longer timeout is passed to any blocking API that accepts a timeout. Can we explain that this config is only used as the default timeout for operations which do not have an explicit timeout option? Similarly for the changes in the upgrade docs.
I think this config property key seems a misfit, and probably reflects an earlier incantation of the design before KIP acceptance. It might be worth - in a separate PR - renaming this to something like `errors.tolerance` to better align with its purpose.
No, we have to fix it before AK 2.0. Once it is in a released API, we cant change it.
We'll need a separate AK issue, then.
Maybe use a semicolon instead: "task failure; 'all' changes..."
nit: matches the old behavior is very relative
The first part of the doc sounds incomplete.
Aren't the actual values that are used in the recommender are lowercase? If so, we should be consistent here.
Extra "for" in "milliseconds for that"
This TODO should be removed
super nit: `with possibly new type` -> `with possibly a new type` Here and a few other places.
Nit: add Oxford comma: `deserializers, and` Nit: `internal state storage` -> `state store configuration`
nit: `internal state storage` -> `state store`
Might want to add that this is not used if the topic already exists.
Seems like this might occur only in a programming error, which we could test for in a unit test.
This is a bit weird notation. To add the dot separately for all the configs but the first.
Would `Integer.toHexString(value)` work here? Not sure if it's exactly the same as the current code.
I'd move the `ERROR_HEADER_PREFIX` here since it really is more of an implementation detail. And as @kkonstantine suggested, I'd change the prefix to be `__connect.errors.` (with the trailing period) so that the above lines can be like: public static final String ERROR_HEADER_PREFIX = HEADER_PREFIX + "errors";
Nit: use `java.nio.charset.StandardCharsets.UTF_8` here rather than `Charset.forName(...)`. The latter has to do a lookup every time.
Would `Long.toHexString(value)` work here? Not sure if it's exactly the same as the current code.
I really like the fact that we are separating Resources from ResourcePatterns! Great job.
Perhaps something like "Represents a pattern that is used by ACLs to match zero or more Resources"
nit: simplify `InterruptedException, IOException` to `Exception`
For regular API calls (ie, "main code") we should list them. For a test, no exception should ever be thrown, and if, the test fails. Which exception is irrelevant IMHO.
If we rewrite to use "absolute position" instead of delta, we can remove this.
This nested invocation is much harder to read than the original version. I believe storing the result of the first replace in a local variable would make it clearer.
This one matches a single backslash, not a double backslash. Also, I think you don't need to abbreviate 'backslash'.
Is it necessary to add SPLIT_ and REPLACE_ to the names of patterns? The operation performed with the pattern is not part of it. In theory one pattern could be used to both split and replace.
This one matches a double quote, not a backslash and a double quote.
nit: could be `private`. For what it's worth, I tend to prefer having these helper methods at the bottom of the file so that you don't have to hunt for the test cases, but it's a bit subjective.
I think it would be more straightforward to have two separate test cases which call `doTestConversion` with a boolean parameter to indicate whether overflow should be tested.
This method no longer seems worthwhile. Same in `MemoryRecordsBuilderTest`.
This only works if the parent dir of generationDir exists. Try using generationDir.mkdirs().
This logic is not exactly the most straightforward. What about something like this? ``` if (pluginKlass.isAssignableFrom(Versioned.class)) { Versioned versioned; if (pluginImpl != null) { versioned = (Versioned) pluginImpl; } else { versioned = (Versioned) pluginKlass.newInstance(); } return versioned.version(); } return "undefined"; ``` or ``` if (pluginKlass.isAssignableFrom(Versioned.class)) { if (pluginImpl == null) { pluginImpl = pluginKlass.newInstance(); } return ((Versioned) pluginImpl).version(); } return "undefined"; ```
Why do we need this check? It seem that `parentNode` cannot be `null`, and that it would be `this` always, too? (assuming one parent node, what is not generic)
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
I wonder if a better location for `null` checking configs is inside `transform`. To me, it seems it is, because this way the check is in one place and we are protected against future uses of `tranform` with a `null` argument. Also, in contrast, seems that `configTransformer` can not be `null`.
Perhaps replace this NOTE with `If the configuration is not null, it will have been transformed...`
This seems outdated.
Use one line per parameter? Ditto below.
I'm wondering if it still makes sense to consider metadata timeout and request timeout to determine the timeout for `selector.poll`, but maybe it can be discussed in another PR..
as above -- guess some more below
Do we need to call remove ever? Since `filteredOffsets` is constructed empty can we just do the following: ``` if (!globalNonPersistentStoresTopics.contains(topic)) { filteredOffsets.put(topicPartitionOffset.getKey(), topicPartitionOffset.getValue()); } ```
It might be a personal preference, but I'd rather just suffer the duplication of having separate catch blocks for separate kinds of exceptions.
nit - "once once"
we need to import LATEST_1_0, LATEST_1_1 version fields
This could be: ``` .reduce( (value1, value2) -> Math.max(Integer.parseInt(value1), Integer.parseInt(value2)) ) ```
`60_1000L` -> `60_000L` ? (originally it was `MIN_SEGMENT_INTERVAL = 60 * 1000L`)
Maybe we should have this as package private and expose it via a test class in the same package. We don't want users to depend on it and this class is public API.
nit: remove empty line
nit: "in `transform()` and `punctuate()`"
nit: remove empty line
nit: `punctuate each second` (might be c&p error and affect other places, too) -- maybe better to address in separate PR (the `void punctuate()` issue and this can be one PR IMHO).
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
nit: remove this line
I'd be in favor of either just adding the new method to the list (without removing another one) or deleting this whole list. Personally, I feel the list is a little redundant with this interface itself.
nit: needs a comma after the `{@link ...}`
Unless @mjsax objects, I vote to just delete these lists. At this point, it almost looks like it's directing you to all the other methods in this interface, which seems redundant. I'm not sure I follow your last question. The list exists to direct readers to other relevant methods. I'm not sure why adding `flatTransform` renders `transform` irrelevant...
Yeah, I'm fine with defining and applying a coherent strategy. In lieu of that, I guess the default thing to do here would be to just add the new method to the list without removing any other items.
This is still true...
nit: missing `<p>` for new paragraph
Why do you remove `transform` ? We only add a new `flatTransform` but `transform` is not removed.
I don't think we can do this. Also, I would only mention it, when we start to deprecate an API.
Why `? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>` and not just `Iterable<KeyValue<K1, V1>>` ? `transform` also has return type `KeyValue<K1, V1>` but not `? extends KeyValue<? extends K1, ? extends V1>`
Generics confuse me regularly, too... I was just comparing to `transform` -- seems our API is not consistent. I guess your argument makes sense.
I hope you don't mind if I jump in... It's probably worth doing some experiments to get the bounds just right. Here are the basic properties we need: Let's say we have a simple class hierarchy: ``` interface Animal interface Cat extends Animal interface Dog extends Animal ``` And let's say we subclass `KeyValue`: `public class KV extends KeyValue`. Given a `KStream<String, Cat>`, we want to be able to call `flatTransform` with a `TransformerSupplier<String, Animal, List<KeyValue<Animal>>>` like this one: `(String s, Animal a) -> asList(new KV(s, new Cat()), new KeyValue(s, new Dog()))`. The `? super K` and `? super V` ensure that any transformer that _can handle_ a `K` and `V` is permitted. You _can handle_ an instance if you can assign it to you parameter types (so your parameter type is a supertype of the instance type). You need to specify this as a bound because the compiler doesn't generally let me assign a supertype to a subtype (I can't write `Cat c = (Animal) a;`). I don't think you actually need the `? extends Whatever` parts to get the right outcome. If anything, you might need it on the stuff inside the iterable to make sure that you can just pass in heterogeneous members, and they'll get "rounded" to their lowest common superclass. I think this works without `extends` because you generally _can_ assign a subtype to a supertype. So maybe try an example like this without the `extends` stuff and see if the compiler chokes on it or not.
IIRC, Java doesn't fully implement variance, but this is the basic concept (in scala) of what's going on with those `extends` return types: https://docs.scala-lang.org/tour/variances.html
Ok, I stand corrected. I did the experiment I was suggesting, and I got the desired results only with the API you have submitted: ```java <K1, V1> KStreamImpl<K1, V1> flatTransform(final TransformerSupplier<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends K1, ? extends V1>>> transformerSupplier, String... storeNames) { return null; } public static class KV<K, V> extends KeyValue<K, V> { public KV(final K key, final V value) { super(key, value); } } public static void main(String[] args) { final KStreamImpl<Integer, Long> stream = new KStreamImpl<>(null, null, null, false, null); // exact transformer final KStreamImpl<Integer, Long> stream2 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) {} @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(key, value)); } @Override public void close() {} }; } } ); // transformer that takes superclass k/v and returns exact results final KStreamImpl<Integer, Long> stream3 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes exact parameters and returns subclass results final KStreamImpl<Number, Number> stream4 = stream.flatTransform( new TransformerSupplier<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Integer, Long, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Integer key, final Long value) { return Arrays.asList(new KV<>(key, value), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); // transformer that takes superclass parameters and returns subclass results final KStreamImpl<Number, Number> stream5 = stream.flatTransform( new TransformerSupplier<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>> get() { return new Transformer<Number, Number, Iterable<KeyValue<Integer, Long>>>() { @Override public void init(final ProcessorContext context) { } @Override public Iterable<KeyValue<Integer, Long>> transform(final Number key, final Number value) { return Arrays.asList(new KV<>(key.intValue(), value.longValue()), new KeyValue<>(1, 3L)); } @Override public void close() { } }; } } ); } ``` If you take out any of the `? extends`, it won't compile.
Thanks for verifying @vvcephei!
nit: add `final`
nit: add `final`
nit: add `final`
nit: add `final` (also within the loop)
nit: add `final`
Checked on the source code history, I think it is piggy backed from the old manner, in which we do restoration along with the registration of the store (before 0.11.0) whereas now we only register the store but delay restoration in main loop. So I think we can safely remove this.
One question: it seems the `bulkLoadSegments` set will only keep increasing in size since we only initialize it once in `bulkLoadSegments = new HashSet<>(segments.allSegments());` and then do this ad-hoc addition here, when segments are dropped they will not be removed from `bulkLoadSegments` as well, right? I'm thinking, if we could just call `final Set<Segment> bulkLoadSegments = new HashSet<>(segments.allSegments());` here every time, since 1) if the store does not exist yet, the `getOrCreateSegmentIfLive` will call openDB that makes the open flag for the newly created store, and 2) if the store does exist already, then `toggleForBulkLoading -> segment.toggleDbForBulkLoading` will make sure the store is open here.
Thanks for confirming @guozhangwang !
nit: add `final` nit: add space `entry : writeBatchMap.entrySet()` (I thought this was a checkstyle rule? Wondering if my memory is wrong because build passed)
Thanks for clarification, @guozhangwang !
Note this is indeed fixed in trunk but not in older versions.
We've observed from the actual logs that it's not actually.. and the reason is this: https://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception
From my understanding, this must either be without the last `{}` or using `e.toString()`? (Cf. https://stackoverflow.com/questions/6371638/slf4j-how-to-log-formatted-message-object-array-exception)
It's worth including the broker here as it was originally intended.
`Node node = awaitLeastLoadedNodeReady(requestTimeoutMs);` should give you the `node`.
You don't need the `else` here, just `return null` if the `if` evaluates to false.
No need to mention the version -- it's also not clear how long we keep it -- it's at least up to the next major release -- but might be longer. I would rather point to the new method to be use: `@deprecated use {@link #topicSet()} or {@link #topicPattern()} instead` This explains users how to rewrite their code and is more helpful.
nit: add `final`
nit: you could just specify one `String` variable at the top of the method and set it accordingly if `topics` is `null` or not then have a single `return` statement. Just a personal preference though.
I'm ready to admit this might be a personal preference, but I think these are cleaner with both `if` and `else` instead of asymmetric `if` followed by a `return`.
We also need to update `equals()` to include `topicPattern`
nit: add `final` to parameters
Personally I think the default form of `someObjectClassname@hashcodenumber` is fine as to identify the extractor, other than a single topic name, is used, which is probably the most important illustration from `SinkNode` anyways.
This is my first time looking at this class, but I noticed that pretty much all of the "payload" of these description nodes are strings. Should we consider returning a string here instead? In fact, if we did that, we could consider calling `toString` on the extractor instead of returning the class name. This would allow authors of the extractors to provide more information about the extractor than just its name. This might be especially useful in the case of anonymous implementations.
Can you please add `{ }` to both blocks? (Please also add them in `topic()` method above).
as above, your "link" markup
NPE: ![image](https://user-images.githubusercontent.com/925755/55269396-d55f3480-5292-11e9-9c29-78c524d63c65.png) I'm not using a topic pattern, equality should still work.
nit: insert empty line
nit: `topoloy` -> `topology`
nit: `final` (also next two lines)
Do we need to test this -- seems to be not part of this test method and should be it's own test, if we don't have one yet.
Might be worth avoiding `null` -- no `null`, no NPE :)
nit: `final` (also next line)
Maybe we can still just do one pass over the records in this function, while passing in stream time wrapped in a `Long` object in parameter, and update it inside this function. Then we do not need to do this refactoring any more.
Could we initialize streamTime as `((StandbyContextImpl) processorContext).streamTime()` instead? Otherwise in line 188 below we should only setStreamTime if the calculated `streamTime` is indeed larger, because if this fetched batch of records happen to have all timestamps smaller than the current stream time, then stream time will be set backwards.
why void now instead of returning `List<ConsumerRecord<byte[], byte[]>>`
Why not init with `new ArrayList<>(records.size())` and avoid the check in the `for` loop? Could be `final` than, too. If required, we can also `return remainingRecords.isEmpty() ? null : remainingRecords;` -- not sure atm who calls the method and what the impact of returning and empty list vs `null` is.
nit: move to next line or indent lines below
Could you just pass the new subscription to `clearBufferedDataForTopicPartitions` and clear anything not in the set? That would also handle the other case since we could pass an empty set.
It is public API for a class that we are using internally. Thus we are able to enforce rule via code review that `assignedTopics` is never null. And in general it is preferred not to call a function with null parameter, since otherwise we will have a lot of null check in the code which will make the code unnecessarily verbose. So it seems simpler not to check whether it is null. Note that a few other public methods do not check whether the input parameter is null. For example `Fetcher.getTopicMetadata` does not check whether request is empty. So it is probably consistent not having to check it. And for public APIs that are exposed to user, e.g. `Consumer.assign(partitions)`, because we can not ensure via code review that `partitions` is not null, we currently throw `IllegalArgumentException` if the value is null, instead of allowing user to call the method with null value.
Do we need to also clear data from `nextInLineRecords`? Also, it seems that `assignedTopicPartitions` is never null, right? Maybe we can skip this check.
The current implementation is problematic in the sense that all cached data will be removed if users subscribe to the existing set of topics and `topicPartitionsToClear` is empty. Can we replace `clearBufferedDataForTopicPartitions(...)` with `updateCompletedFetches(Set<TopicPartition> assignedPartitions)`? updateCompletedFetches will keep only those data whose partition is still assigned. This will fix the problem above and also makes the logic in the caller simpler. Instead of having to determine the partitions that have been removed, the caller only needs to provide the latest assigned partitions as `subscriptions.assignedPartitions()`.
I'm confused about the problem you are trying to solve. The consumer doesn't use this `assign` API itself after completing a rebalance. This API is only used by applications which are using manual partition assignment.
nit: naming topicPartitions as topicPartitionsToClear would make the code more readable
Also, do we want similar logic for `subscribe`? We can drop completed fetches if they are for partitions from topics no longer subscribed.
nit: Use `{}` substitutions. Maybe this could be `warn` since there's nothing the user can actually do about it shouldn't actually cause any problems for the current assignment.
This seems unnecessary since we're throwing away the collection anyway.
You can probably simplify this using `computeIfAbsent`.
One scenario where we may hit this is during an upgrade when we have some consumers on older versions. Might not be worth checking that though.
We could use `SortedMap` (or even `TreeMap`) here instead of the generic `Map`. Then we wouldn't need the ugly cast below.
Effectively what we are doing is sorting the assignments for each partition using the generation and picking the latest two for the current and previous assignments. Is that right? Could we simplify the logic by building a `SortedMap<Integer, String>` for each partition where the key is the generation and the value is the memberID? Then the current assignment would be the last entry and the previous assignment would be the one prior.
This caching logic might work if the leader does not change. But I have the impression that it is possible to end up in the same problem when the leader changes. Because as far as I see, the new leader will not have build up its cache. (or even worse has an outdated cache from a previous time it was leader) Which I think can still cause troubles. I see two other possible solutions: - The easiest one to implement I think is keep the previous code, but change the type of the `currentAssignment` parameter from a `Map<String, List<TopicPartition>>` to a `Map<TopicPartition, String>`, or keep the type but do a post-processing step on this list where double TopicPartitions are removed before doing the `sortPartitions`. - Another possibility is putting a generation counter in the schema, and only keep assignments with the maximum generation counter. (or those where it is still missing for backwards compatibility). The first possible solutions is easy to implement but makes it a little less sticky. (which I do not mind) The second solution is more sound, but implementing it in a backwards compatible way is a challenge.
Detail: just to be sure, I would initialize it to `this.generation`, to make sure the generation always increments.
Small remark: By throwing here an exception you are giving up. But I am wondering if you should not try to fix it. I do not know which consistency guarantees a kafka consumer group exactly gives. But if it is possible to get into a split brain situation (network split), where temporarily a consumer group is split in two and gets two leaders. You get in a situation where the `SitckyAssignor` will never recover. If it was me, I would write a big fat error, and drop one of the two assignments. The same for line#322.
Maybe I am wrong, but shouldn't this be moved inside the `while` loop, so it is initialized every time a new assignment is selected? Because, now I think only the first consumer will be challenged against this previous partitions.
Have you thought about compatibility with the old schema? I am wondering if it would be better to somehow use the generation from the consumer group so that we don't have to change this schema.
Have you verified that this change is forward compatible as well? Older versions of the sticky assignor need to be able to work with the updated version. The client definitely has access to the group's generation. The question is whether and how to expose it to the assigner.
@vahidhashemian when I first proposed the generation counters, I looked at the coordinator code, and I had the impression that you have only access to the current / your own generation counter, but not the ones used/known by the other generation counters. So my interpretation is, that if you make this generation accessible to the `Assignor` you can only use it as an input source to for the generation counter we send in every ConsumerUserData. So I think its value is rather limited.
It certainly makes sense, however, I do not see much added benefit compared to the more or less arbitrary generation counter you implemented in this PR. If it was already exposed, you could use it, but I would not expose it just for this use case.
This `ArrayList` should be generic (that is, `new ArrayList<>(...)`).
nit: seems these could be final? Same in `ConsumerUserData`.
Can you explain the purpose of this? I would expect we'd want to only use the generation passed from the consumer.
We should be able to get rid of this since we have the default method.
nit: It might be a little clearer to use `Optional<Integer>` for the type here.
We still need to add a default implementation which calls `onAssignment` without the generation. Then we can drop the override in `AbstractPartitionAssignor`.
Mainly I was just trying to ensure we do not abuse the sentinel, but it sounds like you have thought through its usage.
Should be `Optional.of(generation)`. `int` values are not nullable.
I was not sure. It seems like having a generation=-1 doesn't cause any spoilers here, but I'm always suspicious when sentinels get mixed into the code. One way to force ourselves to be honest would be to use `Optional<Integer>`. Maybe it's good enough if we just have a good test case.
nit: unintentional? one more in `subscription`
Yes, I'm suggesting that the current generation is not needed when performing the assignment. All we need to know is the generation of the previous assignment reported by each member, which is included in the subscription user data. Does that make sense? Then we get all the information we need for the next rebalance inside `onAssignment`.
Hmm.. Now that I'm thinking about it, this seems like the wrong place to pass through generation. We do not need to know the current generation in order to sort the current and previous assignments. We should only need to know the current generation when we receive the assignment after the rebalance completes. I think we should have the following method instead: ```java void onAssignment(Assignment assignment, Optional<Integer> generation); ``` Perhaps this is the cause of the confusion over propagation of the generation through the Assignment UserData.
we should also change the type to Long at Line no: 168
Can you replace `old_throttling_for_broker` with `old_broker_throttling_behavior`? Same for `old_throttling_for_client`.
Now that we can use Java 8 APIs, you can use `entrySet().stream().filter(...)` to avoid the for loop.
You could just do `selector.poll(100)` instead of `poll(0) + sleep(100)`. `poll()` returns when an operation is ready, so we are not waiting unnecessarily.
`selector.connected()` is cleared after each `poll()`. So we should keep track of the total number and compare the total against `conns`.
We cannot guarantee that this poll will see all completed connections, so it would be better to poll in a loop until the total connections returned from`selector.connected()` after the poll equals `conns`.
The parameters should be the other way round (expected is `conns` and actual is the metric value).
Same as before, parameters of `assertEquals` should be the other way round.
This might fix the issue, but don't you think it's a little weird that it's necessary? Wouldn't we have the same problem anywhere that we call `stop`? I'm wondering if we need to fix this in ducktape itself.
Note that is users wants to produce a time windowed KTable to a topic they would want to construct a serde with the windowSize, but the `isChangelog` flag would be set to false. So I'd suggest we have another constructor of `TimeWindowedSerde` which only takes `inner` and `windowSize` and default `isChangelogTopic` to false, and rename this function to `timeWindowedSerdeFrom` which will call that constructor. Then for changelog topics, since in practice it is not very common that users want to fetch from these topics directly we can add a non-constructor `TimeWindowedSerde forChangelog(boolean)` for `TimeWindowedSerde` which will set the boolean flag (again by default it should be false) and then return `this`, and then for those advanced users can then call ``` TimeWindowedSerde serde = timeWindowedSerdeFrom(...).forChangelog(true); ``` Personally I feel it is the leak penetrating API for normal users.
nit: I'd suggest removing the overloaded function at line 213 below and make all its callers call with `serdes..keyDeserializer()` parameter instead.
Adding a parameter `version` for `encodeVersionThree` is very confusing to other readers. I'd suggest completely duplicate the code in `encodeVersionFour` and remove this parameter in `encodeVersionThree`.
nit: add empty line between functions, ditto below.
nit: add empty line.
+1, we can add an enum inside StreamsPartitionAssignor which can be extended in the future. Also for this error case the name `UNKNOWN_PARTITION` is a bit confusing, I'd suggest we name it `INCOMPLETE_SOURCE_TOPIC_METADATA`. And upon receiving this error code we should log an error that `some of the source topics ( + source topic lists) are not known yet during rebalance, please make sure they have been pre-created before starting the Streams application.`
It seems like this new constructor only supports the "error assignment" code path. Can we just inline it? I admittedly didn't quite follow why we need this version check now.
nit: align parameters.
Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways.
We can remove the code block line 82-85 above since it will be called here.
nit: align parameters.
nit: empty line.
This seems not used.
nit: add empty line
nit: rename `errorCode()`
Nit: rename to `errorCode` ? We try to avoid abbriviations
nit: add `{ }` -- we use curly braces for all blocks nit: remove double space after `==`
nit: remove double space after `=`
nit: add `final`
nit: double space after `=`
nit: add space `configEntry :` -- wondering why this is not detected by checkstyle...
Thinking about this a bit more, using `adminClient.describeConfigs` here would be okay since we never dynamically change the config, we just want to test that the configs specified when the topic was created is expected. So if the topic creation is not yet propagated, it will fail and re-try on admin client anyways. So nvm.
nit: add `final`
If there are race conditions, we need to fix them. Remember that Streams itself has to use AdminClient.
This is a one-node cluster, though, right? > /** > * Runs an in-memory, "embedded" Kafka cluster with 1 ZooKeeper instance and 1 Kafka broker. > */ > public class EmbeddedKafkaCluster extends ExternalResource { >
I'm a bit concerned using `listTopics` than using `JavaConverters.seqAsJavaListConverter(brokers[0].kafkaServer().zkClient().getAllTopicsInCluster()).asJava())` as only the latter can get the source-of-truth on ZK while the former may be subject to race conditions (e.g. if you create the topic and then call listTopics, it may not be included if the metadata was not propagated yet).
`UnknownTopicOrPartitionException` is the cause of the actual exception `e`, so we cannot just catch it here.
nit: add `final`
Might be better to add a catch-clause for this case instead? ``` try { ... } catch (final UnknownTopicOrPartitionException ignoreAndSwallow) { } catch (final InterruptedException | ExecutionException e) { throw new RuntimeException(e); } ```
nit: add `final`
nit: add `final`
Similarly, `adminClient.describeConfigs` does not read from ZK but from broker cache, and hence maybe subject to race conditions.
Let's use try with resources here and the other test so that the file is closed after it's used.
Consider using java.util.Collections.addAll()
See if you can refactor this code which is similar to what beginningOffsets has (apart from the condition between pos1 and pos2)
This flag is better set in LeaveGroupResponseHandler where we know whether the LeaveGroup request succeeded or not
It's better to keep the parameters aligned (having same indentation)
Use log object if this is to be kept
RebalanceConsumerCoordinator -> ConsumerRebalancingCoordinator
consumerThread -> rebalancingConsumerThread
nit: extract the call to this.interceptors.onConsume(new ConsumerRecords<>(records)) above line 1258 - its result would always be used
nit: when records2 is empty, you can return immediately.
This can be moved to ConsumerRecords class
Restore interrupt status by calling Thread.currentThread().interrupt();
Since condition is just a comparison, you can put the comparison here directly
nit: do we still need this function? Could we just reference the `metricLock` object directly? Since it is private my understanding is that it was not intended to be used outside this class.
Ah I see. MVN then :)
Nit: keep first two parameters in their own lines (we either put all parameters in one line, or use one parameter per line -- hybrid formatting makes it harder to read the code).
Why do we need this? Does the build report a WARNING and fails without it? (same below)
If you want a new paragraph, you need to insert a `<p>` instead of an empty line.
This method sends multiple requests now. It would be useful to rename it to `sendGroupRequests()`
As I understand it, handleResponse will always be called by AdminClientRunnable from the single 'network thread' (KafkaAdminClient.thread).
nit: unnecessary whitespaces
I suggest to use HashMap and LinkedList here since only one thread will process all responses.
I think it might be ok for `windowSize` to be greater than `segmentInterval` as the window will comprise multiple segments, but I could be wrong about this. \cc @guozhangwang @mjsax
I created https://issues.apache.org/jira/browse/KAFKA-7245
We usually don't indent but align -- at least, all other code is formatter like this IIRC. But it's a nit anyway...
`WindowStore` is public API -- we need a KIP if we want to deprecate something. Thus, this is not a `MINOR` PR.
I am wondering, if we might want to remove this method? Do use it only at one place and can easily pass the record timestamp there instead? Or is it useful for some special cases? Of course, this would require a KIP and separate PR -- it's might also not be high priority and just creating a JIRA for it might be fine. WDYT? \cc @guozhangwang @bbejeck
Yeah I was not sure if we should do KAFKA-7245 to remove the API as it would be useful for PAPI users.
Is our coding style suggesting to always make multi-line function call with more than one parameters? My preference is only use multi-line if there are 3+ parameters, AND if we put them into a single line it would be too long.
nit: is it necessary for triggering a warning? My personal preference is that if a single line is not too long, then it is okay to do so.
This is not introduced by this PR but: `processorSupplier` can be reused for `addProcessor` and `ProcessorParameters` constructor below for both the physical and logical plan generation. Similarly the storeNames can be reused for both as well.
Should be fine.
nit: Is topology changing the only possible root cause of this issue? I felt `Has the topology changed?` may be a bit too specific for guiding users to trouble shoot.
Would it be easier to understand if this handled all of the unwrap exceptions after the IOException? And then we could call this method `processUnwrapExceptionAfterIOException`.
It would be good to elaborate on why we need to do this as it's not obvious by just reading the code.
I'm wondering if we should log this exception in case `thread_dump` raises an unexpected error. We don't want to lose the original error.
nit: empty line.
Just a question: Why is this not `6L` ? (it should be `5L` after you applied the fix you want to do in a follow up PR).
generally in assertEquals, the thing being tested comes second. If there is an error message, the first thing appears as the "expected value"
Sorry, you're right. The thing being tested should be second.
Good question. I can't think of any right now (maybe someone else can?) so I would lean towards keeping it package-private for now.
Just to clarify, you do not need to create 12 tasks to reproduce the issue, just task10 and task00..05 should be sufficient..
Why recreating these objects? They are existed above: https://github.com/apache/kafka/pull/5390/files#diff-3cf77a4a8be4dc65221a87377c76ad33R52
Could we try to reuse https://github.com/apache/kafka/blob/03e788b29be6da0ef729f38448dc4345e68e398f/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClient.java#L52 in this part of the code? I'm not sure how simple it is since I don't have extensive experience in Java, but reusing code and having explicit separators is always a good thing. If we could do it in the server as well, that'd be great
nit: add `final
maybe use `TestUtils.waitForCondition` here while not probable this could hang.
nit: add `final
Don't we have to do something with the futures returned by `send`? How do we know if the requests completed? Also, cc @hachikuji.
More specifically, there's no point in having this if we close the network client before we even get to the point where the request has reached the network layer.
Or have one that takes a lambda so that the caller can do the `close`. Similar to what we have for Scala.
It probably makes sense to wait a short time, but not very long, to send the request to the broker. I'm not sure how that interacts with the rest of the client close logic
We could update the timer so that the min was the `requestTimeoutMs` for the second `close` too.
We should consider using the Timer abstraction in the Fetcher methods too.
The consumer close code needs to block until the fetch sessions are closed. `KafkaConsumer` has `close()` and `close(long timeout, TimeUnit timeUnit)`. In the case of the latter one, we don't want to wait longer than the specified time.
We should also clear the sessionHandlers map here, just for tidiness.
How about something like this: For consumer group that uses pattern-based subscription, after a topic is created, any consumer that discovers the topic after metadata refresh can trigger rebalance across the entire consumer group. Multiple rebalance can be triggered after one topic creation if consumers refresh metadata at vastly different times. We can significantly reduce the number of rebalance caused by single topic creation by asking consumer to refresh metadata before re-joining the group as long as the refresh backoff time has passed.
\cc @lindong28 -- seem's you forgot to update this when dumping the version in `1.1` branch.
`true` as initial value of `ignoreExceptons` changes default behavior. Users might be relying on exceptions being propagated to callers, thus we should not change this.
I think this way of triggering the exception is not only complicated but it even might be a source of flakiness. Could we have some more straightforward? I think the original solution (overriding getResponse) was better than this.
If logger.error call does not result in an exception, the test will fail anyway because of expected RuntimeException. If an exception is thrown, this line is not executed
I'm not 100 percent sure what's the race condition here, and why it fixes the test.
It seems we are using the same application id twice in `StreamStreamJoinIntegartionTest` ``` STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer"); ``` This might be the root case -- deleting all topics would solve the issue, too, as it prevent to start with a corrupted state.
I seems that `TableTableJoinIntegrationTest` has a similar issue...
One thing I can think of is that the changelog topic of the joining streams's materialized stores were not deleted, and hence upon starting on the next test case, the store would be pre-populated and hence cause join result to be wrong (note that the state dir is a `@Rule` which means it will get cleaned up on each test case, and in the next run it will not be the same directory path). And the reason that it does not always fail is that we have to hit it that the two tests (with and without caching) consecutively. So think with your fix in `2.0` and `trunk`, it is Okay to leave the appID as is for now.
Just ran some more tests locally to confirm my suspicion.
This may be a bit misleading: how about `Heartbeat poll timeout has expired; it means the caller thread has been stalled too long, will explicitly leave the group to trigger a rebalance`.
Shall we consider a detailed log message like we do for commit failed exception? Also, is this just an `info` or should it be a `warn`? ```scala "Commit cannot be completed since the group has already " + "rebalanced and assigned the partitions to another member. This means that the time " + "between subsequent calls to poll() was longer than the configured max.poll.interval.ms, " + "which typically implies that the poll loop is spending too much time message processing. " + "You can address this either by increasing the session timeout or by reducing the maximum " + "size of batches returned in poll() with max.poll.records." ```
Maybe "consumer poll timeout" would be clearer than "heartbeat poll timeout"? The problem is the delay between calls to `Consumer.poll`.
He means that you don't need an `else` in this case.
nit `stays at 2` seems to be correct -- it's `equalTo(2)` below.
Could we actually remove this guard? We don't call `time. milliseconds()` as below.
Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`
I would personally prefer, to keep the condition in the `while` conditions instead of using `if() break` construct.
I see. Fine with my both ways -- as long as it's intentional and we know about it, it's ok.
Fine with me to keep the guard. Was just double checking.
Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery.
I just talked to @hachikuji about this. Not committing is actually fine. Note, that beginTx() is a client local state transition -- nothing is written to the log (there are no "begin tx markers") and the TC state is also not modified. This implies, that the transaction timeout is not started on beginTx() -- the timeout only starts after the first record was written to the log. Thus, we don't need "keep alive heartbeats" and don't need to tell users to increase the tx timeout for low traffic topics that might have longer periods with no data.
Why did this go from 2 to 1? other than not passing an arg to `runOnce` the test logic to this point hasn't changed
I believe that Java (or the ALU) will do exactly the same thing whether you say `maxPollTimeMs >> 1` or `maxPollTimeMs / 2`, but your human colleagues might appreciate the latter ;)
Ah! I misread this as turning `logAll` *on* instead of *off*. Now I get it :)
nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs` IMHO, easier to read this way.
nit: remove `this`
> in case now is reduced How could this happen? Seems to be impossible to me.
I see. I did not make the connection to the other discussion. I think we can leave as-is.
as above `final` and one parameter per line
nit: add `final` to parameters / reformat one parameter per line
It's not about being trivial or not, but about consistency. Code duplication has the danger to only update one copy of the code...
nit: line too long
this is the same as above: we should extract to a method to avoid inconsistencies if one might get updated but the other one slips...
nit: break line (too long)
Note that for now these customized metrics will only be add on the "thread-level" though the tag map contains two: the scopeName = entityName, and threadName = [thread-id].
`addLatencyAndThroughputSensor` and others in `StreamsMetrics` are for users to register their customized metrics, so we cannot remove it..
We should, and I think it was actually a bug that we did not do that before.
I think this says, "we are looking for a parent node that is among my parent nodes". It seems like we can generalize this logic by not assuming there's a single parent node at all.
This loop only needs to happen once, so it seems like you can move it outsize the enclosing loop.
Can we make this method return `OptimizableRepartitionNode` just for symmetry? I.e., you can see that we're replacing a bunch of `OptimizableRepartitionNode`s with a new `OptimizableRepartitionNode` instead of replacing them with a generic `StreamsGraphNode`.
nit: This could be simplified: ``` java final PriorityQueue<StreamsGraphNode> graphNodePriorityQueue = new PriorityQueue<>(5, Comparator.comparing(StreamsGraphNode::buildPriority)); ```
What about `merge()` operation? I think we could have a case for which there are multiple parent nodes.
Since the criterion is StreamsGraphNode::isKeyChangingOperation, I don't see why the call on line 407 is needed: if there is no key changing operation, null would be returned anyway.
oh, actually I think it can. I think this will produce two paths back: ``` changedKeyStream = stream.map(...) left = changedKeyStream.filter(fnA) right = changedKeyStream.filter(fnB) merged = left.merge(right) merged.join(otherStream) ``` And I *think* this code would only re-root either `left` or `right`, but they should both be re-rooted, right? If so, it might make a good test case.
Ok, I think that's what I was talking about. Not multiple child nodes, but multiple parents somewhere on the chain back from `repartitionNodeToBeReplaced` to `keyChangingNode`. This line of code seems to assume that there is always one path back to `keyChangingNode`, but in general, you could have a diamond (since this is a DAG and not a tree). like : ``` keyChangingNode | | c1 c2 \ / (something) | repartitionNodeToBeReplaced ``` If this can occur, then there would need to be multiple of `keyChangingNodeChild` which all need to be re-rooted. But I'm not sure it can occur.
Can we call this property "priority" instead of "id"? I found this statement setting one node's "id" to another node's "id" worrisome, and I had to do some archaeology to determine that the "id" is actually just used as the priority in the topology building.
This doesn't need to be declared outside the loop (it can be final at the assignment).
`downstream` is a bit confusing: Found the child node of the key changer {} from the repartition {}.
Just curious, could we possibly call this function for the same node more than once? It seems yes as you are checking `!keyChangingOperationsToOptimizableRepartitionNodes.containsKey(node)` here, but I cannot tell from the code...
Generally speaking we should not rely on the caller to pass in parameters that are guaranteed to no pass the check. What I suggested (below) is to have a slightly modified recursion pattern which do not rely that the first caller would never satisfy the predicate.
The recursion here seems a bit wonky: if this function is called directly from the caller (i.e. not from a recursive call), then we should not return the `startSeekingNode` even it if satisfies the condition. I think we should refactor it a bit to loop over the parents and validate on the condition on each parent, if not call recursively on the parent node, and then loop to the next parent.
I see what you intended. Thanks for the response.
I think we cannot just pass in `null` and rely on default serde types from configs, since users are not expecting the repartition to happen here from the DSL, and they thought they have provided enough serde informations on places they "think" a repartition will happen. So observing a `ClassCastException` at a place they are not expected would be a bad experience. As of now, without serde inheritance we may have to "restrict" the optimization, to only apply to cases where two or more repartition nodes are direct children of the common key-changing parent node. Moving forward we can consider the options @bbejeck provided, and personally I think serde inheritance would be a good place to start.
Now seeing the example (super helpful) I am wondering if we can/should be more fancy. I think, it's actually three cases: ``` rekeyed = stream1.map(); merged = rekeyed.merged(stream2); merged.groupByKey()... ``` For this case, without optimization, we would insert repartition _after_ merge _before_ groupBy -- this mean, we repartition stream2 for no good reason. Could we repartition _before_ merge? Checking first parent might catch this case? ``` rekeyed = stream1.map(); merged = stream2.merged(rekeyed); // similar to above put change order of childen merged.groupByKey()... ``` This case is similar, but we might not detect if, if we don't check all parent nodes. Thus, we might miss to insert an repartition topic at all in current code? ``` rekeyed1 = stream1.map(); rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` For this case, we should do the repartition _after_ merge -- otherwise, we would create two repartition topics what is not desired (for the case, that `rekeyed1` and `rekeyed2` are not used elsewhere, too). If `rekeyed1` or `rekeyed2` are use like this: ``` rekeyed1 = stream1.map(); rekeyed1.groupByKey() rekeyed2 = stream2.map(); merged = rekeyed1.merged(rekeyed2); merged.groupByKey()... ``` we would still need two repartition topics, because we need content of `stream1` not be mixed with `stream2` in line 2. Also, as `rekeyed1` will be repartitions already before merge(), we don't need to repartition again but `stream2` should be repartition on it's own before merge(), too. Does this make sense? (note, that merge requires that key and value type of both input streams are the same, and thus, we can use same Serdes independent where we insert the repartition topic(s))
Just for reference: fixed via https://github.com/apache/kafka/pull/5588
oh, yeah, it's not a bug, just a little confusing. I neglected to say that.
Good catch. I think you're right.
`ProcessorSupplier#toString` is usually not implemented, and it does not matter much either, hence I'd suggest removing this line.
Similarly `Consumed#toString` is not implemented either, we can remove this line.
Do we really need to print `super.toString`? Ditto above.
aren't these just the defaults? if so, they can be omitted.
these literals for the error tolerance pop up quite a bit in the code, they should probably be constants on ConnectService or somewhere like that.
probably inside the `if` instead of before since it's kind of confusing to see this for a test that isn't going to actually read the DLQ
nit: this doesn't need to be a field, you can just use a local variable
nit: insert empty line
We should initialize `TopologyTestDriver` with a fixed mock-timestamp and use this below instead of calling `System. currentTimeMillis()`
Is it intended that all records fall into a single window / segment? What was the root cause of the bug and how does this data/timestamp pattern cover it? (cannot remember)
So you want to advance `recordFactory` time too? (similar below)
Do you want to pass in `autoAdvanceMs` for record creation? Default is zero.
Thanks for clarification. So it does not really matter what data we use in the test. Would there be a minimal setup that exposes the issue reliably? Just wonder if the test could be "more clear" from the code. If not, we can leave as is.
We could use nested for-loops.
super nit: the delta really doesn't matter in this case I'm wondering if `assertEquals(i, (int) totalMetric.measurable().measure(config, time.milliseconds()));` would be more clear, but this is subjective so feel free to ignore.
I think that the driving condition of the test is the number of samples, not the time. But I thought I'd point out that time won't advance by default, but you can make it by using the `autoTickMs` constructor.
why this override? we haven't needed something like this for the other autogenerated files
same question about override here
ah, i missed that it was in the Validator and not ConfigDef itself.
Not sure if this is the best fix. It seems that the issue is that the customized rateStat is only reflected in Rate, but not in total. It seems that we can extend Total to also allow a customizable stat. That way, rate and total can be updated consistently and we don't need customized code in record().
Is the following error code also retriable? 0x15 | KDC_ERR_CLIENT_NOTYET | Client not yet validtry again later
Can remove if initialize above
We also need a test to validate that some extensions can be ignored (neither valid nor error).
Yes, we could add `ignoredExtensions` and include that in the log in the server.
@rajinisivaram @stanislavkozlovski LGTM with the possible exception of maybe adding support for retrieving/logging any ignored extensions? I'll defer to your preference on this.
extension name must not be empty
Can initialize to `new HashMap<>()` here as is done with `invalidExtensions` below.
nit: do we want to consider setting `producer` to null here as well if `eosEnabled`? I realize this branch of the code should only get exercised when closing, but just in case we make changes I don't think it will hurt.
Should we avoid calling `recordCollector.close()` unless `eosEnabled`? It seems like this block used to be guarded by `if(eosEnabled)`, but it's not anymore.
Ah, didn't realize this was a `RetriableException`. we might then want to add what topic caused the problem too. Otherwise, if the topic is not being created, clients (for example, Connect would be stuck in a retry loop forever without any guidance on what topic needs to be fixed). P.S: hopefully, we aren't skipping writing a KIP here since this does change the behavior of a public API.
Yes, and more generally, my understanding is that we also need to consider if the consumer may return a "super-list" of the expected values, for example (still assume the above expected `{A, 1}, {A, 2}`): * `{A, 1}, {A, 2}`: this should be correct. * `{A, 1}, {A, 1.5}, {A, 2}`: this should be correct. * `{A, 1}, {A, 2}, {A, 2.5}`: this should be wrong, since `{A, 2}` should be the last for the key. * `{A, 2}, {A, 1}`: this should be wrong. * `{A, 2}, {A, 2.5}, {A, 1}`: this should be wrong.
My bad. `computeIfAbsent(keyValue.key, k -> new ArrayList<>());` computes a new `ArrayList` when key is missing. Was skipping over the code too quickly.
@bbejeck I've looked at this example once again (this is the only place that you have multiple same key records to check right?). And I think the semantics I originally did in `waitUntilFinalKeyValueRecordsReceived` may not meet this needs: originally what I did is to make sure that for each key in the expected results, the corresponding value is indeed the "final" record for that key. It is for testing for aggregations where caching effects may dedup some immediate records. What you are doing here though, is actually to check that: 1) `{A, foo:3}`, `{A, bar:3}` and `{A, baz:3}` eventually happens in the output stream, following this order. 2) There are possibly other records in between of them. 3) `{A, baz:3}` should be the final record for this key, i.e. no other records come after it. The modified logic you had did not meet all these checks, and hence we need to do is: * drain all records with a timeout (already done). * check for the expected results in ordering as 1) required. * check for 3) that the last specified record is indeed the last for the key.
nit: can we break this line? (similar below)
This is similar to `branch()` -- we arbitrarily pick left hand side input. Also, if lhs is `null` rhs could provide correct serde. \cc @vvcephei Can you add this case to the JIRA for branch(), too.
Ack, I missed a `not` :)
Using `Long` for `count()` should be first -- no user overwrite should be allowed for this.
This is an interesting question. One low-fi solution would be to think about using `equals()` (I think to pull this off, we'd need to introduce a requirement that serde/serializer/deserializers implement equals in a way that would be semantically sound for us. This would not be a back-ward compatible change. On the other hand, since callers actually subclass `Serde<T>` with a fixed type like `Serde<String>`, it actually should be available at runtime. I don't remember the hoops you have to jump through to get it right now, but I'll revisit it tomorrow.
Ok, I've done enough kicking the tires to say that it is possible to get the generic type argument to Serde in most (but not all) cases. It's not a trivial algorithm. I'd estimate it at one day of work. If the current code is sufficient, then I recommend we go with it in this PR and have a follow-up PR for just this feature. While researching the issue, I found this article that pretty accurately describes what's going on: https://www.javacodegeeks.com/2013/12/advanced-java-generics-retreiving-generic-type-arguments.html
Actually, I think we want to err on the side of correctness, so shouldn't we just drop the serdes at this point and set them to null? It seems like this would preserve the current behavior (where we also don't have serdes for the resulting k/v).
To get rid of the warning, you can just copy the body of `close(long, TimeUnit)`. If you don't want duplication, you can make the deprecated method call the non-deprecated one.
Need to fix indentation below.
This one also shouldn't be deprecated.
Only `close(long, TimeUnit)` has been deprecated.
Do you want to prevent NPE? Maybe easier to rewrite to ``` if (props != null && StreamsConfig.OPTIMIZE.equals(props.getProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION))) { ```
Nevermind. The PR description explains it.
Ah, you're right. Not sure how I missed that.
> This is similar reasoning to why SessionWindows declares its default separately. Something I don't like -- I think we should use the same default for all windows (that need one) and thus share the default value as a single constant -- I would rather prefer to create an internal class that just define this single const and use for all window implementations. I by the argument to not add it to `Windows` as `UnlimitedWindows` does not have a maintain duration. Thus, adding a new internal class for adding the constant that time/join/session window can just use without inheriting anything.
nit: unused variable
Thanks for the explanation. I missed that this method is `static`.
nit: o2[%s] was **equal** to o1[%s]
Did you consider extending `ManualMetadataUpdater`? That would let us skip most of the overrides below.
@kamalcph Though we want to fix KAFKA-4468 to allow window size be passed into the time windowed deserializers, I'd suggest do it in a separate PR and keep this a smaller scope of bug fixes only. Otherwise this PR will drag long since it takes much more time for KIP discussions. Also note that there is already a KIP related to this function that is underway: https://cwiki.apache.org/confluence/display/KAFKA/KIP-300%3A+Add+Windowed+KTable+API+in+StreamsBuilder cc @abbccdda who worked on KIP-300.
This is a public API change and requires a KIP (cf. https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals)
The return type `Serde<Windowed<T>>` is defined explicitly for callers to avoid casting. I think we can keep it as is.
This seems to change the behaviour...
The producer in the `WorkerSourceTask` automatically resends records, but if the producer fails to resend the [WorkerSourceTask enqueues the unsent records in `toSend`](https://github.com/apache/kafka/blob/08e8facdc9fce3a9195f5f646b49f55ffa043c73/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L343-L348) and send them again. It is true that this happens after each call to `poll()`, but if the send fails then `toSend` is non-null and upon the next iteration of the loop it will not call `poll()` and will then try resending whatever is in `toSend`. This will continue to happen as long as `toSend` is not null. However, in the current PR, even though this might happen, the loop may still ask for the source partitions and offsets and will synchronously commit them using the `offsetWriter`. So it is possible that a record with a particular offset `o1`, for a source partition `p1` fails to send and is retried, but then a connector then sets a later offset `o2` for the same partition and the connector commits offset `o2`. If the connector were to fail at exactly that point (which is possible), the `o2` offset may have been committed without the `o1` record being written. I understand that in your particular use case, you probably would only set the offsets for a particular partition if records were not written recently, but that doesn't change the fact that the `WorkerSourceTask` might be attempting to resend the previous records for quite some time. What if your new block of code were only performed if `sendRecords()` succeeded? I think there are a couple of issues with that as well. First, the offset writer is called synchronously, whereas other calls to commit offsets are sent a separate commit thread that calls multiple tasks. Now there are multiple threads committing offsets with potential race conditions and concurrency issues. Second, it still is a complicated API, and will developers truly understand when and how they use `getSourcePartitionAndOffset()`? Can I call it to read the last offset committed for a particular source partition? The worker doesn't ever set the offsets there. The WorkerSourceTask has a single, ordered pipeline for all records that each have their offsets. I still believe the best and most reliable and deterministic way to solve this is to use that same pipeline.
We do not throw `InvalidTopicException` "if [the topic] is not found"
This exception isn't used in createTopics.
This exception happens if the given topic name can't be represented, not if it collides with another topic name.
This is not correct. We return `UnknownTopicOrPartitionException` if the topic is not found.
InvalidTopicException happens when the topic name can't be represented in the request, or if it is not found, not if it collides with another topic name.
Typo: should be "or larger than the number of available brokers"
I do not think this exception can be thrown from CreateTopics.
This exception can't happen in DescribeTopics.
This exception can't be thrown by DeleteTopics.
You should also include UnknownServerException here, which indicates an internal problem on the server side.
Please include TopicDeletionDisabledException here.
Should have a comma after "for example"
This exception can't be thrown by listTopics.
You definitely can determine this automatically from the existing tags. For anything with patch version > 0, it's trivial since you want the reference for previous version to be `patch_version - 1`. For the first release in a major.minor release line, you would need to figure out the correct previous major.minor.patch release and use that. Normally I would say this is pretty easy, just list tags, find ones that match the right pattern, split segments on `.` characters, convert each to integers, and sort. However, this does get a bit messier with Kafka because of the switch in release numbering (from 4 digits in pre-1.0 to 3 digits in post-1.0), so you'd have to normalize to 4 digits, sort, then make sure you drop any extra digits from post-1.0 versions. It'd be nice to get this all automated and the ergonomics of the script are nicer if they it is, but I wouldn't block merging this on that. This is still better than what committers do today, which is to just construct this all manually.
If we run the script to do the actual release, we have this information already. It would be good to reuse this. Ie, we can keep this as-is, however add a second method that takes this information as parameters. This allow us to call the new method from here, after we collected the information, but also at the end of the regular execution of the script and pass in the information directly. Thus, if a committer does a release, it's not required to call the script again but the email template will be generated directly.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
I like this. We will be able to pack port this to `2.0`, `1.1`, and `1.0` branch -- not sure if there will ever be a `0.11.0.4` branch -- thus, might not be worth do back port further to support 4-digits.
Should be ok to do either 3-digit or 4-digit code (for corresponding branches) ? No need to support both in one branch IMHO
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
For an 0.11 release, the committer can still put the email together manually... We need some cutoff point for back porting -- it's just a convenience improvement for doing a release, not a bug fix. Thus, 1.0.0 seems to be a clean cut-off point IMHO.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
This should go to `announce@apache.org` as well, that's actually the most critical one as that makes it "official".
If this checks out, then I think we actually don't need to track this variable.
The fact that we're ignoring it suggests that it's not an `error`. For something that indicates something unexpected has occurred, but that we can handle it and continue correct execution, we should use `warn` level. Also, the type of exception will be captured in the stacktrace, so I would just say something more descriptive like, "Channel closed unexpectedly before lock release."
Hmm.. I think the original logic made more sense. Even if `completeExceptionally` returns false, it's still an error, right? We would not want to then proceed to `future.complete` or the next operation.
I think `handleRetriableError` is a bit misleading. I mean it handles both retriable and non-retriable error. From this perspective the old naming was better (from my perspective).
This is very similar to `handleRetriableError`, perhaps there is an opportunity to use a common code.
Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
Not sure why this needs to be a Suppiler of time rather than the value itself. The code would be more readable with just a value.
Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
Looking at just this code, it looks like we record this for auth and reauth, even though it actually happens only once as expected. It would be more readable to move this code under the `if (channel.successfulAuthentications() == 1)`.
I think in general we shouldn't call this `toString`. People would confuse it for the actual `toString` method and would suppose the same or similar behavior. For instance `toString` methods aren't expected to throw exceptions. So in my opinion we should call this something like `readAll` or `readAllBytes`.
Instead of having an `else` with a nested `if/else`, we could keep it flat: ```scala if (fileAlreadyExists) ... else if (preallocate) ... else ... ```
Why is this needed? This is worse than the previous approach as it opens, closes and reopens the file.
It will result in the same list of versions -- both equally good IMHO.
This test is still using `sleep` and I would assume it might be unstable -- what should have beed the reason why we disabled the test -- I think, we should rather rewrite it that just enable it.
Both should be fine -- not sure if the change improves anything -- but would not make it worth either.
Just out of curiosity, how does this differ from the following? ``` assertTrue(streams.close(10, TimeUnit.SECONDS)); ```
same as above with try/catch
If we reuse shared streams instance do we still need try-catch here as the `cleanup()` should close streams.
We can reuse `streams`.
Ditto here, for this test we can still reuse the shared streams.
none from what I can see, but I'm not sure it's worth holding up the PR for it.
+1 for me on `AutoCloseable`
minor: this variable is not introduced in this PR but it may be good to rename this to `sharedStreams` or any other better name as `streams` instance is created locally in some of the tests.
I think it might be nicer to make the fields in `FilterResult` mutable and move the functionality there. It's a little annoying to have an additional internal class just for accumulating a subset of the filter stats.
I think this can be replaced with `remainingToSearch.keySet().retainAll(value.partitionsToRetry)`.
There seems to be some redundance between `partitionsToRetry` and `remainingToSearch`. It might be nicer if we could get rid of `remainingToSearch` so that we only had to rely on `ListOffsetResult` to know if we should retry. I think the only thing we need is to avoid losing partitions in the call to `groupListOffsetRequests`.
Another idea might be to pass `partitionsToRetry` into `groupListOffsetRequests`.
This method can be package-private.
nit: do we need this line here, we automatically add the `restorer` in line 67 below
nit: -> `shouldThrowForInvalidSocketReceiveBufferSize()`
nit: -> `shouldThrowForInvalidSocketSendBufferSize()`
nit: naming -> `shouldAcceptDefaultBufferSizes()` Also, I am wondering why we check for default buffer size? The ticket was about the issue, that `-1` was not accepted. Thus, while having this test is ok, we should actually test for `-1` to have a test that covers the reported issue.
If `new KafkaStreams` throws, this line will never be executed (same below). Also, `streams` will not have any object assigned. Thus, we cannot even close anything, as the object was never created -- I think we can remove this line and also remove the variable `streams`.
This might not be robust enough if the streams app decides to store the directory elsewhere right? I'm thinking we need another test as part of this file that checks a stateful topology and confirms the storage is actually there. Then the pair of tests would be more robust to changes.
How about also augment the built `Topology` with processor APIs to add more processors, and check that as long as we do not call addStores the resulted topology would still be stateless.
nit: you could add a static import and make these all just `getBytes(UTF_8)
I'm not sure in this. So in case of two properties (k1=v1 and k2=v2) do you generate the `--consumer-property k1=v1 --consumer-property k2=v2` string? The repeated property didn't work when I tried it out and it only picked up the first one. I think you need to pass `--consumer-property "k1=v1,k2=v2"` as with some other commands. In case the "k1=v1,k2=v2" format is needed, there is a nice one-liner way to do it: ``` ','.join("%s=%r" % (key,val) for (key,val) in k.iteritems()) ```
I can't see this being used. Do you think this can be a validation step? (For instance to look at the expiry dates after generating, expiring, renewing tokens.)
Do we need to both consumer_properties and client_prop_file_override? maybe we can just use client_prop_file_override
consumer_properties is still in parameters ```suggestion kafka_opts_override="", client_prop_file_override=""): ```
That makes sense, but is this method currently unused? If it's not used, then I think it's better not to add it. (IMHO, lack of dead code outweighs the value of symmetry)
I wonder if a safer way to do this from a compatibility perspective would be to provide a default method for `close(Duration)` which invokes `close(long, TimeUnit)`. Similarly for the producer.
nit: mentioning the time unit seems redundant.
checkstyle: space before `{`
With the additional context, I'm fine with the PR as is.
Could we do `final StreamsConfig streamsConfig = new StreamsConfig(config, false);` instead? I tried locally and it worked.
Still not addressed -- we should mention both methods return this iterator.
`long,long` is used for `WindowStore` while `Instance,Duration` (or `Instance,Instance` if we correct it) is use for `ReadOnlyWindowStore` that return the same iterator.
Do we still need to verify that `channelMock.position` is called once? Given that the old test works, `channelMock.position(...)` should be invoked. If we don't do `when(channelMock.position(42L)).thenReturn(null)`, what would be the return value if `channelMock.position(...)` is called? It seems that channelMock.position() is called in the old code but not the new code. If so I am wondering which part changes this behavior.
Isn't this case covered by `statelessPAPITopologyShouldNotCreateStateDirectory` ? DSL compiles down into a Topology anyway. Similar below.
Thanks for the cleanup!
Yes, this is a good catch. We used to need the `active` check before #5390, but I missed that during the refactoring. We can remove the redundant `if` condition.
I think this is a good catch. For checking `hasNewPair` the logic for avoiding conflicts on either active-active or active-standby should be the same. cc @bbejeck for another look.
`what users need to instantiate` Unclear what this means.
nit: add `<p>`
If we want a new paragraph, we need to insert `<p>` here.
The `operatingSchema(record)` (line 141 in the PR) obtains the key or value schema, depending upon which transformation is used. So the new PR is a bit more clear, and I believe it is correct. The prior code used `value` for the operating value, but if the `Flatten$Key` transformation was being used the `operatingValue(...)` actually returned the key, so the `value` variable name is a bit of a misnomer.
Thanks for your detail explanation!
nit: this is the user specified name, that is part of the repartition topic name -- we should rename this
ditto to `KStreamImpl`
Cannot follow here... Do you aim for existing topologies with generated names, and user update code to "pin" names? For this case, user would pass it name, without `-repartition` suffix? User, would also need to drop `<application.id>` prefix in the name she passed to `Grouped`.
nit: remove empty lines
nit: 4 space indention only
nit: 4-space indention plus move `builder` down one line
What's the code path that leads to `repartitionTopicBaseName.endsWith(REPARTITION_TOPIC_SUFFIX)`? I couldn't find it.
I think if you mark this method as `@Deprecated` as well, it will also suppress the warnings, which might be better because it preserves the deprecation notice from the interface.
Thanks! I just noticed this yesterday, and it did trip me up a little.
IMHO, it's better to pass along the deprecation instead of suppressing it. They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body. This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired.
nit: remove var `newJoined` (also not used for left-hand-side code)
nit: remove `this.`
I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method). I also don't think we need `@Deprecated` as this annotation is inherited anyway. However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this.
`repartitionTopicName` and `repartitionTopic` is a bit confusing. I'd suggest just keeping the `GroupedInternal` as a field to replace key/valueSerde and `repartitionTopicName` in the constructor and retrieve its fields later. Ditto for other internal class's constructors (you already replaced serdes with the object in some classes, just trying to suggest consistency here).
In that case could we just use the foreach loop after `ConsumerRecords#records` to get the filter list from the returned list? I just felt leveraging on `ConsumerRecords#iterator` is unnecessarily costly.
Yeah I felt it is better to guard against null beforehand. This actually come from the unit test where we do `adapt(null).restore(null)` where the `restore` function is supposed to throw not-supported exception, while I was a bit frowning upon it.
We can just use `ConsumerRecords#records(TopicPartition partition)` since it should only contain one topic-partition's records.
Do we expect `TimeOrderedKeyValueBuffer` to be queryable ever (I saw you changed it to extend StateStore)? If yes we need to make this variable as volatile since IQ threads may access `isOpen`.
nit: seems like a better practice to use the same iterator instance for both `hasNext` and `next`. The construction of the iterator does do a little work. Same thing for the other checks. Also, the `hasNext` check below is now redundant.
Need null-check since `remoteAddress` is null on the server-side since we are currently only setting for client connectons.
We probably want another constructor `ChannelState(State state, String remoteAddress)` for non-authentication-failure states where we store `remoteAddress`.
`@code` -> `@link`
nit: style seems to be to not include braces when there is only one if or else statement
nit: braces unneeded
nit: braces unneeded
Is the `myCommittedToken == null` check unnecessary here since it can never be the case when there are extensions? I think we make sure of this since we only call `identifyExtensions()` when there is a token.
Yes that's correct as this PR stands now. But if we put the name check back to what it was originally, then this line is not needed.
All the callers seems already handles the `name == null` case so this seems unnecessary.
Good catch, but I still prefer to use `Joined` as a single parameter, but overall I think this approach is better. In the Serde inheritance PR the order was switched when creating a new `Joined` object ie `Joined.with(keySerde, otherValueSerde, valueSerde)` so the `otherValueSerde` was used, but the change was subtle and I missed that point. Probably better to keep it this way so things are more explicit.
nit: `repartitionNameOverride` won't ever be `null` from doing the checks for the name on lines 571 and 576, so we can get rid of this line
super nit: should we assert greater than `0.0`? But I don't have a strong opinion here though.
nit: method can be `private`
Making sample age super high sounds better to me, comparing to having a ballpark check.
This is a little hard to read. A vanilla `if` statement may be better: ``` if (connectorName != null) { assertTrue(context.startsWith("[" + connectorName)); } ```
@rhauch I don't believe that's the effect the code here has. In the method call: ```java assertEquals( connectorName != null, connectorName != null ? context.startsWith("[" + connectorName) : false ); ``` if `connectorName` is null, then both arguments are guaranteed to evaluate to `false`. I think your intent may have been something like this: ```java assertEquals( connectorName != null, context.startsWith("[" + connectorName) ); ``` which would probably be acceptable, but may also benefit from a `message` that clarifies the expected behavior, possible something like `"Context should begin with connector name if and only if connector is non-null"`
Nit: every conditional needs to have braces (per the code style) not nit: I find this logic a little difficult to follow. Contrary to what @mjsax suggested, wouldn't it be pretty straightforward to map the old semantics on to the new ones like this: * negative numbers => 0 * 0 => Long.MAX_VALUE * all other arguments stay the same ? Then, the old close method could just transform its arguments and call the new method, with no need to have this "new semantics" flag and an early return in the middle of the loop.
nit: add `{ }` to block
We cannot change the semantics of exiting `close(long, TimeUnit)` -- this would be a backward incompatible change. We can only change the semantics for the new `close(Duration)` method. We also should point out the different semantics in L830: ``` @deprecated Use {@link #close(Duration)} instead; note, that {@link #close(Duration)} has different semantics and does not block on zero, e.g., `Duration.ofMillis(0)`.
@mjsax is right. Just to clarify, state store / changelogs today only do header-agnostic serde, so the scope of this PR is only for sink nodes.
`StateSerdes` is use to read/write into local stores and changlog topics. Because changelog topic should only be read/written by Kafka Stream, it seems we don't need a change there to me.
same here have a `retrieve_generation_num` method for extracting the generation number from captured output
nit: remove `this`
nit: remove `this`
It seems that the checkstyle failed but all unit tests have passed. I can modify the code slightly to fix the checkstyle failure before merging it.
Should this be simply `assignments` for consistency with the accessor name? Same for the construction parameter. Or change the accessor name.
To clarify: there are two entities: the metrics registry which organizes the metrics, and there is metrics reporter which regularly pulls from the registry to report the metric values. Inside metrics registry there is `sensor` which as you understand is just a way of grouping metrics into meaningful clusters. The `SensorName` is just an id for distinguishing sensors in the metrics registry (i.e. you will see logic like `if this sensor as already been created in the registry skip this step)`. A metric name presented in `MetricName` which contains groupName, tags, etc is just a logical entity in the registry. How to represent the metric names is up to the metrics reporter (different reporters can definitely represent it differently). As for the sensor names, they should never be seen outside the registry as the metrics reporter never exposed them.
Thanks @guozhangwang I was just checking, and we also define `"stream-processor-node-metrics"` in `ProcessorNode` -- should we unify both, to have one constant only? Think, we can also simplify `ProcessorNode#createTaskAndNodeLatencyAndThroughputSensors` and remove the `group` parameter.
group name is used as a first "tag" of the metric name in JMX reporter: `xxx-metrics:type=[group-name],[tag1]=[value1],...`; for other reporters they can use the group name however they like.
I didn't think of that before, but now that you mention it, the change makes sense to me.
returned => will be returned
Sorry, my bad, but we should probably say that deallocation varies instead of "no deallocation". `8` implies that at least one of them happened.
You made this stricter (+100 instead of +1000), I'd not do that given Jenkins variability.
Discussed this offline with @rajinisivaram and we think we think the first condition can be removed.
Previously, we would not throw the exception if `flush` returned `false`, but now we do. I assume that's intentional? May be worth explaining.
Can we please add a method `isDone` which is basically `finalState() != null)` and then use it here and everywhere else we do `batch.finalState() == null`? It would be much easier to understand.
Ack, thanks for the explanation. Sounds good to me.
Should be final
Seems like we don't really need inheritance here. Can just have an "if" statement that checks if we have a group or not
Should be `consumerGroup == null ? "" : consumerGroup` to match the other entries. We don't use nulls in JSON
We don't use null entries in JSON, because it gets too confusing. You should check against empty string here.
We should use a randomly generated (and hopefully unique!) consumer group here so that we don't conflict with other people running a test.
Should be final
There's a pattern for all of the Trogdor JSON code where we don't use null anywhere. The problem with null is it gets annoying to check each collection for empty vs. null, each string for empty vs. null, etc. etc. null is also handled kind of inconsistently in Jackson. Sometimes Jackson will serialize a field that is null as `"foo": null` whereas sometimes it will just omit the field. (I think that `"foo": null` is actually not conforming JSON, by the way...) There are probably ways to configure all this, but null doesn't really provide any value 99% of the time, so it's simpler to just treat empty as null.
Should be final
We shouldn't always use assign here. If the developer has not specified any partitions, we can use the partitions of the group itself.
It would be nice to have a unit test for this.
It would be good to have some assertions.
Nit: what if the default changes :P
indentation issue here, if this is passing checkstyle i would be surprised
I'm fine with these as is, but you could also change the methods to be `static` in `Worker` and accept the `WorkerConfig` as a parameter since that's the only class member they use. Would get rid of all the distracting mocks and expect calls and focus the tests on the key functionality of those methods.
Can we simplify the patch by not having this method? We pause a partition after putting it in `recentlyUnPausedTopicPartitions`, we can still use `fetcher.subscriptions` to check whether the partition is unpaused while going over the partition in `recentlyUnPausedTopicPartitions`.
This statement is always false.
There is an extra unnecessary space.
`HashMap` can be replaced by `Map`.
Maybe put a log statement, indicating that we are putting some data aside in the `pausedNextInLineRecordsPerTopicPartition` plus its size.
Existing issue, space should be after the colon.
Existing issue: space after colon. No need for `()` in `getVersion` and `getCommitId`.
Can we swap the `if/else` so that we don't have to negate the first `if`? That seems unnecessarily confusing.
Nit: capitalization isn't quite right, see `reauthenticationLatencyMs` for example.
I think it should be a KafkaException (or a subclass of it). IllegalStateException in this class refers to an invalid configuration that is more compile time than runtime, whereas a partitioning problem is purely runtime and only occurs with custom partitioners.
I'm against the sprinkling of `final` keyword everywhere. But more than that, I'm in favor of consistency w.r.t. the surrounding code. A few lines above, you may observe that `close` has a similar loop without `final`. Every project has its idiosyncrasies and Connect is not dogmatic w.r.t to `final` for local variables.
I've found that avoiding using anything related to the type of a variable in its name is usually for the best. It takes a few more seconds to come up with an appropriate name, but it definitely pays off when reading, and even in generic uses as this one here it's still better IMO. E.g. I'd call this at least `chain` or something.
(especially given that below you use the simple name)
Are you sure you want the FQCN here? simple name is another option which might be nicer to print. But in any case, want to make sure we are deliberate here.
@kkonstantine I don't feel strongly about it, but others do. Probably largely due to (with hindsight) the opposite default of what would have been ideal, i.e. would have been better to have to opt out of by-default immutable references rather than opt in. We definitely already have some divergence within the broader AK codebase (`streams/` uses `final` more aggressively, not sure if it's checkstyle-enforced, Connect tends to use `final` mostly just for class fields). `clients` is a bit of a mix, but less `final` in `for` loops than not. And `core` is scala, so different story altogether. I have no hard and fast rule here, main reason I even asked is because in this diff it is being removed from what was already there. No need to get hung up on it, but the other argument is to not change it in favor of minimizing diffs and maximizing mergability/cherry-pickability :) There are tradeoffs no matter which direction you choose....
There's some redundancy here. SMT=Single Message Transform + transformation Given that you log the name of the transformation I'd use `"Applying transformation {} to {}"`
Thanks @ewencp! Glad I'm still up-to-date on that. Happy to adjust per project and yes, unnecessary diffs are better to be skipped in non-cleanup/non-refactoring PRs. Huge fan of that.
@ewencp I suggested not using `final` in every new loop for consistency (several loops even here don't use it such as the one in `close`), but I didn't imply that we should change unaffected lines. In general in Connect my understanding is that we are not strict in demanding use of `final` in local variables. Let me know if something changed.
nit: Seeing the line right above, one would think this fits in one line
@cyrusv Your example should print something like: `<SimpleClassName: blah blah blah>` But I don't see `<` or `>` in your example. So either the code example is wrong, or you didn't mean to use those "quotes" in the joiner. BTW probably not adding this decoration would be better.
I don't see how wrong usage elsewhere inspires wrong usage here. Despite that, I still prefer `Applying transformation` since this log message is not a paragraph in a text where we need to be precise and need to list the acronym explicitly along with what it means. This is a log message that needs to be succinct and can be read in context. A transformation here implies a connect SMT.
unnecessarily verbose for a log message IMO (see reply in previous thread)
I would consider something other than just `...`. It took me some time to figure out why we were even adding this. `non-null` or something like that, or changing the format so it isn't `keySchema=` which implies (to me at least) that we're putting the actual value. Alternatively, you can include the actual schema, currently it's `toString()` just returns name & type, not a full, nested representation of the schema.
I don't think you're going to want to use `getSimpleName()`. I think for a nested class that only returns the nested class name, and we use nested classes as a pattern with transformations to handle keys and values without duplicating a bunch of code, e.g. https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java#L194
This will end with a comma after the last transformation.
This probably doesn't add value without more information about the name of the schema.
These log messages really make most of the individual log messages within the SMTs less valuable, since the record will include the key schema, key, value schema, and value. I've highlighted some of the log messages in individual SMTs that I think are worthwhile. Also: ```suggestion log.trace("Applying SMT transformation {} to {}", ```
Also, I just looked at `ConnectRecord.toString()`, and it does _not_ print the schemas. I wonder if it's worth changing that to include whether the key schema and value schema are null; e.g., ``` @Override public String toString() { return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", keySchema=" + (keySchema != null ? "..." : "null") + ", key=" + key + ", valueSchema=" + (valueSchema != null ? "..." : "null") + ", value=" + value + ", timestamp=" + timestamp + ", headers=" + headers + '}'; } ```
I'm not sure these add value since TransformationChain has log messages with the records, which should be sufficient to know whether the schemas are null.
I'm not sure these add value since TransformationChain has log messages with the records, which should be sufficient to know whether the schemas are null.
I'm not sure this is worth it, if we're logging messages for each field being converted, plus if the TransformationChain has log messages with the records.
Same question as above.
This is a useful log message. But since in a busy Connect worker it's unlikely these log two messages will be adjacent, how about instead using a single log message: log.trace("Cast field '{}' from '{}' to '{}'", field.name(), origFieldValue, newFieldValue);
This may be useful, since the log messages in TransformationChain do not print the schemas in `ConnectRecord`
Should be "threadsPerWorker"
Same (should be threads_per_worker)
nit: we usually format code differently and put `@Override` is its own line.
Why do we add `KeyValueStore`, but not `WindowStore`/`SessionsStore` ? If this can be any dummy, I would rename to `dummyStore` -- otherwise it might be confusing.
nit: add `{ }`
nit: `ERR_MSG` -> `ERROR_MESSAGE` (avoid abbreviations to increase code readability)
nit: this can be package private (what is more restrictive)
I wonder if it would be worth wrapping the config in another object with a custom `toString` so that we are not tempted to reintroduce this bug in the future.
Could we rephrase this so that we do not need to clarify that it is not an issue? For example, `Successfully processed removal of connector '{}'`.
super nit: maybe reverse statement to `maxInFlightRequestsAsInteger > 5` IMHO easier to grok, but this is highly opinionated, so feel free to ignore.
We should convert null -> none here. Even if Jackson always fills this in, someone may manually create this object
This is unused.
Unless I'm wrong, we move to this directory and that's where we execute all the rest of the commands (such as the echos in output files below). Just want to make sure this is what we want (which looks like it is)
the changes to the optimizer code LGTM
@ijuma I looked into this in more detail. We don't need a new type `R` -- instead, we should update to ``` public class KTableKTableJoinNode<K, V1, V2, VR> extends BaseJoinProcessorNode<K, Change<V1>, Change<V2>, Change<VR>> ``` and use `private final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal;` (ie, `VR` instead of `R`. (Note, the you always pass `<..., Change<X>, X>` in the current PR, what is redundant and can be avoided.)
\cc @bbejeck Can you have a look here? You know the optimizer code best.
It's not necessary to have `PENDING_VALUE`, `RUNNING_VALUE`, etc. since you can just call `PENDING.name()` to get the string `"PENDING"`
I think you need `Utils#join` here. Calling `toString` on an array will give you something like `[Values@3343c8b3`
OK. Sorry, I looked at this first before I saw the update to `TaskStateType.`
Yeah, it's a tricky business... I think the suggestion I had in Max would also apply here, and you wouldn't have to compare them at all.
Sorry if this basically seems like code-golfing, but I'm wondering if this would be equivalent and a little more robust? ```suggestion return samples.isEmpty() ? Double.NaN : max; ``` Then, we could leave the initial values at negative infinity (not sure if it matters).
nit: indention should be 4 spaces
Why do we need this? This is logged anyway.
is it important to have 3 brokers for this test? I'm wondering if the tests would be more resilient and faster with just one broker and replica of each topic.
nit: simplify to `final String propFileName = args[0];` -- was checked above already
You might consider using any `public static` utilities in `ConfigDef`/`AbstractConfig` to help with this here. I know at least `ConfigDef.parseType` could help here since it also does things like trim the string. Reusing that code will keep this closer to the normal behavior of `ConfigDef`s (and if we eventually move this functionality to be part of `ConfigDef` itself, will probably make it a simpler transition).
These aren't needed in these tests anymore, and `anyTimes()` wouldn't be ok here anyway. I think the `anyTimes()` was masking the fact that we could remove these now.
Nit: I would inline this method. It's a single liner and only used once.
I would prefer a separate test: `shouldAllowPrePopulatingStatesStoresWithCachingEnabled()`
Nit: why change `0L` to `0` ? Timestamp is of type `long`. (sam below)
Perhaps this can be private and we can expose a `buildUnsafe` method that sets `validate` to false. Then we will be less tempted to accidentally use the API.
`seems existing already but it doesn't` -- this might be confusion. What about: ``` Could not create topic {}. Topic is properly marked for deletion (number of partitions is unknown). Will retry to create this topic in {} ms (to let broker finish async delete operation first). Error message was: {} ```
nit: indent not aligned, as @mjsax mentioned above.
To get `retryBackOffMs` config, we will need to add it explicitly with prefix `admin.` in `StreamsConfig#getMainConsumerConfig()` similar to retries: ``` consumerProps.put(adminClientPrefix(AdminClientConfig.RETRIES_CONFIG), adminClientDefaultConfig.getInt(AdminClientConfig.RETRIES_CONFIG)); ``` Nit: fix indention (similar below) -- there should not be any tabs, but spaces only.
Personally, I prefer (3) because the others provide redundant information.
Nit: -> `messagePrefix + "It shouldn't be null.")` (remove space -- this allows to hand is an empty prefix without getting an undesired whitespace (ie, `""`, if prefix is not desired). Similar below.
nit: move to line above (however, this is an internal API, and I am not even sure if we need to add this at all.
`advanceMs` is not the same as provided input parameter `advance` -- this would make the error message miss leading.
> I felt the same, but I picked this approach to make less changes in the code, since I am beginner here. That's fair. However, we should not "optimize" for fewer changes but for better code. :) > But, I need to give a thought on kind of prefixes we should use to make it unique for various fail cases. `validateMillisecondDuration()` will just add the prefix. Each called can pass in whatever prefix is suitable for it.
Might be simpler to just update the Jira and do all at once? > Any thought about how the prefix text should look like? The suggestion you made via wrapping one `IllegalArgumentException` with the other, was good. Just you proposed "outer" error message could be used to be passed in as prefix.
```suggestion public static final String MILLISECOND_VALIDATION_FAIL_MSG_FRMT = "The \"%s\" for \"%s\" is incorrect, value: %s"; ```
Like I wrote above, we should just have one function that takes a map. Then we don't need a special function for the case where there's only one task to create (can use Collections.singletonMap or similar if needed).
Like I wrote earlier, this should just be a map, so duplicates should not be a problem. I think it would be good to do all the validation here. There's no reason not to do it and it makes things more robust if the code is re-arranged in the future.
Check TROGDOR.md. > All Trogdor RPCs are idempotent except the shutdown requests. Sending an idempotent RPC twice in a row has the same effect as sending the RPC once. Because the request is idempotent, sending it twice has the same effect, including the same result code.
Cleaner to just check if `tasks.isEmpty` after the loop is over.
We should also check to make sure there are no invalid empty task IDs. In that case we should throw an exception and not try to create anything, similar to the above exception...
This exception gets logged and then ignored by `createAndScheduleTasksAtomic`, right? I think it would be better to keep the original behavior where we create a task, and mark it as done and failed. Then, at least, the user could see the error message if something failed. The error would appear in the failed task status. The user shouldn't have to read the logfile to see the failure message.
Hmm. This still has the problem where things can be partially applied, because we have a bunch of `CreateTask` runnables being processed separately. It would be easier to just have a single `CreateTasks` runnable and pass it the map. Then the whole thing could fail with a `RequestConflictException` if any task had a conflict.
`TaskManager#tasks` has to be accessed through the state change thread. It can't be accessed here by incoming requests since there is no lock or synchronization. Probably the easiest thing to do is to have a CreateTasks runnable which does what you want.
I don't think this `Callable` should return `ManagedTask`. `ManagedTask` is an internal data structure that should only be accessed from the state change thread.
I don't think we really need this function any more... we can just submit to the executor from the other function.
This shouldn't throw an exception. The existing logic where we fail the task is what we want to do here.
nit: same formatting would have been nice for all places with `parser`.
nit: one line per parameter for method call when statement is long acceptable.
For your consideration: message could be more direct . `Is this a plaintext response?` --> `The broker expects SSL, is your client configured for SSL correctly?` (or something of that nature)
Adding an extra "if" statement on every call to `read()` seems like a very heavy price to pay for this feature. We should look at where we're using the total length and check there, I think.
@stanislavkozlovski understood. My proposed wording was aggressively making assumptions, but perhaps we could find a middle ground that still directs users to check their port? The issue with `Received a first response larger than 1MB (Is this a plaintext response?)` is that users won't know what to DO with that response. It would be great to reword it accurately to give some proposed action/suggestions to the user...
Are you referring to my fix? or my problem? I just would like my problem solved :) I'm not concerned about my fix - I was just mainly trying to demonstrate the problem.
I think the right time to check this would be when we originally create the NetworkReceive object, not in the hot path for every read. Also, whatever we choose as the "upper limit" for a size to be interpreted as valid needs to become the new upper limit for `message.max.bytes` (which in turn, means that maybe this needs a KIP, since it changes a public config)
Adding the exception is fine, but you can just throw it directly: ``` throw new OffsetOutOfRangeException(...)` ``` Not need to assign it to variable first :)
Remove `changelog` -- can be any topic
variable exception unnecessary
nit: `/** ... */` -> `/* */`
We usually avoid the get prefix in cases like this
We've gotten several requests not to log the values of any records above debug level. If you think we should still log the value, we should split this into a warning log without the value, and then a debug/trace log including the value.
Hmm.. I was looking into how the producer handles various operations after the call to close(). Unsurprisingly, there is a bunch of inconsistency. For send(), as far as I can tell, the logic looks like this: 1. Check whether the sender thread is closed and raise an error if it is 2. Refresh metadata if needed 3. Check if the accumulator is closed To be honest, I don't know why we don't just check if the sender thread has begun shutdown, but perhaps that's a separate issue. For transactional requests, it seems we don't currently do any checking to see whether the producer has been shutdown. At least I couldn't find any checks in `commitTransaction()`. To make this solution work, I think we should have those checks. Basically the user should see some kind of error indicating that the producer has already begun shutdown and the operation is no longer allowed. As it is, they would probably see an illegal state error indicating the transaction already began aborting or something. This could be a simple check in `TransactionManager` if we add a `close()` method as suggested elsewhere.
I think this should probably be an `info` or `warn`.
To clarify, I was suggesting that we can abort a pending transaction only if `close()` is called before the user has attempted to commit. The transaction would be doomed to abort anyway, but this way we don't have to wait for the transaction timeout.
A couple things to consider: 1. If close() is called and a transaction has not been committed or aborted, should we abort it explicitly? 2. I mentioned in the JIRA that the thread blocking on `commitTransaction()` may be stuck if we shutdown the `Sender` before the future has been notified. That seems to still be a problem as far as I can tell. Maybe we should add a `TransactionManager.close()` which does some cleanup.
nit: "didn't went" -> "didn't go"
Just a nit, but the name of the api suggests that this could be called in other scenarios than closing. Perhaps we should rename the method to something like `abortPendingRequestsOnForceClose`. Or maybe just `close()`? 
This doesn't really work as a general pattern. What I mean is that for `ElectPreferredReplicas` there is no analog of `NewTopic`: The `AdminClient` API is just a `Collection<TopicPartition>`, so there's no handy class in which to put the conversion code. It might be worth having a consistent place to put these conversion functions (for AdminClient protocol messages, at least); either a single common class (`AdminClientProtocolUtil`), or a `ElectPreferredReplicasProtocolUtil` etc. The latter would have one benefit: With the use of nested classes there's the possibility for imports like `import org.apache.kafka.common.message.ElectPreferredLeadersResponseData.Result;` to collide when another protocol uses a nested class named `Result`, and using outer class qualified type names makes the code ugly. Having a `ElectPreferredReplicasProtocolUtil` etc would avoid this.
Maybe worth a sanity assertion before removal of the child sensor? Something like this: ```java assertTrue(metrics.childrenSensors().get(parent).contains(child)); ```
Maybe we could put this to `INFO` but change the message slightly to say "This could be an issue for IQ" or something along those lines. Just a thought.
I don't have a strong opinion on the log level `DEBUG` is fine with me.
@mjsax I think I'm sold on your arguments, let's keep them as WARN then :)
Nit: use `producer != null`
nit: these three can be package private
This test fails on the mac on this line as the path is not `/tmp` but starts with `/var/folders...` by changing the assertion to `startsWith("process-state-manager-test Failed to write offset checkpoint file to [` then the test passes
I guessed the npathcomplexity thing. :) My question is why we have a class instead of just a method.
The last commit added a bunch of indenting changes. We should revert them.
This should be called `topicExists` or something like that. `newTopicName` then becomes `topicName`. The variables inside the method should also be changed and the exception should include the variable, not a hardcoded value.
Seems like the indenting should be adjusted to the left, right? Applied to other changes in this file too.
We are not very consistent unfortunately. But I suggest sticking to the approach that was used previously in the file.
Maybe we should only look for "explicit partition assignment" in the message.
Nit: please use braces even for single-line if bodies
Also below, when calling `stateListener.onChange(state, oldState);` -- would we need to call `stateListener.onChange(newState, oldState);` instead? Otherwise, `state` could change before we do the callback because the lock is released already.
Why could we not fix this? The underlying issue seems to be the state transitions of `StreamThread` -- it allows to go from `CREATED -> RUNNING` -- if we change this, and we can only go to `CREATED -> PARTITION REVOKED` we should be able to tackle this issue? (Maybe a different PR to do this though.)
Why do we want to disallow calling `start()` twice? Could be idempotent no-op, too.
Preexisting, but should this be an error log? (seems to be implied by the text of the log)
This seems to defeat the purpose... If we really want to skip this test in this environment, we should rather put it in the beginning and do ``` if (isUnix) { return; } ```
```suggestion // This handles a tombstone message when schemas are enabled ```
```suggestion // This characterizes the production of tombstone messages when Json schemas is enabled ```
Please double-check the logic, but I think if `enableSchemas` is false, then we would still effectively return `SchemaAndValue.NULL`. So maybe we can leave `enableSchemas` out of this check? One other bit of (optional) cleanup. The validation in `jsonToConnect` below doesn't make much sense to me. We are already ensured that `jsonValue` has the proper structure. Since this is the only use, I think we can get rid of `jsonToConnect` and pull the couple needed lines into this method.
```suggestion // This characterizes the production of tombstone messages when Json schemas is not enabled ```
Previously the records were consumed after every poll. Now I think the intent is to treat the records collection as representing the backing log in Kafka. Is that about right? Assuming so, I wonder if we can make the representation a little clearer. We currently have separate collections for `beginningOffsets`, `endOffsets`, and `records`. Perhaps we can consolidate all of them. For example, in pseudocode, we could have something like this: ```java class MockLogData { List<ConsumerRecord> log; long startOffset() { return log.first.offset(); } long endOffset() { return log.last.offset() + 1; } List<ConsumerRecord> fetch(long offset) throws OffsetOutOfRangeException; } ``` Then we could replace the three collections with a single `Map<TopicPartition, MockLogData>`.
this pattern of ` if (shouldRecord) measureLatency(X) else X` is not very DRY. You have this same condition (shouldRecord) in several places, and the code for measureLatency(X) vs X is essentially copy-pasted. Instead, add maybeMeasureLatency(f, sensor) with `if (sensor.shouldRecord()) ... `.
since this wrapper just exposes a new constructor, consider making it a factory method instead
large and deeply nested method here -- recommend splitting into multiple smaller private methods where possible
same here -- sounds like CachingKeyValue with TimestampStore
recommend sticking with T, U, V (or A, B, C for higher-kinded) type parameters
We should add this API to the `Consumer` interface.
I think we'd want to use `updateLastSeenEpochIfNewer`. The provided epoch just gives us a lower bound on an acceptable leader epoch.
nit: there's some redundance printing both the offset and the OffsetAndMetadata. Maybe it would be most useful to print just the offset and epoch
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
unrelated nit: fix indentions above
Sure. We can address this in a follow up PR.
nit: rename to `processor` because this test uses only one processor (the numbering is confusing otherwise)
Same here and below
I'm not sure... This is in the constructor for KafkaStreams, so the only duplication it would save is if people ran more than one Streams instance in the JVM. Seems like kind of a low ROI for adding even more fields (with just one usage) to the object.
a big +1 for the rebalance metrics
Couldn't we pass the `host` to the constructor here as well and do the DNS resolution with the real host instead of trying to have the canonical hostname? (remove `getCanonicalHostName`)
Unused import - this is causing checkstyle failure in the PR build.
Perhaps it would be better to change host using reflection in the test since this code looks out of place in the implementation? Then the `host` field can also be made final.
visibility could be restricted to be package-level (i.e remove keyword `protected`)
Nit: should the method be named `testOptionsDoesNotIncludeWadlOutput()` instead? The point of this PR is to prevent including WADL in the OPTIONS output, but the existing method name makes it seem like we're testing the content of the WADL output.
this line is still a bit long... You could try a static import for `singletonList`.
Nit: seems a little weird to put the assignment operator at the start of this line instead of the end of the previous one.
I'm a bit skeptical for functions whose name need a number suffix to distinguish with others, since it usually indicates loss of generality :P Could the common logic be shared, or if not I'd suggest we still inline all of them.
Yeah, I'm not opposed to that. Another option is to use inline exclusions. I'd also say that having long test methods is not great and extracting verify methods is not a bad plan. However, having numbers in the name is bad. Typically one can group assertions in a way that makes sense.
Ah, this is unfortunate, but a good enough justification for me.
Ditto on this method, which also appears to be used just once.
I buy it. Thanks!
@mjsax justified this in other PRs... Since the exceptions are unexpected, and they would cause the tests to fail if they happened anyway, detailed throws declarations just pollute the test method declarations.
nit: I think it is not necessary to break to multiple lines.
Elsewhere as well, will omit below.
There's a heck of a lot of code inside the resource block of this `try`. The only thing that really needs to be there is the `new KafkaStreams`. Can we assign the config to a variable prior to the try block? I don't feel strongly about this, but it just seems a little harder to parse visually when there is so much code separating the `try` from the actual block.
Not sure if it makes sense to convert a `TimeoutException` into a `TaskMigrated` exception... Also note that we created https://issues.apache.org/jira/browse/KAFKA-7932, so it might be best to tackle all Streams related changes there? \cc @vvcephei
@huxihx I'd suggest we remove any Streams' related changes in this PR, since we've already created a separate JIRA (above) for tracking any follow-up works on the streams side, and @vvcephei is working on that already.
If we need both the `File` and `FileChannel`, I wonder if we may as well pass a reference to `FileRecords`. We can get to the channel using `FileRecords.channel()` and we can rely on `FileRecords.toString()` for a nicer representation in logs and exception messages.
Can `LogManager.getRootLogger().getLevel()` be `null`? With other loggers you return the effective level if that's the case.
Is this the format required by the mbean stuff? It might be nice to return something more structured here.
@wicknicks would be useful to have some unit test for this class.
this _technically_ changes the public interface and would require a KIP if we're being pedantic about the process. I personally think we can go by without a KIP but we obviously need a committer to say what they think
nit: What do you think about sprinkling a bit of docstrings in this interface while we're rewriting it? I like the original description of `An MBean that allows the user to dynamically alter log4j levels at runtime.`
This isn't a REST extension necessarily, right? It's also used by Kafka via JMX. I think mentioning `worker restarts` and `rest extension` might be confusing
What are your thoughts regarding returning the same `"No such logger"` value? It might be more informative to JMX users
Not sure I have great ideas for improvement, but this feels like a brittle test case. I wonder if we are just trying to handle too many cases in this test.
This call raises an exception, which means the assertions below never get checked and the producer is never closed. It is possible that this leak is causing some of the recent build failures. ``` 00:57:22 Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread "kafka-producer-network-thread | producer-20" ```
you can just compare the primitive longs, no need to do all this boxing to `Long` type
For window / session stores, there's no putIfAbsent function and hence no metrics would be registered.
Do we need to capture-and-rethrow here? Since the caller will always capture and swallow, this seems unnecessary to me.
We avoid using time-based operations in integrations since they usually leads to flaky tests. Consider using `TestUtils.waitForCondition()` with a timeout.
I think we may be able to simplify this and just use `canSendRequest`. If a metadata update is due (which is a rare case), we will still be prevented from sending. It doesn't seem like we get any benefit from having `leastLoadedNode` choose a different node which may have in-flight requests.
nit: Could we just call the above constructor here and avoid duplicating assignment logic? `this(startMs, durationMs, null)`
```suggestion List<String> retVal = new ArrayList<>(); ```
```suggestion if (r.startsWith(rule) || r.equals(defaultRule)) { ```
Nit: we don't normally use exclamation marks in Kafka log messages.
Nit: why reuse `appId` -- if there is a bug, and we set `APPLICATION_ID_CONFIG` by mistake, it would not be detected. Maybe better to set a different id.
Should we still log it, perhaps as a warning? If I understand the background, this case is unexpected except with 0.10 brokers, so it seems like swallowing it could mask an important condition.
If you decide to still log the condition, we could rehabilitate this test to check for the log.
Sorry for my denseness... Why are these "not re-assigned"? They're part of a data structure called "assigned tasks", which seems to imply that they are assigned.
Good point, since standby tasks do not have eos anyways today.
This method is not a public API. It's fine to delete it. Thanks for asking.
``` After each release ... one should first check that the released version has been uploaded to https://s3-us-west-2.amazonaws.com/kafka-packages/ which is the url used by system test to download jars; anyone can verify that by calling curl https://s3-us-west-2.amazonaws.com/kafka-packages/kafka_$scala_version-$version.tgz to download the jar. If it is not uploaded yet, ping the dev@kafka mailing list to request it being uploaded. ``` nit: `each any` -> `each`.
nit: extra lines
The only place we need this duration is ``` duration.negate().addTo(now); ``` `java.time.Duration` has `negated` but its `addTo` is taking a `Temporal`, and all its extends like `LocalTime` etc do not have the right API of `getTime`. So I think we just keep it like this.
We should remove the javax.xml.datatype.Duration usage since it's not part of the Java base module.
@mjsax LGTM Both `kafka/tools/StreamsResetter.java` and `kafka/admin/ConsumerGroupCommand.scala` should be migrated to remove usage of `javax.xml.datatype.*`
Yeah if it exists elsewhere let's just leave it as is for now.
I think this `if/else` block here can be removed and just go with ``` java for (final ProcessorNode child : children) { forward(child, key, value); } ```
+1 on assuming a single children, check-and-throw-otherwise
> Thinking about this once more, can a "global processor" have multiple children to begin with? Yeah, you could be correct about, I don't think it can.
There is a related JIRA about that but whether we'd keep it as is still open questions, I think we can make this assumption still atm but just bring it up FYI. https://issues.apache.org/jira/browse/KAFKA-7125
Let's remove on both side: I think in J8 it is not really a big difference.
Did @guozhangwang suggest to rename this DF to `2.2`? I actually think the descriptive name might be better. It seems like it'll be less work in the long run to remember what exactly is different about the different CFs.
:+1: It looks like the two-arg constructor is unused.
I'd suggest one optimization here, in order to save array-copying every time while we scan over the old CF: we keep the original key and value bytes for `no-timestamp` iterator, and only when we've already decided to assign from this iterator to `next` then we will call `getValueWithUnknownTimestamp` to do the bytes array copying.
if (comparator.compare(nextNoTimestamp.key.get(), nextWithTimestamp.key.get()) == 0) we need to advance on both ends while only returning the one from with-timestamp iterator, otherwise we may get duplicates returned.
nit: maybe we can make it just a general accessor that takes two parameters: `oldCF` and `newCF`? Or we can do this generalizing in the future if you'd like to hard-code for now.
Actually I was only suggesting to make the current Dual accessor to be more general currently it assumes the `old` to be `default`, and `new` to be `withTimestamp`. what I was suggesting is only to make these two parameterized; so that in the future we only have two accessor impls: 1) XX-CF only; which we already do in this PR. 2) XX-to-YY upgrade: old XX CF to YY CF upgrade accessor. All that being said, I'm okay with such refactoring as follow-ups.
I see. Do we guarantee that concurrent IQ will not see duplicated results with the db-accessor updating logic as well? If yes, we can save this check.
I don't feel strong about my suggestion either :) Will leave it to anyone who has a strong feeling here.
I don't feel strongly about it. If we enforce the "no null keys" invariant, then they are equivalent. It seems mildly confusing that we essentially have two different methods of determining when the iterator has run out of data. I leave it up to you.
should both iterators also be reporting `!isValid` here as well? I'm finding he rocksdb iterator api a little confusing... I guess if we never allow a null key into the store, then this is an effective way to check for the end of the iteration.
I looked at it closer. I still think it's better to split them out, but I also don't think it's a correctness issue right now, so I'd be fine with merging what you have.
+1 from me as well for putting both accessors into separate classes
Personally, I like the existing design. The `DualColumnFamilyAccessor` has to do a lot of extra checking that isn't necessary if there's just one cf to deal with. If we collapse them into one class with a flag, we pay for it with a lot of branching. One thing I did find confusing was reasoning about the fact that the Dual accessor is embedded in this (Timestamped) class, and the Single accessor is embedded in the parent (non-timestamped) class. But, we're using it as an accessor for this (the child) class. This seems unnecessarily convoluted, and it's a little hard to see if it's actually ok, or just coincidentally ok, since the parent and child APIs are only semantically, rather than actually, different. It seems simpler to understand if we pull both accessors out into separate classes that take `db`, `name`, `options`, etc as constructor arguments, rather than closing over protected state.
I think it is still possible. Here's one scenario: 1. last checkpoint at offset 100; all writes goes to old CF. 2. continue writes to old CF til offset 110, but no checkpoint written yet. 3. non-graceful shutdown happens, and upon restarting new CF is used. 4. we start restoring from offset 100 to log-end-offset 110, to the new CF. Now we ended with data of offsets 100-110 in both CFs.
nit: I'd suggest we remove this (and also the other default db accessor in the other class) class and call `SingleColumnFamilyAccessor(columnFamilies.get(1))`. Reason is that here we make the assumption that `withTimestampColumnFamily` (and `noTimestampColumnFamily` in the other class) is already not-null but that depends on the impl today. This type of style is a bit vulnerable to future bugs that cause NPE.
:+1: Thanks for doing it.
Makes sense. If it's static, then we could also extract it to a separate file, which might be better than having one of the siblings define a class that both siblings use. Also, it would reduce the LOC in this class, which is nice for readability.
Just following on my other idea about collapsing into a single class here: maybe instead of naming it as keyValueWithTimestamp, we just name it as: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 to now. And the flag can just be indicating if it is 1) or 2) above; in the future if we need to do this again we can then have: 1) "default" -> version 2.1- 2) "2.2" -> version 2.2 - 2.5 (just made that up). 3) "3.0" -> version 3.0 - now. etc.
Meta: this and the existing class still share a lot of common code; I'm wondering if it is possible to just add a flag to the existing class, based on which we can decide whether use Options v.s. DBOptions / CFOptions, and use RocksDBAcccessor.
If not, we should move the exception capturing logic inside the dbAccessor as well.
Now I see why we need to make `name` package-private now; think it is okay.
I cannot find in rocksDB docs whether newIterator.seekToFirst().isValid() is a safe / efficient way to check emptiness, could we had a unit test for this specific case? 1) starting the DB with empty cf at the beginning. 2) starting the DB with non-empty cf at the beginning. 3) same as 2), but delete all keys and restart the DB and check again.
The logic here is a bit over-complicated to me, can we simplify to sth like standard sort-merge here? We have e.g. `AbstractMergedSortedCacheStoreIterator` for a similar pattern.
I'd suggest try-catch each line separately since the underlying `RocksDBException` would not tell you which line actually went wrong, and this piece of info would be very useful for trouble shooting; ditto below.
We should update the tests for setting the `stateStore` and `recordConverter`
nit: line starts with `=`.
nit: this can be simplified to `LONG_DESERIALIZER.deserialize(rawTimestamp(..))`.
Some cleanup needed? Ditto elsewhere.
This seems not addressed? Line 122 below is inconsistent with this one.
I see, in that case it's probably fine.
I don't think limiting the pool to 2 threads is really what you want, since you have 3 runnables that you are creating later on. Check out some of the other newXYZThreadPool functions. There is at least one that doesn't limit the number of threads.
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
nit: extra new line
Can we put the worker ID into the prefix here? We may be running multiple workers at once.
This probably shouldn't be called "prepare", right? It is the main Callable here and we expect to stay in it for a while.
Earlier we discussed sending the task spec on the process' standard input. Did you decide not to do this? It seems like using a command-line option will be pretty awkward...
How about "runs an external command for the worker."
@cmccabe is right about the race condition I think. we should probably check that `controlChannel` is initialized here
It seems like the race conditions for `process` won't turn into any errors. It maybe is better to be explicit about the race conditions and have locks as to ease future maintainability of this task (especially by other contributors)
How about just ``` log.error("{}: (stderr):{}", id, line); ```
Let's call this "StdoutMonitor" since that makes it more clear what it is doing. We may want to pass things other than status eventually. Also, the instance variable is called `stdoutMonitor`, which suggests that this is a better description.
Please add the exception at the end of the line so that we can get a stack trace, including any "cause" exceptions. Example: ``` log.error("{}: Failed to start the external process.", id, e); ```
Please include the problem in the errMsg which is used to complete `doneFuture`. For example: ``` errMsg = "Failed to start the external process: " + e.getMessage(); ```
There is a race condition here. `ExternalCommandWorker#process` is used by the `ProcessMonitor` thread, and also here in a different thread. Similarly, `controlChannel` is accessed by multiple threads and could be be used before it is initialized. I think `process` and `controlChannel` need to be initialized and accessed under a lock. I would suggest something like: 1. take lock 2. check if process is null. if so then EXIT 3. create ControlCommand 4. release lock 5. call executor#awaitTermination for 1 minute 6. if the executor did not finish all tasks, then take the lock again, invoke process#destroy, release lock, call awaitTermination again
I think we should have a `log.info()` here to show the error. I'm afraid users might miss some error statuses once they get overridden, so it's good to have it persisted somewhere
How about just ``` log.info("{}: (stdout): {}", id, resp.get("log").toString()); ```
Suggest using `Executors#newCachedThreadPool` rather than manually counting threads. We don't need a `ScheduleExecutorService` here either... just a plain `ExecutorService` is fine.
Maybe something like "No command specified" would be more descriptive
Originally this was just intended as a sanity check. However, thinking about it more, it might be better to change it to a much longer period, or even get rid of it entirely. It should not be needed since there is a task `durationMs`, of course. That might make sense as a separate PR, since we should change all the workloads.
An error is intended to shut down the worker. So we should call `doneFuture#complete` here.
It seems a little harsh to set a fatal error condition because the process logged something on stderr. A lot of applications use stderr as an output. Maybe we should just log this with `log.error`.
yeah, in general the construction of commands could be cleaned up quite a bit for extensibility -- i think we tend to forget about the importance/value of doing this when writing tests in the Kafka repo, but these classes should be reusable and extensible. this approach seems pretty error-prone, although at least it should consistently fail if the quoting gets messed up since you'll have invalid extra args on the command line.
ok, I get that though I think that's just tech debt. for any test related files, we really shouldn't be using anything other than `PERSISTENT_ROOT` so that we can at least attempt to ensure each test/service gets a clean workspace
The variable name should be changed.
I think the versions of subscriptionInfor / assignmentInfo are coupled, so if we bump up on one side we should do the same on the other.
In the case of version probing, we send an old assignment back. If we encode the new `tasksByHostState` instead of `partitionsByHostState` wouldn't the instances that are not upgrade yet crash? Or course, to be future prove, eg, we bump the version from 5 to 6, it might be ok for a 5 to 6 upgrade to send the new encoding. We need to make the cut-off point base on the smallest version we received.
nit: should be removed (similar below)
Changing old version encoding would break our upgrade path, ie, all existing `encodeVersionX()` methods cannot be modified.
nit: move `new HostInfo("unavailable", -1)` to next line to get proper alignment -- also, we use 4 spaces (not 8).
Avoid reformatting in classed that do not have any change (please revert those classes).
Also I think the checkstyle will fail as well for this code style.
Please add `{}`, too
This function returns true if there is a non-empty intersection between topicNames and partitionForHost. You could reverse the iteration: iterate topicNames (since it is a List) and lookup into partitionForHost (since it is a Set and likely has O(1) lookup time). The overall complexity would be reduced: O(m*n) -> O(m)
It seems this still needs to be executed for `minReceivedMetadataVersion >= 2`
nit: newline for better IDE debugging.
There's no need to copy entry.getValue().sourceTopics into a HashSet just to iterate it. Later down you put individual soureTopic into a topicToTaskMap which already eliminates possible duplicate keys...
For compatibility, we cannot change any message format of old versions, so I think we need to bump up the version to 5 and only do compression at 5 while no compression at version 4. Think about the case: when a streams application with multiple instances are rolling bounce to be upgraded, maybe some instance are already on version 5 and hence sends the assignment back compressed while some other instance do not recognize this version at all. We need to distinguish the cases when to compress and when to not compress. @mjsax has done the version probing protocol and he can provide more.
I think we can just keep the logic to maintain the topic-partition set and not exposing tasks instead, the key is that in `rebuildMetadata`, we can still reconstruct the topic-partition set. What I have in mind is the following: 1) we remember the `final Map<TaskId, Set<TopicPartition>> partitionsForTask = partitionGrouper.partitionGroups(sourceTopicsByGroup, fullMetadata);` constructed from StreamPartitionAssignor at the `StreamsMetadataState`. This can be pass via the `assignor -> taskManager -> streamMetadataState`, i.e. at `setPartitionsByHostState` we pass in both `tasksByHosts` and `partitionsForTask`. 2) And then at `rebuildMetadata` we can get the `Set<TopicPartition>` from `taskId` directly.
I felt this refactoring is a bit messy compared to its benefits, so how about: ``` OutputStream baos = new ByteArrayOutputStream(); // first write the version as byte array directly before switch baos.write(version); // then wrap with compression if it is newer version if (version >= 5) { baos = new GZIPOutputStream(baos) } try (final DataOutputStream out = new DataOutputStream(baos)) { switch (usedVersion) { // ... } } ```
It's unusual to hold a reference to an abstract class like this. I believe the intent is to be able to transparently handle either `KeyValueSegments` or (I'm guessing) `KeyValueTimestampSegments`. The full expression would be to have a `Segments` interface implemented by `AbstractSegments`, which is then extended by your two implementations. Then this line would reference `Segments<S>`. It's fine to collapse this into just the abstract class (although questionable in the presence of package-protected fields). But to maintain transparency, I'd name the abstract class `Segments` instead of `AbstractSegments`. That way, to an outsider class (like this one), you're still just interacting with the interface (i.e., the public interface of the class), rather than specifically an abstract class. Adhering to this pattern leaves the door open in the future to extract `Segments` into a full interface without having to change any outside code (which is what I meant by maintain transparency).
Add @param for recordMetadata
This part and the line 525-529 below can be extracted out of if condition.
@dongjinleekr you could do something like: ```java final Serde<K> keySerde; final Serde<VR> valueSerde; final String queryableStoreName; final StoreBuilder<KeyValueStore<K, VR>> storeBuilder; if (materializedInternal != null){ .... else { .... } kTableJoinNodeBuilder .withKeySerde(keySerde) .withValueSerde(valueSerde) .withQueryableStoreName(queryableStoreName) .withStoreBuilder(storeBuilder); ``` Alternatively you can make them non-`final` and set the default values and update the variables inside the `if` condition, and simply remove the `else` block. It's a matter of personal style so I'll leave it up to you.
The "Swallowing the exception" is a bit alarming at first. It's true that if this method returns false then `TestUtils.waitForCondition` (which is using this method as the test condition) will fail at line 137. Pretty minor, but how about the following? ``` // Log the exception and return that the partitions were not assigned log.error("Could not check connector state info.", e); return false; ```
nit: make a constant for this value? helps to read and understand the time unit.
Thanks for the catch! It was my bad for not capturing it last time I modified this test case.
It's definitely overkill to lock the whole function body, right? I guess we only have one thread running this now, but it could be an issue if we want to have multiple threads run it in the future. Let's just replace the `AtomicLong` with a regular long, and use the lock to protect changes to the `unackedSends` variable.
volatile is not needed here, since this is only accessed under the lock.
We could probably add this to the errors on line 979 above. This is a "normal" error case, so I think we logging at debug level should be sufficient.
I think the metadata update may not be needed. `UNKNOWN_LEADER_EPOCH` means the consumer's metadata has gotten ahead of the broker, so we can just retry. The only thing I am not sure is whether we need additional backoff logic before retrying.
nit: I'd suggest use a constant instead of hard-coded `-1`: we can reuse RecordQueue.UNKNOWN e.g.
Hmm.. we are pealing off two layers here. The second layer is window-store-facade, what's the first layer of WrappedStateStore? Ditto elsewhere.
I guess in the end we will find these classes a better place (currently they are a bit scattered, e.g. `KeyValueStoreFacade` is in the materializer class).
Consider renaming to `safeToDropTombstones`: ```suggestion boolean safeToDropTombstones() { ```
Sounds good. Actually I've seen the same situation for our current caching layer flushing logic as well: e.g. `put(A, v)` -> `delete(A)` and both only hit the cache layer. When flushing we tried to read the old value and found its null bytes, so we know nothing was flushed for `A` and nothing written to downstream before so we can skip putting a tombstone to underlying store as well as downstream. For suppression buffer though, it is harder since you do not have an underlying store to fetch the old value, and of course reading the whole changelog to see if there's any updates on this key `A` costs you everything. But suppose we always have a persistent buffer, this may be an easier task.
Right. But note that the current dirty-key in cache is not enough determining if we have, **ever**, write for a key to the underlying store which is not deleted yet: dirty-key only contains the dirty-keys since last flush, i.e. the key not in the dirty-key is only a necessary, not sufficient condition. And that's why we can only consider not writing the tombstone if the read on this key returns null-bytes, indicating nothing was there.
No, that's fine. Already merged. Thanks! :)
nit: We don't use the 'get` prefix for getters, just `host()` to be consistent.
Somewhat more idiomatic to do: `print(json.dumps({"status": "running"}))`
While looking this up (hadn't seen newSetFromMap before), I found that ConcurrentHashMap has a `newKeySet` method which is basically equivalent to this. Not suggesting we use it, just thought I'd share
I think you should only do this when the version actually is published.
This one is not needed since this tool should always be executed from trunk.
This is s repetition of `"data"` case -- similar below -- we should put all int/long/double cases etc together to void code duplication using "case-fall-through" pattern.
We should pass in `record.topic()` instead of `""` into `deserialize`.
Just one minor point, if we don't have EOS enabled, we could end up pulling duplicates in some tests like the `StreamsUpgradeTest::upgrade_downgrade_brokers`
Nit: line too long
why use `topicName` here? Should it not be `failed: key=X actual=A expected=B...` instead of `failed: key=X <topicName>=A expected=B...`
Setting `num_kafka_nodes` to `replication` does not sound correct to me. The replication factor would be smaller than the number of node.
`num_kafka_nodes` is only actually used for the underlying check here, we are indeed already setting up the cluster as ``` self.kafka = KafkaService(self.test_context, num_nodes=self.replication, zk=self.zk, topics=self.topics) ``` below, and here it is more like "remembering the num_nodes here". As I suggested, instead of checking the sum of topics (and hence needing this parameter) we could be more strict and check that for each broker node, the listed topics include all the required topic names.
And replace this pattern on `StreamsUpgradeTest` as well.
nit: maybe we could add an example here like "e.g a configuration key was specified more than once"
Should we mention this in the KIP? It was not specified there - even though this is the same behavior as the old AlterConfigs, in the context of the KIP this non-transactional functionality isn't obvious I fear
One way to do it is to use 0 as the timeout in `joinGroupIfNeeded`. I tried this out locally and it seems to work.
A better test case would be to return the MEMBER_ID_REQUIRED error code. We could then skip SyncGroup and call maybeLeaveGroup. This is the scenario that this patch is trying to fix.
I think it is a bit confusing to call the generation valid if we are in the pending state. I am debating whether it is worth the complexity to add another `MemberState`, say `PENDING` for the state in which we have a memberId, but are not yet part of the group. Perhaps it would be sufficient to use something like this instead: ```java public boolean hasMemberId() { return !memberId.isEmpty(); } ```
How much effort would it be to have a test case for this? We have a few LeaveGroup tests in `ConsumerCoordinatorTest`.
This docstring could be improved. Perhaps we can just mention that we will may have a memberId before we are part of a group generation.
nit: `expectedErrorMessage` rather than `expectedErrorMessagePrefix`
Do we actually need two fields? From looking at the code, it wasn't clear to me.
> I think cleanest the way to do it backwards compatibly is to introduce a new store hierarchy and deprecate the old one. I am afraid that might be the only way to do it -- deprecation hell -- users won't be happy -- thus we should think hard if it's worth to do or not...
> Why does it do this? Maybe it's a translation layer for other stores? In which case, is it correct for Streams to second-guess the implementation and break its own contract by ignoring the marker interface and delivering non-timestamped binary data? I don't think that would work. Note, on restore, we always get the most inner store and would not call this "translation layer wrapper store" (and thus it would break as we would insert our converter and hand timestamped-bytes to the store that does not understand them). If one want to implement a translation wrapper like this, she need to "hide" it from Kafka Streams and not implement `WrappingStore` (ie, the translation wrapper must be the most inner store).
Cool, I will create a JIRA ticket for now to keep track of it.
`WrappedStore` should have a method `root()` (or similar name) that returns the most inner store.
You mean type? As mentioned above, if we pass in two types to a `WrappedStateStore` we can make it type safe.
I think, we should first check for this condition, because we should only check the most inner store -- if an wrapping store would (be mistake) implement `TimestampedBytesStore`, we would return `true` even if the most inner store does not -- this would be incorrect.
`innerMost = ((WrappedStateStore) innerMost).wrappedStore();` Even if I think, that a `WrappedStateStore` should be able to return the `wrapped` and `root` (ie, recursively wrapped store directly -- ie, there should be methods `wrapped()` and `root()`)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
`name()` -> `wrapped.name()`
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
This is ugly -- we should have access to the wrapped store directly via inheritance. (also below)
I think this should be `protected` to avoid calls to `wrappedStore()` in all child classes
Not sure about this -- why not add two generics to the store, one for "wrapped" and one for "root" and keep this method that return the root type? I would also rename `inner()` -> `root()`
`wrappedStore()` should `return wrapped;` -- that's why I argue for renaming the variable.
nit: sufficient to implement `NameSuppressed` now only
If it's an internal interface, might it make sense to `NamedSuppressed extends Suppressed` such that both internal sub-classed inherit only NamedSuppressed (ie, this insures that we don't forget to inherit both interfaces?) -- just a thought -- it's of course less compossible than.
nit: `If you wish to see fresher results, you can set it...` -> `If you wish to see fresher results, you can set it to a lower value..`
It's internal. So should be fine.
> That's correct. I may not have been clear above, but what I meant is that this change won't break compatibility with users currently **_not providing_** a repartition topic name as it will create multiple repartition topics thus keep their topology the same. Does that make sense? Ack. Just wanted to make sure we are on the same page :)
Don't insist -- would prefer though (for this case) -- in general, this pattern can be useful -- just don't see it for this particular case.
Thanks for renaming! Much better!
> and re-using the `KGroupedStream` results in an `InvalidToplogyException` when building the topology I thought, if there is no user topic-name, old code would create multiple repartition topics? And re-using `KGroupedStream` only throughs if there is a user topic-name (and this restriction is lifted with this PR)
Same with this, we are actually testing the deprecated method here, so we can justify the suppression.
It seems like we can migrate away from the deprecated method in this test.
This method also seems to be testing `get`, rather than `put(k,v)`, so maybe we can just migrate away from the deprecated method.
It actually seems like using the non-deprecated `put` method would improve this test's readability, since the verification depends on specific timestamps having been used in the put.
Heh, I actually just looked it up, because I was surprised all those other `<p>` elements were not closed... Apparently, even in HTML, you don't _have_ to close `<p>` elements: > Paragraphs are block-level elements, and notably will automatically close if another block-level element is parsed before the closing \</p\> tag. > https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p Sometimes, you just have to stop and marvel at HTML...
I think we may be able to remove this if we just initialize `nextSequenceNumber` to 0. Then we wouldn't need `hasSequenceNumber` as well.
I think this call might not strictly be needed. We cannot return to the INITIALIZING state after beginning any transactions.
By the way, do you think it would be safer to return `Optional<Integer>` and keep the use of the sentinel confined to this class. Same thing for `lastAckedOffset`.
nit: conventional in Kafka to drop `get`. Same in `getLastAckedSequenceNumber`
How do you feel about dropping `Number` from these names? This would be consistent with the methods we expose from `TransactionManager` itself (e.g. `lastAckedSequence()`).
One thing I was considering is whether we should be explicit about when we expect these entries to be inserted. We could raise an exception from `get` here in cases where we expect the state to exist. For example, for the transactional producer, we have `maybeAddPartitionToTransaction` where we could explicitly insert. For idempotent producer, probably the first time we try to access the sequence for a partition or something. Anyway, just a thought, but it might make the code a little easier to reason about.
Might help to have the partition in this message. Same thing in `add` below.
nit: seems we don't need this method anymore since `addPartitions` is idempotent.
Hmm.. In fact, there is not necessarily any relation between the partitions that are being consumed and those that are being written. Usually you would expect them not to overlap. I wonder if it actually makes more sense to track these offsets in a separate map.
nit: all the other collections are initialized in the constructor.
Hmm.. This is an interesting case. As far as I can tell, the state would only get reset if we are changing the producerId in `resetProducerId`. So the question is whether it is valid to associate the last acked offset of the old producerId with the new one? I suspect the answer is no. The last acked offset is used in order to try and detect spurious UNKNOWN_PRODUCER_ID errors, so I think our assumption is that this offset is associated with the producerId at the time of the request.
Alternatively, we could make this method idempotent. Seems like we only call it from `ensureHasBookkeeperEntry` anyway.
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
`toString` is not required.
I wonder if `task-thread-` is preferable since it's a smaller departure from the previous prefix that was `pool-<poolId>-thread-<threadId>-`. I'm a bit on the fence, but also not too important.
Thanks for the details. Should we instead do a "clean up" within `getWriteBatches()` and only return open segments? Seems to be a better separation of concerns.
Can you elaborate? Seems to be orthogonal to the timestamp fix.
nit: missing whitespace before `retries` (or after `and/or` the line above)
Sure. I'd suggest that around line 220 in RecordCollectorImpl, we catch 1) TimeoutException, and 2) RetriableException, and 3) ProducerFencedException, and 4) all other exceptions separately and log a different error message on those four cases. For 1) usually users did not override `retries`, so we'd just suggest users to tune `deliver.timeout` while keeping obsoleted `retries` with their default value, and for 2) we'd suggest users to check if they've overridden `retries` and if yes do not do that and tune `deliver.timeout` instead.
I'd +1 for hinting differently for TimeoutException and other retriables.
Thanks for the details. Seems I did not have full context.
Can you elaborate? Why is ``` You can increase the producer configs `retries` and `retry.backoff.ms` to avoid this error. ``` not suitable for a `RetriableException`? (With "current" I referred to the old/existing one, not the updated/new one.) As said above, I think it would be best to have two hints -- one for timeout, and one for all other retriable cases. Thus, we would add a second hint.
> What would be the hint for `RetriableException`? The current hint seem to be appropriate.
Maybe it's better to have two checks: first test for `TimeoutException` and if it's a different one, test for `RetriableException` second.
`TimeoutException` extends `RetriableException`. Thus, I think catching `RetriableException` is correct.
Nit: It's probably a bit better to have this be near the `setPollException(...)` method.
Nit: let's not add an unnecessary extra line.
Nit: remove the `:` after "type", since this forms a readable sentence.
Nit: it'd be better to avoid changing lines that don't need to be changed. Helps to keep the PR as small as possible.
This is good, but it may be more consistent to move the remaining lines in this method to another static method. That would make this `masked(Object)` method a bit easier to understand, too. If you add a new static method right after this method and use `value` for the parameter, the next few lines will remain unchanged (other than moving into a new static method).
This call to the replacement mapping function may result in an exception, for example if the replacement value of `foo` is used on a field of type `DECIMAL`. It is proper to throw an exception, but would we not want to wrap that in a `DataException`? And speaking of failed replacements, it'd be good to have more such conversion failures in the `testReplacementTypeMismatch` unit test.
Is it valid to have a blank string as the replacement value? If not, then the `replacement` config should have a validator that prevents using invalid values, and it probably would be good to succinctly describe the limitations in the doc string. And it may be better to only set `this.replacement` to a non-null string that should always be used. This would centralize the logic of determining whether it should be used in one place, and line 135 becomes a lot simpler and more efficient: ```suggestion if (replacement != null) { ```
There are lots of occurrences of `checkReplacementSchemaless(singletonList(`. Perhaps it would make sense to create a second `checkReplacementSchemaless(String replacement, Object value)` that returned `checkReplacementSchemaless(singletonList(replacement), value)`, and then most of these lines could be simplified a bit. Same for `checkReplacementWithSchema(...)`.
Nit: for better or worse, our convention is to use `test` as a prefix for the test methods, so this would be: ```suggestion public void testSchemalessWithReplacement() { ```
Maybe: > Mask specified fields with a valid null value for the field type (i.e. 0, false, empty string, and so on). > For numeric and string fields, an optional replacement value can be specified that converts to the correct type. ```
It's a nuanced topic. It would be good to clarify if we can find a good way to say it. The operator itself never processes data out of order. It always respects the order defined by the topic. But upstream repartitions don't provide any guarantees about the order they _write_ to the topic. This is how data can logically become out of order, even if users take care to populate their input topics strictly ordered, which might in turn violate some expectations with `null`s in the picture. I'm concerned that if we start saying, "Kafka Streams processes data out of order", with no context, it'll create FUD. We should be up front about the limits of the system, but it doesn't benefit anyone to create the impression that we don't guarantee things that we actually do guarantee.
Should we mention the "problem" with out-of-order data for this case? Should we ever recommend to _not_ return `null` ? We had a discussion at some point to actually disallow returning `null` because a "delete" is not a valid aggregation result.
How about this: `If the reduce function returns null, it is then interpreted as deletion for the key, and future messages of the same key coming from upstream operators will be handled as newly initialized value`.
nit: `was` -> `were` (also below)
In other words, I'm recommending that we specifically say something like "Producing deletes from your aggregations may cause unexpected results when processing dis-ordered data. Streams always processes data in the order it appears in the topic. If the topic is populated out of order, you may have late arriving records, which can cause records to become unexpectedly re-created after they have been deleted. Out-of-order data can be a problem for non-deleting aggregation functions as well, but it's especially surprising with aggregations that produce deletes." :/ ... you see what I mean by saying that it's a nuanced topic.
Fair point, I don't have an issue with it annotation itself, but maybe we are better off with not using an IDE specific value. Just my 2 cents though, I'll go with the majority on this one.
I think both tables at line 319 and line 321 are materialized anyways even without the `Materialized` object: for 319, the `groupBy` operator would cause its parent to be materialized.
Huh. Bummer that you have to make it queriable in order to use the in-memory store. I never noticed that before.
Frankly, I am not sure if it makes a difference. Leave it up to you.
Just to pile on other testing ideas, we can verify an exact type bound by checking assignability from both sides (since assignability is `<=` for types). Like `store.getClass().isAssignableFrom(RocksDBStore.class)) && RocksDBStore.class.isAssignableFrom(store.getClass())`. But I'm also in favor of doing this in a follow-up, since the fix itself looks "obviously" correct.
We we improve this test class further (this method is ok I guess). Methods below for example: ``` @Test public void shouldBuildKeyValueStore() { final KeyValueStore<String, String> store = Stores.keyValueStoreBuilder( Stores.persistentKeyValueStore("name"), Serdes.String(), Serdes.String() ).build(); assertThat(store, not(nullValue())); } ``` Only check for not-`null` what seems to be a poor verification. (Maybe others can be improved, too).
Can we retain Set type? maybe required to eliminate duplicate mechanisms.
I found out recently about the fancy pants `assertThrows` api. It seems like a nicer pattern because you can localize the block of code that should be raising the exception. As an aside, are there any assertions on the state of the coordinator falling raising this exception? Maybe at least we could verify the assignment in `SubscriptionState` is empty. I think generally this should be viewed by users as a fatal error.
nit: `toString` can be used implicitly
Fair enough. So basically if we have reached an illegal state, the user continues at their own peril. :wink:
nit: You can use the `topic(...)` helper function instead of `Arrays.asList(...)`. See examples in other unit tests.
yeah, seems valid. we already chop off whatever we don't need in `docs_version`, I guess just leftover from the `0.` days...
Do we need this new config? According to the ticket, we want to used `KafkaStreams#close(Duration timeout)` and "forward" it to the `Producer.close()` call.
Wouldn't this be a bit cleaner? ```suggestion private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); ```
Ah, nevermind, the topic is in the message's `toString()`.
nit: parameters on a separate line
nit: parameters on a separate line
At a high level, our store ecosystem looks like an onion. On the outside, we have a <K,V> store, and on the inside, we have a <bytes,bytes> store. All the layers in between have different responsibilities, like changelogging, caching, add metrics, etc. The nice thing about this PR is that it gives us one clean layer that's responsible for the transition `<K,V> <=> <bytes, bytes>`. When we need to look at the de/serialization into/out-of the stores, we have exactly one place to look. The prior code did mostly this, but to accommodate cache flushing in conjunction with the fact that the cache layer is below the transition from objects to bytes, we had to poke a hole in the onion and tell the caching layer (a bytes layer) how to deserialize. So, then there were two layers that independently know how to de/serialize, and the onion had a hole in it. This idea to move the serialization out to the TupleForwarder is basically the same, but in the opposite direction. Again, there are two components that need to perform serialization (the serialization layer and the tuple forwarder), and again, we need to poke a hole in the onion so that the tuple forwarder can communicate directly with an inner layer. It's not always practical to go for a "pure" design, but if readability is the goal, then it seems like we should try to avoid mixing layers, unwrapping layers, etc. as much as possible. To be fair, this is just my take on the situation.
nit: I'm wondering if just using would suffice (IMHO slightly easier to immediately grok the meaning). ```java if (!cachingEnable) { context.forward(key, new Change<>(newValue, oldValue)); } ```
nit: `cachingEnable` -> `cachingEnabled`
To be more strict, should this be `keyFrom == null && keyTo == null`? I know that today they either are both null or neither are null, but it is future-risk vulnerable if we ever change the apis to allow null values on one side.
Understood. Thanks for the clarification! Yea, that's a tricky aspect of nullable fields with defaults -- how to determine whether the value is explicitly null or has been defaulted to null :)
Would it make sense to push this down? ``` } else if (tolerateMissingWithDefaults && fields[i].def.hasDefaultValue) { objects[i] = fields[i].def.defaultValue; } else { ```
To clarify, what I was suggesting is this: ``` if (buffer.hasRemaining()) { objects[i] = fields[i].def.type.read(buffer); } else if (tolerateMissingWithDefaults && fields[i].def.hasDefaultValue) { objects[i] = fields[i].def.defaultValue; } else { throw new SchemaException("Missing value for field '" + fields[i].def.name + "' which has no default value"); } ``` I was just trying to consolidate the logic, but it's not a big deal. By the way, since you drew my attention to it, it might be worth catching the `SchemaException` that we throw here and and raising directly rather than wrapping it in another SchemaException.
Should we set the default value here or let it be null and handle the defaults in `Struct#getFieldOrDefault`
Nit: ```suggestion * consumes and counts records. This class provides methods to find task instances ```
all the usages of this method are of the form: `assertWorkersUp(..).orElse(false)`. Should this method simply return `false` when it fails to determine if all workers are running.
would `RECORD_TRANSFER_MAX_DURATION` be a better name here? `RECORD_MAX_DURATION_MS` seemed to indicate that we are recording the max duration.
nit: extra newline
nit: extra newline
I think we can just use `NumberFormat` and avoid the cast below.
This is two methods calls, thus it should not be a one liner (test would pass if `getStateStore()` would throw `UnsupportedOperationException`. Should be: ``` @Test public void shouldNotAllowInit() { final StateStore store = globalContext.getStateStore(GLOBAL_STORE_NAME); try { store.init(null, null); fail("Should have thrown UnsupportedOperationException."); } catch(final UnsupportedOperationException expected) { } ``` Similar below.
might want to highlight the `balanced` pieces below here, that's ultimately the critical bit beyond just running what we expect (which tbh, makes the most important part of these tests hard to find)
just be wary that this entire loop capturing state is a bit dangerous. of course you don't expect it to happen, but it's possible there is a rebalance (unintentionally due to timeouts) and you get some inconsistent set of results wrt connector state/location. at a bare minimum, tons of repeated tests locally and many repeated tests on jenkins would be warranted here to avoid any potential flakiness, especially given AK jenkins' penchant for unexpected timing issues.
this is the kind of assertion that could become flaky given incremental population of `connectors`.
This was probably unintentionally reduced to debug level while fixing conflicts.
Already mentioned, but I think we can be smarter about caching some state to avoid unnecessary work here. Validation is only needed if we do an unprotected seek or a metadata update arrives. Probably this can be left for a follow-up. It's only a concern when the number of partitions and the poll frequency is high.
This is interesting. There is always a race with the next leader election for a valid position. Do you think we need to be strict about the timing? I guess if you provide an epoch in seek(), this would be a good way to force validation before fetching.
nit: unneeded newline
That does not sound right. If we throw a `StreamsException` the thread will die.
nit: fix indention (move each parameter to own line)
Not important -- just look funky to me. We concatenate multiple strings and thus all should have the same indent -- why would we indent the first differently? I would format as follows (even if I know that you don't like that the plus goes into the next line): ``` "string1" + "string2" + "string3" ``` This avoid the "ambiguity", of multiple string parameters, vs one concatenated parameter: ``` method( "param1", "param2", "param3", "param4"); // vs method( "param-part-1" + "param-part-2" + "param-part-3", "new-param"); // vs method( "param-part-1" + "param-part-2" + "param-part-3", "new-param"); ``` Thirst and second hard hard to distinguish (where do parameters start/stop), but third makes it clear, that it's two parameters but not one or four, what is hard to tell in the middle case. Of course, double indent also fixes this but it's weird to me: ``` method( "param-part-1" + "param-part-2" + "param-part-3", "new-param"); ```
I don't know the IDE setting -- this case is rare enough that I "fix" fit manually if it happens.
Let me know if you plan to address or ignore this -- I am fine either way.
Of course we can then remove the whole `disableAutoTerminate` thing from StreamsSmokeTest.
hey @vvcephei I'm wondering if we can simplify the code further: with `timeToSpend` now can we just remove autoTerminate? 1) SmokeTestDriverIntegrationTest.java: we are sending 1000 * 10 = 10K records only, so likely that will stop even before the second instance can be started, similar to the issue you observed in system test. I'd suggest we just set `timeToSpend` to 10 seconds so we have some enough time to start up to 10 / 1 (sleep time) = 10 instances. 2) As for StreamsSmokeTest itself, we only disableAutoTerminate in three of StreamsUpgradeTest cases, since we do not need to verify the number of records sent / received but only check upgrade completed. We can also use 30 seconds (not sure if it is sufficient, but we can get on average how much time those three tests will take from our nightlies), and even if the test completes before that it will call `driver.stop` to force-stop the driver anyways.
@rajinisivaram I would keep this method name unchanged because the limitation really is a hard-coded 1 second as per https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java#L530. Other than that, though, LGTM.
@C0urante, what do you think about doing something like the following instead of the `instanceof` block? ``` Integer rebalanceTimeoutMs = config.getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG); if (rebalanceTimeoutMs != null) { herderRequestTimeoutMs = Math.min(herderRequestTimeoutMs, rebalanceTimeoutMs.longValue()); } ``` This still uses the `DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG`, but otherwise is simply checking for the existence of that property to determine if `herderRequestTimeoutMs` should be set to a new value.
Could we have a separate test case which covers the `forceClose()` path. We can assert that we do not wait for the producerId and all pending sends are aborted.
Ah, this is a good point. I was thinking that downstream processors were directly manipulating the record context, but if they do that, it's clearly their responsibility to restore it when they're done. I think I won't bother with a ticket right now, it seems like the mutable-headers thing is actually just part of the key and value also being mutable. Plus, there's a bunch of other stuff that's not well defined about header handling in Streams. It seems like altogether too much to scope in a ticket without investing some serious design work up front. I think the current change is fine as-is. Thanks!
You should use `assertThrows` instead. Otherwise we need a `fail` after `close()` to make sure we actually raise an exception.
Is the change in behavior intentional? `SecurityException`, which was previously caught and logged, is an instance of `RuntimeException`.
Since we have several things that all need to be closed, perhaps we could use `ClientUtils.closeQuietly`? Maybe something like this ```java try { for (String id : connections) close(id); } finally { AtomicReference<Throwable> firstException = new AtomicReference<>(); closeQuietly(nioSelector, firstException); closeQuietly(sensors, firstException); closeQuietly(channelBuilder, firstException) if (firstException.get() != null) throw firstException.get() } ```
I think we can remove the mention of the rebalance listener since it is not invoked in this context.
This logic looks a little different. The "group subscription" is a little hard to understand. It's intended to be the union of the subscriptions of all consumers in the group. When we change the local subscription, we should still keep the subscription from the rest of the group. Perhaps separately we can consider how to simplify this bookkeeping.
Yeah, I have a slight preference to just lock `SubscriptionState` every time since it is the simplest option. I don't think contention is a major problem since there's only the heartbeat thread which is sleeping most of the time. Unless there's some reason to think the cost of lock acquisition itself is a concern.
Hmm, but we're not actually calling the listener here. We do that separately.
I think we should try to optimize for the simplest locking that is possible even if it is unnecessarily coarse. `Metadata` is a good example where it might be possible to do finer locking, but it's not really worth the effort. Otherwise we need to put a lot of effort into understanding current usage and making sure we don't break locking expectations in the future. There is really only the background heartbeat thread which is contending for these locks, so I'm not too worried about lock contention in general. > Also we can say that we don't provide synchronization for listeners (like ConsumerNetworkClient), it seems pretty independent of the other data in there anyway. Your alternative is also good, honestly I'd be good with either :). That's fair. As long as we can invoke listeners outside of the lock, it's probably ok. I think I'd still like to get rid of the listener because it makes the code harder to understand, but perhaps we can treat this as a separate issue.
Actually I'm not sure why we need this additional block if we are already holding the lock. The predicate wouldn't be used after this method returns.
We're returning the collection directly here which violates the synchronization. Actually there's some inconsistency between `groupSubscription` and `subscription`. The latter is effectively immutable in the sense that we do not update the set once it is created. Perhaps we can do the same for `groupSubscription` which would fix this problem.
just `name` should be fine
We should annotate `withName()` as `@Override`
Just thinking about is once more: why do we need to make this interface public? We have `Named` as public method to use `NamedOperation` and other public control objects (`Consumed` etc) that implement it -- but I actually think, users don't need to know about this interface? \cc @fhussonnois @bbejeck @guozhangwang @vvcephei @ableegoldman
nit: `this method will....`
nit: maybe something other than `internalName`? `baseRepartitionTopicName`? I don't have any great suggestions ATM
+1 to @vvcephei's suggestion here.
nit: `remove` -> `removed`
Was the intention here to avoid the deprecation warning? If so, you can just call this method `name()` and do this: ```java @Override @SuppressWarnings("deprecation") // TODO remove this when {@link Joined#name} is removed public String name() { return name; } ``` Callers won't see the deprecation warning as long as they access the method via a `JoinedInternal` and not a `Joined`.
While I'm in favor of code re-use, in this case, the code in `Topic.validate` is not too large and could be easily ported to a `Named.validate` method. By doing so, Kafka Streams can change naming rules as needed. I realize that `Materialized.as` uses`Topic.validate` to validate the name of the store, but I'd suggest updating to use `Named.validate` there as well. NOTE: If we do this, we'd need to update the KIP EDIT: Actually I'm not sure we'd need to update the KIP as most likely this method would not be publicly accessible.
nit: we can just throw a `TopologyException` exception here.
According to the KIP `Printed` should add a `Printed.as(final String name)` method
nit: avoid `this` if not required.
depending on the decision regarding naming sources for `builder.table` we'll need to update this test.
@fhussonnois thinking about this some more, what is the motivation for doing a validation here for processor names? When Streams starts up the `AdminClient` will attempt to create any internal topics and the full topic names are validated at that point, so we don't need this check up front. \cc @guozhangwang
You have a point about checking via unit tests, but when the `AdminClient` attempts to create the topic, the check is against the full name. The check here only looks at the user-supplied name, which is only part of the topic name so that we could have a situation, although admittedly rare, where the unit tests pass, the full name fails. So I'm inclined not to have the check here or in the `Materialized` class. Let's see what others think. \cc @guozhangwang @mjsax @vvcephei @ableegoldman
That sounds good to me, and if I recall correctly we had similar reasons for not adding `as` to `Suppressed`. \cc @guozhangwang @mjsax @vvcephei @ableegoldman
Thanks for this reformatting to make code readable on Github!
nit: avoid double blank lines
nit: move first parameter to next line, too
Why do we need to call `build()` here? (similar below)
nit: insert space `String... expected`
To re-start this thread, I also feel like we should have our own check for Named operations to use. * we may want to make operation names more restrictive than topic names, for example to prevent collisions with automatically-generated partition names * the topic validation throws an exception that mentions the name is "an invalid topic name". This statement is nonsense if I'm naming an operator. We should throw an exception that says it's an "invalid operator name" or similar.
nit: move parameter to it's own line (same below)
nit: move `topology.globalStateStores(),` to next line.
nit: it is naming a source node, not a processor node. -> `"source"`
We should fix right away -- otherwise it might slip.
nit: use `"table-source"` ? It's naming a source node, not a processor node.
Nit: remove unnecessary spaces (same below)
appending `KTable` source operators with `-table-source` is not in the KIP, so we'll either need to remove this or update the KIP
SGTM. If we update the KIP, we should send a follow up email to the VOTE thread summarizing the changes (we can do this after all PRs are merged in case there is more)
I'm ok with the names, but I don't have a strong opinion. We still have time to address between now and the final PR though.
Not sure about this test the title says `shouldUseSpecifiedNameForGlobalTableSourceProcessor` but it's asserting the names of state-stores. But we can fix this in one of the following PRs.
Nit: add `@Overwrite`
nit: remove empty lines
Because `Named#name` is not `final`, it is not guaranteed that `EMPTY` will have an `null` name (one might call `#empty()` and modify it) -- seems to be a potential source of bugs. Can we instead remove `EMPTY` and return `new NamedInternal()` in `empty()` each time? It's not on the critical code path, so should be fine.
Other classes implement this as: ``` this.processorName = name; return this; ``` Why the difference? I we think that using this pattern to guaranteed immutability is better (what might be a good idea), we should consider to rewrite _all_ code -- of course, if a separate PR). I cannot remember atm, why we did not implement similar method immutable? Can you remember @bbejeck? We introduced this pattern with KIP-182.
I am personally a little confused what `orElseGenerateWithPrefix` means? It's a personal preference, but I don't think it's easy to read. Similar for `suffixWithOrElseGet`. (Maybe it's just me, being not use to fancy Java8 constructs that are mimicked here...) Curious to hear what others think.
nit: `e` -> `fatal`
Ack, I get it now. Thanks for clarifying.
nit: avoid `this` if not necessary
nit: we use 4 space intention
I think we need to use some suffix because otherwise we would generate two names, ie, end up with a naming conflict -- problem is, that a `KTable` results in two processors and we need a name for each.
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
```suggestion * is {@code null}, no records are emitted. ``` Please also fix this on the original method
```suggestion * {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
Please also fix this on the original method (line 1739)
```suggestion * @param named a {@link Named} config used to name the processor in the topology. ``` Or something similar, it just reads a little bit on the terse side, which I think would confuse people initially. Please also apply this feedback to all the other param docs.
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
nit: add opening and closing curly braces for all the `if` / `else` blocks on lines 1102 to 1108
nit: should be `named` can't be null
nit: add `{@link Named}` here and elsewhere below
Can we just inline `doFlatTransform` now? There's no need to have 3 layers of indirection to build these processors.
```suggestion builder.addGraphNode(streamsGraphNode, transformNode); ```
```suggestion final JoinedInternal<K, V, VO> joinedInternal = new JoinedInternal<>(joined); ```
@fhussonnois can we move the logic from here to the `NamedInternal` to always generate the name to ensure we increment the counter for backward compatibility? Something along the lines of ```java private String orElseGet(final Supplier<String> supplier) { final String generatedName = supplier.get(); return Optional.ofNullable(this.name).orElseGet(() -> generatedName); } ```
Any idea why these processors changed order? It could indicate a deeper problem.
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
same for the store
nit: change `mapValue` to take a lambda vs. an anonymous `ValueMapper`
We didn't specify this in the KIP but I think `toStream()` and `toStream(mapper)` should also have overrides with`Named`
Yep. Thanks, @bbejeck !
should this have a suffix at all? I don't think the `funcName` requires one. If it does, at a minimum we should not use `-sink` as it's used above and doesn't accurately describe its role.
Would this result in a different name for the source than the prior code? (Not sure if it matters...)
nit: add `a {@link Named} config`
nit add `a {@link Named} config`
nit: add `a {@link Named} config`
This should not have the `sink` suffix
Sorry, I meant `JoinGroupResponseMemberSet`. Anyway, a list is fine too-- I don't feel that strongly about it.
nit: line too long
nit: line too long
nit: line too long
Should we keep one operator per line to make potential stack traces easier to read? Also simplifies setting breakpoint.
Nit: `Note, this maximum number offsets` is redundant right? Seems like "Only used in ListOffsetRequest v0" would be a more concise version and achieve the same thing. Same applies for every similar instance in this PR.
"// will be removed after these versions are deprecated" seems wrong and we should probably remove it.
It's not thread-safe without the `synchronized`, since we're accessing variables that need synchronization. But let's move the `synchronized` keyword into `ConnectionStressWorker#updateStatus`, as described above, since that's a bit cleaner
This function should be synchronized since it accesses totalConnections, totalFailedConnections, etc. which are only ever accessed under the lock.
nit: Provide a message to the IllegalStateException constructor
nit: use `RecordQueue.UNKNOWN` instead of magic number so that if we've added negative timestamp support in the future we can just change one place.
Just to follow the question above, could we directly restrict the range at this caller as: ``` (Math.max(earliestSessionEndTime, currentSegmentBeginTime()), Math.min(latestSessionStartTime, segmentEndTime)) ```
nit: Provide a message to the `IllegalStateException` constructor
Although the constructor was pre-existing, I'm thinking we could clean things up a little bit by adding a constructor ```java public TableProcessorNode(final String nodeName, final ProcessorParameters<K, V> processorParameters, final StoreBuilder<KeyValueStore<K, V>> storeBuilder) { this(nodeName, processorParameters, null, storeBuilder); } ``` Then it's more clear in the code when we call ```java final StreamsGraphNode tableNode = new TableProcessorNode<>( name, processorParameters, storeBuilder ); ``` And we can leave the existing constructor using all 4 parameters alone.
nit: remove `this` here and line below
Ah got it, my bad :)
`KafkaStreams` is AutoCloseable now so you can include its construction inside the `try` block. Ditto elsewhere.
nit: move `UUID...` to next line
I was curious and still have Java7 installed and could reproduce the issue. The lists are not sorted here and thus the check fails. The reason is the following exception: ``` java.lang.UnsupportedOperationException at java.util.concurrent.CopyOnWriteArrayList$COWIterator.set(CopyOnWriteArrayList.java:1049) at java.util.Collections.sort(Collections.java:159) at org.apache.kafka.streams.integration.RegexSourceIntegrationTest$TheConsumerRebalanceListener.onPartitionsAssigned(RegexSourceIntegrationTest.java:445) ``` Seems `CopyOnWriteArrayList` cannot be sorted.
Could also purge the local state before tests, just in case.
How about adding a bit of detail about how this works and behaves, including: * what do variables look like? * how the map of properties for config providers are used? * what are the config provider properties (probably warrants a separate paragraph and list of known properties)? * what happens if the config provider properties don't define the config providers? * what happens if the config provider properties are provided in the `originals` rather than the `configProviders` map (or rather `configProviderProps`)? I also think because this class is subclassed a lot, it's worth being a bit more explicit than we typically are in the `@param` to say whether the parameter can be null. For example: ``` * @param definition the definition of the configurations; may not be null * @param originals the name-value pairs of the configuration; may not be null * @param configProviders the map of properties of config providers which will be instantiated by the constructor to resolve any variables in {@code originals}; may be null or empty * @param doLog whether the configurations should be logged ```
Could store `entry.getKey()` in a local variable since it is used several times
unnecessary type in constructor, can use `new HashMap<>()`
`instantiateConfigProviders` since this is potentially creating multiple providers
`instantiateConfigProviders` since this is potentially creating multiple providers
`original` => `originals`
unnecessary type in constructor, can use `new HashMap<>()`
Looks like this constructor was removed? We can't remove public constructors from a class in the public API.
This for loop is pretty similar to one in the `resolveConfigVariables(...)` method. Can we extract to a static helper method? I think it would also help make things a bit more clear, too.
Remove unnecessary `toString()` once the Entry type is set above. Other similar cases below too.
Map.Entry<String, String> to avoid the check below
We pass this map to `resolveConfigVariables`, which does: ``` providerConfigString = (Map<String, String>) configProviderProps; ``` That is unsafe if we are supporting props that are not actually a map of strings.
Then the log message should state the reason (e.g., the class was not found) and that the named provider will not be used. However, it does seem strange that this is technically an invalid configuration, so why would we not just throw a ConfigException? Note around lines 723 how any configuration property whose value is a Class name will be invalid if a `ClassNotFoundException` is caught, and this leads to a `ConfigException`. Silently ignoring seems at best inconsistent but at worst a bad idea.
Nit: include brackets on all blocks, even if they are a single line. This makes it easier to read, but also reduces the number of affected lines if we have to add another line in the block in the future.
We should specify whether the keys in this map have the `config.providers.` prefix, like they would in the `originals`.
Again, `ConfigProviders` doesn't make sense here. The `config.providers` property is listing the _names_ of the ConfigProviders, whereas the other `config.providers.<providerName>.class` (for example) specifies the name of the `ConfigProvider` implementation class for the named provider. IMO, we should use the term `ConfigProvider` to mean one of two things: 1. The name of the `ConfigProvider` interface 2. The instances of the `ConfigProvider` implementations that are instantiated by this class. Always using "instances" in these cases will help disambiguate the use of `ConfigProvider`. Also, it's probably worthwhile to begin this paragraph with: > The "{@code config.providers}" configuration property and all configuration properties that begin with the "{@code config.providers.}" prefix are reserved. The "{@code config.providers}" configuration property specifies the names of the config providers, and properties that begin with the "{@code config.providers.<providerName>.}" prefix correspond to the properties for that named provider. For example, the "{@code config.providers.<providerName>.class}" property specifies the name of the {@link ConfigProvider} implementation class that should be used for the provider.
"If ConfigProvider is not provided" --> Strictly speaking, the parameters don't pass `ConfigProvider` instances, so this sentence is not very accurate. We need to be very careful about the terms we use, and we should not use the term "ConfigProvider" when we really mean "ConfigProvilder properties".
Nit: this is really primarily where the configuration properties go, plus any optional config provider properties. How about: ```suggestion * @param originals the configuration properties plus any optional config provider properties; may not be null ```
This block may be a bit easier to follow if the if-else block sets up the Map of properties that will be used for the config provider properties, and then instantiate them in one place. Something like: ``` Map<String, String> providerConfig; if (configProviders == null || configProviders.isEmpty()) { providerConfigs = indirectVariables; } else { providerConfigs = mapValuesToStrings(configProviders); } Map<String, ConfigProvider providers = instantiateConfigProvider(providerConfigs, originals); if (!providers.isEmpty()) { ... ``` I know the if-else could be simplified further, but I think this is simple to follow.
nit: rename -> `instanceStateShouldTransiteToErrorStateIfAllThreadsAreDead` Also, just throw `Exception` for simplicity.
I pasted the code from `skip` which is called from `skipBytes`. I know we don't call `readFully`.
According to the result of CPU reduction, the skip operation does not reduce data decompression, but it reduces CPU consumption. Is a large amount of CPU consumption in the GC pressure @ijuma
`skipBytes` doesn't avoid decompression though, the `readBlock` call below decompresses the buffer: ```java class: KafkaLZ4BlockInputStream public long skip(long n) throws IOException { if (finished) { return 0; } if (available() == 0) { readBlock(); } if (finished) { return 0; } int skipped = (int) Math.min(n, available()); decompressedBuffer.position(decompressedBuffer.position() + skipped); return skipped; } ``` We do avoid GC pressure though.
Instead of this, I think people should just use a separate method to iterate the simplified version.
Thanks for the clarification.
Need to change ">=" to ">" in the exception statement here.
Thanks for the background @rajinisivaram. By the way, I noticed one of the checks is defined inconsistently: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java#L344.
nit: we try to avoid wildcard imports.
`ConsumerConfigs.MAX_POLL_INTERVAL_MS_CONFIG` default value is 5 minutes, so I think it is not necessary to further decrease it to 1 minute in Streams; maybe we can just remove this override to defer to Consumer's default value? BTW we should have a tiny KIP for this default value change.
This type is not parameterized. It's generally better to list the parameters when you reference a parameterized type.
nit ```suggestion private static final long RETENTION_PERIOD = 10_000L; ```
nit: ```suggestion private static final String storeName = "InMemorySessionStore"; ```
missing coverage on: * expired segments * retention time * fetchSession, which doesn't find a session * fetch
Suggest just using "return" here. Then you can have the loop be a regular for loop.
IMO, this would be a bit cleaner with a separate executor with a single thread for the status updater.
We use milliseconds everywhere, so this can be confusing.
The argument that `NullPointerException` is not appropriate holds for almost all cases when `requireNonNull` in use. Thus, I would prefer code consistency and use `requireNonNull` here, too.
nit: move `key` to new line
If I understand correct, a record read from the changelog topic should only be: 1) having no headers at all (old version) 2) having a singleton header with `v --> byte`. All other cases should indicate a bug in the code. So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value: 1) byte == 1: do as below. 2) otherwise: to not support forward compatibility, we can just throw unsupported.
- It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm. - And we know that KafkaStreams never writes headers into changelog topics. Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case. We can also do a version check as suggested by Guozhang.
Ack. So we don't need it, but we want it :)
I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content. Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true; it's guaranteed that there is a headers object. Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.
It's not super-important, but it probably makes sense to set -1 as the default for `ProducerId` in `InitProducerIdResponse`. Same for `ProducerEpoch` But maybe keep the explicit assignment as well since that makes it very clear...
the method `restorePartition` is no longer used and can be removed
Hmm I'm still not clear where did we break the topology order here: let me go through my reasoning and lmk where I got it wrong: 1. in `InternalTopologyBuilder` when we construct the `InternalTopology` the following parameter is constructed: ``` new ArrayList<>(stateStoreMap.values()), ``` So `ProcessorTopology#stateStores()` is in order. 2. in `AbstractTask#registerStateStores` we get stores from `ProcessorTopology#stateStores()` which is in order, and hence we are calling `store.init` in order, and hence call `ProcessorStateManager#register` in order as well. 3. The resulted `stores` in `ProcessorStateManager` should be in order then as well.
I see. So it seems this issue will only be exposed with EOS still, right (reinitializeStateStoresForPartitions is only triggered when EOS is enabled and offset out-of-range detected)? As for the fix: since the `storesCopy = new HashMap<>(stateStores)` is just for fixing concurrent access exception, how about we just use an iterators instead, i.e.: ``` iterator = .. for (iterator) { if (storeToBeReinitialized.contains(..)) { ... iterator.remove() stateStore.init(processorContext, stateStore); } } ```
I see your point now, this is exactly the messy code that we were trying to fix. I've looked at the source code again, and I think we can actually not remove the state at all since the same object will be add to the state stores via `store.init` immediately within the same function call. So I think we can actually do: ``` if (storeToBeReinitialized.contains(A)) { A.close; delete state dir; A.init(); } ``` In that loop.
nit: we usually try to keep lines under 120 characters.
Sorry, you are right! My bad!
Could you please extract the values `"Key1"`, `12345L`, `"Key2"`, and `"6789"` into variables and use the variables in the checks below? I am sorry, I missed that in my previous review.
nit: remove empty line
Add a check to verify, whether the iterator has no more elements.
Add a check to verify, whether the iterator has no more elements.
nit: remove empty line
Here it is better to use to distinct records, because if the code contains a bug that adds a record twice, you would not discover it.
The indentation should be four spaces instead of eight.
nit: I would also create a local `consumerRecord1` as you did with `consumerRecord2`. Alternatively, you could use the global `consumerRecord2` instead of creating a local one. In my opinion, mixing the global and local makes the code less readable.
nit: remove empty line
FYI: There is the static nested class `Record` in `TopologyTestDriverTest`, that can be used to compare records.
I first questioned in myself if an int is enough for this or not but we're not using any kind of ordering property so if it overflows then we just end up with a funny ID but nothing more. (So I'm just noting this.) Also it seems to be more effective than other methods. I was just thinking about whether we can solve this by reference comparison of the return value of `subscription.assignedPartitions()` and a weak reference stored here but the first one is a final map, so that doesn't help in this case.
> Is there any official style guide or export of accepted settings? Not really :( Rule of thumb is, avoid reformatting if possible :)
@mjsax Should we include indentation settings here: http://kafka.apache.org/coding-guide.html ? It's mentioned in the scala guidelines...
Nit: avoid unnecessary reformatting. Also, we use 4 space indention, not 8 space.
Feel free to do a PR against https://github.com/apache/kafka-site
nit: how about just using letters alphabetically from "A" than using multiples of `XYZ` only? Relying on numbers of letters may be bug-prone (see below).
Is there any reason not to accept this suggestion? I will go ahead and push an update to this PR next week if there are no further responses so that we can get this fix into the next release.
@sdreynolds I would suggest we use my suggestion since it addresses the problem with the relative time adjustments. It could also happen that `now` somehow gets far ahead of `nextCommit`. In this case, we we do not want a flurry of useless offset commits in order for `nextCommit` to catch back up. It seems simpler to reset the offset commit interval.
I don't think it makes a big difference either way. The intent of the offset commit interval is just to make sure committed offsets don't fall too far behind. It does not need to be a strict schedule. It seemed more intuitive and simpler to me to reset the interval. In any case, we should get rid of this relative tracking of the next commit. If we use an absolute time, then we will not have problems like this in the future.
Maybe this is a little simpler? ```java nextCommit = now + offsetCommitIntervalMs; ```
Could be default access if we only need it for testing.
Yeah, I think that should be fine. I just wasn't sure how much I wanted to trust the request completion handling logic. I guess it would break a lot of expectations elsewhere if it were broken though, so probably no additional harm from relying on it here.
One idea that I had was to make this a `Map<Integer, Long>`, with the value being `System.currentTimeMillis()` at the time the fetch request is sent. That would allow the "Skipping fetch for partition" log message to include the duration that the previous request has been pending for (possibly adjusting the log level based on how long ago that previous request was sent), and also enable a fetch request time metric to be easily collected if someone wishes to add that enhancement in the future.
FYI, there is already a metric for fetch request latency.
This whitespace change isn't needed
This only needs to be set when we first create the `OffsetCommitRequestTopic`, right? So it can be set inside the `getOrDefault` block.
Does it make sense to actually declare the kafka cluster id as in the previous test (member field) and return it here? Maybe the code might start using it at some point.
whenever referring to Connect as the framework (as opposed to the verb) I'd use `Connect`. Same as `Kafka cluster` and `Connect framework` a few words after. It's easy to miss that you are referring to the framework if it's not capitalized.
I'd keep it as member field. Having to declare the type in every test case to which you use it is be a bit annoying. Also, if you reuse this variable in consecutively in the same test case, the fact that in the first occurrence you have to specify the type but in the next ones it's already declared, breaks a symmetry in assignments that's otherwise good. Finally, having it `final` is not all that useful.
I was referring only to the newly introduced methods and their tests. Not the whole class. Not a big issue.
Do you mean the `assert` keyword in Java? IIUC assertions need to explicitly turned on at execution time. So you need to rely on the user to turn them on.
You basically specified the `differentName` check twice, in this and in the previous method. I would specify the following tests into separate methods: - same - different names - different topics - different patterns Makes the test better readable and avoids repetitions of checks.
My preference would be `requireNonNull` and add the message. First IMHO it's always better to have some sort of description v.s just a `NullPointerException` and second, there is less chance for error since we need to explicitly enable assertions. Just my 2 cents.
OK, I overlooked that the overload of `requireNonNull` without the message is used in the code under test. However, I think it would be better to use the overload with message, i.e., ``` public static <T> T requireNonNull(T obj, String message) ```
OK, makes sense. Didn't know about policy of internal checks. Would be good to have it written down somewhere.
Here and for the next two tests, it would be better to rewrite the tests to following: ``` final Exception e = assertThrows(NullPointerException.class, () -> new InternalTopologyBuilder.Source(null, Collections.emptySet(), null); assertEquals(<expected message>, e.getMessage()); ```
Well, it's not at the end of the file right? But if you'd prefer to keep it that's fine too, was just a "super nit" suggestion 
This may indicate a bug in `SessionWindowedDeserializer`
super nit: extra blank line
Yes. The user can use the config two ways: ``` // as string props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, "my.fully.qualified.package.MyInnerSerde"); // or as class props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class); // or short (it the class is imported) props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class); ``` Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type.
Yes, I think we should. And it's not even a diversion from the approach elsewhere because there's a KIP in progress to do so in classes like `SessionWindowedSerializer` as well
Cool. I think the fewer configs overall, the better. If we can get away with just the Serde configs then let's do so to keep the API surface area smaller for users 
Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so
That sounds good to me 
>Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it 
Can we actually include UUID type? It always 16 bytes.
nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases.
nit: remove `this` (not required)
We should use try-with-resources here (for `DataInputStream`).
Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
I think we should call `deserializer.configure(...)` here
I think we should call `deserializer.close()` here
As mentioned on the KIP discussion, `BytesDeserializer` should not be included.
nit: should be `Deserializer<?>` to avoid warnings about using a raw type
nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The "SERDE" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line).
Update return type to `L` (if we introduce `L`)
`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
We should limit this suppression to the method for which we really need it instead of the whole class
I am wondering, if we should get the `List` type as generic (not sure). `public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`
Should it be `Class<L> listClass` ? (or `Class<List<T>` if we don't introduce `L`)
It's better to avoid "double-brace initialization", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement. A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`.
This shouldn't be necessary. I believe the config parser will coerce the value to the type you declared the configuration as, `Type.CLASS`. Might be worth to double-check, but we shouldn't add a bunch of branches if they're not necessary.
That class is different because it doesn't actually `define` the config, it's just an undeclared "extra" config that gets passed around to be interpreted inside the serde. Actually, this _is_ a bug, and that config _should_ be `define`d there the way you do it here.
Update return type to `L` (if we introduce `L`)
nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
can be simplified to `@SuppressWarnings("unchecked")`
Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments. WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you
nit: use `private static` ordering (for consistency with the rest of the code base)
```suggestion throw new ConfigException(innerSerdePropertyName, innerSerdeClassOrName, "Deserializer's inner serde class \"" + innerSerdeClassOrName + "\" was not a valid Serde/Deserializer."); ```
Update return type to `L` (if we introduce `L`)
`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently: ``` if (listClass == null) { String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS; listClass = (Class<List<Inner>>) configs.get(listTypePropertyName); if (listClass == null) { throw new ConfigException("Not able to determine the list class because it was neither passed via the constructor nor set in the config"); } } if (inner == null) { String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS; Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName); inner = Utils.newInstance(innerDeserializerClass); inner.configure(configs, isKey); } ```
Use `KafkaException` instead of `RuntimeException`
Use `KafkaException` instead of `RuntimeException`
We should add a `null` check to allow closing a deserializer that was not properly setup
I think that if you were to actually pass in a map here and then assert you get the same map back from `logConfig()`, it would be enough test coverage.
Turns out I was wrong. Since in java all parameters except parameters of primitive types are references, modifications on the state of the objects the parameters point to are visible outside the method (what I meant with side-effects) also if the parameters are declared `final`. With `final` on parameters one can merely avoid modifications of the references stored in the parameters (not the modifications on the state of the object) within the method. I still think that `final` is great and that putting final on parameters makes code a bit better. Just wanted to correct my statement. Sorry for the confusion.
If the object is created with the no-argument constructor, this will throw an NPE.
Overwriting the parameters `keyFrom` and `keyTo` might be dangerous, because it has side-effects outside of the method. Unless you want to have the side-effects. If you do not want them, please use local variables and specify the `keyFrom` and `keyTo` as `final`.
Is the logic changed here? It was `this.allKeys = (keyFrom == null) && (keyTo == null);`
What's the benefits of using a callback here than calling `openIterators` directly? I think adding to `openIterators` outside of the constructor makes sense, but cannot think of the rationale of doing this upon closure.
Ack, thanks for the explanation.
You actually do not need `this` here, right? The values are the default initialization values in Java. And `super` is also called in the default constructor. So actually, you could remove this constructor completely.
nit: remove extra line
`stores` -> `stored`
nit: than -> that
`{@link TimestampedWindowStoreBuilder}` should probably be `{@link TimestampedWindowStore}`.
Could you use more meaningful names for these variables? Especially for `rpMsgPrefix` and `wsMsgPrefix`.
I would move line 328 and 329 to before this line.
I would move line 330 and 331 to before this line.
Whoops, that was my bad
`stores` -> `stored`
`than` -> `that`
`than` -> `that`
nit: move `Serde.String()` into next line (also, I would prefer to not have static import `Serde` but prefix. I was initially confuse why this compiles and why it's not `new String()`... (because the method name starts with capital `S`...)
The verb following 'should' would be in active tense. consider naming this test: userSerdesShouldBeInitialized
A verb is normally expected following 'if no xx' How about naming this test: shouldGetSerdesFromConfigWithouUserSerdes
Why we do not want to verify serde classes here? Although it's trivial seems no harm either.
How about we get rid of the problem altogether -- define `WorkerConfig.rebalanceTimeout()` that returns null by default, and then override it in `DistributedConfig` to return `getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG)` since that will always be an integer. The latter's method is trivial, and the code in `RestServer` becomes simpler and easier to read. I think there's enough precedence in `StreamsConfig` and `ConverterConfig` to add getter methods (without `get` prefix). The fact that it cleans this up this significantly is also good justification.
Would it be better to get the value only when the return type is an integer rather than do that when it the type is non-null? For example, something like: ``` Integer rebalanceTimeoutMs = null; if (config.typeOf(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG) == ConfigDef.Type.INTEGER) { rebalanceTimeoutMs = config.getInt(DistributedConfig.REBALANCE_TIMEOUT_MS_CONFIG); } ``` This may be a bit easier to read, and it's also going to cast to an integer only if the value actually is an integer.
Nit: ```suggestion * @return list of {@link ConfigValue} instances that describe each client configuration in the request and includes an {@link ConfigValue#errorMessages error} if the configuration is not allowed by the policy; never null ```
There are some bugs here: * If you call remove twice in a row, it should fail. Remember: > Note that the remove() and set(Object) methods are not defined in terms of the cursor position; they are defined to operate on the last element returned by a call to next() or previous(). So you can't `remove` twice in a row-- you have to call `next` or `previous` again before you can call `remove`. Since lastReturned can be non-null here after we call `remove`, something bad will happen if we call it twice. * If you get to the end of the list and then call `previous`, it should work. It fails here because we check `cur` against `head` * Indices (`nextIndex` / `previousIndex`) get screwed up after calling `remove` * In general, there seems to be no value in having both a `cur` and a `next`. We can always follow the doubly-linked list backwards, so having an extra field doesn't add any efficiency. We also have `lastReturned` to track what was last returned, and `cursor` to track whether we're at the end or the beginning of the list (which is implemented as a circular linked list, as you've no doubt noted) If you choose to have both of these fields, you need to keep them properly updated when you remove an element. Sounds like a pain.
Good call-- thanks for the correction.
This does work, but for a reason that is a bit obscure. When using an `ImplicitLinkedHashMultiCollection`, `remove` will remove the element b such that a == b, if it exists. This is necessary since if it just took the first element where `a.equals(b)`, it might be a different one than expected. It might be clearer to directly call `removeElementAtSlot`, since we already know the slot number.
We should test that delete twice in a row fails with `IllegalStateException`
This causes an ArrayIndexOutOfBoundsException: `i <= threads.length`
Thanks for confirming @hachikuji!
We actually did not call `close()` for this case on purpose. IIRC, the producer contract is to not call _any_ method (not even `close()`) after a `ProducerFencedException` was thrown by the producer, as indicated by the `isZombie` flag. \cc @hachikuji to confirm.
@mjsax @bbejeck Just to clarify, I think we should always close the producer even if it was fenced. Resources like network connections only get cleaned up in `KafkaProducer.close()`.
Maybe it would be good to weaken a bit this statement like "This is conceptually equivalent to calling ..." or similar. See also the other occurrences.
Ayayay. Thanks for adding it!
nit: move `windowBy()` to it's own line -- also `count()` (similar below)
nit: missing empty line
we can make method this public in `EmbeddedConnectCluster`.
this global variable isn't great. Can't we hit some rest endpoint that can return this internal values out of the extension? probably makes for a better end to end test too.
nit: some extra newlines here.
another ideaa: add an assertion here? so that we are guaranteed that this method is called.
Ah I missed this part in my review as well.
There's a potential KIP for allowing negative timestamps (so you can represent time older than 1970, duh), I think we leave space for such extensions in the future back then.
Nice catch. Though I'm curious how the unit test could expose this? :)
The `connectorStatus(connector)` call might fail with a `NotFoundException` if the connector was removed after the `configState.connector()` method is called but before the status for the removed connector is asked for. Although this shouldn't happen within the process (since requests are handled sequentially by a single thread), it may be possible that this herder is not the leader, that the leader performed the change, and that this herder's config state backing store read that change after the `configState.connector()` method was called but before the `connectorStatus(connector)` method is called for that connector. Should be easily handled with a try-catch, and if the connector with the specified name is not found then simply continue to the next connector name. Something like: ```suggestion try { out.put(connector, connectorStatus(connector)); } catch (NotFoundException e) { // do nothing with connectors that were just removed } ``` Note that if a connector is *added* with similar timing, the new connector name will not be returned from `configState.connectors()` and the new connector will not be included in the results. I think that's fine, considering the result of this method call would still be consistent with the state at the time the `configState.connectors()` call is made. Call it again, and you'd see the new connector.
Removing this will mean `DistributedHerder` is not touched in the PR.
This should call `configState.rawConnectorConfig(connector)`, which returns the _user-supplied_ configuration _with variables not resolved_. The `connectorConfig(connector)` call returns the configuration _with variables already replaced_, which means we might be leaking passwords and other secrets specified using variables.
Let's wait until the KIP is merged before this one.
Ditto for LRUCache: `Cache extends RocksObject`. We need to close it explicitly.
Ah right, I think then it is not blocked on anything (the KIP was semi-orthogonal to this).
But grace and retention are two different things. In fact, I just had another conversation about this issue, and it seem we need to fix this by allowing people to specify a retention time IMHO. Not sure if we need to add a `Materialized` parameter or add `Joined#withRetention()` that we use to specify serdes etc.
We already import the class on L26, so let's remove this import.
Seems unnecessary L123
How about ``` if (!cachingEnabled) { context.forward(key, new Change<>(newValue, oldValue)); } ```
This is for statically use this single util function (otherwise we have to call `ValueAndTimestamp.getValueOrNull` when calling).
nit: we can still use the ternary operator here.
Why was this removed? This is testing reconfiguration without a truststore.
I think we tend to use `forXxx` rather than `create` for mapping names to enums.
What is the rationale for this change? It will be better to handle defaults consistently for keymanager algorithm and trustmanager algorithm. And since the doc says we use the JVM default, it seems to make sense to set the default here.
`SSL context` => `SslEngineBuilder`
Can replace the three lines with: ``` assertEquals(Utils.mkSet("TLSv1.2"), Utils.mkSet(engine.getEnabledProtocols())); ```
This is a validation failure (e.g. admin client validateOnly request), sobetter to say `validation failed`.
This probably doesn't work. Better just throw an unsupported exception instead of implementing the value getter.
yes, I think so.
Yeah, I think that there's a larger "lookback" feature that I wasn't aware of when I implemented Suppress. It seems like it needs a first-class solution, and probably just mixing in this interface would just surface a different exception at run time. I'm not sure without spending some time to look at it, but it seems we need to enable "old values" upstream and then add the ability to store the old values as well. Actually, this may already be partially supported, with the FullChangeSerde. The other missing API is the valuegetter. We might need to actually implement that, acting as a cache (look in the buffer first, then look upstream), or, since we know the buffer _always_ reflects the upstream state anyway, we can just directly look upstream.
```suggestion public void groupByAfterSuppressShouldRun() { ```
We could make this field access `public`
Same for generation and member.id. We could just call `request.data().generationId()`
We could remove this function
This is a blocking call, and @guozhangwang just proposed KIP-520 to make it more efficient by allowing to pass in multiple partitions at once. Should we wait for KIP-520 to be implemented? If now, we should make sure the update this code after KIP-520 is merged. I am also wondering how we should handle `TimeoutException` for this call? Maybe not, but might be worth to clarify? \cc @guozhangwang
I am wondering whether we can do better here. Encoding partition time in Base64 seems to me a bit a waste of space. As far as I can see, a 8 byte value is encoded in 11 bytes with Base64. Would be great, if we could store partition time in 8 bytes. I am also wondering why `metadata` in `OffsetAndMetadata` is a `String` and not something more bytes friendly.
nit: remove `get` prefix (similar below for `getPartitionTime()`
I would put methods to write and read record metadata in their own classes. Those classes would be kind of SerDes for metadata. Such SerDes would make the code better testable and separates the concerns of a task and reading and writing metadata which are completely independent. It does not need to be done in this PR. I just wanted to mention it.
@guozhangwang @bbejeck @vvcephei Do you think it's worth to add a version number for the binary format of the committed offsets (I tend to think we should add a version number). I would also not encode the timestamps as `String` but as 8-byte binary long.
I don't have the full context on the history, but it would not be easy to change the API... I talked to Jason about it, and it seem we can just move forward with this PR as-is, and could do a KIP later that allows us to store metadata as `byte[]` type if we really need to change it. Atm, the metadata is just a few bytes and the overhead does not really matter IMHO.
In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
Just realized I need to do another rebase on my PR. So if this PR is closer to be merged I'd suggest @RichardYuSTUG @mjsax you guys just move forward and I will rebase mine later.
So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly: ``` log.error("Could not initialize partition time. Committed metadata is corrupted.", e); ```
is old metadata missing expected after we start off? Might be useful to add a debug log or trace if this is not normal.
would be favorable to order comparison result according to first citizen. Like `metadataTimestamp >= localPartitionTime ? metadataTimestamp : localPartitionTime;`
To be future prove, we should encode a version number as prefix in case we ever what to change this metadata. What about `<version>:<partitionTime>` with version number "1" ? Also, line is too long. Move both parameters to their own lines.
nit: use RecordQueue.UNKNOWN instead of -1
This block can be moved outside of the `try-catch-block`
Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
return type is `void` -- remove this line
Nit: (simplify to) `"Unsupported offset metadata version found. Supported version {}. Found version {}."`
Input parameter `partitionTimes` should always contain the correct partition time, hence, we can just get it: ``` final long partitionTime = partitionTimes.get(partition); ```
I think we don't need this method if we apply my other suggestions
nit: `partitionTimeMap` -> `partitionTimes`
Good pointed. I missed that the type is `String` (expected it to be `byte[]`). Hence, for efficient encoding, and to allow us to add a magic/version byte, we should first serialize the timestamp, prefix it with a magic byte and then "deserialize" it to `String`. ``` byte[] bytes = ByteBuffer.allocate(9) .put(MAGIC_BYTE) // add a corresponding "final static" variable .putLong(timestampe) .array(); String metadata = StringSerde.deserialize(bytes); ```
Good find! > However, gradle does not allow the usage of Base64 since it "could not be found" according to the compiler anyways. How did you try to use it? Everything from the standard library should be available... I would prefer to use Base64 if we can. If not possible, we can still fall back to using String, but I would really like to avoid it if we can.
Why not do this unconditionally? If it's not `clear` we won't commit anyway. It's seems cleaner to avoid to many branches and it's not on the hot code path so the overhead of updating `partitionTime` is not relevant.
Passing in `null` is not idea IMHO. At this point, we _know_ that we want to get the timestamps from the `PartitionGroup`. Hence, seems better to build up the correct `Map< TopicPartition, Long>`, by looping over all committed offsets: ``` final Map<TopicPartition, Long> partitionTimes = new HashMap<>(consumedOffsets.size); for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) { partitionTimes.put(entry.key, partitionGroup.partitionTimestamp(partition)); } commit(partitionTimes); ```
Same as above: Also, we should log a WARN message there, that the the found metadata is corrupted and cannot be decoded.
We should return `RecordQueue.UNKNOWN` instead.
I am not an expert on this API, but I would expect that even if we commit using the producer, the consumer should still be able to read the metadata. Did you verify that we cannot retrieve the metadata if EOS is enabled? If this is the case, I would claim it's a bug that need to be fix.
Thanks. Understood. It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information. In `close()` before we actually "loose" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`). This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies.
:+1: Actually, I think `@link` would be preferable in all three cases, but they need to be properly qualified.
nit `RocksDB` without space
nit: as above, use `@link`
nit: insert `<p>`
Use `@link` and also link to `should have {@link RocksObject#close() close()} called`
nit: insert `<p>`
And then here ``` Any object created with @{code new} in {@link #setConfig} and that inherits from {@code org.rocksdb.RocksObject} should have {@code close()} called on it here to ```
I would rather write the one line description in imperative. ``` Close any user-constructed objects that inherit from {@code org.rocksdb.RocksObject}. ```
```suggestion * Example objects needing to be closed include {@code org.rocksdb.Filter} and {@code org.rocksdb.Cache}. ```
We need the default implementation in any case until the next major release not to break existing code. Afterwards, I would be in favour of removing the default implementation, because then there is a bigger chance that existing memory leakages are found and future leakages are avoided.
RocksObject -> {@code RocksObject} setConfig -> {@link #setConfig} close -> {@code close}
That's a tricky question... The problem is that there's no way to signal ahead of time that you *should* override the method to avoid breakage when we remove `default` (unlike when you deprecate a method, which indicates you should make a change ahead of the breakage). So, whenever we do make the change, it will be a breaking one. If we think it's very likely that implementers would have some objects that need to be closed, and thus are leaking memory today, maybe we should actually just not default it at all, so they fix their memory leaks asap. Conversely, if we think it's unlikely that the average implementer is going to leak memory, there's no reason we shouldn't just leave it defaulted forever.
I would use the full qualified name of `Filter` and `Cache`. If anybody wants to look them up it is easier. Additionally, I would enclose it into `{@code ...}`.
same... `@link` > `@close` for this case.
```suggestion * Any object created with {@code new} in {@link RocksDBConfigSetter#setConfig setConfig()} and that inherits ```
nit: extra space between `structure` and comma.
We can use `assertThrows`. It is much more concise.
nit: should we merge this into existing test? If not, rename to `shouldChooseNextRecordBasedOnHeadTimestampe`
nit: there is not "partition stream time" -- there is "stream time" and "partition time" :)
Since we're only using a batch size of one, we should be able to just verify that we got a list of size 1 and it contained what we wanted, right? Also, we should also try to be robust against malformed responses that don't include the group we asked for (this will currently throw an NPE if we get one of those).
nit: also add java doc for type `T, O` here
Minor nit, but I think something like this would be more accurate/complete. Applies to this and the broker config. What do you think? If the `MAX_COMPACTION_LAG_MS_CONFIG` or the `MIN_COMPACTION_LAG_MS_CONFIG` configurations are also specified, then the log compactor considers the log eligible for compaction as soon as either: (i) the dirty ratio threshold has been met and the log has had dirty (uncompacted) records for at least the `MIN_COMPACTION_LAG_MS_CONFIG` duration, or (ii) if the log has had dirty (uncompacted) records for at most the `MAX_COMPACTION_LAG_MS_CONFIG` period.
If `valueAndTimestamp != null` should this be `max(valueAndTimestamp.timestamp(), context.timestamp())`
nit: we could just call `numPartitions()`. Alternatively, we might consider a friendlier representation such as "none" or "undefined."
@agavra I don't think these should be public API.
Should we be a little careful calling `array()` here? It might be safer to use `Utils.toArray`. We should probably have been more careful accessing this ByteBuffer in the first place since it is provided by plugins. I think we have the same problem in `ConsumerCoordinator.metadata()`.
Same for this class upgrade.
Yeah, `FetchResponse` will most likely be the last one to convert because we'll have to figure out how zero-copy will work.
We'll need to fix this in a follow-up so that followers send the right replicaId.
Hmm this would fail for the old schema. I think we have to use `getOrElse`.
Maybe we should make this type `Optional<Integer>`. Otherwise it's a little confusing if you should use `null` or `UNSPECIFIED_PREFERRED_REPLICA`.
Ah. That makes sense. Thanks! Not sure if we want/need to change the behavior. Also, it would require a KIP imho, because people may have tests in place testing for the current behavior... Not sure if it's worth it.
I feel that generally speaking the `commit-on-every-pipeInput` of TopologyTestDriver is debatable, especially since we call `pipeInput` recursively from repartition topics, which means each of the new / old records via the repartition topic would be triggering once. Will merge this PR still as-is and we can discuss if we want to change this behavior later.
@mjsax @vvcephei I ran the new commit locally and I think I get the difference here: In the ToplogyTestDriver#pipeInput: ``` // Process the record ... task.process(); task.maybePunctuateStreamTime(); task.commit(); captureOutputRecords(); ``` I.e. each record would cause a commit immediately, and in this case, when processing the two records from the repartition topics, each of them will trigger the `pipeInput` once and hence commit once, i.e. the processing of the original one `pipeInput` would cause two `pipeInput` from the repartition topic, and hence commit twice, and flush twice. While in the old `KStreamTestDriver`, we do not commit from the repartition-topic piped record, hence only result in one flush.
This seems different to the old expected result.
The source KTable is materialized for this case. That's why `groupBy().aggregate()` is only executed after the flush on the last value per key. However, with `TopologyTestDriver` we cannot mimic this behavior any longer. Hence, it would make sense to delete this test, as it's the same as `testAggBasic`
nit: move this to next line
Not sure if I can follow. The old code calls `driver.flushState();` after each `driver.process(...)` call.
nit: the expectation should be the first argument
This import was not needed, so checkstyle is failing.
There are a couple of checkstyle failures in these two lines because of missing spaces
Could use `MockVaultConfigProvider.class.getName()` instead of hard-coding the class name so that changes are reflected automatically.
Curious: why should we need this in the integration test utils? There's no need to make something public for later use, since we can just make it public if/when we need it to be.
nit: this looks like it wants to be a static constant now.
thanks for clarifying this.
IMHO, it would be better to keep this one in favor over `processed` that does not provide type safety...
`MockProcessor` does not support callbacks...
Seems to be a step backward to use `String` instead of `Record` -- also, `MockProcessor` actually also supports `KeyValueTimestamp` as type...
configSetter.close() will clean up any resources constructed in configSetter.setConfig, and should only be called in RocksDBStore#close()
ditto here, should be moved to RocksDBStore#close
A brief description of the purpose of this adjustment would be helpful.
nit: since we're not doing anything in the EAGER case, couldn't we simplify this: ```java if (protocol == COOPERATIVE) adjustAssignment(ownedPartitions, assignments) ``` Similarly in `onJoinPrepare`
Did this slip? Seems it should be a `WARN` as you said above.
Slight readability improvement: could we factor this into a method, maybe #assignPartitions? We could also then have just #revokePartitions and the cooperative case could call both -- this way it's immediately clear what each protocol does/how they differ
s/`is the protocol`/`the protocol is`
Ok. We also have a check in `subscribe` to ensure that the set of assignors is not empty. There might be a way to remove the redundant checking. By the way, there's a typo above: `confingure`.
I can't tell if you're trolling me  . The method `prettyString` below still does not handle manual assignment. Maybe it could be something like this? ```java public String prettyString() { switch (subscriptionType) { case NONE: return "None"; case AUTO_TOPICS: return "subscribe($topics)"; case AUTO_PATTERN: return "subscribe($pattern)"; case USER_ASSIGNED: return "assign($partitions)"; } } ```
nit: no need to hyphenate unrecognized.
Since this is a general `toString()`, we probably should handle manual assignment as well.
I think we can simplify these functions. Something like this: ```java private static byte readByte(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 1) readMore(buffer, input, bytesRemaining); return buffer.get(); } private static long readVarLong(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarlong(buffer); } private static int readVarInt(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException { if (buffer.remaining() < 10 && bytesRemaining.value > 0) readMore(buffer, input, bytesRemaining); return ByteUtils.readVarint(buffer); } ``` I think we shouldn't need more than one call to `readMore`.
It's a nullable argument. When set at the batch level, it overrides the record-level timestamp.
Would it make sense to pass this through as a byte array? That would make the call to `buffer.array()` below safer.
Now that we have a benchmark, we could test `System.arrayCopy`. I suspect it doesn't make much difference either way since the number of bytes we have to copy with this logic would never be more than 10.
I wonder if we could have a simple `IntRef` or something like that in the `common` classes to make this a little clearer. It would also help us in awkward lambda situations where we are not allowed to use a normal variable.
It doesn't rely on the OS, it's a JVM intrinsic (this is a critical distinction). And it's used all over the place by the collection libraries. We only really care about Linux performance. If it's a small number of bytes, it may not matter, but generally I think perf driven changes have to be measured.
@guozhangwang I think we shouldn't relax schema validation. We would have to think through all the implications of accepting bad data. Is the performance really that much worse? I thought the biggest benefit was avoiding the allocations, which we should still be able to do.
Also, there are a number of crashes due to misoptimized loops too (Lucene filed a number of these over time, it doesn't mean we can't use loops).
I've been thinking about this function a little bit and the savings we are trying to achieve. From a high level, I think we are trying to minimize the number of reads and the number of allocations. I am considering whether we can pull the `skipBytes` approach up to this level. So first thing we do is allocate a buffer of size `min(sizeInBytes, FIXED_BUFFER_SIZE)`. Then we turn the parsing into a state machine. The logic would look something like this: ``` buffer = allocate() needMore = true state = READ_METADATA while (true) { if (needMore) input.read(buffer) switch (state) { case READ_METADATA: // read offset and timestamp // transition to READ_KEY_SIZE case READ_KEY_SIZE: // read key size // transition to SKIP_KEY_BYTES case SKIP_KEY_BYTES: // skip until key is consumed // transition to READ_VALUE_SIZE case READ_VALUE_SIZE: // read value size // transition to SKIP_VALUE_BYTES case SKIP_VALUE_BYTES: // skip bytes until value is consumed // transition to READ_HEADERS case READ_NUM_HEADERS: // read num headers // transition to READ_HEADER case READ_HEADER: // read one header // return when all expected headers are consumed } } ``` The basic idea is to always use the same buffer and never have any trivial reads. In fact, this would allow us to reuse the same buffer across multiple calls to `readPartially` which would improve GC even further.
I think we may be able to get rid of some of the `bytesRead` bookkeeping. As far as I can tell, it is only used in the exception message below and it seems redundant there (i.e. sizeOfBodyInBytes - bytesRemaining = bytesRead).
I was playing around with some ideas to simplify this parsing logic since we're mostly doing the same thing over and over. I think some helpers might do the trick. This is what I'm thinking: ```java private static int readMore(ByteBuffer buffer, DataInput input, int bytesRemaining) throws IOException { byte[] array = buffer.array(); // first copy the remaining bytes to the beginning of the array // do not use System.arrayCopy since it will allocate a new array for same src/dest int stepsToLeftShift = buffer.position(); int bytesToLeftShift = buffer.remaining(); for (int i = 0; i < bytesToLeftShift; i++) { array[i] = array[i + stepsToLeftShift]; } // then try to read more bytes to the remaining of the array int bytesRead = Math.min(bytesRemaining, array.length - bytesToLeftShift); input.readFully(array, bytesToLeftShift, bytesRead); buffer.rewind(); // only those many bytes are readable buffer.limit(bytesToLeftShift + bytesRead); return bytesRead; } private static int skipLengthDelimitedField(ByteBuffer buffer, DataInput input, int bytesRemaining) throws IOException { boolean needMore = false; int bytesRead = 0; int bytesToSkip = -1; while (true) { if (needMore) { if (bytesRemaining > 0) { int bytesToRead = readMore(buffer, input, bytesRemaining); bytesRemaining -= bytesToRead; needMore = false; } else { throw new InvalidRecordException("Invalid record size: expected to read more bytes in record payload"); } } if (bytesToSkip < 0) { if (buffer.remaining() < 5 && bytesRemaining > 0) { needMore = true; } else { bytesToSkip = ByteUtils.readVarint(buffer); if (bytesToSkip <= 0) return bytesRead; } } else { if (bytesToSkip > buffer.remaining()) { bytesToSkip -= buffer.remaining(); buffer.position(buffer.limit()); needMore = true; } else { buffer.position(buffer.position() + bytesToSkip; return bytesRead; } } } } ``` With these helpers, the state machine becomes a little simpler. For example: ```java case READ_KEY: bytesRemaining -= skipLengthDelimitedField(buffer, input, bytesRemaining); state = ReadState.READ_VALUE; case READ_VALUE: bytesRemaining -= skipLengthDelimitedField(buffer, input, bytesRemaining); state = ReadState.READ_NUM_HEADERS; ... ``` If we could get this working, then we may not need to reuse states.
Safer side in what way? If it's a performance thing, then you have to measure. `arraycopy` is usually fastest for what it's worth.
This is definitely much clearer than the first try!
It's not any more JDK dependent than running a loop is JDK dependent (JIT optimizations vary more than this method that changes less often). Anyway, if it's 4 bytes, it doesn't matter.
Default access is probably good enough and is consistent with `PartialDefaultRecord`.
Would it be a lot easier to do the following? ``` Map<String, Object> prefixedOriginals = connectorConfig.originalsWithPrefix(prefix); Map<String, Object> clientConfigs = configDef.parse(prefixedOriginals); ``` This does check a few more things (dependencies are set and validators are run), so maybe that's not really want we want to do here.
That's a good addition too. We should definitely clean up the regex if possible too, though.
I would like to see: ``` connectorsResource.createConnector(FORWARD, NULL_HEADERS, bodyIn); ``` to make it much more clear what the second parameter is and that most of the tests are not specifying headers.
Then, this method should return an immutable version of the resolved originals: ``` return Collections.unmodifiableMap(resolvedOriginals); ```
I'd suggest naming this variable `resolvedOriginals`.
@tadsul We could perhaps convert this to a immutable map and store `in `originals` like we do for `values`.
nit: remove `use this to` Or `use the {@code Processor} to`
Nit: `late enough version` reads a bit funny. Maybe we can say "Could not validate fetch offsets for partitions {} since the broker does not support the required protocol version (introduced in Kafka 2.3)" or something.
Maybe we should have a static method in `OffsetForLeaderEpochRequest` that this class can pass the version to which returns `true` if it's 3 and higher.
This was a separate bug, I guess? Might be worth mentioning in the PR description.
nit: IMHO we should just get rid of the `try/catch` as if the cause is anything other than a `ClassCastException` this test will still pass.
Another option would be to put a "deprecated" annotation here.
Maybe say something similar to the PR about this method being commonly used to set up ssl clients even though not public API so we're temporarily keeping it for backwards compatibility as of 2.3 (that will give a sense of timing when people see this code a year or two from now).
That's a good point.
Also note, that txn timeout is one of the reasons of `TaskMigratedException` only, so the title seems inconsistent with the actual change here. If we indeed intent to reset for any `TaskMigratedException` root causes, then we should change the title of the PR; otherwise, we should get the cause of the exception and check if it is `ProducerIdFenced` exception and only reset then.
So we need this log? We already warn a WARN and `numIteration` is an internal detail that I think we should not "leak" into the logs.
+1, the above check for empty leader and epoch obviates the extra check we were doing in safeToFetchFrom. I missed that
This can be taken out. When the `StreamThread` goes into the `runOnce` method and the state is `PARTITIONS_ASSIGNED` the `StreamThread` calls `taskManager.updateNewAndRestoringTasks()`. If the lock is held by another thread still cleaning up this call will return false and the state will not transition to running. In the next iteration of `runOnce()` the `StreamThread` will attempt the same operation again since the state has not yet been set to `RUNNING`
I think the logic here is not correct: we should still resume the main consumer and assign standby partitions if active tasks are all running; we should only alter the logic of returning flag with both active / standby all running.
This fix seems preferable because other rest calls might fail too, which we might not want to retry. Such as: `validate_config` and others.
This debug message seems like it would appear _before_ we actually attempt to do any checking. It's probably worth keeping the old message (or something similar) _after_ the checking has been completed.
It's a little confusing that this is named newValue, but is sometimes actually priorValue
nit: put args on separate line
I was just reading up the original KIP-98 design doc. Actually, when `beginTransaction()` is called, it's just a local producer-client state change. The transaction timeout should start ticking only after the first `send()` for the transaction. Hence, it's seems ok to call `beginTransaction()` pro-actively in `initializeTopology()`.
We no longer need `DEFAULT_OUTPUT_TOPIC_NAME` if all caller will use `createTopic` now
nit: since we are changing this, probably should be one of the following: `Configurations shared by Kafka client applications: producer, consumer, connect, etc.` or `Configurations shared by Kafka client applications: producer, consumer, connect and adminclient.` adminclient is one that is missing, so the version with `etc` might be preferable.
Also these declarations are masking member fields. We should avoid that because it might lead to bugs that are hard to detect. If we have member fields for any of the variables we need to customize in a test case, we should use those and avoid masking with local variables.
Is this method used anywhere? The only caller I can find is `DeleteGroupsResponseTest` and that also test the Errors is None. If we decide to keep it, we can remove `.code()` from both sides of the equals
Do we need this Constructor? It looks like it's only called from `parse()` which has the version so it could call the other constructor
Let's use `Map` on the left side instead of `HashMap`
The suppression operator should not do this (evidenced by the fact that this change breaks the SuppressTopologyTest). The KIP specifies the behavior when the name is present: ``` * Use the specified name for the suppression node in the topology. * <p> * This can be used to insert a suppression without changing the rest of the topology names * (and therefore not requiring an application reset). ```
Can we call this `toHtml` to go along with the generically named `toRst`? If we want to change the output in the future, we won't need to add a new API.
Seems this method and `getConfigValue` are only used in `toHtmlTable` and `toRst`. Would it make sense to move these implementations into `ConfigDefUtils` as well? I tried this out locally and it seems to work. Then we don't need to expose these methods and the definitions here just become stubs. For example: ```java public String toRst() { return ConfigDefUtils.toRst(this); } ```
Your description was very helpful. No need to change the value.
For future-proof: if we pre-register the members then starting all three would still have one rebalance, and hence stableGeneration would be 1 here. So instead of hard-code it to `3` assuming always three rebalances, we can just use extract_generation_from_logs to assign to `stableGeneration` instead.
We just need to make sure the extracted generation from all three processors are the same.
nice cleanup to extract to a util file
We should not use wildcard imports. Check your IDE setting to disable this auto-rewrite.
Why is it useful to include `this.getClass()` in the log message? Are we missing information from the log context? Possibly more useful would be to mention that this is due to a JoinGroup response.
nit: the name is a bit awkward. How about `maybeInvokeOnPartitionsLost`? We can change the others similarly.
nit: "failed on partition being lost {}" -> "failed on invocation of `onPartitionsLost` for partitions {}"
We seem to have lost the `info` message from the original.
nit: was -> were
nit: configure -> configured
Perhaps: `With the COOPERATIVE protocol, owned partitions cannot be reassigned...`
Hmm.. Do we know how this is getting propagated in all cases? Some of the responses are handled using a `RequestCompletionHandler`, but NetworkClient currently eats any exception raised from this callback.
Do we really need to set a fatal error for every consumer, or could we just remove the problem partition from the assignment and set NEEDS_REJOIN? Or is it better to just fail fast
nit: **revoke** partitions that **were** previously owned but **are** no longer assigned
nit: "User rebalance callback **threw** an error"
Or possibly move it to the Cooperative case, and specify that we should only change after we've triggered the *onRevoked* callback -- as it is it's just incorrect
typo: moreq -> more
Also: should only call onPartitionsLost on owned partitions that no longer exist
I don't think this logic is quite right...when we call maybeRevokePartitions we calculate revokedPartitions = assignedPartitions.filter(tp -> !assignedPartitions.contains(tp)) which is an empty list.
Could we rename this to something like "remainingPartitions"
Hmm.. I am not sure this is sufficient. Any of the responses could return from the heartbeat thread.
It might be worth mentioning in the upgrade notes the fact that we now invoke revocation logic in `unsubscribe` and `close`.
```suggestion * session timeout, errors deserializing key/value pairs, your rebalance callback thrown exceptions, ``` Same reasoning.
nit: would be nice to be consistent on the pattern we use here
Yes, I was suggesting separate methods. Something like this: ``` private void resetGeneration() { this.generation = Generation.NO_GENERATION; this.state = MemberState.UNJOINED; this.rejoinNeeded = true; } public synchronized void resetGenerationOnLeaveGroup(String causeMessage) { log.debug("Resetting generation due to consumer pro-actively leaving the group"); resetGeneration(); } protected synchronized void resetGenerationOnResponseError(ApiKeys api, Errors error) { log.debug("Resetting generation after encountering " + error + " from " + api + response); resetGeneration(); } ```
I understand that for cooperative, we should call onPartitionsRevoked before actually revoking the partitions. But it seems like for eager we should first assign, then notify that the partitions have already been assigned through the callback. So maybe for cooperative we should update the assignment between `onPartitionsRevoked` and `onPartitionsAssigned`
Note this correction
Hmm.. This method is invoked from `unsubscribe`. It seems like we might want to use `onPartitionsRevoked` in that case because it can still be a graceful handoff.
I think the idea is to validate that the new assignment doesn't contain any partitions that are being revoked during this rebalance
We can make this a little clearer and save some duplication with a signature like this: ```java protected synchronized void resetGenerationOnError(Errors error, ApiKeys api) ```
Nope, up to you. Just thought this might be more readable: ```java maybeInvokePartitionsAssigned(addedPartitions, firstException); ```
Do we need to use AtomicReference here? Seems we only call `maybeInvokePartitionsRevoked` once per branch
I think we can just relax ``` public int numberOfPartitions() { if (numberOfPartitions == -1) { throw new IllegalStateException("Number of partitions not specified."); } return numberOfPartitions; } ``` To allow it returning UNKNOWN.
`int` is what you want here, not `Integer`, right? It looks like we don't want or need this to ever be null. Should we throw an exception if latencyMs is set to a non-positive number? It's not clear what that would mean, or who it would be useful to. A millisecond is a relatively large amount of network latency. I suppose we can add a nanoseconds field later, though, if that becomes an issue.
getters should not use get. i.e. use `networkDevice` here, etc.
we don't typically use "get" in our getters, right? so this should just be `latencyMs`
In addition to passing through the timeout to `createRequest`, we also need to pass it through in `newClientRequest`. Only some of the APIs support an explicit request timeout. This appears to have been a pre-existing bug.
So our options here are either to raise an error to the user or adjust one of the configurations. Since `default.api.timeout.ms` is a new configuration, it is possible that a user has explicitly provided a `request.timeout.ms` which conflicts with the default `default.api.timeout.ms`. I think the logic should be something like the following: 1. If a `default.api.timeout.ms` has been explicitly specified, raise an error if it conflicts with `request.timeout.ms`. 2. If no `default.api.timeout.ms` has been configured, then set its value as the max of the default and `request.timeout.ms`. Also we should probably log a warning. 3. Otherwise, use the provided values for both configurations.
Nit: might be a bit clearer to add "WithoutSchema": ```suggestion public void insertConfiguredFieldsIntoTombstoneEventWithoutSchema() { ```
Nit: let's avoid moving all of these imports around. Our convention is to place the `java` and static imports at the end, and moving them unnecessarily just complicates maintenance.
I remember some of the metrics were lazily registered, i.e. they would only be registered if the corresponding action is called for the first time. Have we refactored it to always register all metrics up starting the task / process-node etc? Otherwise waiting for the stream state to transit to RUNNING may not guarantee all metrics should be already registered.
Thanks for double checking this! Then it lgtm.
If we call `getWindowStore`, it will give us a null because of type mismatch (expected window store, actually timestamped key-value store)
nit: move .collect to new line
Why replace `null` with `"null"` ? (similar for value)
nit: avoid inserting random blank lines.
nit: Avoid unnecessary reformatting.
I suspect the test failures in this class are due to the fact the value here is a String `"(1<-null)"` for the expected value but what is returned from processing is a `Change` object so the test fails. For example the expected values are created like ```java new KeyValueTimestamp<>("B", "(1<-null)", 10) ``` but should be ```java new KeyValueTimestamp<>("B", new Change(1, null), 10) ``` I suspect the issue is the same in some other failures as well when removing `toString` from the `equals` method.
I would favor a complete long number in this case instead of removing `L`.
nit: why double space? (similar below and further below)
nit: avoid unnecessary reformatting
as above (similar below)
as above (similar below)
nit: move `}` to next line
Why remove the empty line? It make it harder to read the code, as logical blocks are separated by blank lines atm. (similar below)
nit: move closing `}` to next line
nit: avoid unnecessary reformatting (similar below)
nit: line too long
value type is `long`
nit: preserve empty line after `checkAndClearProcessResult`
I disabled all IntelliJ auto-reformatting to avoid any unrelated reformatting.
nit: avoid double spaces
nit: with two parameters we usually do not need newline.
A little annoying to have to pass through `groupInstanceId` here, but I can't think of a better option. The only thing that comes to mind is passing through the full `JoinGroupResponseData.JoinGroupResponseMember` object, but that's annoying also.
If you want to do this, you could use `ThreadLocalRandom`.
As mentioned above, we can make this constructor default access.
Actually the constructor of all such description classes can just be default package-private since they are only used by KafkaAdminClient, and hence can just be private APIs, and we do not need to deprecate-overload any more.
nit: `leaveReason = "consumer poll timeout has expired..` So that the whole log entry would read as `Member sending leaveGroup request to coordinator due to consumer poll timeout has expired ..`.
I think we can remove this warn and just rely on the info entry above.
I was thinking that we can just pass in the statement in the above warn as the root cause into `maybeLeaveGroup`.
nit: line to long (break each parameter in it's own line)
Hey, I'm sorry, but can you explain what's going on here? `Sensor` doesn't override `equals`, so I'm not seeing how this assertion works.
Just chiming in, I think we're just defining the metrics in this PR and then actually recording them later. We might change the actual metric type later on when we look at how it's going to actually get recorded, so maybe we shouldn't worry too much about the selected metric implementation right now.
Ok, I took a closer look and had a bit of a flash-back to how confusing these metrics are... Maybe https://github.com/apache/kafka/pull/7057 will help.
In `threadLevelTagMap(String...)` there is a check: ``` if (tags != null) { if ((tags.length % 2) != 0) { throw new IllegalArgumentException("Tags needs to be specified in key-value pairs"); } ``` Should we do the same here
Can we just add https://checkstyle.sourceforge.io/config_sizes.html#LineLength so we never have to have this discussion again? :)
```suggestion public static void addAvgAndSumMetricsToSensor(final Sensor sensor, ```
Yes, each time the method is invoked (I.e., once per store), a separate copy of the string is placed on the heap. I previously didn't think this would be a big factor, but someone in the community profiled the memory usage of a long-running topology and found that these strings tend to accumulate over time. There's no need to worry about this for short-scoped strings like exception messages, but metrics are long-lived objects, and we benefit from making them static constants.
Oh, right, I forgot that the metrics registry returns the same copy of the sensor when all the name, description, and tags are the same... Thanks.
`Moving average duration` may be a bit confusing to readers, maybe just `Average duration of ..`.
Thank you @vvcephei !
Are the `storeType` supposed to have the dash already? If not this should be `storeType + "-" + STORE_ID_TAG`.
This is not part of the PR but I realized that `CumulativeCount` is in `streams.processor.internals.metrics` but not in `common.metrics.stats`. Is this intentional? @vvcephei
This is to be consistent with existing state store-level ids.
here it might work better to use `TestUtils.waitForCondition` instead of an arbitrary sleep call
nit: I slightly doubt whether these exact match tests are necessary
I would suggest we co-locate the description with metrics and refactor out the sensor creation part, this may help reduce code redundancy.
If we are using `MockTime` then it should be fine.
nit: for method _calls_, we usually format like this: ``` addAmountRateAndTotalMetricsToSensor( sensor, ...); Breaking the first parameter already reduced line length and seems preferable.
One thing I would suggest we do is to create an intermediate struct to store all the parameters, in case later we need to add more fields for sensor creation.
This introduces a circular dependency. We could move `listener_config.py` to `tests/kafkatest/services/security` so that SecurityConfig doesn't depend on Kafka. We could call it `ListenerSecurityConfig` instead of `ListenerConfig`. Alternatively, we could remove the dependency of SecurityConfig on ListenerConfig.
We should add doc which says that these are listener overrides without the listener prefix since we are adding listener prefix to these,
Obviously a pre-existing problem, but we should probably try to avoid brittle expectations like this.
If the reason is important, maybe we should put an enum type behind it or something. Anyway, not a major issue.
Do we have a test case which covers the case where the user seeks to a new offset while a partition is paused with data available to return? In this case, we expect the data to be discarded when the partition is resumed.
nit: extra space before `anyObject`
nit: should we add a new method that takes this additional parameter so we don't modify the existing calls.
Its quite hard to read a function call that has a null parameters being passed in. In this context its easy because the change was just made. This is a nit.
record.toString could be expensive, so it would be best to surround this in an if statement, like ``` if (log.isTraceEnabled()) { log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition); } ``` Of course, it's a bit unclear that we should be logging `record.toString` at all here, since we don't log it in the other cases. it would probably be enough to just log (at trace level) that we are retrying the partitioning for the particular topic because there was a new batch.
Yes, I think that's a good idea. We would want to know that we are retrying due to the current batch. We should also be surrounding this with an if statement-- the fact that it isn't right now is a (small) performance bug, which we don't want to perpetuate :) We don't know how expensive `record.toString` is but we can guess it's not free. Let's fix the call above too
nit: use final.
This should be able to be simplified to `return keyBytes == null && !explicitPartition`
Thanks for the explanation. Make sense.
I was thinking of adding a java deprecated tag. Let me reply to the voting thread on this so that we can get some consensus on this.
Let's avoid making whitespace changes like this, since they tend to make the change harder to read
Maybe "notifies the partitioner that a new batch is about to be created... when using the sticky partitioner..."
Maybe "attempting to append " would be better than "sending", since we might fail to append.
It would be nice to have information about what the "previous" partition was here in this log message. Also "retrying because of a new batch..." might sound nicer.
We need to call `transactionManager#failIfNotReadyForSend` here, so that we don't try to append to the batch when we are not ready to send. Also, we should remove `failIfNotReadyForSend` from `TransactionManager#maybeAddPartitionToTransaction`
OK, makes sense. I was missing that context.
nit: all of these calls seem to have two spaces before the false
We could actually show the client state by: `"Some clients didn't reach state RUNNING without any stand-by tasks. Eventual status: [Client1: {}, Client2: {}]", client1IsOk, client2IsOk`
nit: extra line
Could you try to factor out some setup code? For example, in each test you create and initialize the names of the topics, member IDs and the consumer IDs. You could initialize a bunch of topic names and IDs globally and reuse them in the tests. Another example is the creation of `partitionPerTopic`, which is really similar in all tests, only the number of partitions vary. Factoring out setup code, would make the tests more readable IMO.
I do not say, you have to use the setup method for `partitionPerTopic` in all tests. You could implement a method `setupPartitionsPerTopicWithTwoTopics(long numberOfPartitions1, long numberOfPartitions2)` or similar and use it in those tests that need two topics.
IMO, you can get rid of `topic` and use `topic1` instead.
I would move this assertion out to the assertion on line 351. With this move you save one parameter and you have both assertion next to each other.
Throttle time defaults to `0`, so this is not required
Why are we changing this? The constructor with a single argument takes a message and not the groupId
nit: new line
nit: new line
I don't feel too strong about this change. My points where a) to discourage using seconds for delays b) that this is not a general library such as the JDK or other in which I'd be all up for the fully customizable method version. Seems fine to leave as is though and not block this PR on this suggestion.
do we need both the `expectedStarts` and `expectedRestarts`? It seems like the former should be just one more than the other.
Sounds interesting, cc @kkonstantine
This might be miss leading for users who don't know the details. What about: ``` log.debug("Ignoring request to transit from PENDING_SHUTDOWN to {}", newState); ```
Not sure if we need this? If the shutdown is not clean, we logged an ERROR before in `StreamThread#run()`
There are two different error codes: - `VERSION_PROBING`: for this case we continue and rejoin the group; also the assignment is empty - `INCOMPLETE_SOURCE_TOPIC_METADATA`: for this case we shut down, and this case is already handled above. Hence, I think we don't need to log anything for this case? (Note that we log the version probing in StreamThread later and the metadata error is logged above already)
Why is this an ERROR? If we receive a SHUTDOWN signal, it's just an "eager exit" to not create tasks IMHO. We might want to log a DEBUG though. Would also update the message: ``` log.debug("Skipping task creation in rebalance because we are already in shutdown phase."); ```
Thinking about this, I am wondering if we should just change the FSM to allow this transition and simplify the code here? \cc @guozhangwang
nit: `clean flag` sound like an implementation detail, that may not be a good way to phrase it for an INFO log...
Well. Its double logging... Not a big fan of that. But if you insist to keep if, also fine.
I would give this test a more meaningful name, e.g. `shouldNotThrowWhenPendingShutdownInRunOnce`.
Shouldn't the indentation be as follows? ``` log.debug( "Skipping task creation in rebalance because we are already in {} state.", streamThread.state() ); ```
I would not use variable names in log messages.
```suggestion log.debug("Skipping task creation in rebalance because we are already in {} state.", ``` (minor suggestion)
Why did you remove the `return`? Without the `return`, the code will reach the `finally` block and log that the assignment took place.
Same as above, e.g. `shouldNotThrowWithoutPendingShutdownInRunOnce`
Can you elaborate? What do you mean by > otherwise the state won't proceed
Sure. Why not. I just would avoid the term "invalid" because it might confuse users (they may thing something bad happens, but it's expected and not bad).
Oh sorry, I misunderstood the code.
Shouldn't the indentation be as follows? ``` log.debug( "Encountered assignment error during partition assignment: {}. Will skip the task initialization", streamThread.assignmentErrorCode ); ``` The first parameter is one or two characters too long, though.
I thought about this when tightening the FSM before but the unit tests reminds me of one thing: our current contract is that we only transit to PARTITIONS_REVOKED when calling onPartitionsRevoked, which is called only once at the beginning of the rebalance today, so keeping it strict is better just in case we have incorrect partial rebalance procedure. With KIP-429 this may be violated so we need to revisit our FSM once Streams adopt cooperative protocols. cc @ableegoldman who's working on this.
My fault! I missed the parameter. I looked at the next parameter in the `StateRestorer` constructor which is a `long`.
```suggestion public void shouldIgnoreIrrelevantLoadedCheckpoints() throws IOException { ```
```suggestion final OffsetCheckpoint checkpoint = new OffsetCheckpoint( new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME)); ```
I would assign `storeToChangelog.getKey()` to a variable called `storeName` to make the code more readable.
I liked this refactoring a lot, thanks @vvcephei !
If `max.poll.interval` is still relevant, then this seems a little confusing. Assuming `max.poll.interval` is still in the picture, is the expiration determined by `max.poll.interval` + `session.timeout.ms` or `max.poll.interval` + `max(session.timeout.ms - max.poll.interval, 0)`.
Thanks for splitting out https://github.com/apache/kafka/pull/7076 @ableegoldman Please review the new PR, too, @pkleindl
Will do. My question regarding the null setting was about the DBOptions & ColumnFamilyOptions in RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter, why they are not set to null too. As for the long name, it's probably a german language thing ;-)
This is a meta question: I think we should really maintain functional methods that would be used for all types of tasks here, otherwise let each extended class have their own function.
I see what you mean. I think I'd still do it since the cost of these flaky tests are high and I bet people will use the wrong one if it's available.
Minor simplification we can do below: ```java if (targetNode == null || !awaitReady(client, targetNode, time, requestTimeoutMs)) { transactionManager.lookupCoordinator(nextRequestHandler); break; } ```
Not necessarily something we have to do here, but I think we should be able to get rid of this loop and just rely on the next iteration of `runOnce()` to handle retries.
It reads a bit strange to fall through to `lookupCoordinator` if we know the request doesn't need the coordinator. Maybe clearer with a slight restructure: ```java transactionManager.retry(nextRequestHandler); if (nextRequestHandler.needsCoordinator()) { transactionManager.lookupCoordinator(nextRequestHandler); } else { // For non-coordinator requests, sleep here to prevent a tight loop when no node is available time.sleep(retryBackoffMs); metadata.requestUpdate(); } ```
A helper for `awaitReady` might be useful as well. Might be a chance to consolidate the `awaitLeastLoadedNodeReady` path.
This is a breaking change, right? Same for the other `create` method in this class.
This was probably discussed in the KIP, but obviously the downside is that users won't get any warning or hint that they should transition. But I guess we don't get a substantial benefit from removing `AdminClient`, so maybe we'll just never do it.
It seems I also could approve it. I will read all code tomorrow and work with you to get this approved.
ok. Just curious what is target date when all implementation for this will be merged? cc@saisandeep
adding the registraiton part into Config file doesn't look good to me. Generally speaking Config classes are only there to express the config members and validation around them but not to take any action. SecurityUtils.java would be a better place.
Here we rely on insertProviderAt() programatic way BUT if in the application's context somebody else calls Security.insertProviderAt(provider,1) that provider will be given the priority for any conflicting Provider services+algorithms. This code works well if you have exclusive services+algorithms example SPIFFE but if you are writing a provider for Standard algorithms example TrustManagerFactory.PKIX then you may run into trouble since your insertProviderAt() call got overridden by somebody else in the application context/startup. When that happens I don't know easy way to fix it. I think It is important to call this out.
We have this kind of challenges today in some other context. Hence speaking from the real experience! :)
Do we need follow same pattern as other properties to define a new property which refers to SECURITY_PROTOCOL_CONFIG? Example: public static final String SECURITY_PROVDERS_CONFIG = SecurityConfig.SECURITY_PROVIDERS_CONFIG;
So let us say - we already have a single provider for JSSE but for Kafka we need one more to override with some of the common services+algorithm (example only override TrustManagerFactory.PKIX). We are at the mercy of the initialization sequence of calls, isn't it? Here the different providers could be owned by different teams and when we are in bigger infra setup it may be difficult to overcome this technical limitation. Only for Kafka applications the init sequence is different vs rest of the infra in the company. Basically, we are making an assumption which may not hold true and then we will be really stuck. What I am suggesting is - If we are calling out that limitation with these changes it is okay but otherwise it will result in a bug.
Suggestion on above function addConfiguredSecurityProviders: 1. I think using following format for security.providers: security.providers=provider1_classname,provider2_generator_classname key1:val1 key2:val2,... 2. So when parsing above config, if there is no parameters following provider1_classname, then we can think provider1_classname is java Provider, then insert it; if there are key:value pair parameters following provider2_generator_classname, we can think provider2_generator_classname is SecurityProviderGenerator, then create "Map<String, ?> config" from key:value pair parameters, then call configure and getProvider of instance of SecurityProviderGenerator. This way we can handle all different scenarios in the future.
Remove about two lines code and something like below? copyMapEntries(nextConfigs, configs, SslConfigs.NON_RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SslConfigs.RECONFIGURABLE_CONFIGS); copyMapEntries(nextConfigs, configs, SecurityConfig. SECURITY_PROVIDERS_CONFIG)
@jeffchao traditionally Kafka used key,value pairs in properties and pass it everywhere and each implementation takes look at this config and pulls their interested key,value pairs. Example, authorizer interface https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/security/auth/Authorizer.scala#L35 . The pluggable class when it gets instantiated a configure method will be called and all the key,value in server.properties will be passed and it will pick whats relevant to the class. We can do the same here instead of asking users append key,values into the a config which is hard to configure and hard to get it right.
That has its own complications because if my provider is only providing TrustManagerFacotry.PKIX then it can't provider other services+algorithms that might be expected in calls like SSLContext.getInstance(String protocol, String provider). SSLContext.getInstance might look for TLSv1.1 or TLSv1.2 etc which my provider doesn't really have and I don't have a way to fallback anymore once I go route of specifying "provider" in getInstance() calls. In short - once we have a Provider providing a Standard service+algorithm we may have to implement other services+algorithm also otherwise it may not work (like I mentioned for SSLContext)
On the other hand I was debating whether it's worth to use `buf.duplicate().get()` instead of flipping on the expense of creating a lightweight ByteBuffer object. Advantages is that we don't mutate the current buffer but instead work on a throwaway object.
nit: use `ApiKeys.LEAVE_GROUP.latestVersion()` here also
we only need the lock for setting the seed and calling `random.nextInt`, right? for the rest of the function we can avoid holding the lock since we're just iterating over an immutable list that can't change and calling stuff that is threadsafe anyway
oh sorry, I forgot this is the condition for staying in the loop. my mistake.
I think we should also check for empty here, so that we could give a nicer error message than "all components don't sum to 100" etc.
I think it would be better to call this something like `RandomComponent` since that's more consistent with the JSON
should this be `&&`? As it is, this loop could terminate even if we always return null and never non-null
we can get rid of the else block here and save some indentation. if we threw an exception then we're out of the function
For `shouldNotReturnDuplicatesInRanges` it seems best to use `processorContext.timestamp()` -- `processorContext` is passed in `init()` do you just need to add a member variable to the `Transformer` to store is so you can use it in `transform()`
That is expected. You need to add a member variable `ProcessorContext context` to the class and assign `this.context = context` in `init()`. In `transform()` you can use `context.timestamp()` to get the current timestamp.
nit: maybe drop "serializable"? Reads a little redundant.
nit: The mention of join group comes out of nowhere
If we are using this in KIP-447 as well, perhaps it makes sense to move it int the `consumer` package.
It should be public as it will be used in `producer` APIs.
Also add `@params topics`
nit: remove `which is`
nit: extra line
nit: could be organized as: ``` return assignors.stream() .filter(assignor -> assignor.name().equals(name)) .findFirst().orElse(null); ``` but it's up to you
This is for possible future extensibility if we want to add new fields: with raw types (map) we'd have to change the signature of the API.
s/assignments/oldAssignments, and name could be simplified as `toNewAssignment`
static import maybe better
same here, s/subscriptions/newSubscriptions and `toOldSubscription`
This name seems backwards.
I think we can use `Class.isAssignableFrom` to see what type it is rather than catching the exception. See `ChannelBuilders.createPrincipalBuilder` for a similar use case.
nit: It seems clearer to use `ConsumerPartitionAssignor.class` directly below.
```suggestion // Value `null` is valid for an optional field, even though the field has a default value. ```
```suggestion // Only return a default value fallback instead of `null` when the field is actually required ```
```suggestion // Value `null` is valid for an optional field, even though the field has a default value. ```
```suggestion // Only return a default value fallback instead of `null` when the field is actually required ```
I think we can remove the `to()` operator to verify if we don't fail with a NPE.
The stack trace on the ticket indicate that the NPE is thrown in this line, because `keyChangingOperationsToOptimizableRepartitionNodes.get(keyChangingParent)` returns `null` -- can you elaborate why it becomes `null`? From my current understanding, `keyChangingParents` should be a `Set` and hence we should get each item once and remove it once from `keyChangingOperationsToOptimizableRepartitionNodes` (in the old code). Why would we `get()` the same item twice? Seems I am missing something.
Yea I can understand the argument as I often debate it during development but the JIT compiler would inline these kind of methods so you actually don't lose any performance yet the code looks more understandable :)
It would be something like this: ``` private void assertResponseCountMatch(Map<TopicPartition, ApiException> errors, int expectedResponsesCount, int receivedResponsesCount) { if (errors.values().stream().noneMatch(Objects::nonNull) && receivedResponsesCount != expectedResponsesCount) { String quantifier = receivedResponsesCount > expectedResponsesCount ? "many" : "less"; throw new UnknownServerException("The server returned too " + quantifier + " results." + "Expected " + expectedResponsesCount + " but received " + receivedResponsesCount); } } ```
Oh, indeed, I missed that. And in this case you'll need to have an extra line with put.
Even if reassignments isn't empty, topicsToReassignments may be empty, if all the inputs are invalid. We should be checking topicsToReassignments here, not reassignments.
This could be more concise: ``` topicsToReassignments.getOrDefault(topicPartition.topic(), new TreeMap<>()).put(partition, reassignment); ```
nit: This block might be refactored to a method like `assertResponseCountMatch` so `alterPartitionReassignments` becomes shorter.
This logic is dependent on the broker-level KafkaApis on which errors are set at the top-level, and once the other side is changed this is vulnerable to lurking bug. I'd suggest we add a `topLevelError()` in LeaveGroupResponse that just returns the `topLevelError` only, and then here just check `topLevelError() != null`.
Throwing an exception here would just cause a `caller.fail`, and then caused a `handleFailure` instead. I think it's better just setting the exception in the future directly.
Also, we should handle all errors as well so that when constructing the result we know it's successful and hence we do not need to set any errors below.
I think the result does not need to include anything here if we organize the top-level future as a map of members -> the corresponding futures of `Void`.
This logic seems not correct? If there's a global error, we would fail all the internal futures. But that global error could be set because only one of the member response has an error as well.
Some of the internal awkwardness seems to be the result of not having a clear "list of classes" type. Not something we have to do here, but potential room for improvement.
If the subscription changes, the `onPartitionsRevoked` would be triggered and the owned-partitions in subscription would not include them. The original reasoning is just to make sure all partitions except the ones that are attempted for reassignment-hence-revocation-first are owned by someone. But like we discussed in the other PR, today we are not checking these conditions anyways, e.g.: 1. assignor assign one partition to multiple consumers. 2. assignor assign one partition to no one. They are both bad situations but Coordinator does not check for those today. We discussed about this and think it worth a more general solution as a separate JIRA.
Better be `cooperative-sticky`? `cooperative` is too general I think.
Even if you are in 2.4+, if you want to switch from EAGER to a different COOPERATIVE assignor that new assignor still need to support both in order for a simple rolling bounce path. I think it worth mentioning this as well (maybe better in web docs than here in java docs, just wanted to bring this up).
I think we would remove the `StickyAssignor` in the future if we realized that there's rare use cases that needs a eager sticky assignor; until then I'm fine with the current way as is.
nit: the phrasing "to never revoke" is a little awkward. Maybe something like this: > Note that you can leverage {@link CooperativeStickyAssignor} so that only partitions which are being reassigned to another consumer will be revoked.
We should mention somewhere that users should prefer this new assignor for newer clusters.
Assuming this interface is not implemented often, that seems OK.
I see, the assumption is that existing implementations will have overriden it.
I think the main downside is that type safety was lost. The compiler doesn't help you if you forget to implement the method.
This loop is a little puzzling to me. Is it not the same as the following: ```python assert all_captured_preferred_read_replicas[non_leader_idx] > 0, ... ```
Thanks for clarifying!
Ok. Thanks for clarifying.
> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation. I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)
Maybe consider: ```suggestion final CountDownLatch latch = new CountDownLatch(1); kafkaStreams.setStateListener((newState, oldState) -> { if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) { latch.countDown(); } }); kafkaStreams.start(); try { latch.await(IntegrationTestUtils.DEFAULT_TIMEOUT, TimeUnit.MILLISECONDS); } catch (final InterruptedException e) { throw new RuntimeException(e); } ``` Then, this method won't return until Streams is actually started, which we've seen can increase test stability.
This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology.
Not sure what the intent is here, to increment the number between each test, or between each instance of this integration test class within the JVM... It actually does the latter.
likewise, this one can be static
method should be static
Ah, yeah, there are some limitations to the linter. Thanks for taking care of it.
should be final
These three fields should be final as well.
generics can be inferred here.
Ok, I'm convinced :) Thanks for clearing up my confusion.
I see. But even though the number gets incremented after each test method, the string `inputTopic` is already fixed when the class is constructed, so it won't automatically get incremented. I think you need to make this a method to achieve the effect you intended.
Do we need an integration test for this? Using `Topology#describe()`, I think we could verify this with a unit test.
I had a similar thought, that it looks like good fodder for unit testing, but I did like the safety blanket of verifying the actual partition counts. I guess I'm fine either way, with a preference for whatever is already in the PR ;)
Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
Similar to above: we should be able to test with via unit tests using `Topology#describe()`
The reason why `ConcurrentSkipListMap` did not implement it in this way is because it is designed to be async, and we need it that way to support IQ / stream thread concurrency. But since only stream thread would put / delete into the map (assuming we forbid iterator.remove from the other PR so it is not allowed from IQ), then it is okay to have this field as non-volatile.
unused import checkstyle error.
If we're going to go down the route of wrapping everything, then we should hide the `MetadataResponseData` field so that it can't be modified to be different than the wrapped fields. So we shouldn't have `data` as a public method (it should be private, or absent)
Why not leaf the code "as-is" and change this line to: ``` if (suspended.values().contains(task)) { task.closeSuspended(clean, false, firstException); } else { task.close(clean, false); } ```
This does a system call every time. We can use `InputStream`'s `readAllBytes`: https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/io/InputStream.java#L389
If we do this, we should have an array of `byte`s and can just call `String(byte[] bytes, Charset charset)`.
I think this can only catch `IOException`s. We can be more explicit.
Hmm, why did we do this? I thought we'd have a try/catch block.
modifiers should be in the order `private final`
Very nit: we can let the `setUpMetrics` to return the `statistics` object, and then here since we already has the unwrapped `metricsRecorder` we can directly have `metricsRecorder.addStatistics(name, statistics, metrics, taskId);`
nit: extra space, ditto below.
nit: This last check is not needed, since it verifies functionality of the `Map` returned by `Collections.unmodifiableMap()` and not of the code under test.
Please remove `()`.
nit: I would remove this line
nit: use `assertThat` instead
nit: use `assertThrows` instead of try-fail-catch construct.
nit: unneeded boxing
I think this one and `ADD_OFFSETS_TO_TXN` should have magic v2 set has well.
Maybe this should read: > Perform leader election for the partition of the topic passed in as argument and waits until leadership changes replica.
Hmmm. If that is the case then at least `kafka-leader-election.sh` won't exist in older versions of Kafka. I think we added it in 2.3.
Yeah, let's remove if it's unused. We can add back if we need it.
nit: `fetchOffset` instead of `fetchedOffset`? I think this is meant to describe the offset from the fetch request.
What do you think about taking the name `CompletedFetch` here. It seemed a little more descriptive.
nit: indentation is 2 tabs (total 8 spaces) and its best to match the similar method below.
I should have used a better exception type initially. Because the class is indeed found. Probably `ClassCastException` would be more appropriate. Let's leave as is here, since the exception does not bubble up as cnfe anyways.
why the declaration is here, far from where it's used? Probably leftover
Should the error message not point out what went wrong, ie, "messages in the first batch were [not] processed in a timely manner" -- same below
If case of failure, we detect the failure only after `session.timeout.ms` (default 10 seconds) hit -- to speed up the test, we could decrease the session timeout via `StreamsConfig`
In most cases we don't have any message, so should be fine to remove. I see your point about `assert that bla` -- however, I think if the assertion hits, the error message reads different (ie, with reversed logic) and hence rephrasing would make it easier to read the error message if it fails (please correct me if I am wrong).
While I think it would be fine, it would be "new" -- if we think `instance` is better, we might want to migrate all other code lazily to use `instance`, too. It's always best to have the same naming conventions throughout the whole codebase IMHO.
nit: simplify to `throws Exception` -- in test, we don't intent to catch them anyway and just let the test fail
Should we produce the input before we start the KS instances? If there is no input, it's clear that Standbys won't restore as there is not data for restoring.
nit: `client1` -> `kafkaStreams1` nit `createClient` -> `createKafkaStreams` (similar for all other variables eg `clientStates` etc) It's technically a client, but also consumer and producer are and we also use `consumer`/`producer` as variable name there -- makes it easier to read -- otherwise it unclear what client it is.
nit: we prefer the following formatting (similar below) ``` public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) { ```
We can try that -- but we need to reduce `HEARTBEAT_INTERVAL_MS_CONFIG`, too, for this case -- its default is 3 seconds and it must be smaller than session timeout. Checking some existing tests, we have some that set them to 500ms / 1sec. Some other test reduce session timeout to 2 or 5 seconds...
Yeah from looking at the validation code, I'm not sure we'd hit this point. But if we did wouldn't we want to throw an exception as we do in the validation section? Although I think we may use a number higher greater than latest supported version for version probing.
Clear as crystal 
Could we still keep the log entry? `log.info("Unable to decode subscription data: used version: {}; latest supported version: {}", usedVersion, latestSupportedVersion);`
Would we want to consider building the `Set` the first time this method is invoked then caching for subsequent calls? Although looking at the code this doesn't seem to be on a hot code path and the task assignments could change, so I'm not sure. The same goes for the method below.
nit: move this `if` statement below/above version check on line 62
The topic may not be in the metadata due to some edge conditions, we cannot throw the exception in this case.
We have a `org.apache.kafka.streams.internals.QuietStreamsConfig` to use.
Yeah, let's do that: we should try our best to avoid code duplicates.
Hmm, in that case we can do this as-is then: I need to think through what's possible under the context of KIP-429 anyways so we can have a thorough discussion in a later PR.
Maybe clarify the path only needs to be unique when two instances are sharing the underlying FS. I can imagine users getting into weird trouble thinking they need to launch new instances with a uniquely generated state dir...
That's true, we log this before setting the state to PARTITIONS_ASSIGNED/REVOKED. That suggestion sounds good
@xin-au good idea with the timeouts, but I think we can simplify lines 310-329 down to a single `waitForCondition`. Something along the lines of: ```java TestUtils.waitForCondition(() -> !globalStreamThread.isAlive(), "Failed to shutdown a global stream thread,the global stream thread is still alive"); ```
Should we check for the thread has eventually joined, say when the streams instance is closed? With 10min we should not block on waiting, but also we should make sure we are not accidentally leaking threads.
The logic of `closeMetricsRecorder` and `metricsRecorderIsRunning` is a bit awkward here since the logic of `RocksDBMetricsRecorder#removeStatistics` and `#close` are actually the same except `stopRecording`. How about simplifying it to the following: 1. Just have a hard-coded recording interval inside `RocksDBMetricsRecorder` instead of letting caller pass in the value. Also maintaining reference to the background thread but not starting it at construction time. 2. In `addStatistics`, if the map is no longer empty afterwards (in practice it should be), starts the thread if it has not been started. 3. In `removeStatistics`, if the map becomes empty afterwards, stop the thread. Then in the caller, we only need `addStatistics / removeStatics` and do not need the flags for closing any more, and the logic of this function can be simplified also. For segmented store, we need to make sure that if the last segment was indeed removed, it would not try to create a new one, which means that at least two segments are maintained at the same time which I think can be guaranteed programmatically. To be safe, we can also maintain a flag or a enum `state` of the recorder of `created -> (addStatistics) -> started -> (removeStatistics) stopped`, and if `addStatistics` was called after it has transit to `stopped` throw an exception.
After closer look, I don't think there's a conflict between 429 and how the metrics are currently closed 
But @cadonna , will this remove the metrics whenever any store on the instance is closed? That seems to make sense with the current eager rebalancing, but with KIP-429 we will only close the ones that are migrated to another consumer.
We need to be able to remove just a few specific metrics without disrupting the others, while also making sure to actually close/cleanup during an actual shutdown
I think it should be safe to call remove here multiple times, the important thing is just to make sure it's actually *at least* once
Just wanted to say I really like the way this unit test is written! With the right usage of mocks we would avoid having any time-dependent flakiness.
Ah good catch.
Hopefully, there's no danger of these counters overflowing, but regardless, I think the math would still work out if we did reset them here.
I like this parity check. :+1:
(nit): not sure if this is any clearer that `1` (and `0`) above.
So there are metrics we would like to add but can't until we upgrade RocksDB? Can we create a 3.0 blocker ticket to add them back in when we bump rocks (and/or maybe a separate ticket to consider a major version bump of rocks with the next major version bump of kafka)
This seems suspicious... * Shouldn't all blocking operations have timeouts? * Should we be swallowing and ignoring InterruptedExceptions? It seems like a recipe for Streams to hang forever un-killably. But I feel like I'm missing something.
One caveat is that when we are closing the Kafka Streams instance with a specified timeout value, this function may violate that timeout and wait for longer time since we call `thread.join()` without a timeout value.
nit: maybe we can just merge `NEW` into `NOT_RUNNING`? I.e. the initialized state is just `NOT_RUNNING`.
Ah, you're right. I was thinking that the `numberOfOpenFiles` variable was a field (i.e., persistent).
```suggestion * Grace period defines how long to wait on out-of-order events. That is, windows will continue to accept new records until {@code stream_time >= window_end + grace_period}. ```
Hmm. I think we're just following the same pattern used elsewhere, but I think the version check is redundant. We already know it's a valid version because the request constructor verifies it.
How about the following to simplify the string construction below: ```java String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT); String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME); ```
Ack. Thanks for the details.
This is the essential line of the test that verifies the change in #7097. Without the fix in #7097, the listener will get an update with *all 3 task IDs*, and to pass this line would then need to be changed to: ``` configUpdateListener.onTaskConfigUpdate(Arrays.asList(TASK_IDS.get(2), TASK_IDS.get(0), TASK_IDS.get(1))); ``` However, we don't want *all* task IDs to be updated. Instead only want only the task ID(s) for the tasks that were indeed updated. That's why this test case expects that the task config update only includes the one ID of the task that is actually updated (i.e., `TASK_IDS.get(2)`). So as is, this test method will fail unless the fix for #7097 is actually applied.
No need to pass the spec in the constructor here, as all the connections have access to the internal spec.
I think these need to be `volatile` if we want them to work cross threads. I was thinking we should consider using `AtomicInteger` to avoid the need to increment these variables inside synchronized variables. I know it has a smaller cap (INT_MAX) but I imagine that should be enough for such a test
I think `totalAborts` will always be 1 because this branch will abort the task, meaning `!doneFuture.isDone()` will be true for all the other MaintainLoop threads. Is there a reason to save the total abort rate in such case? Might as well change it to a boolean I think
Shouldn't this be called once we refresh only? As far as I understand, this code will greedily refresh all possible connections (more than 1 every 10ms) if they are available. I think we should have a separate sleep call when there isn't a connection to maintain
suggest returning an `Optional<SustainedConnection` rather than `null` when none can be found - it helps avoid NPEs
We're only reading the variables here, I think we can get away without a lock.
nit: extra line
This throttle can cap our `refreshRateMs` per connection, right? e.g if we have only 2 threads and 4 tasks with a refreshRateMs of 5ms, I think only two of those tasks will ever see their connections being reset. This seems to expose a flaw in the way we find connections to maintain - by simply looping over the list we can't ensure that all tasks get an equal chance of a connection refresh. If it isn't too hard, maybe we should use some sort of heap ordered by last update time . Or maybe we can not throttle at all
I don't believe this is resolved. I still think we shouldn't catch Throwable
I suggest passing all of the variables we access in this constructor via`SustainedConnectionWorker.this.spec` to be switched to parameters we pass upon instantiation
Because we're not using the `subscribe()` functionality, we don't need to configure a consumer group. It will use a null group by default (https://cwiki.apache.org/confluence/display/KAFKA/KIP-289%3A+Improve+the+default+group+id+behavior+in+KafkaConsumer)
Do we need this config? `producer.send(record).get();` ensures we get a response from the request so I don't see the value in the config
Any reason to not initialize these in the definition? e.g ``` private long totalConsumerFailedConnections = 0; ```
nit: Could just to `new ArrayList<>();`
Yes it is. I was asking whether we might want to alter it for the default case. I think it's fine to keep it as it is
Never seen it in Kafka or in other software? I don't think it's a bad idea to accept the current time as a parameter. It makes your code way more testable for one thing
I think that's fine, I don't think there's a way to recover (nor if it makes sense) from an OutOfMemoryError - https://stackoverflow.com/a/352842
Ha, that existed. Strange, I remember searching for such a thing before and not finding it
I don't think we need a synchronized block here since `start()` is always called once at the very start
This won't log the error. We want to use the `public void error(String msg, Throwable t);` overload, so we need to change `e.getMessage()` -> `e`
What do we want to achieve with this throttle? Do we just want to backoff for `THROTTLE_PERIOD_MS` whenever we can't find a connection you sent? I think we should simply use a `Thread.sleep` call. To be concrete, I recommend we instantiate a `org.apache.kafka.common.utils.SystemTime` class and use both its `sleep()` to sleep and `milliseconds()` to get the current time
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
This won't log the error. We want to use the public void error(String msg, Throwable t); overload, so we need to change e.getMessage() -> e
Sorry about that. In the end I think I prefer passing it in but I don't have a strong opinion
Suggest creating this once and reusing it throughout the instance's lifetime. Is there a benefit to use a random number here? We could simply start from partition 0 and cycle through all partitions throughout all the refreshes
I suggest logging a message and passing in the error to the log message. e.g `log.error("Error while refreshing sustained admin client connection", e);` and ditto for all others. This makes it way easier to nail down errors
Yeah might as well change it I think, it results in shorter code
Not really against the style, just a nit of mine (My IDE recommends removing it). feel free to ignore.
I suggest only catching `KafkaException` and only close the client then. About other exceptions, we can probably try retrying or fail the thread, I don't have a strong preference. I don't think we should catch Throwable though, as that catches errors like `OutOfMemoryError`
Technically we may still have a connection. Do we need to close the client first before we decrement these? (ditto for others)
Why did you decide against passing in the spec or the arguments in the end? It makes the class more unit-testable (not that we have unit tests) and it's generally easier to read I find
Yeah. I think it's good given it's a variable - we sort of expect it to be set I guess. It would be really cool if Java supported having `final abstract` variables
nit: `completeRefresh` to be more concrete in what it completes? Complete can be understood to mean a total completion of the connection, whereas this completes a single refresh (something finer-grained and more often)
This needs to be `totalAbortedThreads`
nit: this can be `private synchronized Optional...` and we wouldn't need the synchronized block below
nit: this can be final
This isn't related to AK as it's not server software but rather a test client. I think it's fine if we continue with 1 thread less
This is exactly the same as the isStruct / isArray case and can be merged into that clause.
UUIDs can never be null. Therefore, it doesn't make sense to use null as a default. Even if it did, we don't choose null as a default for anything unless the user selects it specifically by writing `"default": "null"`. The default value you should use should be something like the all zeroes pattern. You can create a single (public, static, final) instance of one of those in MessageUtil or somewhere, to avoid creating new ones each time we instantiate a Message class with UUIDs.
nit: I know the admin client has an established pattern of packing everything into a single giant class, but maybe we don't need to follow it in all cases. We can pull stuff like this into `admin.internals`. That also makes it easier to setup a good pattern for unit tests.
My concern here is that we are retrying all the calls, not just the one that failed. If there are N calls and each one fails, then will will retry N*N requests. The simple approach is just to retry this call only. What I am thinking is something like this: ```java for (final Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry: leaders.entrySet()) { final int brokerId = entry.getKey().id(); final Map<TopicPartition, ListOffsetRequest.PartitionData> partitionsToQuery = entry.getValue(); calls.add(new Call("listOffsets on broker " + brokerId, context.deadline(), new ConstantNodeIdProvider(brokerId)) { @Override ListOffsetRequest.Builder createRequest(int timeoutMs) { return ListOffsetRequest.Builder .forConsumer(true, context.options().isolationLevel()) .setTargetTimes(partitionsToQuery); } @Override void handleResponse(AbstractResponse abstractResponse) { ListOffsetResponse response = (ListOffsetResponse) abstractResponse; Set<TopicPartition> partitionsWithErrors = new HashSet<>(); for (Entry<TopicPartition, PartitionData> result : response.responseData().entrySet()) { KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(result.getKey()); PartitionData partitionData = result.getValue(); Errors error = partitionData.error; if (context.shouldRefreshMetadata(error)) { partitionsWithErrors.add(result.getKey()) } else if (partitionData.error == Errors.NONE) { future.complete(new ListOffsetsResultInfo(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch)); } else { future.completeExceptionally(result.getValue().error.exception()); } } if (!partitionsWithErrors.isEmpty()) { partitionsToQuery.keySet().retainAll(partitionsWithErrors); rescheduleMetadataTask(context, () -> Collections.singletonList(this)); } } @Override void handleFailure(Throwable throwable) { for (TopicPartition tp : partitionsToQuery.keySet()) { KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp); future.completeExceptionally(throwable); } } }); ``` The important part is `rescheduleMetadataTask(context, () -> Collections.singletonList(this));`. We only reschedule calls individually. And even better would be to just create a separate operation context for each retried call so that there are no concurrency problems.
We can save this for a follow-up, but there's a lot in common with `ConsumerGroupOperationContext`. There is probably an opportunity to consider generalizing the notion of `OperationContext` and trying to share some logic.
I'm not sure I follow this logic. If we get a failure on one call, why would we fail _all_ futures? Seems like we'd just want to fail the partitions associated with this call. This seems to also be a bug in `deleteRecords`. By the way, I was looking for unit test coverage for these new APIs and didn't find any. I think we will need some test cases before we can merge.
We would want to check for `InvalidMetadataException` here as well. We need to go back to the Metadata call if we find a metadata error. This is similar to the cases when we call `rescheduleFindCoordinatorTask`.
I think `requireTimestamp` is only needed if we are not requesting the earliest or latest offset.
I think it's probably fine to use `Optional.empty` for the leader epoch in the ListOffset request. The admin client doesn't have the need for strict epoch validation like the consumer.
See also `handleGroupRequestError`. If the coordinator is in the middle of being moved, we want to retry rather than fail.
nit: could use `computeIfAbsent` to simplify this
Yes, that is the reason for the inconsistency. Obviously it was a mistake to let `OffsetAndMetadata` be serializable in the first place.
This is a bug. We can leave this field unset. The default will be -1 if needed by the schema.
It's not introduce in this PR, but while reviewing it I found in `OffsetAndMetadata` we are storing nullable `Integer` directly whereas in `OffsetAndTimestamp` and here we are still maintaining the `Optional<Integer>`. Maybe it's better to make the first one consistent with others.
The topic/partition-level errors are the following today: ``` /** * Possible topic-level error codes: * UnknownTopic (3) * LeaderNotAvailable (5) * InvalidTopic (17) * TopicAuthorizationFailed (29) * Possible partition-level error codes: * LeaderNotAvailable (5) * ReplicaNotAvailable (9) */ ``` For 5) we should be able to retry, and for 9) we can ignore -- right now we only check topic-level errors but not partition-level errors (line 3642 below).
On a side note, maybe we should add a global error code fields in addition to per-partition for those global errors like not-coordinator, illegal-generation, unknown memberId, etc.
nit: we can use `offsetData.compute` to replace the `getOrDefault` and `put` below.
nit: I think parenthesis are a little more conventional.
Not sure this makes sense. I'd expect we'd attempt to retry the metadata request if needed (e.g. to find the leader). I'm guessing this logic was copied from deleteRecords? We could possibly leave this as a follow-up and fix both APIs at the same time. If we don't fix it, it would be a regression in the consumer group tool because the consumer's list offset APIs have proper retry logic.
I think we should probably retry on coordinator level errors. Take a look at some of the other consumer group APIs to see how we handle errors. For example, see `ConsumerGroupOperationContext.hasCoordinatorMoved`.
nit: a little annoying for a newline in the middle of a map type
This returns the resulting map, so we can avoid the `get` below.
nit: in spite of the getter convention, I still prefer setters be prefixed with `set`
If we follow the same pattern as `ConsumerGroupOperationContext`, then this could be a static method which takes the response as a parameter.
Could we expose this like the following instead? ``` public KafkaFuture<ListOffsetResultInfo> partitionResult(TopicPartition partition); ``` Then we can keep the map internal.
Is the copy necessary? Seems like we just use this map to get access to the future. No harm if it's a superset of the partitions the retry is interested in.
The user is trying to access a partition that was not requested. I think we could raise `IllegalArgumentException` directly to the user.
This is a bit subtle, but I think we want to raise the `InvalidMetadataException` rather than constructing a new `Call`. The problem is that we lose the retry bookkeeping which means these retries will not respect the backoff. By throwing the exception, we let the retry logic in `Call.fail` kick in. This would be consistent with the logic in `getFindCoordinatorCall`.
Maybe doesn't matter, but seems a little more intuitive to check that the error is not NONE.
Given usage, this could probably be a Set.
nit: can simplify a little bit ```java return allErrors.stream().anyMatch(this::shouldRefreshMetadata); ```
nit: drop reference to consumer groups
Not something we have to do here, but one way we could improve this in the future is by taking into account leader epoch information from individual partitions. We can ensure that epochs increase monotonically in order to prevent using stale information during retry. Another thing we could do is reduce the topics we are fetching metadata for as the ListOffsets requests complete. Ideally we'd only be refetching metadata for topics with metadata errors.
nit: usually we drop the `get` prefix on getters.
If you don't mind, let's defer this to https://github.com/apache/kafka/pull/7449, just so we have the JIRA to go along with it.
Does this work for you? I think you need `-E` instead of `-e` to have a regex in sed.
Is there evidence that we really need all this stuff? It will just be another thing that we need to change when we move to a new version of the JDK. I think a better approach would be to take this stuff from a local file or an environment variable passed to the ducktape process.
This is Linux-specific (and even potentially Linux-version-specific). We should not fail the test if this doesn't succeed. Let's catch the error and log it, but continue, in that case.
This is needed if you want to persist the limit across a reboot. But we are not rebooting here. Get rid of this.
Instead of using `.format` and `+` to create the string, maybe use same way as `cmd` is constructed (using % to format, and multiline string without `+` but by ending with `\`)
might want to do this as an alternative, package-private constructor so we can at a minimum make `topicMetadataTimeoutMs` `final`. If we don't change this, convention in Kafka is to not use `get`/`set` prefixes on the method name.
Since this doesn't change functionality, we probably don't want to change this just to update to modern syntax. The more changes we make like this, the harder it is to backport other fixes that might overlap with this diff, and ideally we backport fixes aggressively (and in fact, this could be an example where we might want to backport to a version that supports jdk7).
+1. That should not be needed.
@bbejeck @guozhangwang Oops, looks like I missed this. Bill has a point here. I will probably log a JIRA to get this done.
rewrite test as above using `assertThrows()`.
remove try-catch and replace with: ``` final StreamsException s = assertThrows(StreamsException.class, () -> testDriver.pipeInput(consumerRecord)); ``` assert afterwards and don't re-throw.
`ConsumerRecordFactory` is deprecated via KIP-470 (merge recently) -- please update the code to use `TestInputTopic` -- we cannot merge as-is, because the build would fail if we use deprecated classes.
As we catch the exception we should swallow is and don't expect it
```suggestion "consumer is not part of an active group yet. You can try completing the rebalance " + ```
It seems a bit odd that `CommitFailedException` is a `KafkaException`, whereas `RetriableCommitFailedException` is an `ApiException`...
I've been trying to follow this. Looking into `ConsumerCoordinator`, I think I see what you're talking about, there are several opportunities to return early if the timer expires. But is `1ms` enough to guarantee that we'll actually get through to where we fetch the data? Is it possible to just modify the conditionals farther down to make sure that we can always make progress even when the timer is set to 0ms (i.e., the request is fully asynchronous). To me, a timeout of 0ms doesn't mean that the operation should take no time at all, just that it shouldn't block. It seems "within bounds" to still perform any operation we need in order to guarantee we make progress.
```suggestion "part of a group or is participating in a rebalance right now. You should first call poll to complete " + ```
```suggestion * This is a synchronous commit and will block until either the commit succeeds, an unrecoverable error is ```
Isn't this the "main point" of this PR -- we don't want to return nothing just because a rebalance is needed or pending
"Can be retriable" does seem to imply that in some circumstances it can be retried, while in others it _can't_ be retried at all. "Can be retried" implies it's up to the user whether they want to retry
I just happened across this in the broker-side code, turns out `commit` _is_ taken as proof that you're alive, in that it gets effectively treated as a heartbeat.
nit: If a user tries to commit during an eager rebalance, the `CommitFailedException` will still be thrown -- we should consider making this explicit, I worry users might read this and think it's ok to commit during an (eager) rebalance as long as they catch the `RetriableCommitFailedException`
nit: Can we make this more specific, or maybe provide examples? eg this happens if you try to commit offsets for partitions you don't own, or you fell out of the group
We probably need to either make this `volatile` or an `AtomicReference`.
Regardless of where the `lookupCoordinator` is triggered, we are only raising it from `ensureCoordinatorReady`, so I am not sure I follow the point about raising from other contexts. Note there doesn't appear to be any logic preventing multiple listeners from getting attached to the future. I think it would be better to always attach the listener when the future is created.
Hmm.. Are we safe to remove these given that this is a public API. It's _probably_ unlikely anyone is using them, but still..
Did you consider the compatibility of this change? We probably should have been more restrictive from the beginning, but is it a risk to change now? An alternative might be to just log a warning.
nit: The callers of this iterate the committed offsets twice. Once in order to check partition ownership and then a second time to invoke `updateLastSeenEpochIfNewer`. I wonder if we could consolidate these two loops.
Just want to check my understanding. The user may attempt to commit offsets while the broker has a JoinGroup in purgatory. In this case, we would send the older generation which would be doomed to fail with `ILLEGAL_GENERATION` once the join completes. In this case, should we still reset the generation as we do below? I am wondering if it is useful to remember the generation that an offset commit was sent with (perhaps inside `OffsetCommitCompletion`) so that we only reset if necessary.
I think we should not say that the commit can be retried. I would just say that the rebalance needs to be completed by calling `poll()` and that offsets to commit can be reconsidered after the group is rejoined.
`deteremined` => `determined`
typos: Drop "is an" before "fatal error"
typo: "partitions that *are* no longer assigned"
Does it make sense to move this check above a little bit? ```java if (offsetAndMetadata != null && subscriptions.isAssigned(tp)) { ``` That will make the log message less confusing.
nit: remove newlines
Ah, I see. Edit: Actually, upon closer inspection, it appears that I had got the order mixed up. Looks like all things are accounted for. That was my mistake.
I'm ok removing this API in spite of the compatibility concern. It's just that the other constructors are "standard" exception constructors and we have no real need to remove them.
nit: this probably breaks checkstyle
nit: I'd suggest "Ignoring the fetched committed offset"
Why did you separate the two cases? The respective `if`-bodies still contain the same code.
nit: `immediately` -> `immediately`
This exception message is not really clear to me. What about `"Cannot commit offset of topic partition " + tp + " since the partition was not dynamically assigned to this consumer"` or similar? In the code such messages usually start with an uppercase letter.
There's already an expected exception, we can remove the `fail(...)` call here.
Gotcha. Okay, two points: 1) It may be overkill, but you may want to consider separating the input (source, resource, etc.) files from the output (compiled class, JAR) files. It'd more closely mirror the build setup that's commonly used for Java projects and would probably make this code easier to modify in the future. 2) This bug initially surfaced with use of the `ServiceLoader` mechanism; it'd be great if we could have a test that verifies that the changes here fix how that works in isolated plugins.
Is it guaranteed that the contents of `errors` will only actually be compilation errors and not, e.g., warnings? Might be worth just using the return value of [call()](https://docs.oracle.com/javase/8/docs/api/javax/tools/JavaCompiler.CompilationTask.html#call--) to determine if compilation succeeded.
nit: let's keep calling this `savedLoader` as in the other places where we call this method because: a) naming consistency might pay off here more than elsewhere, b) calling it old might be misleading because you are keeping the reference because you want to restore it somewhere (rather than replace it as "old" would imply)
Nit: this line is redundant in combination with the casts that occur later on lines 208 and 220.
Cheating the compiler, woohoo!
How are dependencies for test plugins handled here? If someone wants to expand these tests to include a test plugin that requires some library that isn't available in the classpath for compilation during these tests, what (if anything) would they have to change in order to make things work? It seems unlikely enough that this would be needed any time soon so proactively making changes to support that seems unwarranted, but it'd probably be useful to at least note that the test plugins here are a little fragile if there's any current limitation on what dependencies they can rely on.
If these aren't expected to change at runtime, I'd suggest initializing the `HashMap` in the static block as a temporary variable , then wrapping it to create an unmodifiable map which is then used for the constant here: ```java Map<String, File> pluginJars = new HashMap<>(); try { pluginJars.put(..., ...); // More of the same } finally { PLUGIN_JARS = Collections.unmodifiableMap(pluginJars); } ```
Nit: I personally dislike being shouted at by exception messages ```suggestion throw new IOException("Could not delete old class file"); ```
Nit: could just throw the exception directly here; doesn't appear to be much benefit to putting that in a separate `setup` method.
If the assertion here isn't necessary for the test, we should probably remove it.
If all we're doing here is instantiating a map and then placing a single property into it, is there any reason to do so in a separate `beforeAll()` method instead of moving everything into the `setup()` method? Additionally, if that's the case, we can make `pluginProps` a local variable instead of an instance variable.
I wasn't aware that `finally` blocks had a large impact on performance, and haven't been able to find anything definitive on the subject that indicates they are. Can you provide a reference that backs up this claim? Fwiw, using a `finally` block would be more concise and readable here: ```suggestion try { return Utils.newInstance(klass); } finally { compareAndSwapLoaders(oldClassLoader); } ```
`ConfigProvider` was introduced in AK 2.0, so this change can't be backported to earlier branches.
`HeaderConverter` and this method don't exist prior to AK 1.1.
Default methods are not going to port backward past `2.0`, since default methods are a Java 8 feature. To work with Java 7, you'd have to make this an abstract class. I'm not sure it's worth that change, since there are quite a few other things that will likely have to change when backporting. For example, `HeaderConverter` was introduced in AK 1.1, and `ConfigProvider` was introduced in AK 2.0. Removing these won't be trivial, so that means we're probably going to need separate PRs for some of these older branches.
I don't think this needs to be static, since we're rebuilding this for every test method in `setup()`.
This method is called from within `newConverter`, `newHeaderConverter`, and `newConfigProvider`, so mentioning "converters" and getting the available converter implementation classes is actually wrong when used to find the config provider implementation class. One way to address that would be to pass in additional parameters to the method, but since we want to backport this to branches before `2.0` we have to make that work without method references. So one simple option is to rename the method to `converterClassFromConfig` and add a new method for `configProviderClassFromConfig` that does essentially the same thing but is tailored for config providers. Perhaps a better alternative is to dynamically determine the name and the `pluginNames(...)` based upon whether `klass` is a subtype of `Converter` or `ConfigProvider`. This keeps a single method, but is a bit more dynamic.
The other benefit is the way we are loading converters it could technically be loaded in a different plugin loader that what are keyed in the map since we're loading the ConnectConfig after swapping the class loader in `startTask`. If we prefer to keep it the way it is here, we need to revert that change.
Thanks. Not a blocker.
We can let this function to return the list of suspended tasks, so that in rebalance listener we can print it in log4j entry; by doing this we do not need the `suspendedActiveTaskIds`.
nit: these two functions can be private.
More specifically, during `onPartitionsRevoked` the restore-consumer is assigned either: 1) with active restoring task partitions, or 2) with standby task partitions With 1), upon `onPartitionsAssigned` we would continue restoring active tasks while no standby tasks would be processed yet, hence we could update restore-consumer with revoked partitions and newly added task partitions that needs restoring; With 2), upon `onPartitionsAssigned` there are two cases: 1) if there are newly added task partitions that needs restoring, we should "revoke" all standby tasks since we need to first go to restore those active tasks; 2) no new active tasks need restoring, we should only "revoke" those standby tasks that are re-assigned.
We cannot rely on the rebalance protocol from `setRebalanceProtocol` in task manager to determine the logic, since during a rebalance, one can send with two supported protocol but the `EAGER` protocol is still used --- only the leader knows which protocol is used when doing the assignment. At the moment it is almost impossible to let the listener know which protocol was exactly used (not what protocol was supported); but on the other hand I feel may be it is not necessary as well: If the COOPERATIVE protocol is used, then we are effectively suspend tasks in onPartitionRevoked, and then immediately close those suspended tasks in onPartitionsAssigned since they are called consecutively in `onJoinComplete`, and no one would ever be resumed at all (i.e. that `resumeSuspended` call would always be no-op with no passed in task parameters). So I'd suggest we make the listener to also be agnostic to the rebalance protocol, and just follow the current logic of suspending resuming, and we can even see if we could in later releases remove the whole suspending/resuming all together regardless of the protocols.
Note that the restore-consumer is shared for both restoring active tasks, as well as for updating standby tasks, and it would only start updating standby tasks until all active tasks has completed restoring and now in running state. This means at a given time, the restore-consumer would only be assigned with restoring active tasks, or with standby tasks; on the other hand, the `changelogReader` is only used for restoring active tasks. So far this logic is still correct, but I'd suggest we handle such two cases like `closeSuspendedActiveTasks` and `closeStandbyTasks` separately, and also clear standby records / reassign restore-consumer separately.
Line 124 here seems not correct any more: now that we've initialized `assignedActiveTasks` it would never be null. And since we added `addedActiveTasks` we should not condition on `assignedActiveTasks.isEmpty()` anymore.
nit: revert this change.
Maybe checking that `get(tp)` never returns null? If not it should indicate a bug.
Are there unit tests for the `StreamsRebalanceListener`? If yes, it would make sense to extract them to its own class `StreamsRebalanceListenerTest`.
I think I'm convinced :) Let's keep them as removed.
Yes that's right, but code-wise if the current restore consumer only contains partitions of standby tasks then the `updatedAssignment.removeAll(closedTaskChangelogs);` below would be a no-op since the `updatedAssignment` would not contain any of the `closedTaskChangelogs` (well, unless we assign the active and the standby of the same task to the same thread, which is even worse).
My only motivation is to simplify the usage of the flag a bit :) I feel that some conditions on this flag is not necessary.
Ah I think I was looking at an old commit :P LGTM now.
Should we make a copy of this collection, if the purpose is to save them before they're updated? It looks like otherwise this is a reference to the same collection as `active.runningTaskIds()`.
That's a good point that the same taskID may change with different partitions. I think what I'm trying to explore is whether we can make the logic a bit clearer to read while being protocol agnostic. That is, we can either 1) reason about the source-of-truth for added / revoked / remained tasks for both active and standby in `assignor#onAssignment`, and then we do not need to rely on the parameters passed in the `listener#onPartitionsRevoked / Assigned` at all. 2) rely on the `listener#onPartitionsRevoked / Assigned` to get the list of revoked / remained / added tasks.
I cannot see how a unit test for the rebalance listener does not make sense. I would rather unit test the listener in isolation and then mock it in the other unit tests.
I am always a little "concerned" about bumping timeouts if we don't understand why it actually fails. 30 seconds seems like quite some time. How long is deleting topics supposed to take? As far as I understand, we send a single request to delete all internal topics via AdminClient to the brokers. Is there a relationship between the expected completion time to delete all topic the the number of topics in the request? \cc @cmccabe
I looked at the test and at least one failed test namely `testReprocessingFromDateTimeAfterResetWithoutIntermediateUserTopic` does not have any internal topics to create, and hence none to delete. So it's not clear if bumping up the timeout would help here. I'd suggest we first augment the error messages to include the expected topics and the actual topics
nit: maybe use `finally` to restore this value.
Do you really need to verify this? You already verify (indirectly) in `getMetrics()` that your code specifies the correct group `consumer-coordinator-metrics`. IMO, the creation of the corresponding `MBean` is not part of the code you want to test here.
Also, does the below `assertEquals` not subsume the `assortNotNull` ? Seems redundant to check for not-null first.
Should we `assertNull` for all three before anything is recorded to check that they are created lazily? (similar below)
I am not sure if I completely follow what your are saying. Looking again at the code, I think, it would suffice to pass a mock `Metrics` object to `AbstractCoordinator`. The mock would return a `Sensor` mock for a call to `sensor()`, with which you can verify if the parameter passed to `add()` is an object of class `Avg` (or whatever `MeasurableStat` is used). This should be possible with `PowerMock`. We have to use `PowerMock` since `Sensor` is `final`. If you think, it is too much work given the upcoming release, I am fine with creating a Jira and postpone.
Should we have a separate `UnsupportedVersionException` for multiple endpoints? For example, we could have multiple plaintext endpoints.
Nit: wouldn't it be easier and perhaps less error-prone to pass the `JsonConverterConfig` into this constructor and set these fields directly from the config? It would also make it easier to call this constructor above, turning the multi-line call into a single line call.
nit: let's use `DECIMAL_FORMAT_CONFIG` instead of the literal string here.
That was my other thought. We might wrap one DataException into another but that's not a big deal. In this code `Decimal.toLogical` is now used, and if you follow the refs, there's a `new BigInteger(value)` in there that might throw `NumberFormatException`. Improbable, but thought we could be on the safe side with the more general catch block.
nit: typo at `withe`
@agavra instead of adding the `CachedConfigs` class could we perform the parsing at instation time in the `JsonConverterConfig` class and store results there, exposing them through getters? For example: ```java public class JsonConverterConfig extends AbstractConfig { private final DecimalFormat decimalFormat; // ... public JsonConverterConfig(Map<String, ?> props) { super(CONFIG, props); this.decimalFormat = DecimalFormat.valueOf(getString(DECIMAL_FORMAT_CONFIG).toUpperCase(Locale.ROOT)); } // ... public DecimalFormat decimalFormat() { return decimalFormat; } } ``` The `CachedConfigs` class adds a bit of cognitive overhead to this part of the code base; if we can avoid adding another utility class to have to track it'd be nice.
My concern was that, unlike most of the places where we check configs, several of these checks are used in the (de)serialization methods that are called with every record. Prior to this change, those configs were cached inside the converter instance and not cached by the `JsonConverterConfig` class, and my concern was that we were slowing the overall performance of the (de)serialization with the multiple checks. I think it's fine to cache them as final members in the `JsonConverterConfig` class, and reference them here, which is exactly what the current PR does.
Why are we removing these cached configuration values? The `JsonConverterConfig` class does not cache them, so every time we call a getter on the `config` instance -- which is at least one per value that is (de)serialized -- we are looking up and converting the string value of the configuration. That's quite inefficient at runtime. It's probably fine to remove these here as long as we add the cached config values inside the `JsonConverterConfig` class *and* (ideally) ensure all of the getter method calls on `JsonConverterConfig` can be inlined (e.g., making `JsonConverterConfig` final or making the getter methods final) to maintain performance. However, the latter part is more restricting and would not be backward compatible for anyone already subclassing the `JsonConverterConfig` class. So one option is to simply cache the values as final fields in `JsonConverterConfig`, have the non-final getter methods return these cached values, and hope that either the JIT inlines the getter methods (as long as there's no subclass loaded, non-final methods may be inlined) or the impact is negligible. The other option is to keep these final fields here in this class where we know we're using them very heavily and continuously. This may require changing the `LogicalTypeConverter.toJson(...)` method signature to pass the converter instance rather than the config. That's a tiny bit more messy, but we know we'll get faster evaluation of the various config options. I would prefer the second option simply because we can ensure this `JsonConverter` logic -- which is used very heavily -- is as fast as possible.
By returning here, we're losing the logic immediately following this line that checks for a null schema. For example, if this method is called with a `BigDecimal` value, then the `logicalConverter.toJson(...)` call will ultimately result in calling `Decimal.fromLogical(...)` with a null schema that will result in a NPE when that method attempts to get the scale of the decimal schema. We should always avoid NPEs, but also the KIP says that a `DataException` will be thrown in this case. BTW, we should have test cases where the JsonConverter is called with a null schema (i.e., the schemaless case) to verify the behavior in the KIP.
This should be `org.apache.kafka.common.message` to match the directory that the generated source files are outputted to
This should be `org.apache.kafka.common.message` to match the directory that the generated source files are outputted to
This should be `org.apache.kafka.common.message` to match the directory that the generated source files are outputted to
should be `final`
If that's the case, then we may need a separate PR to go back further than 2.1. I'd say we'd want to correct this issue (which has irked quite a few people) as far back as 1.0.
I think it'd be useful to verify the behavior of casting all of the logical types to strings, just so that we verify the formats (e.g., timestamps, times, and dates should use the ISO 8601 representation) and have some regression tests for the future to help ensure we maintain backward compatibility.
No, I'm talking more about the `encode*` and `castTo*` methods in this class. I'm not suggesting we make them public, but instead I'm asking whether it would make sense to make these *protected* and *non-static* so that it would be easier to create subclasses. I guess the problem with that, though, is that they are implementation details and not part fo the public API. If anybody did create a subclass and we later changed the implementation, their subclass would no longer compile and they'd have problems running on different versions of Connect. Given that, I think it's fine the way it is: private non-static or private static doesn't really matter except for subclasses.
Could also do the following to be a bit more succinct: ```suggestion assertEquals(Schema.Type.INT32, transformedSchema.field("date").schema().type()); ```
It would be great to have a few more test cases to cover more scenarios: 1. cast to types other than `int32` and `int64` (e.g., `float64` and `float32`), including where we lose precision 2. verify we can cast these to strings 3. verify the cast fails in expected ways 4. casting null values should not fail
Is there a reason not to include support for casting `DECIMAL` to other primitives? Yes, this might be lossy, but it also would help in cases where there would be no loss of precision.
Can we easily make these non-static and protected so that they can be customized by a subclass if needed? We should avoid making too many changes other than what's required, but if this only affects a few other lines it might be worth it.
Nit: this is actually a timestamp column with a null value, and it might be worth calling that out here. ```suggestion "null_timestamp_to_int32:int32" ```
Should we chain the `UnsupportedVersionException` `e` when creating the new `InvalidRequestException`? E.g. ```java throw new InvalidRequestException("Unknown API key " + apiKey, e); ```
nit: chain these c'tors to consolidate code. Makes it easy to do validation etc in case a need arise in future.
Is this used anywhere? I see we have changed client code to use the other C'tor.
nit: replace with `this(new RequestHeaderData(struct, headerVersion), headerVersion)`. Consolidates c'tor logic to one place.
There's a bit of a change in behavior in this PR. We would previously return an empty string for older request versions and now we return `null`. Is that intended @cmccabe? cc @hachikuji
Actually, it's not just older requests, the default was always `""` whereas I am seeing `null` after this PR. It looks like there are two issues: 1. We didn't set a default of `""` for clientId in the json schema. 2. The generated protocol code behaves differently with regards to default values. It only uses the default if the field is not present. The `Struct` code uses it if `value == null`. ```java Object value = this.values[field.index]; if (value != null) return value; else if (field.def.hasDefaultValue) return field.def.defaultValue; else if (field.def.type.isNullable()) return null; else throw new SchemaException("Missing value for field '" + field.def.name + "' which has no default value."); ``` This is the generated code _after_ I add the `""` default for `clientId`. ```java public void read(Readable readable, short version) { this.requestApiKey = readable.readShort(); this.requestApiVersion = readable.readShort(); this.correlationId = readable.readInt(); if (version >= 1) { this.clientId = readable.readNullableString(); } else { this.clientId = ""; } } ```
The protocol behavior is that `null` is valid while we use a default empty string so that it's never seen by users. The generated protocol classes definitely change behavior here as it caused a previously passing test to fail with a NPE. I'll share the details with you offline.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
Since we have a Jira ticket that is even referenced here, I would prefer to remove the ToDo from the code.
This must be `OUTPUT_TOPIC_2`
The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess.
yes, it seems to be not what this test is checking on. I think we can drop it here.
It seems like the purpose of this test class is to verify the test driver. Since the partition is effectively not part of the test driver's (non-deprecated) API, I think we can drop it. Technically, as long as the deprecated interface is still in the public API, we should still exercise it, but I think in this case it's extremely unlikely we would break the partition field in the future, and it's probably not that risky anyway. I'd vote just to skip the partition assertion here.
Please get rid of this volatile. There is no longer a circular dependency here.
Add a reference to KIP-511 here
we should not hard-code version 3, but programmatically find the highest supported version and send something higher , so that this code doesn't have to be revisited if there is a new version
Should we restrict the values for name and version? Maybe we can just test that they are non empty? nit: the non-capture group isn't really necessary and this regex matches stuff like `"."` and `"---"`.
sounds good! Thanks for the confirmation.
If it is no more an integration test, this should be removed.
Are these cases already covered by existing tests over there? I didn't see them moved as part of this PR.
Not sure I follow the logic of this test... We want to show that it doesn't block, but then we just assert that the call doesn't take longer than 30 seconds. It seems like this test is sensitive to timing, which is not ideal. I guess that what we really wanted to test is that the close method never calls `Thread.sleep()`/`Thread.join()` or maybe just never calls a specific list of blocking methods on the clients. Now that we have all the mocks above, it seems like we can make this assertion directly.
Why do you need to prepare `KafkaStreams` for testing? Only classes that are mocked and that are either `final` or have static methods need to be prepared.
Can we just rely on the equals function of `map` and `list` here? I think it checks for "exact equality", while here `otherAssignment` being a super-set of thisAssignment can still pass.
SG, thanks for the explanation!
Is it ever possible that the `clientState.ownedPartitions().get(partition)` is not null but `allOwnedPartitions.contains(partition)` is false? It seems the second condition is redundant (and hence the `allOwnedPartitions` parameter throughout the `assign` function seems not needed).
This is not introduced by this PR, but: is line 934 necessary? We checked `sortedTasks.isEmpty()` at the head of the loop and there's no other operations on this list anywhere else.
nit: It's a bit weird to do the allocation of standbys inside `addClientAssignments` logically. I'd suggest we move this out of the function, and just do that in the parent caller, in the order of: 1. deciding the assignment of active (interleave or give-back). 2. deciding the assignment of standby (always interleave). 3. set the partition assignment accordingly (maybe remove owned partitions).
nit: again this is not introduced in this PR, but let's use `AssignorError.NONE` here and elsewhere to be less vulnerable to enum changes.
It's a bit anti-pattern to use `null` and `""` indicating two different sentinel cases: the partitions is owned by different clients, or the task is new. I think there's a better way to save on not redundantly iterating through taskPartitions: e.g. we let it to return a `Collection<String>` (and rename to `previousConsumers...` which is the union of the claimed owners of the partitions, and then in the caller we can just treat: 1) it's a singleton, 2) it's empty, 3) it's a plural, differently.
I think the test coverage is not sufficient with the augmented logic here, we should at least test the following code path: 1. among clients, with EAGER set we still try to honor stickiness based on user metadata (this may be covered already). 2. among clients, with COOPERATIVE we still try to honor stickiness based on owned partitions. 3. within a client, we try to reassign tasks back to consumers if the prev consumers are fixed, and load balance is not violated. 4. within a client, we if we cannot satisfy 3), we interleave. 5. within a client, upon version probing we interleave.
nit: move those to lines 46 below.
My bad, mis read the lines :)
Is this suggested by IDE? :P I feel it is not necessary but if I do not feel strongly either.
nit: how about just put these two in one line? We usually only use multi-lines if there are 3+ parameters.
I think the name of the function is better defined as `interleaveTasksByConsumers`
I think the logic here is not very intuitive as we are doing a double loop here. What we could do is to loop through all the unfilled consumers and fetch unfulfilled amount of new tasks to fed them. This could also reduce the number of loop cycles we have to go through.
Similarly here, I think we can move these checks into `TransactionManager` and just pass the batch.
Mainly my goal is to encapsulate as much of this logic in `TransactionManager` as possible to make testing easier. After revisiting this logic, I feel there's a bit of inconsistency and/or redundancy between this check and `TransactionManager.canRetry`, which will return false if the producerId and epoch are not an exact match. As far as I can tell, the only reason this parallel logic exists is because of `Sender.canRetry`, which includes the following: ``` ((response.error.exception() instanceof RetriableException) || (transactionManager != null && transactionManager.canRetry(response, batch))); ``` So we seem to be trying to prevent unsafe retries in the case of a retriable error. I am wondering if we can consolidate this logic in `TransationManager.canRetry` and then change this check to use `&&`. That is: ``` ((response.error.exception() instanceof RetriableException) && (transactionManager == null || transactionManager.canRetry(response, batch))); ```
We should spell out that if the string is empty, it's because the remote end didn't send this information
Please get rid of the old constructor. If people want to create a RequestContext without the software name and version information, they can easily do it explicitly with the other constructor. Keeping this around just leads to confusion.
This is wrong. Unsynchronized access to a map could cause more than just "stale or inconsistent" data. It could cause null pointer exceptions or other issues. We cannot access this without synchronization.
I don't think it's necessary to keep a central registry of all this information for all connections. We really just need the metrics, most of which can just be simple counters. If we need more information about a connection, we can look at the request context of that connection. But it doesn't have to be stored here.
We should spell out that if the string is empty, it's because the remote end didn't send this information
Thanks for the explanation, and thanks for fixing the synchronization. As an optimization, I think we should use something like `ConcurrentMap` here rather than synchronized blocks, so that we can minimize the amount of time threads spend waiting. This will be a bit more tricky to use, but more scalable. I guess when I thought about KIP-511, I thought of it in terms of metrics and perhaps occasional samples of connections (similar to how we do request sampling by logging a few selected requests). I don't see why we'd ever need a full snapshot of all existing connections. The snapshot would probably be out of date by the time it had been returned, since new connections are closed and opened all the time. Reading the KIP more carefully, I see that KIP-511 does specify a metric which essentially requires each connection to register itself. Considering we don't even have a way to visualize or graph this metric, I'm not sure this belongs in JMX. I have to think about this more...
nit: we should update the name too if we're changing this from a list
nit: formatting -> only one parameter per line
nit: `COGROUPKSTREAM-AGGREGATE -`
I think this is a good to add, about the metric name, wdyt about `rebalance-latency-total` or `rebalance-latency-cumulated-total` instead? I felt it is more consistent with other namings.
Why have an `addAdminResources(...)` method when it could simply call `register(...)` like the previous and next lines? The method is private, so it's not useful for subclasses. I understand that we'd be duplicating some code, but it seems more complex to have the method abstract the one call.
Might be good to add a debug log message right before this.
Should we handle the case when `adminListener` is empty or blank? See line 126 above.
Wouldn't it be much easier to do the following: ``` public Connector createConnector(String listener, String name) { ... String hostname = ... int port = ... ... if (name == null || name.trim().isEmpty()) { name = String.format("%s_%s%d", PROTOCOL_HTTPS, hostname, port); } connector.setName(name); ... ```
reassignment to a final? Recommend: ``` final int totalCapacity; if (...) { totalCapacity = ... } else { totalCapacity = ... } ```
I think having this inline in the constructor is nicer because it prevents someone from finding `maybeGetRocksDbMetricsRecordingTriggerThread` and using it thinking that this would just return the thread vs. create a new one. An alternative would have been to call this `maybeCreateRocksDbMetricsRecordingTriggerThread` to make the intent clearer, but inlining still seems preferable.
Very good read thanks!
maybe instead to check that `rocksDBMetricsRecordingTriggerThread` is `null` instead? Here and elsewhere.
IIUC, `rocksDBMetricsRecordingTriggerThread` is either going to be `null` because it didn't meet the criteria for creating the thread or it is going to be non null because we did create it. Given that, checking for `null` is a nicer way to determine if we need to shutdown because as long as we have a thread, regardless of the condition that created it (e.g. `RecordingLevel.DEBUG`), we should shut it down. This is also safer if the creation conditions ever change and we forget to update them here.
Those are good points, making a one-pass num.partition decision is not critical in our framework, and I think it's more or less a brainstorming with you guys to see if it is possible :) To me as long as we would not be stuck infinitely in the while loop it should be fine. If user pre-create the topic with the exact `xx-repartition` name, then yes I think that could make things tricker. Also with KIP-221 the repartition hint, I'm not sure how that would affect this as well.
Basically, when ordering the non-source node groups we do not rely on `Utils.sorted(nodeFactories.keySet()` but rely on some specific logic that those non-source sub-topologies with all parents as source sub-topologies gets indexed first.
This dates before this PR, but while reviewing it I realized that line 898 in prepareTopic: ``` topic.setNumberOfPartitions(numPartitions.get()); ``` is not necessary since the `numPartitions` is read from the topic.
I'm +1 on removing this logic, we could not assume that the leader did not change even if it is not the one who get's bounced, since it may join the group late.
I'm confused by this, we removed the `StreamsConfig.METRICS_LATEST` config, but we've replaced it with a literal string value of the same thing. EDIT: NM re-reading the PR I think I get it now
Maybe get the value of this expression on the line above. IMHO it will make it easier to see what the `assertThat` is doing.
Nice refactoring to put this code in a separate method!
It's better to not use this annotation, because if any other line than the expected one throws, the test would incorrectly pass. It seems some existing tests, also use this undesired pattern. Feel free to do a MINOR follow up PR to improve the test code.
Use: `StreamsException e = assertThrows(StreamsException.class, () -> stateManager.initialize);` and also add `assertThat(e.getMessage(), is("..."))`;
```suggestion Arrays.setAll(topics, i -> topics[i].trim()); ```
do we really need to process this in parallel? seems like `setAll()` should be simple and clean enough.
Here, not necessary either.
Below we check and construct a subset of `validTopicNames`; we should skip if that subset is empty, and just return the future with exception with the rest invalid topic name.s
Ditto, check `topicNamesList` below.
Ditto: check `aclCreations` below.
Below we break into `brokerResources` and `unifiedRequestResources` and send two requests; we should check them and skip if possible respectively.
This is not necessary, since the for loop below would be a no-op.
This is not necessary, since the for loop below would be a no-op.
This fails checkstyle. We need a space after `if`
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
nit: add a newline here too.
What kinds of failures are we trying to mask here? Frequently retries also require delays between retries.
Probably fine currently given max number of retries we specify, but would really be preferable to do this iteratively (which also doesn't require copying/mgmt of kwargs like this).
What do you think about renaming this local `generation` variable (and the one below) to distinguish them from the `AbstractCoordinator` field (or vice versa)? To make it easier to keep track of which `generation` variables can be null (these) and which cannot (instance variable). Not sure there's a better name out there, just something to consider.
Ah sorry, didn't see it there at the bottom 
Can you explain this a bit further? Why do we return null when the group is stable, I assume stable means everyone is successfully on the same generation? So why return null rather than the actual generation.
I think you should have a separate method `generationIfStable` or something.
Not clear why we want to use a separate thread to call `joinGroupIfNeeded`? In unit test we would try to avoid any unnecessary multi-threading as it can easily cause flaky tests.
Hmm, why `client.hasInFlightRequests()` is false? I thought it should have the sync-group request queued up here? Could you explain just for my own education.
Thanks for looking into this!! I tested it locally and it passes here.
In order to reproduce this issue, we need to reset the generation via `maybeLeaveGroup` before the `onJoinComplete(gen.generationId, gen.memberId, gen.protocol, memberAssignment);` is triggered, but after the join-group response handler is exercised to set the generation id. I think this can still be doable with a single thread, to execute in the following ordering: 1. we only prepare the join-group response in the mock network client, but not the sync-group response, and we also make the MockTime to be able to advance time automatically, then by calling `joinGroupIfNeeded` with a small timeout instead of Long.MAX_VALUE, it should be able to finish the join-group round trip, trigger the handling logic to set the generation id, and then send the sync-group request, but then time out on https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L400 waiting for the sync-group response and return false. 2. Then we call `maybeLeaveGroup` within the same thread. 3. Then we prepare the sync-group response in the mock network client, and call `joinGroupIfNeeded` again, this time the response would be received, and the rest of the logic https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L406-L415 would be executed.
Also ran client:tests and all extended `BaseConsumerTest` and all passed.
This is not related to change in this PR, but after second read, should we setup non-empty topology here. to prevent IllegalStateException to be thrown due to empty topology after some future change. ```suggestion testDriver = new TopologyTestDriver(setupSourceSinkTopology(), config); ```
This is not related to change in this PR, but after second read, should we setup non-empty topology here. to prevent IllegalStateException to be thrown due to empty topology after some future change. ```suggestion testDriver = new TopologyTestDriver(setupSourceSinkTopology(), config); ```
Could we also move this only to the `StreamTask`? Doesn't have to be in this PR.
Apparently it is not completely save to call instance methods from the constructor since the object and its member variables are not completely initialised yet. Static methods would be fine, though. I do not think that `initializeCommittedOffsets()` is currently problematic since it only accesses member variables from `AbstractTask` which should be already initialised. However, `initializeTaskTime()` accesses `partitionGroup` which is created in the constructor. Since we cannot know what reorderings a compiler does, it would be better to specify `initializeTaskTime()` as static and pass in `partitionGroup`. To make `initializeCommittedOffsets()` future-proof, it would also be good to also transform it to a static method.
Oh, I see. I did not think about that dependency. Nevertheless I think somehow it would be possible to move `initializeTopology()` to `StreamsTask` without an `instanceof` but that needs a bit more thoughts and refactoring and it is out-of-scope for this PR.
nit: use the `SecurityManager` interface
Minor: maybe it's better override the override the acks to always be trimmed string inside `postProcessParsedConfig`, and then here we just check that the value is `all` or not; this way we can keep `parseAcks` as private static inside `ProducerConfig`.
Here we can further refactor a bit: ``` if (config.idempotenceEnabled()) { // read out the acks string value if (string value is null) // just log an info saying we are gonna override it to -1 else if (string value is not "all") // throw exception } ```
How about we reset the offsets between tests? I think that may be what you're suggesting. It would be cleaner not to accumulate metadata over time.
Can be final
I've been doing the delete / reset offset path in all of my tests and it seems to be going fine. For the output topic I delete and do create in a loop because the list command sometimes does not show the topic but then create fails because it thinks it still exists. We could get the committed offset at the beginning and fail if it is greater than 0.
I think we need to remove the leading space here and on the next three sections? It would be good to not change the line of code if the only difference is whitespace. It will also keep your name out of `git blame` unnecessarily :).
Nit: If we do fix up the above example of this it makes sense to fix this up too.
Since it looks like we expect numRecords to always be set, should we error out here if record is null? That would seem to indicate that our source topic was under-sized or some other issue (GC pause?) caused poll to timeout.
Nit: I know this was inherited from the existing code, but it would be nice to do something like `ByteArrayDeserializer.class.getName()` for serializers and deserializers.
Should be something like: ``` assertThat("KafkaStreams did not transit to RUNNING state within " + timeoutMs + " milli seconds.", countDownLatch.await(timeoutMs, TimeUnit.MILLISECONDS)); ```
nit: make this string and the one below static final.
I would be more concrete: ``` RocksDB version will be bumped to version 6+ via KAFKA-8897 in a future release. If you use `org.rocksdbOptions#WhatEverMethodWasRemoved`, you will need to rewrite your code after KAFKA-8897 is resolved because those methods were removed from `org.rocksdbOptions` class without a deprecation period. ``` Or something like this.
I like the addition, we can reuse this when they inevitably do something else we'll need to warn our users about 
Personally I think we should call out the specific method that was removed so we don't cause undue panic. It was only a single method, after all.
> It seems to me a lot of work for little result to research all methods removed without deprecation period. Exactly -- we shouldn't push this to every user on every upgrade. It's annoying, so, we should just do it exactly once, and only before considering an upgrade.
Need to make this public to allow `KafkaStreams` to use the helper method.
nit: put `@Override` on its own line
Why do we need a `try` for this case? The `close()` is a no-op anyway. (Similar below)
Since the calling code already knows whether it's a key or value, how about just having separate methods? Yeah, they'd be mostly the same, but we could avoid the superfluous logic and could simplify things a bit. Also, would it be better to wrap the exception rather than just log the error? Especially with the retry operator, it's possible that the error won't get logged near this log message, so we'd lose the correlation.
It actually would be helpful to include the exception's error message in this line, since the message alone might be bubbled up via the REST API. ```suggestion log.error("{} Error converting message key in topic '{}' partition {} at offset {} and timestamp {}: {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e); ```
It actually would be helpful to include the exception's error message in this line, since the message alone might be bubbled up via the REST API. ```suggestion log.error("{} Error converting message value in topic '{}' partition {} at offset {} and timestamp {}: {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e); ```
Indeed, we could. I am not sure that it brings much more information though so I am fine with keeping it as it is.
If you want to check that records for non-pause partitions are cleared, we should have a separate test.
nit: rename to `shouldNotClearRecordForPausedPartitions()`
> Was also wondering if there could ever be an exception thrown by addListener which would cause the listener to not be added or the completion handler to not be called? Hm good question ... find it hard to imagine as implemented unless we end up with multiple listeners executing on the consumer thread & a listener that precedes this one throws or something along those lines. And in that scenario right now I think we'd expect the exception to bubble out of KafkaConsumer.poll(), which would at least give us a clear signal that something went terribly wrong.
(@thomaslee reminds me that `send()` can throw and then we're left with a dangling reference)
nit: do we need the "" in the end for conversion? I think Java is smart enough without it.
Ah ok fair enough -- thanks!
Nit: add static import
Ahh! https://github.com/apache/kafka/pull/7500 allows you to wait for running state while still using an existing listener. But still waiting for it to get merged.
@kkonstantine, thanks for the correction -- the admin properties for the sink worker task (and specifically the DLQ reporter for the sink task) should use a combination of the `producer.*` and `consumer.*` properties that are connector-specific.
Personally I'd vote for focusing on correctness now and saving any larger scale cleanup/refactoring for a follow-up PR
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
My bad, I thought the `upgradePhase-Starting a REBALANCE` message is used somewhere in verification and thought with the augmented log4j it can now be replaced, but now I see it is only for debugging purposes.
Yeah my point here is that we can just hard-coded it than passing it through.
Not saying you need to change this or anything, but just want to point out that once everyone is on cooperative this state change actually indicates the _end_ of a rebalance.
nit: conifg -> config (applies elsewhere too)
Is it necessary to make `TASK_DELIMITER` configurable? I feel it is unnecessarily complicating the setup..
We should first check to see if this is closed, and if so simply return.
It is now possible with your changes for `ConvertingFutureCallback` to throw an exception during `cancel(boolean)`. If that happens, then this reader instance will not be properly/completely closed, so that needs to be handled.
@vvcephei we should fix the return statement -- should not be empty
nit: `<p>` not needed because there is no following paragraph
`This conversation was marked as resolved by wcarlson5` :)
This is defined in `CogroupedKStreamImpl` and here -- seems to be redundant -- make the other one package private and reuse in this class instead.
This overload does not take `Materialized` parameter
This was not addressed yet. the whole sentence can be removed
nit: formatting -> single parameter per line (same next line)
nit: line too long
nit: single parameter per line
nit: move parameter to next line
nit: remove empty link
nit: why not `k2` ? Should we use `A`, `B`, `C`, `D` for the values to make it easier to understand the expected result? It's unclear which A is which below.
I think we should have this in this PR already.
`WithOverlappingKeys` (or `WithSharedKeys`)
nit: add missing empty line
Nit: we don't need this tag before the parameter list.
nit: `{@code KCogroupedStream}` Typo `grouopStream` What does "and aggregator linked" mean (I guess I can infer what you try to say, but I would just remove it)
It seems to be clumsy to get a "random" `AbstactStream` to call the method. Better make `ensureCopartitionWith()` a static method and pass in the full set of `groupedStreams` -- for the join case, you would pass in both `AbstractStream` that needs to be copartitioned.
There is no `CoGroupedStream#reduce()` -- we can remove this
Wrong signature of `aggregate(...)`.
Ah. I missed that we have only only `CogroupedKStreamImpl` object (my mental model was that we have one for each input stream). What's unclear to me atm (maybe I need to do more detailed review) is, how repartitioning works? For that case, when do we insert a "source" node that is reading from the repartition topic, and where does the source node get the `Serde` information from? We could also have multiple independent repartition steps for different input streams.
This method does not take an `Aggregator` as parameter...
added `per key` (twice)
type: `the this`
The output type is only `KTable<KR, VOut>` is we don't use windowing, right? Otherwise, it should be `KTable<KR, Windowed<VOut>>` ? Do we actually need two different `build()` methods for both cases? Maybe we can clean this up in the follow up PR though. Just something to think about
as above (more often below -- please fit all)
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
`get()` returns a `ValueAndTimestamp<Long>` -- not sure if we should add a `.value()` call to the right hand side or change the return type left hand side? Maybe easier to change to ``` Long aggForKey = localStore.get(key).value(); // ... ```
Sure, I understand that. That was not the question, but might not be too important anyway.
Fair enough :)
This issue might resolve itself if you get rid of the `KTableSource` processor (the purpose of `KTableSource` is to model the case when a topic is consumed as table).
`KeyValueStore` -> `TimestampedKeyValueStore`
Removed this and pass in the `processorName` as `String` parameter directly.
Changed this to generate a name `<userName>-cogroup-merge` to align to `<userName>-cogroup-agg-<counter>` instead of just `<userName>` for the merge node.
No need to pass in a `Named` -- we can just pass in the actual name as `String` directly -- otherwise we call `suffixWithOrElseGet` twice for no reason
Unified the non-null check into a single place.
remove unnecessary `this.`
New default name for the merge-node
Removed this, as it describes `CogroupedKStream` what is not appropritate here.
I was using this test to print the topology and it shows two sub topologies while it should be one (seems the reason is that you use the same `StreamsBuilder` as in `setup()` method. Also, the naming of the operators seems to be incorrect. Also wondering if `KGroupedStream#cogroup()` needs on overload that takes a `Named` parameter? Maybe not, but the specified `Named` from `aggregate()` would need to be used for other processors, too. Atm there is this weird `COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test` ``` Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 (topics: [topic]) --> none Sub-topology: 1 Source: KSTREAM-SOURCE-0000000001 (topics: [one]) --> COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> test <-- KSTREAM-SOURCE-0000000001 Processor: test (stores: [COGROUPKSTREAM-AGGREGATE-STATE-STORE-0000000002]) --> KTABLE-TOSTREAM-0000000005 <-- COGROUPKSTREAM-AGGREGATE-KSTREAM-SOURCE-0000000001test Processor: KTABLE-TOSTREAM-0000000005 (stores: []) --> KSTREAM-SINK-0000000006 <-- test Sink: KSTREAM-SINK-0000000006 (topic: output) <-- KTABLE-TOSTREAM-0000000005 ```
With regard to naming: we should also check that the store in only queryable is a name is specified via `Materialized`.
Could also be neither. Error message should be fixed
Also set the store name in this test.
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
nit: fill in `@return` docs
nit: insert empty line
nit: parameters on a separate line
Since we merged in KIP-429, under normal processing the state of streams would be in REBALANCING for very short time in between poll calls only during restoration, I think that's why you observed that it is queryable, as the state transition to RUNNING happens instantaneously after line 454 (i.e. the restoration takes no time). Just for the logical reasoning itself, I think it is still better to check state transition first, and then verify queryable results.
Thanks for the suggested fix, @NarekDW. The convention for the various Log4J method calls is that when the exception is passed as the _(n+1)_ parameter where _n_ is the number of `{}` placeholders in the message string, then the exception's stack trace is included in the log immediately following the error message. In this particular case, the first `{}` placeholder is replaced with the result of `this.toString()`, and before this change there is no second `{}` placeholder and so the `error(...)` method call will generate a stack trace with the exception `t`. You are correct in that the error returned by the Connect API does not include the message of the original exception. That probably is helpful, but your proposed change would cause Log4J to not log the stack trace. I think the following change would give us both and modify the message to help to identify the problem: ```suggestion + "recover until manually restarted. Error: {}", this, t.getMessage(), t); ```
Maybe this should be called `fromBindings`.
I would say it's important _not_ to be able to create bogus requests. ;) We can introduce specific mechanisms for testing, but a public constructor for a request should do its own validation.
Since this is a serialization/deserialization test, I think it should not rely on `prepareResponse`.
I think we should convert these tests not to rely on `AclBinding` unless they are testing `prepareResponse` specifically.
`<code>` -> `</code>`
Nice cleanup, it makes looking at `StreamTask` bit easier having `TaskMetrics` in a separate class.
Are we ever going to need to use the return value here? Instead of using a `Supplier` maybe we could use a `Runnable` in the signature and we could get rid of the `return null` statements.
+1. I think the appropriate functional interface is Java.util.function.Consumer
Calling nextTokens with negative n would cause infinite loop.
`Failed to flush accumulated records` -> `Failed to flush all accumulated records...` Also, it would be much more useful if we could say `%d of %d` batches although I see we would have to expose a `size()` method in `IncompleteBatches`
exception can be improved a bit - "failed to flush within X ms, successfully completed Y/Z batches". wuold help distinguish between slow connection and no connection.
In addition to this, this way of taking vararg allows a user to pass `null` (and this code will throw NPE). Normally these methods are declared like: `of(Integer replica, Integer... rest)` to make sure that invalid values can note be passed in.
Why do we return `Optional` here? Doesn't makes sense just by itself, unless some bigger picture requires it.
This is public API meant to be used by users. I don't mind if our tests are a bit more verbose but we should aim to have succinct public APIs
wow, I don't think we actually catch the case where the replica set is empty. Let me open a separate PR for this
It might be slightly more efficient to describe all consumerGroups once (outside the for-loop) and then use the resulting map here.
Should we also include group state `DEAD`? New groups in the source cluster appear as dead in the target cluster.
From my tests it doesn't seam to work. The CG doesn't show up in the target cluster when listing with `kafka-consumer-groups.sh`. Also, when I start a consumer it resets the offset to what is configured in the consumer (latest in my case).
Use `Time.SYSTEM` instead of creating a new instance
Looks like you forgot to shutdown the scheduler in close()
This part is duplicated -- can we make DRYer? I suggest adding a checkpointsForGroup() method, and use that in both places.
This would be a side-effect in an otherwise pure function.
Nit: the scheduler understands negative intervals as "disabled", so you don't need to have this extra check here. Just have syncGroupOffsetInterval return -1 if it's disabled.
We should also close `targetAdminClient`
We could use `Collections.emptyMap()` here and in a few places below
This looks unused, same below for `c2t2p0`
On second though, using `describeConsumerGroups()` may be more predictable in terms on work to do, as you describe only the groups assgined to this task
We can use `new OffsetAndMetadata(50)` if we don't set any metadata. same below
It's a bit unusual to have `consumer3` and `consumer4` without 1 and 2 =)
can we change the type definition of these 2 to be `Admin`? Then we don't need the cast
I would like to propose the following change to take care of the source consumer group changes ```suggestion for (Map.Entry<TopicPartition, OffsetAndMetadata> convertedEntry : convertedUpstreamOffset.entrySet()) { TopicPartition topicPartition = convertedEntry.getKey(); for (Entry<TopicPartition, OffsetAndMetadata> idleEntry : group.getValue()) { if (idleEntry.getKey() == topicPartition) { long latestDownstreamOffset = idleEntry.getValue().offset(); // if translated offset from upstream is smaller than the current consumer offset // in the target, skip updating the offset for that partition long convertedOffset = convertedUpstreamOffset.get(topicPartition).offset(); if (latestDownstreamOffset >= convertedOffset) { log.trace("latestDownstreamOffset {} is larger than convertedUpstreamOffset {} for " + "TopicPartition {}", latestDownstreamOffset, convertedOffset, topicPartition); continue; } } } offsetToSync.put(convertedEntry.getKey(), convertedUpstreamOffset.get(topicPartition)); ```
@thspinto I think i am running into the same issue which you pointed here , so the source consumer group has different topic and is active and the target consumer group is idle but having different topic, but the code only checks for the topics and partitions matching the target site and adds them only when the target offset is less than source, it ignores other topics at the source
Thanks @ning2008wisc. I'll let you know it I find any other corner cases in my tests.
Missing a null pointer check here. ```suggestion Map<TopicPartition, OffsetAndMetadata> convertedUpstreamOffset = getConvertedUpstreamOffset(consumerGroupId); if (convertedUpstreamOffset == null) continue; ```
I managed to get to work adding the DEAD consumer groups in the new consumer group list: ```suggestion if (consumerGroupState.equals(ConsumerGroupState.EMPTY)) { idleConsumerGroupsOffset.put(group, targetAdminClient.listConsumerGroupOffsets(group) .partitionsToOffsetAndMetadata().get().entrySet()); } else if(consumerGroupState.equals(ConsumerGroupState.DEAD)){ newConsumerGroup.add(group); } ```
what about `assertEquals("consumer record size is not zero", 0, records.count());`? It can also be applied in a few other places
We could also use `hasRemaining` for these checks
Yes, I understand the need to have a length at the tagged field level. Mainly I was thinking about the redundancy when the type of the tagged field is known. It's a little annoying to serialize the string length twice, for example. Anyway, thanks for explaining.
We could also check the value of that metric.
We could have the same test in the non-SSL case which checks that the value is zero when there are connections.
From the default implementation and netty implementation, it looks like they would never be null. But I guess it is ok to do the null-check here since we may make SSLEngines pluggable.
How is this better than a method that returns the map? Is it because we pass the map with a lock held? Could we expose a map get method instead? Calling callbacks with locks held tends to cause problems and I'm a bit concerned that someone will see this method and use it outside of tests.
@rajinisivaram Can you please double check if these methods can actually return `null`? It looked to me like they would not, but I didn't look deeply.
Still not sure about the name since this already includes more than just `cipher`. But since it is an internal class, it should be ok to leave it as is for now.
Hmm, maybe we can move this check into the KafkaConsumer's catch block before `close` also.
Because when an error is thrown at initializing groupInstanceConfig, the log would not be initialized yet where we checked it ` == null` below.
Does it make sense to change the parameters to `Integer firstBroker, Integer... rest)`? Then assert if `firstBroker` is null. That will make sure that we don't get empty list of brokers and rest of the code that handles this object doesn't have to bother about this case.
In the former, you can handle the Optional.empty() where the latter does not? I'd even say the latter code is less clear from that perspective.
+1 for fixing the api. I would prefer a traditional api (either make C'tor private and `of` as a builder pattern w/o returning Optional or provide C'tors to create objects).
That's correct. The implementation becomes a bit tricky, as we can't just use `Arrays.asList` and be done.
To a client of api its preferable if the contract is visible from the api signature. If we want this restriction, I think better to make it visible in the signature.
I just mean the code snippet you passed doesn't handle removals, which is the whole reason for passing `Optional`.
nit: Update the unit test to use this new method. Jose changed them to use C'tor.
Does this need to be public? Making it private forces use of `of` method, which I think is good.
Is there similar behavior in the map parsing? I see a similar comma consume call being ignored. Consider the following test: ``` SchemaAndValue schemaAndValue = Values.parseString("{foo:bar baz:quux}"); assertEquals(Type.STRING, schemaAndValue.schema().type()); assertEquals("{foo:bar baz:quux}", schemaAndValue.value()); ```
Unless i'm misunderstanding something, I think these test names are inverted. ```suggestion public void shouldParseUnquotedEmbeddedMapKeysAsStrings() { ```
This case duplicates the one inside the while loop and it'd be nice to replace this with control flow that re-used that single case. I understand that the goal in the restructuring here was to catch a list of elements with no commas between them. I think you can do this by just skipping the comma consumption at the end of the loop if you look-ahead and see an `ARRAY_END`, but require it if the parser is not at the end of the array. Here's my thoughts in a longer form, feel free to skip reading this if you get it already: Before: ``` ARRAY_BEGIN // next token is one of (element, array end) while { // next token is one of (element, array end) ARRAY_END -> finish with at least one element (NPE when there were no non-null elements) // next token is an element !COMMA -> else fail from empty element parse element // next token is one of (comma, array end) COMMA? -> consumption ignored // next token is one of (element, array end) } ``` Your fix: You're changing the loop invariant to assume that the next token you see is an element, rather than possibly being an array end. This allows the loop to assume that element appears first, and then in each iteration force the loop to consume either a following comma, or an end bracket. ``` ARRAY_BEGIN // next token is one of (element, array end) ARRAY_END -> finish with zero elements // next token is an element while { // next token is an element !COMMA -> else fail from empty element parse element // next token is one of (comma, array end) ARRAY_END -> finish with at least one element, maybe null // next token is a comma COMMA -> else fail from missing comma // next token is an element } ``` My suggestion: Skip the final comma check if the next token is an ARRAY_END, but if the check happens, assert that the comma was consumed, rather than the condition being ignored in the original code. ``` ARRAY_BEGIN // next token is one of (element, array end) while { // next token is one of (element, array end) ARRAY_END -> finish with any number of elements // next token is an element !COMMA -> else fail from empty element // next token is an element parse element // next token is one of (comma, array end) if (ARRAY_END lookahead) { // next token is an array end } else { // next token is a comma COMMA -> else fail from missing comma // next token is an element } // next token is one of (element, array end) } ```
I read these as "should not parse X as Y", as in X is in the input, and a Y is the output. The input was a string which included an "unquoted embedded map key", and the output was a string, rather than a fully-parsed map. I now see the reading you're using, and it makes sense, since it refers to the innermost element that's being tested, rather than the whole input object.
I was not aware of the restriction on JSON object keys, and that seems like a fine standard to follow. I can't imagine it being too useful.
Can/Should these be immutable values? Maybe you can use the singleton `Collections.emptyList()` to avoid making memory allocations.
Sounds good. We can consider this resolved.
We probably only want to set the DeleteHorizonTime if the batch contains tombstone.
Hmm, it seems that we can just get deleteHorizonMs from batch directly, instead of through filter.
Hmm, a more intuitive check is probably !batch.isDeleteHorizonSet() && messageFormat >= V2.
Instead of adding a new setDeleteHorizonMs() method, would it be better to just add another constructor for MemoryRecordsBuilder that includes DeleteHorizonMs? This way, it's more consistent with how DeleteHorizonMs is set in another classes.
We could probably just bump up the number of parameter limit in the checkstyle file to 14.
We only need to write the DeleteHorizonTime if the batch contains a tombstone or a control record. This increases the chance for more optimized writeOriginalBatch case to happen.
nit: Please fix code style.
nit: Please fix code style.
nit: This should be ``` cache = new ThreadCache( new LogContext("testCache "), maxCacheSizeBytes, new StreamsMetricsImpl(new Metrics(), "test", StreamsConfig.METRICS_LATEST) ); ```
see my question above about using mocks.
nit: This line is too long. ```suggestion private final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
Q: Why do you use a mock here? In the ticket you said you just want to replace `MockStreamsMetrics` with `StreamsMetricsImpl`. req: If there is no specific reason, I would propose to either create a common mock that can be used everywhere as I proposed or to consistently replace `MockStreamsMetrics` with `StreamsMetricsImpl`.
req: Could you please rename `StreamsMetricsImpl metrics` to `StreamsMetricsImpl streamsMetrics` and then format the code like this ``` final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST); ```
See my question above regarding using mocks.
nit: Please fix code style.
See my question regarding using mocks above.
nit: This can be put in one line
nit: This should go in one line.
nit: Please correct code style as mentioned above.
nit: Please fix code style.
Did we save a heap to heap copy? I thought we only saved the reallocation of new buffers.
At this point, it's not clear if all bytes from src have been read. So, it might be useful to loop through the writes to sslEngine until flush() can't write all bytes to sockets and then return. This can potentially avoid the caller from re-populating the same bytes from fileChannel again.
Why not just use the static value `ByteUtils#EMPTY_BUF` here? All empty buffers are the same (well, aside from whether they are direct or not...), and they can never be enlarged or changed.
If you allocate this as a direct buffer here, you need to force it to be deallocated in `SslTransportLayer#close`. Otherwise these off-heap buffers will build up over time and starve the system of memory. The garbage collector doesn't "see" direct buffers and in at least a few versions of Java, they never get cleaned up until a full GC.
I can see two reasons that this might be set up this way. One is that the author didn't feel confident that they could distinguish EOF from other errors. I'm not entirely sure that checking for EOFException is enough. Another reason is because if an exception is thrown, whatever data has already been transferred will not be acknowledged to the caller, which would result in an inaccurate count at the upper layer.
We could reuse the remaining data in fileChannelBuffer, but those remaining bytes need to be included in bytesWritten so that the caller can issue the next transferFrom() from the right position.
Probably not worth it. The tricky thing is that we have to factor the remaining bytes in fileChannelBuffer into TransportLayers.hasPendingWrites(), which requires more work.
This whole logic could be simplified as: ``` private void verifyExceptionalState(ThrowingRunnable action) { assertThrows(TaskMigratedException.class, action); // This task should be closed as a zombie with all the other tasks during onPartitionsLost assertThat(assignedTasks.runningTaskIds(), equalTo(singleTaskId)); EasyMock.verify(t1); } ``` so that new test just needs to put in the intended action. Here `singleTaskId` is a class level parameter I defined to replace the singleton list, which is not highly required.
nit: empty line
How about this: in `Sensor` we expose a public function `hasStats` which checks if its `stats` is empty or not. If it is empty then it means no `sensor.add` has ever triggered yet (e.g. all the `emptySensor` would be the case), in which case we can skip measuring at all. By doing this we can also remove the extra check on the caller for the config value --- i.e. we consolidate the logic to be independent of the config, but only relying on the sensor's `stats` map.
For create / destroy maybe that's okay, but `process` is at the very critical path so we have to be careful not to incur any overhead.
I asked you exactly that a few months ago :) You referenced some old PR but basically the takeaway was, a restoring task hasn't initialized anything but its state, therefore needs to close only the state manager
Hmm this is interesting: for created the running tasks we call its `task.close` function whereas for `restoring` we only call `task.closeStateManager`, is it intentional? If yes why? cc @ableegoldman
No I'm referring to the `generation()` itself: we call this function twice within the function and in between the generation object may have been changed.
nit: maybe use meaningful names? e.g. `topic_creation_start` Even better would be to add some kind of `timed` function
Not for this patch, but we should do a KIP to add support for batch topic creation.
I don't know of any, but I haven't checked. I thought it might be straightforward to add one, but it's up to you.
Adding batch deletion here would be useful also. This has caused problems in Kafka previously.
I think 2 is the more common setting these days. It is a bit more unforgiving, which perhaps makes sense for this test case.
Why is this necessary? In the `for` loop above, each partition that finished restoring should be removed vai `it.remove();` -- hence, if `allTasksRunning()` return `true` `restoring` should be empty ? Similar for `restoringByPartition` given the two new lines this PR adds.
+1, we should just throw in this case.
Btw: we maintain state quite redundantly (what is a general issue). Would it make sense to extract a helper class that wraps ``` private final Map<TaskId, StreamTask> restoring = new HashMap<>(); private final Set<TopicPartition> restoredPartitions = new HashSet<>(); private final Map<TopicPartition, StreamTask> restoringByPartition = new HashMap<>(); ``` and we only modify the state via the helper? The helper class can insure that all three data structures don't diverge. (Also ok to do in a follow up cleanup PR)
We need to add `@SuppressWarnings("deprecation")` to make the build pass.
specify generic -> `MockProducer<byte[], byte[]>`
Remove this suppress and fix generic.
remove try-fail-catch and rewrite to ``` final StreamsException expected = assertThrows(StreamsException.class, () -> collector.flush()); assertTrue(expected.getCause() instanceof TimeoutException); assertTrue(expected.getMessage().endsWith(topic1TimeoutHint)); ```
+1. All for user readable assertions. If the bytes format is standardized then it should be tested in SubscriptionInfoTest.
Nice one on projecting the value!
Okay I think this makes sense, let's just follow this pattern then.
It is wall clock indeed.
Could we do the same pattern as I did fixing some flakiness of the SmokeTest integration test? Just wait for the state transition in listener. Found sometimes it could cause many more concurrent instances running.
I'd suggest we put the time span as 4 days to be on the safer side, the 7 day log retention may kicks in a bit earlier.
When error, could we print all the received records from all four topics too? Just 2 cents from my past flaky test fighting experience.
extra space after `*` needs to be removed
extra space after `*` needs to be removed here too
I think this definitely helps clarify! A copy suggestion: ``` Attaching a state store makes this a stateful record-by-record operation (cf. {@link #map(KeyValueMapper) map()}). If you choose not to attach a state store, this operation behaves similarly to {@link #map(KeyValueMapper) map()} but additionally provides access to the {@code ProcessorContext} and associated metadata. ```
Overall LGTM. But can we format it differently? We should start a new line for each sentence. If we update the docs later, it make the diff simpler to read.
I don't think it's safe to reuse `nowMs` here since we may have blocked on `free.allocate`.
I think we should make the `MetricsReporter` interface itself extend `Reconfigurable`, rather than just `JmxReporter`. We might want to reconfigure other metrics reporters at runtime, and there is no reason to special-case just this one.
This doesn't fit with the pattern that we typically use for configuration keys. Prefixing should be done outside the class in question. Check out `StreamsConfig#getAdminConfigs` for an example of doing prefixing. "kafka" is also an odd prefix to use since both the client, broker, etc. all all part of kafka. If we want to use a prefix just for the broker it should be something like "broker".
It's true that there are two kind of weird and old yammer config knobs, `kafka.metrics.reporters` and `kafka.metrics.polling.interval.secs` that are prefixed with "kafka." But no other broker configurations are. For example, `metrics.sample.window.ms` isn't prefixed, `metrics.num.samples` isn't prefixed, etc. etc. And of course, there are hundreds of other broker configurations that are not prefixed. It doesn't make sense to prefix configurations with "kafka" since logically, every Kafka configuration is for kafka. Kafka Client configurations are for Kafka, Kafka command line configurations are for Kafka, etc.
By the way, `kafka.metrics.reporters` is a horrible config key name because it suggests that it is configuring the "Kafka metrics" system (which, as you know, is separate and different from the Yammer metrics system), but actually no, it configures Yammer. :disappointed:
I might be missing something, but in both cases, you just want to use regular expressions, right? There is no need to mess around with predicate functions.
On second thought, I'm fine with keeping the predicate.
@xvrl: The only reason the interface was optional is that we needed to retain support for Java 7, which didn't have default methods. Since we don't have to worry about Java 7 any more, let's make the interface part of the type.
MetricsReporters have been optionally reconfigurable since 1.1.0, before Java 7 support was dropped. Any metrics reporter implementation that implements Reconfigurable is reconfigured by the broker when any of its configs changes. Since then, we have adopted the same approach for other interfaces like Authorizer as well.
Yeah, I think it's worth considering renaming those two configs. If someone created a KIP for that, I would vote for it. We would have to support the old names for a while, of course.
If we add these to `toString` here, we should also add `runningByPartition` to `AssignedTasks#toString`
```suggestion "restoring by partitions map contained {}, and the restored partitions set contained {}", restoringByPartition, restoredPartitions); ```
need to update `./gradlew uploadArchivesAll` at line no: 644
I think debug logging would be sufficient for this one as well as the log entry below for normal disconnect.
This doesn't read well. Assuming it needs to be as long as `inactivity-gap` plus `grace-period` then the following reads better to me: ```suggestion * @param retentionPeriod length of time to retain data in the store (cannot be negative) * Note that the retention period must be at least as long as * inactivity-gap plus grace-period. ```
It's better to do `instanceof` than this. The JVM folds the `instanceof` into the trap it needs to ensure JVM semantics.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
Nit: dowrade -> downgrade.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
"*" is considered "LITERAL" for compatibility reasons
Also, it's really ugly for "validate" to mutate the request. Can we have another function for this... :pray:
Mockito is mysterious to me -- someone else should review this please.
I don't have a better suggestion, but targetTopicPartitionsUpstream is a kinda confusing name.
I think it would be good to check that the registry doesn't already have something registered before we do this (since otherwise we'll be overwriting whatever that is).
If this already exists, we should check to see if it's equal to the new information we wanted to register. If it is, we can skip the decrement metric + increment metric dance.
Unfortunately, that would not be a compatible change. Older clients will not expect the REBALANCE_IN_PROGRESS error. The only way we could do it would be to bump the protocol version and return the error code only for the new version.
What are your thoughts on having `InvalidGroupIdException` here? In all regards the KafkaFuture will wrap it in an `ExecutionException` - I'm just wondering if user code could be checking whether the cause is a `KafkaException`.
I'm ok with `IllegalArgumentException`. I think `InvalidGroupIdException` is typically used to indicate a groupId which is structurally invalid in some way (e.g. null).
Yeah it's not straightforward. Let's see what others think
```suggestion WORKER_SETUP_DURATION_MS, "Initial group of workers did not start in time."); ```
It seems that we compare with this value to check if there is no leader or epoch. It's a bit more robust to check if both `leader` and `epoch` are empty, no? Then it still behaves correctly if we have some code that passes empty to both constructor parameters.
This method name is a bit confusing since `Empty` here refers to a `LeaderAndEpoch` with `noNode`. Could we use `currentLeader` everywhere and remove this method? It's a bit difficult to reason about with both of them present.
Shouldn't the workers discover that the coordinator is unavailable while it is down? I'm imagining this test going like this: 1. steady-state workers are running 2. brokers stop 3. workers discover the coordinator is unavailable 4. workers stop their tasks 5. brokers start 6. workers discover the next coordinator 7. workers start their tasks 8. workers are running unaffected
Line too long (also some lines above and further below)
This method seems to be the exact same as `TimeWindowedKStreamImpl#materialize()` -- we should share the code.
Just see the test from below with "many windows" -- I think we can merge them into one test
Just see the `timeWindowMixAggregatorsTest` below -- seems it's a super set of this test and hence, having the second one only seems to be sufficient.
There are two input streams in this test, and thus we should create a second `TestInputTopic` to pipe input via both.
We should use `to()` and use a `TestOutputTopic` instead of the processor
Can we "duplicate" this test and use default serdes from `StreamsConfig`? We recently discovered some bugs in other parts of the code with this regard and it would be good to verify that the code work correct upfront.
line too long
nit: `KCOGROUPSTREAM` -> `COGROUPKSTREAM` (to align with the class name)
nit: move `windows` to next line
Overall, confusing indentions and line breaks...
I would disable auto-formatting...
If there any case for which we would pass a non-null valueSerde here? If not, we should remove the parameter entirely.
Overall hard to read because of formatting. Easier to read like: ``` condition ? // then : // else ``` Ie ``` materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>( materializedInternal.keySerde(), windows.size()) : null ```
What value does this test provide? Seems it verifies that `setup()` method is correct? Seems unnecessary to me.
only one parameter should be `null` -- otherwise it's unclear what this test actually does
prop: Use `assertThat()` here and in the other overload of this method.
prop: Should we use `MockTime` here? prop: Could you use a more meaningful name for `ts`? The above is also valid for the overload.
prop: Would it make sense to also have an overload with just a flat list, i.e., ``` void runTestWithDriver(final List<TestRecord<Long, String>> expectedResult) ``` Maybe it would simplify the code of some of the tests. Hopefully, you can share some of the code in the overloads.
Okay, sounds fine.
nit: addUnathorizedTopics => addUnauthorizedTopics
This is a very confusing method. There is a risk that it may be invoked inadvertently when making changes, even though it is meant to be only for tests.
This seems to be the same program as in the test above -- if yes, I think we should merge both tests into one.
This syntax is a bit hard to follow with the conditional at the end. Can you rewrite it to something like: ```python self.jmx_tool = None if jmx_object_names is not None: self.jmx_tool = ... ``` (and also check the attributes as mentioned above)
@brianbushree I don't mind going back to the mixin if you think that's better. I'll leave it up to you.
nit: these can be replaced by `if self.jmx_tool`
Actually there are a few places like this. Maybe we can figure out how to let jmx_tool always be set even if we're not scraping any JMX objects? I think that's how the mixin worked before.
nit: Could we also briefly explain the issue in the tests? Personally, I tend to read tests to understand the expected behavior and the issue with versions earlier than 9 is not immediately apparent
should this also be moved into `if (sendable && !backingOff) {` ? for example, if the batch is not full, it's unnecessary to put `tp` to `readyNodes`.
Could you point me where do we eventually cancel the commit? Since it is not in diff I bet it happens somewhere later in the call trace but did not get it.
Looks like we could deprecate it, but it's not listed for deprecation in the KIP. I'll defer to @guozhangwang on this one.
We also need to make sure that inside StreamThread all the blocking calls that can possibly throw interrupted exceptions now are gracefully handled since otherwise it may cause undefined behavior.
Unlike in the standalone, it's a bit more difficult to check how the connector configuration has changed. This `startConnector(...)` method seems to be called in two places: from `restartConnector(...)` and `processConnectorConfigUpdates`, which IIRC happens whenever the connector configuration has changed in some way. I'm wondering how often a connector might not change it's task configs when the connector config has changed. What do you think about just always restarting the tasks even if the generated task configs have not changed? WDYT? @kkonstantine, the `processConnectorConfigUpdates(...)` method is called during rebalances, and maybe I'm missing cases where the recent rebalance improvements handle the task configs more frequently than I recall.
nit: could probably push this loop into `check_protocol_errors`
Can we just set the `recordCollector` in `setUp`, instead of keeping a persistent one? It just makes it easier to see that the state really will get cleaned in between tests.
Wow, test inheritance made this a truly mysterious change... Managed to track it down, though.
Sorry, `False` is not a number ;) Seriously, it looks like the usage of this method expects a number, so if we're coercing a failure, we should explicitly coerce it to a number.
The error message of `RecoverableClientException` does not match any longer... (I would recommend to _not_ add the error name to begin with, as the root cause exception is pass into the constructor anyway)
we should avoid to use this construct but use `assertThrows()` instead.
nit: could be set to final and refactor as: ``` final Integer partition; if (partitioner != null) { final List<PartitionInfo> partitions = producer.partitionsFor(topic); if (partitions.size() > 0) { partition = partitioner.partition(topic, key, value, partitions.size()); } else { throw new StreamsException("Could not get partition information for topic '" + topic + "' for task " + taskId + ". This can happen if the topic does not exist."); } } else { partition = null; }
req: This is unnecessary
req: no longer used
prop: unit test this as well
prop: `suspend` and `closeSuspended` could cause confusions as the meaning was very close. Maybe name `closeSuspended` to `closeSuspendedIntervalState`
I mean the `ProductionExceptionHandlerResponse` class itself
If we are not exposing this detail, then it would be a bit weird to classify them in the first place.
req: typo unknown Pid
req: I don't think we should call `maybeBeginTxn`, as we do that call during every send. If we are not in a transaction but calling `commit()`, that sounds like an illegal state to me, or we should just bypass the whole commit logic as it indicates we didn't do any send call in the past when EOS is turned on.
Can we actually just get rid of "test only" constructors? It couldn't be used by _that_ many different tests...
```suggestion "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " + ```
Just checking... Is the intent to roll back the "hack" to also catch UnknownProducerId and initiate a rebalance to recover? Note, if this was not the intent, then there are similar catch blocks below.
These methods can throw any number of other exceptions, which we would catch and wrap when called via `send`, but would **not** catch and wrap when called via `commit`...
I guess this method no longer throws at all? (also applies to close)
req: this member should be removed.
Same thought as above.
If it's no longer used, we should remove it.
Thanks for clarifying this... Maybe we should update the Producer docs, since this is enormously subtle, but also important for handling correctly.
Just my 2 cents: having a lot of factored-out code in tests usually hinders, rather than helps, maintainability. In the long run, the overall number of lines in the test file doesn't hurt anything, because you rarely sit down to read all the methods (typically, just while doing the review like this). After this PR is merged, you would almost always just be trying to read and understand a single method. Thus, it pays to optimize for single-method legibility. Having a test harness to go read, and other support methods to go read, just to understand this method is only going to get in the way. As it is right now, this method is 28 lines long, perfectly legible and clear. Trading clarity for de-duplication is a bad deal.
+1 I personally decided not to worry about it, but just keep a mental todo to come back after all the cleanups are complete to revisit test coverage (lines and semantics). But it would be nice to know why it's ok that they were removed.
Well. I see your point. Maintaining the docs in many different places is quite a challenge. Curious to hear what @vvcephei thinks about this question.
This change doesn't look quite right. It should still be a ReadOnlyKeyValueStore, but the generics are different: `ReadOnlyKeyValueStore<K, ValueAndTimestamp<V>>`. Also, if the intent is to recommend using the Timestamped variant, then folks would actually have to specify `QueryableStoreTypes.timestampedKeyValueStore()`.
nit: can you remove the blanks? Those would be rendered: ``` // with blanks code would be intendet view.get(key) // but we don't want indention view.get(key); ``` This applies to all `<pre>{@code...` sections) -- it was not introduced by you but we should fix it.
The framework already retries this step if there is a retriable exception. I'm not sure if the operator is needed in the source connector (especially after resetting the processing context, which removes all useful context for the reporters).
nit: I think there's no need to reference the JIRA. The explanation seems clear enough without any additional context.
Needs `@param logContext` (and also `@param time the time instance`)
Thanks. Will merge after Jenkins is green.
We should not use Java `assert` statement but proper unit testing asserts, ie, `assertThat(e.getMessage(), equalTo("..."));`
We can just hard-code `250` here.
As above: use `assertThrows` and verify error message
While existing test are written this way, we try to move off this pattern and not use the `expected` annotation. Instead, we should use `assertThrows` and also verify the exception error message.
prop: change `stateStoreNames` -> `stateStoreName` here.
Yeah, Java's type system makes this stuff a pain. I think you can fix it with: ``` final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(new KStreamBranch<>((Predicate<K, V>[]) predicates.clone(), childNames), branchName); ``` which should be safe If you want to also get rid of the cast, you can do it by coercing each of the predicates to a Predicate<K,V> when you loop over them at the beginning: ``` Predicate<K, V>[] kvPredicates = new Predicate[predicates.length]; for (int i = 0; i < predicates.length; i++) { final Predicate<? super K, ? super V> predicate = predicates[i]; Objects.requireNonNull(predicate, "predicates can't be null"); kvPredicates[i] = predicate::test; } ```
This is synchronized and will await on the produceFuture. `await()` is called by `awaitFlushCompletion()` which is called when a user calls `flush()`. I am concerned that a user can call `flush()` and end up effectively dead locking other operations on the ProducerBatch, as getChildrenProducerBatch and addChildrenProducerBatch will not be able to be called by other threads - my concern is that the sender thread may become deadlocked in splitAndReenqueue in this state.
This should also be synchronized
This function is only used in unit test, we should not expose.
We have multiple null check for `childrenProducerBatch` which is not necessary, instead we could just reject here if the given `childrenProducerBatch` is null to ensure it's non-null.
If we could get rid of null check, `addChildrenProducerBatch` and `getChildrenProducerBatch` could be removed as well.
We can save this for a follow-up, but it doesn't seem too difficult to allow the consumer to seek to the offsets that were successfully fetched. I think we just need to change the type to something like `Map<TopicPartition, Either<Errors, OffsetAndMetadata>>`.
Note that today for the public `#committed / #endOffsets` call if we do not get all the committed offsets after timeout we throw the exception -- i.e. we do not return partial results. So we may return no results and throw timeout but internally silently reset the position for some of the partitions.
You can drop the "public" here, since the tests are in the same package.
All the interval configs (sync topics acls, sync topic configs, refresh topics, ...) use negative values to indicate that the Scheduler should not run the corresponding task. I suppose we could have used an Optional or something instead. If we drop the negatives here, we should do so with all the other tasks. Mox nix to me.
It feels like allowing intervals to be negative interval to disable connectors is not a great pattern. Instead preventing the interval to be negative and relying solely on `enabled()` would have been a better alternative.
It's not ideal but yes we don't want to change the current behaviour here. Let's keep the negative values.
It's customary to use `Objects.requireNonNull(topicPartitions)`. Otherwise LGTM assuming all tests pass.
The overhead is minimal. The benefit is clarity: if someone looks at the code it is immediately obvious that null is not allowed.
Well I was thinking two threads could be two threads, one calling `consumer.groupMetadata` and one calling `consumer.poll` which is creating a new object, but on a second thought this race condition maybe fine anyways since we cannot guarantee time-ordering if it happens.
Since this callback is not synchronized and nor does the getter `groupMetadata()`, we should declare this cached object volatile.
Why we want to return a clone of the groupMetadata? If we can declare all the fields as final, and whenever we update we always create a new ConsumerGroupMetadata object, then I think we can safely return the cached object directly.
I like the use of `Optional`. I think, you could make it even simpler: ``` final Sensor sensor = Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { final Sensor newSensor = metrics.sensor(fullSensorName, recordingLevel, parents); threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return newSensor; }); ``` Please use the correct indentation. We use 4 spaces. Same applies to the changes below.
Just FYI, for KIP-360 I'm doing this check for both idempotent and transactional, since it triggers an epoch bump instead of a producerId reset. I'll just pull this call out to a shared code path, the rest of this method shouldn't need to change.
qq: is the second condition necessary given the first one? If we are not in transactional, then right now the only place to transit to INITIALIZING is line 496 below. I'm actually fine to leave it here to be less bug-vulnerable, but just to clarify my understanding.
Would you consider using a `long timeout` instead and move this to TestUtils? Perhaps we need this somewhere else too.
nit: parens unnecessary
Yeah, I get that we want to make sure the same instance is returned. But since `Sensor` doesn't override `equals`, `is(sensor)` should still do an instance equality check. It's really a minor point, so I don't care too much if we keep it as is.
That is on me. In this verification it is important to check for reference equality, because you want that `threadLevelSensor()` returns the sensor that is registered in the `Metrics` object or that was created by `Metrics#sensor()`.
Please extract this to its own method since the same code is also used on line 220.
Should be `final` here.
Please reformat as follows: ``` setupGetNewSensorTest( metrics, THREAD_ID + ".task." + storeName + SENSOR_PREFIX_DELIMITER + storeName + SENSOR_PREFIX_DELIMITER + TASK_ID, recordingLevel ); ``` This applies also to the other locations where the second argument is too long for the line, i.e., lines 322, 343, 364, 385, 406. Sorry if I missed that in my previous review.
Although, you simplified this code, you did not apply all simplifications I proposed in PR #7914. I think you can even simplify this code further as shown here: ``` return Optional.ofNullable(metrics.getSensor(fullSensorName)).orElseGet(() -> { threadLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); }); ``` This simplification can be applied also to the other methods below. If you have any concerns about this simplifications please share your thoughts.
Please use 4 spaces instead of 8 for indentation. Same applies to the changes below.
If a line is too long, either move right hand side of assignment to a new line. If it is still too long put each argument and the closing parenthesis on its own line. Examples are: ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor(THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel); ``` and ``` final Sensor actualSensor = streamsMetrics.storeLevelSensor( THREAD_ID, storeName, TASK_ID, sensorName1, recordingLevel ); ``` In this case please use the former. Please check also the other changes for too long lines.
This line is too long. Please move `streamsMetrics.storeLevelSensor()` to new line.
This should be `private` not `public`.
This test replaces `shouldGetThreadLevelSensor()`. Thus, you can safely remove `shouldGetThreadLevelSensor()`.
I used `equalToObject()` because it makes the intent more explicit.
Fair enough, let's just leave it as is then. Thanks for the explanation.
There several issues with this test: - First of all the test fails. - According to the name of the test you want to verify `threadLevelSensor()`, but you call `taskLevelSensor()`. - Since the `Metrics` mock always returns the same sensor, it does not make sense to compare the sensors that are returned by the different calls to `threadLevelSensor()`. Such a verification will always be true. You should rather verify if method `sensor()` is not called on the `Metrics` mock. For example, the following two setups could replace `setupGetSensorTest()`: ``` private void setupGetNewSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(null); final Sensor[] parents = {}; expect(metrics.sensor(fullSensorName, recordingLevel, parents)).andReturn(sensor); replay(metrics); } private void setupGetExistingSensorTest(final Metrics metrics, final String level, final RecordingLevel recordingLevel) { final String fullSensorName = fullSensorName(level); expect(metrics.getSensor(fullSensorName)).andStubReturn(sensor); replay(metrics); } ``` and the following two tests would replace `shouldGetTaskLevelSensor()`: ``` @Test public void shouldGetNewThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetNewSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } @Test public void shouldGetExistingThreadLevelSensor() { final Metrics metrics = mock(Metrics.class); final RecordingLevel recordingLevel = RecordingLevel.INFO; setupGetExistingSensorTest(metrics, THREAD_ID, recordingLevel); final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, CLIENT_ID, VERSION); final Sensor actualSensor = streamsMetrics.threadLevelSensor(THREAD_ID, sensorName1, recordingLevel); verify(metrics); assertThat(actualSensor, is(equalToObject(sensor))); } ``` Similar is true for the other tests below.
There may be an assumption that the collection will be mutable because it is passed around and used elsewhere -- potentially being mutated. It feels to me that it would be better to embrace immutability here and wherever else such a collection is used. This ticket (`"Modify CreateDelegationTokenOptions Renewers Implementation"`) would be closed in favor of one that has a broader scope (e.g. `"Embrace immutability in delegation token processing"`). This would also require a KIP since the public API would be changing (maybe not in signature, but definitely in semantics).
I guess we could restore this location with the change to the group subscription maintenance? I don't feel too strongly since the new location works also.
Do we need this? Seems like we have access to the `SubscriptionState` object already in the test case.
I'm confused about a couple things. First, if a subscription changes while we are in the middle of a rebalance, do we need to call `onJoinPrepare` again? With the old semantics, we probably didn't need to because we revoked everything, but with "cooperative" rebalancing, perhaps we should? cc @guozhangwang The second thing is that it seems surprising that we raise auth errors from the "group subscription." If consumer 1 has subscribed to "A" and consumer 2 has subscribed to "B," I probably wouldn't expect to get any auth errors for topic "B" from consumer 1. So maybe we should be ignoring auth errors not part of `subscription()`? Not sure if that is straightforward or if there are any additional implications.
@rajinisivaram I think I understand the problem a little better now and I'm coming around to the approach in this PR. It does make sense to me to reset the group subscription at the time that we set `joinedSubscription` in `ConsumerCoordinator`. Then I guess my only question is whether we still need to reset it inside `onJoinPrepare` as well? Another (possibly worse) idea I had is to change the implementation of `resetGroupSubscription`. Perhaps instead of setting it equal to the current local subscription, we should set it to null or empty. We could try to maintain a cleaner separation between the local and group subscriptions. We would then need to update `ConsumerMetadata` to work with both the local and group subscription.
Regarding the first question, if a subscription changes while we are in the middle of a rebalance we would call `onPartitionsRevoked` immediately on those still-assigned partitions that are not part of the new subscription any more, and for the returned assignment we check if they match our join-subscription and if not we trigger another rebalance (this is pre-429). So I think we still do not need to re-trigger `onJoinPrepare` when subscription changes in the middle.
We can do it separately if you like.
`assertThrows` is what we use for some time now, and it's available to the branches that this PR will be backported. (same below)
no need to use `this.` outside the constructor. Here and below
Could combine these into a single `testCycleCollection()` method if you put a `null` value in the list (e.g. `"A", null, "C"`) and for every one of the 4 positions (0-2 and cycling back to 0) you also check the `peek()` value. I think it would be clearer compared to what you have currently since the last 2 methods you have now are a bit haphazard.
Should simply return true here, right? As written now it is adjusting internal state, and it is best to isolate state manipulation to as few places as possible.
Should add that the class is not thread-safe.
nit: I don't think there's any reason to mention this. Unexpected errors fall under `KafkaException`, which is listed below.
I think these should all be `IllegalArgumentException`. The producer is not in an illegal state.
nit: we may as well move this check into `ConsumerGroupMetadata` since we have some other null checks there.
nit: usually we write this like this: ```java this.groupInstanceId = requireNonNull(groupInstanceId, "group.instance.id can't be null"); ```
Might be nice to have an overload which sets only `groupId`.
Might be worth adding a null check here for `groupMetadata`. Another simple validation is ensuring that if `generationId > 0`, then `memberId` should be non-empty.
nit: if you want a new paragraph you need to add `<p>`
Not from this patch, but this should be `private`.
I'm wondering if we should use `CommitFailedException`. In the consumer, we do not expose illegal generation and unknown member id errors directly to the user.
We can give a clear message saying null is not supported. If it's an NPE somewhere down the stack then the user doesn't know if it's a bug or not.
nit: as mentioned elsewhere, I don't think we need to add all of these exceptions to the method. They are all runtime exceptions, so it's still up to the user to read the docs.
It seems like we don't need to mention 0.11 here since the requirement for 2.5 is stricter.
I guess we could also get an auth error for the groupId.
How about this? > ... if the commit failed and cannot be retried (e.g. if the consumer has been kicked out of the group). Users should handle this by aborting the transaction.
This makes stopbugs complain ... We should use `%n` in the format string instead of `\n`
No need for the extra `:`, `addConfigDetail()` already inserts it
Same here, I think `computeIfAbsent` would simplify the logic
I think we can `computeIfAbsent()` here. It will replace `containsKey`, `put` and `get`.
Let's keep the `L`
I think we can be more concise and simply say `// Single writer thread, multiple reader threads`
Let's remove the brace changes please.
Kafka doesn't mandate braces in `if` statements.
Yes, reading can be done from multiple threads. `volatile` would probably be enough for this use case.
Generally, replacing an `isEmpty` check with `count` is not a good idea as you can easily regress in performance. In this case, the deque implementation may have a cheap `size()`, but it's brittle and not something I think we should do.
may be rename `partitionsByHostState` in other places too? cosmetic change. your call
Ah, yes, it's `org.apache.kafka.test.TestUtils#tempDirectory()`. My mistake. The protocol is for all temporary state in Kafka tests to use that method. The change I made in `QueryableStateIntegrationTest` is basically what we should do here as well.
This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.
thinking aloud: guess there is nt much value in wrapping a single provider.. so +1
To close out the earlier thread.. This test is okay, since `NOT_RUNNING` will make that instance go to DEAD state (or some non functional state like that) where the store cannot be obtained.. the lines below check that we can stil retrieve the keys from the other replica
this is creative :)
state.isRunning() is as below ``` public boolean isRunning() { return equals(RUNNING) || equals(STARTING) || equals(PARTITIONS_REVOKED) || equals(PARTITIONS_ASSIGNED); } ``` if its not too much work, we can rename to something like `state.isAlive()` , that captures what we want to check .. your call.
thinking aloud: even though we are binding to a specific StateStoreProvider implementation here, it seems fine,since there are n't any other really in a topology
is the kafka way to align this with the `(` .
I also feel that we can collapse the layered interfaces a bit further after we've changed this, since the original motivation of having it is just to "stitch" the global stores and local stores together when exposing as `KafkaStreams#stores`. We can consider that in a separate, cleanup PR afterwards.
cool. this should now allow standby to be queried
cosmetic: extra space at the start
With this change, just Global and Wrapping StateStoreProviders exist? IIUC, most of the `xxxStore` classes are just accessing the `Wrapping..` store provider? Makes me wonder, if we should just use the `QueryableStoreProvider` everywhere and cull the interface.. Anyway, I am not familiar enough with this part of the code. So I leave it to you..
Thanks for the explanation. I was more curious about this, than anything.. Personally, the latter is very natural and easy to do. I just go with the flow, in these things. :)
makes sense... I will take another look at this when I rebase against the lag PR
I can work on the follow-on change. WrappingStoreProvider -> WrappingStateStoreProvider GlobalStoreProvider -> GlobalStateStoreProvider QueryableStoreProvider is left as is.
I think in a previous commit this was `static` and I had concerns that it wouldn't work, so I'm glad to see you changed this.
nit: I understand the change here for compactness, but I find it a little hard to follow. This is subjective however so feel free to keep as is.
Do we need to do this at this point? I guess so at it makes sense to have `sourceTopicNames` match what's in `nodeToSourceTopics`. I'm only asking as we never had this before and I'm curious as to why.
Since topics Set can be quite large, I doubt the intention was to show the contents. '{} topics' reads like the count of entries should be shown.
nit: missing space between logPrefix and 'found'
topics is a Set. What's your intention for the second parameter ? If you want the number of topics logged, you should use topics.size().
This V0 schema should be deleted too
prop: do you think we should name this as `threadProducer` for readability? cc @guozhangwang
With further reading, I think the timer should still be 7 days for pid expiration.
Looks like txn.id timeout is 1 min as of current: ```val coordinatorEpochAndMetadata = txnManager.getTransactionState(transactionalId).right.flatMap { case None => val producerId = producerIdManager.generateProducerId() val createdMetadata = new TransactionMetadata(transactionalId = transactionalId, producerId = producerId, lastProducerId = RecordBatch.NO_PRODUCER_ID, producerEpoch = RecordBatch.NO_PRODUCER_EPOCH, lastProducerEpoch = RecordBatch.NO_PRODUCER_EPOCH, txnTimeoutMs = transactionTimeoutMs, state = Empty, topicPartitions = collection.mutable.Set.empty[TopicPartition], txnLastUpdateTimestamp = time.milliseconds()) txnManager.putTransactionStateIfNotExists(createdMetadata) ``` As we are also considering setting the txn timeout to 10 seconds, this might be a real problem
Adding a bit more on my thoughts about initTxns call: in eos it is per-task so we can delay the initTxn call as much as possible to make sure the pid does not get expired; in eosBeta however, initTxn has to be triggered at the thread-level, and suppose after that has been triggered and then a rebalance happens which assigned more tasks to you, and restoring them takes quite long time during which we would not process any active tasks and hence not calling txn APIs causing the pid to be expired -- note that since we still call poll regularly we would not be fenced on the consumer side. It might be a big issue if we still have large restoration time practically, and in near future what I can think of for tackling it are in two folds: 1) try optimizing our restoration time so that we are confident such thing should be very rare. 2) allow active task processing at the same time while restoring others, hence keep the txns going; note that it is not always comprehensive since if all actives are restoring then it would not help.
Moving forward if we managed to make producer logic inside RecordCollector only then we do not need to capture those "raw" exceptions on the stream thread but only task-migrated exception.
I think moving forward if we are always going to commit all tasks owned by a thread anyways, we should consider separating the logic of 1) flushing stores, writing checkpoint files 2) calling consumer / producer APIs for commit (or sendOffsets) For 1) it is per-task, while 2) is per-thread. In the refactoring atm we move all of 2) into the RecordCollector which is per-task, but maybe in the future we can make the RecordCollector per-thread shared by tasks to make step 2) "global".
prop: We could do ``` committed += taskManager.commitAllStandby(); if (committed > 0) ``` for the rest of the logic so that we don't need a separate `if (committed > 0)` later
@mjsax , just to clarify, are you asking whether https://github.com/guozhangwang/kafka/pull/6 would conflict in some way with restructuring the commit logic? I don't believe so. I'm only changing how the tasks' lifecycles are managed, so it ought to be encapsulated from the thread's perspective.
Do you know why the sleep is needed ? From my experiment, 1 millis sleep suffices. The timeout for completed.await() below can be shortened as well since the allocate() call would return immediately.
-> `storeName()` (without `get`)
As above. Simplify to: ``` Enable querying of stale state stores, i.e., allow to query active tasks during restore as well as standby tasks. ```
`function` -> `method` ? `{@code null}`
nit: line too long. `final` not required -- a static method cannot be overwritten anyway
nit: `{@link KeyQueryMetadata}`
The primary functionality of this method is to _set_ the partition... I am confused by `with stale(standby, restoring) stores added via fetching the stores` -- what does this mean? Maybe just simplify to: ``` Set a specific partition that should be queries exclusively. ```
Nit. Line too long
As above -- incorrect return type description
`will be returned` -> `will be queried`
`to fetch list of stores` -- that describe some internal implementation detail. Better: `Get the store partition that will be queried.`
`this information` is ambiguous -- unclear from the context to what it really refers -- maybe better: `If no specific partition is specified the default behavior...`)
This method does not return a `String`. Maybe ``` @return StoreQueryParams a new {@code StoreQueryParams} instance configured with the specified partition ```
`storeName` -> `state store name` (we should use natural language if possible, and avoid variable names)
It might be simpler to add a private constructor that allows to specify all 4 parameters: ``` return new StoreQueryParams<>(storeName, queryableStoreType, partition, staleStores); ``` Similar in other methods
`it is` -> `they are` (we user is a person :))
The test itself is great -- but the name does not really match -- there is no `GlobalKTable` in this test. Might also be good to pipe some data using the TTD in this test.
I am frankly not sure, if we need to test left-table-table join explicitly (the table-table join test from above should be sufficient)
As above: add a `topologyDescription` verification step -- we should see a repartition topic. We should also not get a state store when reading the data from the repartition topic into the `KTable` for this case.
Might be good, to add a verification step for the `topologyDescription` similar to above (base on past experience, I am a little paranoid to make sure we do the correct thing when building the topology).
For this case, the input `KStream` key was not changed, and thus no repartition topic should be created. We should only get a single sub-topology.
For this case, we should not get a state store.
This first three sub-topologies are weird -- I guess it's because you don't create a new `StreamsBuilder` object but reuses the predefined one -- this make the test somewhat confusing. Would be better to create a new `StreamsBuilder` as in the other tests to get rid of them.
As above, I think we should create both tables using `toTable()` operator
`nodes` is not a good name -> `subTopologySourceNodes` is better.
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
nit: avoid unnecessary `this.` prefix
This seems to only verify the direct parent -- however, we should rather use `repartitionRequired` flag to determine if we need to insert a repartition topic or not.
As for all tests, we should verify `topologyDescription`
This method is only used once -- we should inline the code and remove the method.
We should create this table from a stream. Otherwise we don't verify that `toTable()` does materialize the table (as required if used in a foreign key join) automatically. (ie, `toTable()` should not take a `Materialized` parameter in this test, but the join must still work).
Same here. We should use `builder.stream().toTable()`
We should not use `MockProcessorSupplier` but write the result into an output topic and use a `TestOutputTopic` for verification of the result (the `MockProcessorSupplier` should be removed completely mid-term...)
Using `TableProcessorNode` implies that we make all decision about materialization in `doToTable`? -- However, I am wondering if this is correct. When we call `StreamsBuilder.table()`, we create a `TableSourceNode` (within `InternalStreamsBuilder#table()` that allows us to defer decision when we build the actual `Topology`. This has the advantage that we can take into account how the `KTable` is used downstream. For example, if we use it downstream in a stream-table join, we need to materialize the table -- but if we decide to non materialized here, we can't change this decision later on using `TableProcessorNode`? (Correct me if I am wrong.)
nit (rename): `shouldMaterializeKTableFromKStream` (a good naming patter is "should <doSomething> <condition/operation>"
This seems to always enforce a materialization, but I think we should materialize only if we need to.
Add missing `null` check for `materialized`
This null check is redundant as we check for null in `toTable(Named, Materialized)` anyway -- can be removed
Nit: in `KTableImpl` the corresponding name is `KTABLE-TOSTREAM-` hence, we should use `KSTREAM-TOTABLE-` here
`no longer treated as an updated record` -> c&p error; it was a "fact/event" and is re-interpreted as update now.
Just a personal request: we try to keep the method somewhat "ordered" base on their semantics (stateless: no-side effect, with side effect, othes; stateful: grouping/aggregating/join; last PAPI integration). Hence, I think it would be best to not add the methods at the end, but after the `to()` overloads and before `groupBy(...)` -- please use the same order in `KStreamImpl` to keep both classes aligned -- it make is easier to navigate through the code.
This change is a good suggestion.
I think this change is actually less clear. Workers (members of the cluster) can be leaders, while connectors and tasks are assigned to workers. But connectors/tasks are not workers (members), so it's actually incorrect to say "Cannot restart task since it [the task] is not a leader". About the only improvement to the original message that adds value is to change "member" to "worker", though the value is debatable since we do sometimes use "member" in other log messages (including in this class!).
```suggestion // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks ``` Just marking this for later follow-up.
nit: typo `success` Also mention that it tests that insecure protocols are not enabled by default.
This selector is never used since we create a Selector a couple of lines below using `createSelector`. We should remove these three lines and the `logContext` created at the start of this test.
Sorry, missed this earlier: We are creating a new `selector` in `checkAuthentiationFailed`, so we should ensure that the previous selector is closed. You could call `selector.close()` just before calling `checkAuthenticationFailed` here and also a couple of lines below.
We tend to use different `node` value when multiple connections are created by a test. You could just replace `node` here with "1" and a couple of lines below with "2".
We should also create a client connection with one of the newly disabled protocols like TLSv1.1 and verify that the client connection fails.
Not sure if these configs actually come from the default configs. I think these may be explicitly configured in the tests.
It would be better to use `NetworkTestUtils.checkClientConnection(selector, node, 100, 10);` which actually uses the connection.
I think the main reason why we would not deprecate the existing overload is a "producer only" application for which there is not even a consumer.
`.size() == 0` => `isEmpty()`
Should this say > client operations that do not explicitly accept a timeout or > client operations that do not specify a timeout
Nit: I think `0.0` is idiomatic in Java.
Nit: do we need the `D` suffix? Typically, the decimal point is enough to indicate a double in Java.
Similarly here, when suspending tasks we can just have this logic inside stream-thread and stream-task.
Hmm.. I think we should just close all tasks in onPartitionsLost (this is done as in https://github.com/apache/kafka/pull/7997), then we do not need to have the rebalance listener executing the embedded clients logic here.
As a further thought, I think TableProcessorNode should be used for KTableSource as well (today they are only used in filter, map, transformValues and joinForeignKey that results in a KTable), so that we do not need this extra condition. But we can do this cleanup later (our current internal representation has a few such gaps already so some refactoring might be in request in future anyways).
nit: not related to this PR, but the above `TODO` can be renamed as `TODO KIP-300` to be more specific.
`KTableSource#materialize()` should be good enough -- it will set the `queryableName` (AFAIK, this won't make the store really queryable, but it's just a proxy to enforce materialization).
I don't think we need all this, if we delay adding the store to `writeToTopology()`
We can just use `ktableSource.queryableName() != null` to do this check (we can get the source ktable node via: ``` final KTableSource<K, V> ktableSource = (KTableSource<K, V>) processorParameters.processorSupplier(); ```
Many of the other methods don't involve an optional feature. It may be possible to change the return type to `Response`, and then when the feature is disabled return an error response like in `resetConnectorActiveTopics(...)` but then when the feature is enabled build the map to be returned and then: ``` return Response.ok(map).build(); ```
This looks better than what I did, go for it! My original hotfix PR is just to unblock the JDK11 jenkins job.
```suggestion * This will turn off fault-tolerance for the suppression, and will result in data loss in the event of a rebalance. ``` This isn't a "store", but rather a buffer internally used for suppression. Also, it seems appropriate to be a little more dire in our warning here because the internal nature of the buffer may make it less obvious to people what the downside of disabling this changelog is.
```suggestion * Disable the changelog for this suppression's internal buffer. ```
```suggestion assertThat(CLUSTER.getAllTopicsInCluster(), contains(changeLog)); ``` Equivalent, but gives a better error message when it fails.
```suggestion public void shouldAllowOverridingChangelogConfig() { ``` Not a big deal, but it's a little nicer when the test methods explain exactly what they are testing.
```suggestion public void should createChangelogByDefault() { ```
It seems like this test would be more useful just as an assertion of the default behavior.
```suggestion assertThat(CLUSTER.getAllTopicsInCluster(), contains(changeLog)); ``` And we can remove line 426, `final Properties config = CLUSTER.getLogConfig(changeLog);`, since it would be unused.
Maybe this should be in `ProduceResponseTest`? We started writing this type of test in dedicated classes as this class was getting too large.
prop: abortTransaction can also throw ProducerFenced.
It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this: ```java try { producer.beginTransaction() producer.send(...) producer.sendOffsetsToTransaction(...) producer.commitTransaction() } catch (Exception ) { producer.abortTransaction() } ```
Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.
Why we need an atomic long here? Seems there's no concurrency.
```suggestion * @param storeQueryParams the parameters used to fetch a queryable store ```
Should we include the brokers instead of removing them? Same below.
Nit: I don't think you need the `and`
the type of pollTimeMs is *Duration*. It seems to me that the "ms" is a bit redundant.
This would be less mysterious if this method were inlined into `updateLimitOffsets`. Right now, it's not terribly clear why it's ok to set the "last update offset time" in a method that doesn't update the offsets.
Yes, you have to check each task individually, but nothing prevents you from submitting them all at the same time. I am assuming that any overhead here is really associated with task startup time, so this allows for it to be parallelized.
Unless I'm missing something, we are not actually using the boolean value anywhere. All we do is check if the optional is present. Would it be simpler to just return the boolean directly? also nit: having `assert` in the name is misleading since there are no assertions. I would suggest `isConnectorAndTasksRunning` or `checkConnectorAndTasksRunning`
This assertion tries to capture 3 cases: 1. All good, return `Optional.of(true)` 2. The boolean logic in the method is `false` (e.g. tasks are not ready, connector is not ready, an error response was returned etc). Please return `Optional.of(false)` 3. Exception was thrown and I can't say. Return `Optional.empty()` So based on this, how you'll interpret (3) is up to you on the `waitForCondition`. And here you want to treat it as `false`. That's why `orElse(false)` is more appropriate.
If you need to assert that _at least_ `numTasks` are running, probably it's safer to use `>=` here. If you are starting more than 1 tasks, these tasks might start fast enough and you might miss your assertion, e.g. the number will go from 0 to 4. Again assuming that your assertion is that _at least the connector and 1 task are running_.
I'm mostly concerned about the time to run tests. If it does not make a substantial difference either way, then we can leave it.
I'd suggest copying and using what we've been using elsewhere to assert that a connector and its tasks are up and running: https://github.com/apache/kafka/blob/trunk/connect/runtime/src/test/java/org/apache/kafka/connect/integration/RebalanceSourceConnectorsIntegrationTest.java#L324-L337 It works fine with the `waitForCondition` method that accepts a timeout. Eventually (maybe soon) this type of assertion will go to `EmbeddedConnectCluster` and will be made available for every Connect integration test. However, I suggest we don't take on this refactoring now and we just copy the method from the test above.
Could we use `TestUtils.waitForCondition`? Similarly in `waitForTasksToBeUp`.
If we let this take a set of connector names, we could test them all at once.
That's fine. There are cases that you might want to invert the answer that says "I can't say". But we haven't finalized the design, that's why this helper method hasn't found its way to the base class yet. Still the goal will be to provide generic methods. As a reference, here's an example of negation of the above with `orElse(true)`: https://github.com/apache/kafka/blob/trunk/connect/runtime/src/test/java/org/apache/kafka/connect/integration/RebalanceSourceConnectorsIntegrationTest.java#L227 It's fine to simplify here. We can change if we ever generalize this checks.
nit: can we make this debug level? Otherwise it will make this test a little spammy.
nit: unneeded newline
nit: do we need the `numTasks` argument since the only caller is passing MIN_TASKS? Alternatively, we could replace `info.tasks().size() >= numTasks` with `!info.tasks().isEmpty()` and get rid of `MIN_TASKS`.
How about this? ```java // The consumer fetch position needs to be restored to the committed offset before the transaction started ```
I think we can leave `WakeupException` and `InterruptException` out of this. In both of these cases, we would probably just want the application to close. I think the main thing we want this example to show is the "normal operating" exceptions.
Hmm.. Is it actually safe to abort following a TimeoutException? I think this would cause an illegal state error in the producer. To handle this correctly, the application should retry the commit.
Could we do "group mode" only in this example? The example doesn't really extend to multiple instances otherwise.
For the purpose of understanding EOS, the main exceptions that are worth calling out are `ProducerFencedException` and `FencedInstanceIdException`. I would suggest we write the example like this: ```java try { ... producer.commitTransaction; } catch (ProducerFencedException e) { throw KafkaException("The transactional.id $transactionalId has been claimed by another process"); } catch (FencedInstanceIdException e) { throw KafkaException("The group.instance.id $instanceId has been claimed by another process"); } catch (KafkaException e) { // If we have not been fenced, try to abort the transaction and continue. This will raise immediately // if the producer has hit a fatal error. producer.abortTransaction(); } ```
```suggestion * @throws ConnectException if the configuration fails to be serialized or if the request could not be sent ```
BTW, I think changing the return type from `void` to something else would be considered backward compatible, so this type of change is acceptable.
I'd suggest rephrasing this: > If the group does not have a committed offset for this partition, the corresponding value in the returned map will be null.
Let's use `OffsetFetchResponse.INVALID_OFFSET`
The intent was to remove the input partitions from the map any time we remove a task from `tasks`. It looks like your code maintains this (in a clearer and cleaner way).
Ah, then it was my mistake before! Good catch.
`can't` -> `could not` ? I would remove `even though TopologyTestDriver is shutting down` (read cleaner this way IMHO.)
What does `Such occurrences` means? Might be good to be more elaborative.
nit: add a blank line before this one to make it easier to read
Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.
Personally I think it reduced readability 
`ProducerRecord` is actually guarantee to have a non-null timestamp -- not sure if we need this check -- actually similar for other fields like topic or header
Yeah, good point. I've been on the fence here, because the calls might block one way or the other anyway. Good thing is that the locking is per connector instance. Let's keep the synchronization. We can always return with some performance benchmarking on the topic and revisit locking for connectors _and_ tasks if we can afford to do that. But let's follow the `synchronized` method style (e.g. `onPause`, `onResume` in your examples above) if the synchronization applies to the whole method.
I don't think locking buys you anything w/r/t to that. The producer#send in the status backing store is asynchronous. So what would describe above can happen anyways, regardless of whether you lock this object. Of course, if it wasn't asynchronous things would be much worse. A bottleneck would be created by the lock, waiting for the `send` to finish in every locked block, so that's not an option. Wdyt? That's what I see at the high level without spending to much time on it, but see if you can check this assumption and we can return to this question.
nit: seems unnecessary? similar for `State.FAILED` below.
if you endup locking the whole method, there's no reason to use a block, you can use `synchronized` on the method. Applies here and any similar method.
Thanks for checking the underlying implementation @C0urante . That takes us to my earlier concern about this operation potentially blocking for too long to be in a `synchronized` block. And the potential of blocking does not have to do with acknowledging that the record was written only. The producer call has a metadata update call too. Going over the uses of `KafkaBasedLog` in Connect, I didn't find an example where we have `KafkaBasedLog#send` running in mutual exclusion. Contrary, similar concerns are probably the reason why we call `OffsetStorageWriter#doFlush` outside the synchronized block in `WorkerSourceTask`. I think we might be able to live with a rare race condition as the one you described, in order to avoid introducing unintended side-effects due to locking.
same question here and below about locking around a `volatile` variable. Is this the only reason to lock here? One would think so based on previous usage.
Can we use `Optional<String>` for this? Using magic string values feels messy, and will leak into the API
This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.
`MetadataResponse` allows us to get the `Cluster` directly, so we can do something simpler: ``` Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1); Node newLeader = updatedMetadata.cluster().leaderFor(tp1); assertNotEquals(oldLeader, newLeader); ``` Since the metadata doesn't change, we can just do this check once.
This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.
Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully.
Can we add an assertion like the following to ensure that both requests were sent? ```java assertFalse(client.hasPendingResponses()); ```
Not really sure this has value if the test case expects the leader change correctly.
What about just making `childClasses` a `Set<String>`, since `String.join(...)` takes an `Iterable`? Arguably it's not much different, but it's more in line with other uses of `String.join`. Or, maybe better yet, what about just joining with the stream? Something like: ``` String childClassNames = Stream.of(transformationCls.getClasses()) .filter(transformationCls::isAssignableFrom) .filter(c -> !Modifier.isAbstract(c.getModifiers())) .filter(c -> Modifier.isPublic(c.getModifiers())) .map(Class::getName) .collect(Collectors.joining(", ")); String message = childClassNames.trim().isEmpty() ? "Transformation is abstract and cannot be created." : "Transformation is abstract and cannot be created. Did you mean " + childClassNames + "?"; ```
git: The sentence "So setting the strategy ... matching the given strategy name" reads a bit confusing here. I think we only need to state that when the change of the policy would take effects (the next time when compaction is triggered by the log cleaner thread), and emphasize that for "timestamp" and "header" we would always still retain the last record.
nit: ... for "header" compaction strategy.
This is not used.
The two cases are differ that one throwing KafkaException (fatal) and the other throwing ProducerFencedException (task-migrated).
If the serialization failed we wrap it as a StreamsException -- with this PR it seems a regression that we do not do it any more.
We should remove `K9113` here since in the scope of KAFKA-9113 we are going to treat this as fatal (which is already incorporated now). If we want to use the buffer-and-retry logic then it would be out of 9113's scope.
nit: can we consolidate `producerFencedOnCommitTxn` to the more-general `commitTransactionException`? I.e. if you want to fence on commit, you just register the `commitTransactionException` as a ProducerFencedException
"Task " + taskId + " could not get partition information for topic " + topic
I will make a separate PR to fix this.
The passed in value would be true for both alpha and beta right? if yes we can just name it eosEnabled.
These first line should be removed -- I did it in my current on-going PR.
When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.
nit: could be simplified as `eosEnabled`
req: Could you please remove `context` from the `reset()`? Since we did not specify any behavior for it, we also do not need to reset it.
req: Could you please replace `.andReturn()` with `.andStubReturn()`? This avoids the verification of the call to `valueTransformer.transform(inputKey, inputValue)` which I consider setup and not validation.
req: Since we do not need to validate `valueTransformer`, could you please remove it from the `verify()`.
Good catch! I would prefer to remove it then.
This function is called by two callers: `xxxFromMetadata` and `xxxFromAssignment`. For the former we do not maintain the old topics but just replace with the passed in value, for the latter we still maintain the old topics -- this it to take care if the leader did not assign all tasks / partitions due to assignment error. We should still keep that logic here.
I think now I understand the reason that part of the partitions can be removed here. If a task is indeed being removed, it should be triggered in the TaskManager#onAssignment; here TaskManager#onRevocation is triggered after the previous call so the tasks map should have been updated --- i.e. the task would not be inside task-manager anymore, and if it is due to regex pattern (like this test) the current condition is okay: we should not suspend the task unless all its input-partitions are included. Otherwise, we can just update the input partition of that task --- right now it is final so we cannot update it, but I think thats fine since we would no longer pipe any records to that task and there will be no committed offsets for that partition either --- in either case, we can remove it from the remainingPartitions.
Yeah I know that :) I was referring to the one with `SUM` not `COUNT` too but I copied the wrong name. We already have a public `addRateOfSumMetricToSensor` which is only used for rocksDB today, and I was wondering if we can leverage that.
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<Long>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<VR>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<VR>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
```suggestion * ReadOnlyWindowStore<K, ValueAndTimestamp<V>> localWindowStore = streams.store(queryableStoreName, QueryableStoreTypes.<K, ValueAndTimestamp<V>>timestampedWindowStore()); ```
Just to clarify we should only swallow under `dirty` suspension (in trunk it is wrapped in `close`) right? For clean close / suspend if it throws we would capture it and wrap as TaskMigrated still and then handle them by closing the task as dirty (in that second try we would swallow).
Hmm.. I don't think it's safe to modify `topicPartitionOffsets`. This is shared by multiple calls following the initial `getListOffsetsCalls`. It would be better to create a new map. It might bee worth having a test case which uses two partitions with different leaders to verify this case is handled correctly.
Currently in this case, we do the following: ``` future.completeExceptionally(error.exception()); ``` I am suggesting we do something like this: ``` future.completeExceptionally(new KafkaException("Unexpected partition in response...."); ```
Would this be any clearer? ```java OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp); if (offsetRequestSpec == null) { future.completeExceptionally(error.exception()); } else if (shouldRefreshMetadata(error) { retryTopicPartitionOffsets.put(tp, offsetRequestSpec); } else { ... ``` Also, in the case of that we got back an unexpected partition, I think we can raise a new `KafkaException` and provide a clear message indicating what happened.
The slf4j `{}` placeholders will not work here since we are constructing the message ourselves.
nit: conventionally, we put the `else` on the same level as the previous branch ```java } else if (... ```
req: Due to this deletion, line 327 becomes a no-op. Please remove it, too.
```suggestion info.userEndPoint(), taskManager().getTaskOffsetSums()) ```
I'm not sure this should be necessary either. IIUC, the "future" subscription info isn't supposed to really be a descendant of the current protocol, just a stand-in for _some_ protocol version bigger than ours, in which case all that really matters is the version number. Its role is just to join the cluster and get downgraded to the "latest" version, in which case it should be able to defer to SubscriptionInfo.
Oh, actually, here's the reason a constant sentinel is nice, but we didn't actually use it!
req: After this deletion, `REBALANCE_PROTOCOL` defined on line 66 is not used anymore and can be deleted, too.
req: Is it possible to use a defined constant (e.g. `ACTIVE_TASK_SENTINEL_LAG`) here and also use it in `TaskManager`? I think it would be good to have this constant defined here and then use it in `TaskManager`.
req: `subscription` -> `subscription info`
Currently we are not passing required security configs (using --command-config) to the tool. This change may not work for with secure broker listeners. seems like we are not using these methods in any security enabled tests.
```suggestion put(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, "/tmp/foo"); ```
Nit, suggested rewording ```java log.info("Fetch offset {} is out of range for partition {}. We only have log segments in the range of {} to {}. Resetting offset.", fetchOffset, tp, partition.logStartOffset, partition.lastStableOffset); ```
`logStartOffset` and `lastStableOffset` are not getting filled in when server responds with OffsetOutOfRange error. As @hachikuji pointed out, you need to fill that in on server side here: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1031
Hmm.. do we want to capture all Exceptions as TaskMigratedException? I actually think it's okay to not capture anything here since only StreamsException or TaskMigratedException would be thrown here.
I think Guozhang's point is that `handleRevocation` _might_ throw other exceptions (eg `IllegalStateException`) which we should not convert to `TaskMigratedException` because they are fatal while `TaskMigrated` is recoverable
Actually I think we should not interpret everything as `TaskMigrated` --> if it is due to a recoverable fenced issue e.g. the thrown exception would already be a TaskMigratedException. So I think we should not capture any here instead.
nit: the `20` here means version 2.0, since we mistakenly made a compatible breaking change in 2.0 and this test is specifically for that. So let's just keep the suffix.
I thought about an alternative idea here and just leave it to your call whether you like it or not: I think we can modify line 1047 above to only `if (stateFactory.users().contains(node))`, since the `storeToChangelogTopic` should contain the source-changelog topics already, so we can just add it to the `stateChangelogTopics` of the TopicsInfo. Then, in StreamsPartitionAssignor, before `prepareTopic(changelogTopicMetadata);` we only prepare the topics as `changelog-topics minus source-topics`. In this way we do not need this flag either. My rationale is that this is more consistent with the name `stateChangelogTopics` (a source-changelog topic should be included in this map, as well as in `source-topics`, we just need to let the partition-assignor to be aware of the possible overlapping).
Right, if retries are exhausted and it's a retriable exception, then it seems like it should be a TimeoutException.
One of the annoying aspects of the code below is that we have a lot of redundant logic for constructing a call in the context of a previously failed call. I wonder if it would simplify the logic if we added a constructor which accepts the failed call as an argument and then adjusts `tries` etc accordingly.
nit: I feel like a jerk for saying this, but can we avoid the exclamation points? The enthusiasm can be a bit vexing for an operator who hits this error.
nit: I think technically we don't need this. For internal classes, I tend to favor accessing fields directly when the class is nested. I don't feel strongly, so up to you.
Mainly just trying to consolidate the logic for updating the retry state so that we don't have to remember to do it for new APIs. Also, eventually we want to have exponential backoff logic. I was thinking `CallContext` would be a good way to encapsulate any additional state that we would need.
Ok, let's leave this as a potential future improvement (perhaps as part of the the exponential backoff kip).
I missed this on the first pass, but would there be any harm moving this call to `enqueue`? That way we do not need to repeat this check for other scenarios involving chaining or partial failures. (For example, I assume we'd end up doing the same thing in `rescheduleMetadataTask`). We should probably also try to reuse `failWithTimeout` for a consistent exception message.
nit: add some sanity check on these numbers (like should be non-negative etc). Also update `toString` method to include this information.
nit: Maybe update the message to "Max retries maxRetries for <call> reached."
Not sure if we need to make these `Optional`. `0` seems to be a good default value for these.
To keep this logic same as one in `Call::fail` method, lets set the new time as: > nextAllowedTryMs = now + retryBackoffMs
nit: this alignment is a little weird. Typically we would align with the arg above
I am not sure we enable java asserts when running Kafka server. Lets check the condition and throw `IllegalArgumentException` instead.
nit: As we always pass same value to this method, we can drop the parameter and use `time.milliseconds` within this method.
Opened a minor PR: https://github.com/apache/kafka/pull/8250 (let me know what you think)
`clear` is also called in `suspend` in which we should not clear the partitionTime, but I think we should reset the `partitionTime` in close.
nit: make the test name more descriptive, like `testFlushCompleteSendOfInflightRecords`
nit: we might want log different message if we're ignoring due to a fatal state instead of due to a bumped epoch or ID.
Yes, that's more or less what I was concerned about. We are trying to achieve a specific result here (and brand-new 'Header' instance each time), but it's only achieved via a side effect. On the other hand, I just took a closer look at the code, and I see that there's a better reason to keep using `ConsumerRecord`, namely that `updateProcessorContext` is used both for regular processing (with "real" records) and for punctuation here with this dummy record. It seems like a good idea to prevent those code paths from diverging, so I'm +1 with keeping this change as-is.
From my understanding, neither the `ConsumerRecord` nor the `ProcessroRecordContext` are the issue, but the shared `Header` object -- it's just a "side effect" that creating a new `ConsumerRecord` creates an new `Header` object internally.
This is fine, but note that the new consumer record here is just a roundabout way to create a ProcessorRecordContext. It'd probably be better to just directly instantiate the context we want.
Just to be safe would it make sense to keep this at `60`? I'm not insisting, just asking it's fine to leave as is if you are comfortable with it.
Should we use "NO_CURRENT_THREAD" to replace "null"? It seems to me "NO_CURRENT_THREAD" is more readable.
```suggestion * Options for the {@link AdminClient#alterConsumerGroupOffsets(String, Map, AlterConsumerGroupOffsetsOptions)} call. ```
```suggestion * Options for {@link Admin#electLeaders(ElectionType, Set, ElectLeadersOptions)}. ```
```suggestion import java.util.Collection; ```
rearranging the imports to match the style might make sense here, given that there are only two import statements.
```suggestion import org.apache.kafka.common.annotation.InterfaceStability; import java.util.Arrays; import java.util.Set; ```
This has been fixed by: https://github.com/apache/kafka/pull/8291 Please omit and rebase to get the latest changes.
```suggestion import java.util.Map; ```
AclAuthorizer is not a public class, so it may be ok to make this class public in AclAuthorizer instead of duplicating it here.
We should not be using mockito internal classes.
Sure @vitojeng . cc @mjsax
`To query state stores, it's required to first start Kafka Streams via {@link KafkaStreams#start()}. You can retry to query the state after the state transitioned to ...`
`Indicate[s] that Kafka Streams is in state {@link org.apache.kafka.streams.KafkaStreams.State#CREATED CREATED} and thus state stores cannot be queries yet.`
typo `to to` (also missing `.` at the end of the sentence)
Nit: `Note that {@code InvalidStateStoreException} [is] not thrown directly but only [its] sub-classes.`
Nit: we don't need the `<p>` tag because there is only one paragraph.
nit: These two functions are not for testing only.
1. Let's make it a warn instead of a debug. 2. The error message can be more specific here: `Error closing task producer for task {} while handling lostAll`.
Ditto here about error message
This seems not used.
Similarly here, this state check could be internalized.
Having a `prepareCloseDirty` makes the calling of `closeDirty` a bit cumbersome as we always need to call `prepareCloseDirty` first. To simplify or just do a reminder, I have two suggestions: 1. Internally create a task state called PREPARE_CLOSE or just a boolean like `closeDirtyPrepared` as the state check, so that closeDirty will throw illegal state if the flag is false 2. Following #1, instead of throw, if we don't see the prepareClose is being called, the `closeDirty` will invoke `prepareCloseDirty` first internally.
I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
Similarly for `closeDirty` and `prepareCloseDirty`
We don't have unit test coverage for this exception case
We lack unit test coverage for this case
Could we verify the assignment stack and lost stack separately, by doing `handleAssignment` verify first before calling `handleLost`
We should also verify the thrown cause
Same here, for verifying the thrown cause
Yea, a TODO is also ok.
I would prefer a second loop to guarantee a consistent reflection on the task committed state.
Same here: not only CREATED, but also RESTORING and SUSPENDED tasks should not be included in `consumedOffsetsAndMetadataPerTask` and we should not let the task-manager to peek its state.
`For the next PR`: I think we can save prepareClose (or more accurately, merge prepareClose and close together again) if we make a state diagram change that only suspended state can transit to closed state, i.e. at task-manager level whenever we want to close a task we call its `suspend` function first, which would, depending on its state, be a no-op, or flushing, or closing topology etc, and then after that the task is always in SUSPENDED state, and then we call "commit" if necessary, and then we call close (a minor thing is that today when the state is in SUSPENDED we would omit committing inside task, and we need to lift this restriction; and also the transition actions to transit to SUSPENDED need to rely on the clean flag, hence we need `suspend(clean-flag)`). AND we can further merge prepareSuspend and suspend as well by just making the checkpointing logic as part of post-commit instead of post-suspend, since as I mentioned above you only have three cases: 1) do not need to checkpoint: if you are in CREATED. 2) checkpoint written and consumed offsets: if you are in RUNNING, in which you need to commit offsets as well. 3) checkpoint only store offsets: if you are in RESTORING, and in which case you do not need to commit offsets. In fact, if we are not in the RUNNING state yet, the `consumedOffsets` as well as `recordCollector#offsets()` are always going to be empty, so it is always safe to call `stateMgr.checkpoint(checkpointableOffsets())` and not condition on the state and call `stateMgr.checkpoint(emptySet())`. And if we now allow committing in SUSPENDED state as part of closing (i.e. suspend -> commit -> close), similar rules apply: if we are suspending from a RESTORING state, then in `postCommit` while we ``stateMgr.checkpoint(checkpointableOffsets())` the `checkpointableOffsets` would always be empty; if we are suspending from a RUNNING state it would contain some offsets.
Probably need to change after rebase
nit: allBuffered &= newInputPartitions.isEmpty();
remove should be a no-op if the topicPartition doesn't exist. Might be able to rephrase: ``` if (!newInputPartitions.contains(topicPartition)) { totalBuffered -= queueEntry.getValue().size(); queuesIterator.remove(); removedPartitions.add(topicPartition); } newInputPartitions.remove(topicPartition);
The map itself should still be final
nit: let's put iterator initialization closer to the start of while loop
We should unit test this function
The prefix for kilo, is lowercase `k`
Do we need this extra variable? It seems like `defaultValueBytes` is not used
minor: we can just calculate the `numPartitions` in this overload, and then in the other which has `numPartitions` passed in always blindly use that one.
Nit: `null` -> `{@code null}` Same for all other cases
 I "have a friend" who sometimes puts new classes in the same package so that they can use package private methods. This friend understands that such code is brittle.
Why is `false` (inexact decimals) the default for the no-arg constructor? If this is an attempt to maintain backward compatibility, we should consider whether this bug, when fixed, compatible. Seems like it would be, since having the deserializer use the trailing zeros would be fine/better than not using them.
Passing in a `JsonNodeFactory` instance might be better here, because then the `JsonConverter` instances and `JsonDeserializer` in each converter will all use the same `JsonNodeFactory`. As it currently stands, the `JsonDeserializer` has it's own instance of the `JsonNodeFactory`, and it's possible that they could be set up differently in the future and not caught.
Cool, yeah that addresses my concern 
```suggestion // when the interval has elapsed we should try to update the limit offset for standbys reading from // a source changelog with the new committed offset, unless there are no buffered records since // we only need the limit when processing new records ```
Could we just add one more boolean condition into the filter and check whether `changelogsWithLimitOffsets` is empty or not.
nit: usually we would write this is `this.state = requireNonNull(state);`
Hmm.. We have an `UNKNOWN` state in `ConsumerGroupState` in case the group coordinator adds a new state that the client isn't aware of. Currently we're going to pass this through the request, which is a bit odd. Furthermore, if the coordinator _does_ add new states, we will be unable to see them using this API. I think it might be better to use a `null` list of states in the request to indicate that any state is needed.
Do we need this API? Seems this is the default behavior.
These two cases don't seem to be different. I'd recommend just always wrapping the exception and throwing (currently the else block). If we just re-throw the first exception, reading the stack trace becomes very confusing. Especially since a lot of those exceptions don't even include the stack trace.
In newest trunk we always call `task.closeDirty` .
nit: `task.id()` -> `taskId`
Do we need to log here? All errors are logging in L163 already (and I think we would log it again in upper layers)
We should wrap `KafkaException` as `StreamsException` but rethrow all other `RuntimeException` unwrapped (at least this is the pattern we use everywhere else, and thus we should follow it here, too)
How about instead keeping this private and only exposing `reOpenTaskProducerIfNeeded`, which would take care of doing nothing if there's no task producer, etc. I'm concerned that otherwise, someone might call `createTaskProducer` when there's already one there, leading to a "producer leak".
Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.
Basically yes. I would extend the composite test with multiple occurrences of each case so that we also cover the scenario where we have -- for example -- n active running task, m active non-running tasks, k standby tasks etc. If this makes the test too clumsy, you could cover each of n active running task, m active non-running tasks, k standby tasks etc in its own test and make one composite test with one occurrence of each case. Choose what is better readable. My point is, that the current test does not cover multiple occurrences of the same case.
prop: I would use `taskId01.toString()` here, since you are not testing the `taskId01.toString()` method. Our assumption is that the folder has a name that is equal to the result of `taskId01.toString()` and not `0_1`.
req: Please also verify `stateDirectory.unlock("0_2")`. Only verifying `lockedTaskDirectories()` seems too weak to me.
prop: You can remove this line. You only need `expectLastCall()` if you need to do some further expectation settings on a call that returns `void`, e.g., `expectLastCall().times(3)`
req: Could you compute those from the above maps? It might make maintenance easier.
I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it _would_ be in, given all the mocks above.
req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.
req: You do not need to verify the `activeTaskCreator` here, since you are not testing `handleAssignment()`.
prop: Please use `assertThat()` since it makes verifications a bit better readable.
req: I assume you do not want to test `handleAssignment()` here, so you should not specify behaviour verification on the mock. You could simply write ``` expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment))) .andStubReturn(singletonList(task00)); ``` `.andStubReturn()` is behavior that is not verified in the `verify()` method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test. Same applies to other similar locations in this test.
Oh, I guess I missed `shouldNotReportOffsetSumsForTaskWeCantLock()`. No, I did not have something different in mind.
Ah, sorry to say, one more thing slipped by me before. We should `verify(inner)` at the end of both of these tests. It should actually fail for `shouldNotPutIfSameValuesAndGreaterTimestamp` because we should _not_ call `inner.put` in this case. To fix that, we would just delete L202, where we set up the mock for `inner.put`. Then, the mock would be initialized to expect no calls, and the verification would fail if we did call it.
Oh, I see. Definitely wasn't obvious by reading the test.
The purpose of this change was to highlight that the data structure is required to be concurrent. Of course if a method that existed in `ConcurrentMap` and not in `Map` was used, that would be a hard requirement. `putIfAbsent` used to be such a method but that's not the case after 1.8. In any case, the use of the more accurate interface is valid even if we don't explicitly use methods that don't exist in the parent. That's because the need for this implementation to be thread safe is a requirement here.
Keeping the static block here, because it's a block and that's what we have in `PluginClassLoader`. Our style is not too strict with respect of this ordering. The class overwrites the `loadClass` method. But it's delegating the loading to different classloaders and the locking is embedded in these classes.
looks like we don't have any matching prefixed ACLs for this resource. It is good to add few matching prefixed ACLs
resourceWildcard => resourceWildcard
nit: new Integer(1) => Interger.valueOf()
we can maintain `context`, `actions` as class variables initialized in setup().
prop: make the value a `SortedSet` so we can just insert clients as we build the map and use a custom comparator to automatically sort the clients based on lag
req: I think we want to introduce some `acceptableLag` config within which a task is considered caught-up, otherwise this is way too strict. ie the condition should be `lag <= acceptableLag`
req: rename to `tasksToPreviousClients` or something similar that works "previous" into it
```suggestion // If a task was previously assigned to a client that that is caught-up and still exists, give it back ```
req: rename `clientHostingTask` -> `previousHostingClient` (or similar)
```suggestion * @return map from tasks with caught-up clients to the list of client candidates ``` or something similar to make it clear the map only contains tasks with caught-up clients
Hmm...I'm wondering if `Map<String, List<TaskId>> previousAssignment` is sufficient to pass in, won't we lose all the tasks that were assigned to a client that no longer exists for whatever reason? Maybe we should just pass in `Map<TaskId, String> tasksToPreviousClients` (aka `tasksToHostClients`) directly. We can build that map up during other steps in `assign`
req: we'll never hit this, as `taskToCaughtUpClients` only contains tasks _with_ caught-up clients IIUC. Can we just construct `unassignedTasksWithoutCaughtUpClients` as the set `totalTasks - taskToCaughtUpClients.keySet`? We can do that in `assignTasksWithoutCaughtUpClients` and remove `unassignedTasksWithoutCaughtUpClients` from `assignTasksWithCaughtUpClients` entirely
Not sure if this will actually be cleaner or end up more complicated, but you may be able to reuse some of the `StickyTaskAssignor` code here which does similar things
```suggestion // If a task's previous host client was not caught-up or no longer exists, assign it to the caught-up client with the least tasks ```
Actually, do we even need this at all? It seems like we get everything we need from `statefulTasksToRankedClients` -- it should have all tasks (and clients), and either a) the previous client was caught-up, in which case it should be the first rank and we can determine it was the previous host from a lag of `Task.LATEST_OFFSET`, or b) the previous client was not caught-up, in which case we don't really care what the previous host was for that task
req: drop the `!caughtUpClients.isEmpty()` check here, if it's in the map it should have at least 1 caught-up client
req: Lag should be a long
req: The `clientId` type should be `UUID`, or a generic for easier testing (c.f `StickyTaskAssignor`)
TBH, I'm a little skeptical of using this style too much. Nothing against functional programming; it's just that, having done quite a bit of FP-heavy programming and maintenance for quite a few years, I've settled into an opinion that it's most efficient when employed in simple contexts. When you get into nested transformations like this, it becomes harder to come back to the code in three years with a completely blank slate and read it. Plus, it has a tendency to steer you away from efficient code and you can wind up doing multiple iterations over the same collection when one would have done. So, I tend to use the FP APIs to do stuff like turn a list of Tasks into a list of TaskId, and I'm happier to see regular loops and conditionals for stuff like this. This is very much a matter of preference, though, and I'm only expressing mine.
Actually, WDYT about adding this class in the "add configs" PR and then rebasing this PR on top of that? Then I could do the same (since I need this class in my next PR as well)
```suggestion for (final Map.Entry<TaskId, SortedSet<ClientIdAndLag<ID>>> taskToRankedClient : statefulTasksToRankedClients.entrySet()) { ``` Just to resolve a warning about referencing the subclass instead of the interface.
prop: rename `taskCount` -> `minTaskCount`
maybe also add a `lastAssignedTask(List<TaskId>)` helper to clean up `source.get(source.size() - 1)` used here and below
It looks like the primary purpose of this logic is to compute the list of clients, given the `statefulTasksToRankedClients`. Let's instead just make the set of clients an input to the assignment method instead. I realize it's not in the method signature I specified in the KIP, but then again, that was just algorithm pseudocode. This method isn't used anywhere (yet), so let's take advantage and just assume we'll be passed everything we need in the most convenient format for us.
req: move this to `StreamsPartitionAssignor`, where we'll be building and passing the `Map<TaskId, SortedSet<ClientIdAndLag<ID>>> statefulTasksToRankedClients` map around
WDYT about renaming `lag` to `rank`, or `effectiveLag`, or something else that reminds us this isn't the actual literal lag? cc/ @vvcephei
(just a suggestion based on what I personally find easier to read)
Nice helper, this makes the `balance` code a lot easier to read
Since this only returns tasks whose previous client was caught up, I think we can simplify the first half of `assignTasksWithCaughtUpClients` as just ``` tasksToPreviousClients.forEach((t, c) -> assignment.computeIfPresent(c, (k, v) -> { v.add(t); return v;})); unassignedTasksWithCaughtUpClients = new ArrayList<>(tasksToCaughtUpClients.keySet()); unassignedTasksWithCaughtUpClients.removeAll(tasksToPreviousClients.keySet()); ```
```suggestion final Map<TaskId, List<ID>> taskToCaughtUpClients = statefulTasksToRankedClients.entrySet().stream().collect(Collectors.toMap( Entry::getKey, t -> t.getValue().stream() .filter(c -> c.lag() == 0 || c.lag() == Task.LATEST_OFFSET) .map(ClientIdAndLag::clientId) .collect(Collectors.toList()))); ```
It's a warning you can enable in IntelliJ IDEA. Not a huge deal.
prop: rename `taskToCaughtUpClient` to `taskEntry` or something, looks confusingly similar to `tasksToCaughtUpClients`
```suggestion * Assigns tasks for which one or more caught-up clients exist to one of the caught-up clients. ```
These utility methods can be static.
Fine with me -- it's just a "prop" after all. FWIW I generally find these functional-style methods harder to read, but for whatever reason in this case I was finding the original a bit hard to understand and thought this suggestion helped to "get to the point" faster. But of course it's always easier to read your own code than someone else's 
Good thought. Lag was originally proposed in the KIP, but it's not what we're using anymore.
```suggestion // the validated configs don't need to be logger. return herder.validateConnectorConfig(connectorConfig, false); ```
Actually, it's the new method that needs a `default` implementation if we choose to add it to this interface.
nit: move `false` to it's own line
Why not use `THROW_ON_STABLE_FLAG_UNSUPPORTED` instead of hard-coded string
Atm we cannot say for sure, but it's likely that what's observed on KAFKA-9701 is a broker-side issue; we can either 1) add the check on broker-side across all members to make sure the selected protocol is consistent for everyone, so if the broker already made a wrong choice itself would log an ERROR, or 2) let it check on the client side. I think for trouble-shooting purposes option 2) is fine, and if we later discovered that this bug is actually on the client side I'm happy to revert this change after fixing it.
I think it was never inferred before, it was actually the type `ProcessorSupplier<Object, Object>` (just my theory). I think if old code will fail to compile, we should not change the public API. Note, the existing methods would be deprecated as part of implementing KIP-478 anyway, so maybe we should just leave the public APIs alone.
Ditto here: are we always restricting to a kv-store.
nit: remove blank line
nit: remove blank line
nit: remove blank line
in AK tab space is equal to 4 single spaces. We need two tabs here. Checkstyle might not complain because it's tests. But line 139 has an example of correct indentation.
prop: - Remove `Target` - `thread` -> `stream thread`
Have a look into `StreamsConfigTest`, e.g., `shouldSetDefaultBuiltInMetricsVersionIfNoneIsSpecified()`.
Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.
I actually like `RankedClient` more. Thank you!
The value is that if anybody changes the default value or the allowed values by mistake, maybe during a refactoring, there is a test that shows the mistake. Of course it is not a 100% protection, but at least it won't go totally unnoticed.
prop: I find the `assignment.` prefix a bit clumsy and I think we do not really need it. Other configs do also not have its context prepended like for instance `built.in.metrics.version`.
> I've been going back and forth on whether to add this as a "per-cluster" or "per-instance" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts? per cluster: - (+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client. - (-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth. per client: - (+) Each client host might differ in performance and users might want to adapt the value of this config for each client - (-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader? Since assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.
prop: ``` The maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task. ```
Thanks for the discussion, all. One thing to note is that this config has "no" impact on high availability. Anyone who wants HA would configure num.standby.replicas, and this config applies on top of that one (for this reason, I like the name you picked. It is "extra".) I think you can make a case for per-node or per-cluster, but if we assume we only want to add one config, I think per-cluster is more valuable, since it lets you protect the brokers from overload, which a per-node config may not. Regarding whether we set the default limit low or high, I'd advocate for low. Some clusters are undoubtedly running close to the limits of their disk space or broker capacity, so suddenly letting every node double its traffic would result in serious operational consequences. On the other hand, if we start out low, then the probability of a crash becomes much lower, and the only problem is that the overall balancing process takes a long time. But, since the config is set low, there's a low impact on processing capacity while it's happening, so maybe there's no real impact over that long time. If you buy the argument that a low default is good, then the obvious choice is "1", but that would take a _really_ long time to complete balancing. The default of "2" is basically a compromise. It lets you balance twice as fast, and it's "probably" still low enough to cause a problem for no one. "5" also seems fine-ish, but the farther from "1" you move, the riskier the choice is. One final thought, when you say that some people would "need to change" the config, say from the default (2) to MAX_VALUE... It seems like this population is restricted to the people who really need to make sure that balancing happens absolutely as fast as possible (maybe because their application can't keep up unless the whole cluster is processing, or because they're scaling up to cope with a spike in input data). Hopefully, these people know that they need extra horsepower and are motivated to read the docs and locate the config they need to change. Also, hopefullly, this isn't the common case.
Nit: this can be written more concisely by using `Arrays.asList`.
Nit: this brace should be on the previous line.
Nit: there should be a space before and after the colon.
If we use a `List`, we should be able to simplify the logic below by using an enhanced for loop.
```batch.closeForRecordAppends``` does NOT release the byte array hosted by ```MemoryRecordsBuilder``` ```java public void closeForRecordAppends() { if (appendStream != CLOSED_STREAM) { try { appendStream.close(); } catch (IOException e) { throw new KafkaException(e); } finally { appendStream = CLOSED_STREAM; } } } ``` ```appendStream``` is a wrap of ```bufferStream```. Not sure whether calling ```batch.closeForRecordAppends()``` can resolve the OOM or not. ```java this.appendStream = new DataOutputStream(compressionType.wrapForOutput(this.bufferStream, magic)); ```
We probably also want to close the last batch for append in line 279.
That's a good point too. But what I wanted to highlight is to be explicit and return the exact collection, that being `Collections.emptyList()` or `new ArrayList()` (the former should be fine as you noted), instead of returning what's stored in `result` (whose declaration is good to be close to the use as much as possible). That's to guard against `result` being used earlier by code in the future. Improbable, but also doesn't hurt and it's a good practice IMO.
`result` is unused in this code block. To be future proof, I'd suggest being explicit by returning an empty list here, and declare `result` right above the block that is being used at.
nit: probably checkstyle won't catch this, but if you format the block, there's an extra tab that you can remove here too.
a tab is missing here for alignment too.
@ncliang checkstyle fails because there's an extra single white space here: ``` [ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.13/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java:345: 'try' child has incorrect indentation level 13, expected level should be 12. [Indentation] ```
nit: plural (`Reflections`) seems more appropriate because it refers to the library/class.
Could it print the origin exception also? ```log.debug("xx", e)```
```suggestion return Collections.emptyList(); ```
I will leave it up to you, as long as you ensure the tests itself are mutated correctly, it's not easy to eyeball such a change for no-op.
We could iterate through v1 to v5 here to test every case.
Seems that this data construction logic has been reused in elsewhere (`FetcherTest`), we could get a helper like ``` ListOffsetResponseData getSingletonResponseV0(TopicPartition, Errors, OldStyleOffsets); ListOffsetResponseData getSingletonResponseV0(TopicPartition, Errors, Timestamp, Offset, leaderEpoch); ``` in the ListOffsetResponse to reuse.
Similar for topic request topic construction, let me know if you think we could refactor out a helper like `singletonRequestData(...)` in `ListOffsetRequest`
`Arrays.asList` could be replaced with `Collections.singletonList`
I had a look at this and your are right. It seems that keeping `TopicPartition` is better and difficult to change. In this case, have you considered pushing the conversion to the `Builder` by providing an overload of `setTargetTimes` which accepts a `Map<TopicPartition, ListOffsetPartition>`? That could make the code in the `Fetcher` a bit cleaner.
As we don't have the list of TopicPartition available to filter the list of futures, we could actually directly complete the future within the loop instead of populating the HashSet. It avoids building the HashSet and having to traverse the futures.
nit: We usually put a space before and after `:`.
nit: replace with <>
Let's move this helper into `ListOffsetRequest`
nit: space 4 after `=`
I think this check is redundant and should be removed, otherwise we should have it for all general RPCs with topic => partition structure.
Is this necessary? The leader epoch is -1 by default.
This could move to `ListOffsetResponse` as a helper, and maybe name as `singletonListOffsetTopicResponse`
nit: What about creating a small helper to create a `ListOffsetTopicResponse` for a given `TopicPartition` & co? That would reduce the boilerplate code.
I would call this one `topics()` as you did already in the request.
I just noticed that we don't ensure that all futures of the current broker are completed. It would be great to ensure it by using `completeUnrealizedFutures` method if `retryTopicPartitionOffsets` is empty. We already do this in `alterReplicaLogDirs()` if you want to see an example.
`new ArrayList<>` is suffice
This is not related to your PR at all. It seems that if `offsetRequestSpec` is `null` here, `future` will be `null` as well cause `futures` is initialised based on `topicPartitionOffsets`. If it turns out to be correct, it may be better to just log a warning here like we do in `createTopics()`.
This conversion is a bit unfortunate as we have to traverse all the partitions again to build the `List<ListOffsetTopic>`. Instead, we could compute it directly within `groupListOffsetRequests` and could receive `Map<Node, List<ListOffsetTopic>` directly here. That seems doable but I may have missed something.
I think we could reduce the change of this PR by reverting the numbering change which seems unnecessary.
Well, the purpose is more about encapsulation to reduce the import paths in this test class.
nit: Just my usual habit, I think we should get a separate test file `HostInfoTest` instead.
nit: the mocked task manager would just call `getTaskOffsetSums(...)` so maybe we can just call `taskManager.getTaskOffsetSums()` here? Ditto elsewhere.
Not for this PR: we can clean up the task-manager code to not pass in the checkpoint at all.
Standby task should never be in RESTORING since we always transit from CREATED -> RUNNING -> RESTORING in one call. Did you observe this was not the case from failed system tests? Even in unclean close case you described I did not see why it could be possible..
This part will have some conflicts with @mjsax 's PR, just a note.
I think this is what the below TODO (191) was added for, Thanks :) Please feel free to remove that TODO marker then.
That's true, but I suspect that this leftover folder is not from this test suite but likely from others? I think the right fix here should be to check which test (highly doubt it is one of the `StateDirectoryTest` because of `cleanup` ) has this leftover and fix that one instead.
We don't want to remove the `stateDir` entirely, because then `stateDir.exists` will return false and we won't actually call `listFiles` which is what this test is trying to test. Can we just rename to `"state-renamed" + TestUtils.randomString(5)`? Or, clean up the previous contents at the beginning of the test? That said, `cleanUp` should be called after every test, so there shouldn't be any left over state...right? cc/ @guozhangwang
We can use `assertThrows()` here. Same below
`assertNull`s shouldn't be here but few lines bellow.
We can use `TestUtils.assertFutureThrows()` here too
we can use `TestUtils.assertFutureThrows()` here too
nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with `env.getCluster().nodeById(..)`.
Couldn't we just remove `tpr0` from the test then? It does not seem to bring much.
nit: We could use `assertNull` here and in the other tests when the future returns `null`.
It looks like we are not calling `configSynonyms()` anymore, so we can remove that private method
I wonder if we can just get a Map with the default size. I don't expect this code path to be very hot
Yea, my suggestion would be to reuse the existing constructor as the construction of the `AlterConfigsResponseData` seems non trivial for a caller to do, compared with passing a map of errors.
This data construction seems better to be put in the `AlterConfigsResponse` constructor.
If you guys are not feeling strong, then let's just do this :)
You mean `now`? :) If yes please feel free to resolve the ticket when you merge this.
+1, it could make the reading a bit easier.
Out of curiosity, why do we add two `{}` around the code logic here
nit: is there a way to verify a function is never called? Like `consumer.commit()`
Fair enough, let's keep it inside StreamThread for now. In a longer term refactoring, maybe we could have an StreamsUtil class where such static functions / fields can be stuffed in.
This could be named as `commitInternal`
Seems like we could extract the `processingMode` initialization logic to the StreamThread level to share between task creator and task manager
`Not for this PR`: this is getting really messy here... instead of interleaving the suspending and committing logic, we should just do one loop over ALL tasks gathering their offsets, and then do another loop over the tasks from `revokedPartitions` doing a suspend.
It reminds me that sometimes I did the same thing of piggy-backing re-orging in the code base with actual changes that makes the reviewers life harder :P All of us should be careful in the future. That being said, for this PR since Boyang and I have bite it already, I think we can save reverting the piggy-backed reordering and just merge as this.
With 2) under eos-beta, when committing the txn we would avoidably commit the sent records from other tasks; so I'm wondering if we should also commit them as well, otherwise some outgoing records would be included in the txn while the incoming partition offsets would not be committed.
For TaskCreator and TaskManager here's my thoughts: ideally the taskCreators could be constructed inside task-manager, but for unit testings we want to mock the creators sometimes so we create them outside of task-manager, inside stream-thread. So for this special case I think passing the ProcessMode into these two classes are okay, since for most other cases the hierarchy is natural and we would only see the construction of such parameters in a single place.
Yeah with KIP-429, during the rebalance some tasks would still be processed and hence records being sent out; so the resuming tasks could be in RUNNING state as well.
nit: we could use mkMap helper here as well.
Could you elaborate why we check commitNeeded for task00 and task01, while check for commitPrepared for task02 and task10 here? I'm needing some clarification here.
Assuming we do not need eosUpgradeModeEnabled, I'd suggest not using multiple booleans to specify a single state as it is more error prone. Instead we can just have enum if we have to.
`Not for this PR`: I think a better place for these static methods is StreamsConfig.
We should not rely on `threadProducer == null` etc conditions here any more: at the construction time of the TaskCreator, based on the config we already constructed one of the thread-producer, or the empty task producers. So here we should just: ``` if (eosAlpha) create task producer and update the producers-map else do nothing since we know the thread-producer should have been constructed. ```
Similar here, we can cache the result in case to be reused.
Same here, we can cache the result of `Builder.getPartitions(data)` for re-use.
nit: we can use `map#compute` to replace getOrDefault + put.
this offset is added deliberately so as to make task corrupted.
Should we report the lag as the whole log in this case? Even if the log is truncated it is not guaranteed to throw the invalid offset exception and hence task-corruption logic would not necessarily triggered.
Stream instance "one" -> "two"
prop: Could you explain a bit better what the warning is about? If somebody does not know the code, it is hard to understand what is going on.
@guozhangwang if the end offset is less than the checkpointed offset, how is it possible to _not_ throw a `TaskCorruptedException`? I thought that was thrown after checking this exact condition? edit: what I mean is, do we think this is a possible state? If so, we should explicitly check for it and throw `TaskCorrupted` if detected. (If not, it's an illegal state and thus the check here is appropriate)
> I think it's could easily happen that any past, present, or future versioned member reports a task sum that's beyond the end-offset sum of what the leader thinks the task contains. If the corruption comes from our code, we should throw. The leader fetches the end offset shortly before that code, so they are most probably up-to-date. If the corruption is caused by failure we should handle it gracefully.
@ableegoldman If the log is truncated and then immediately more records are appended to go beyond the original log-end-offset, in old versions we would not throw the exception. After some thoughts I think it makes sense to report the lag as the whole log.
I think parameterize this class with `eos-alpha` and `eos-beta` is a better idea --- for some tests that do not rely on the flag and hence would be a duplicated one, we can move it to a separate non-parameterized class.
It's simplified because we have a clear mapping from eosAlphaStreamsProducer to its corresponding internal producer, which in the existing code reader always has to build this mapping manually by checking the initialization code.
Not necessarily, we could pass in both processing mode and the expected output as parameters, if the test workflow looks essentially the same.
From the comparison here: https://wikidiff.com/common/generic, the opposite of `generic` is `specific`, which in this case I guess you want to express is `mutual`. So IMHO `common` or `mutual` tests are more accurate here.
could be named as `processingMode`
Sounds more like `common tests`
I feel we could actually simplify the test by calling `streamsProducer.kafkaProducer()` every time for the check for the internal producer, instead of keeping a reference here, as the `eosAlphaMockProducer` and `eosAlphaMockProducer` look quite similar.
Also could we try to parameterize this part as well? Like passing processing mode flags to each test
```suggestion public HighAvailabilityTaskAssignor(final Map<ID, ClientState> clientStates, final Set<TaskId> allTasks, final Set<TaskId> statefulTasks, final AssignmentConfigs configs) { ```
I can't understand why there's no warning about this, but it looks like clientId should always use `.equals` instead of `==`.
Here also. It looks like you used the IDE code generator to make these, but they don't seem to be correct. Perhaps there's a configuration wrong somewhere? Here's what mine produces: ```java @Override public boolean equals(final Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; final Movement<?> movement = (Movement<?>) o; return Objects.equals(task, movement.task) && Objects.equals(source, movement.source) && Objects.equals(destination, movement.destination); } ```
This is a bit suspicious... If we're polling the queue, we should just loop until the queue is empty, not iterate over another another collection we happen to know has the same number of elements. More specifically, `poll` might return `null`, but `offer` throws an NPE if `client` is `null`.
Please avoid raw types, even when you don't need the type bound. ```suggestion for (final RankedClient<ID> rankedClient : rankedClients) { ```
I'm starting to lose track of the details... What is the impact of setting these tasks' ranks as `-1` instead of `0`? If memory serves, we proposed to just treat all caught-up clients as the same for the purpose of assignments.
Oh, yeah, I'm 100% on board with you there. `// visible for testing` always reads to me like `// setting a trap for the future:`.
Q: IIUC, we do not check if the movement is for free. That is, if the destination is a caught-up client. If it were we would not need to assign a warm-up replica and could consider one more movement. I am also fine with post-poning that to a follow-up PR.
It looks good to me.
Q: I might have missed the discussion. Why does an unknown offset result in `1` and not in `Long.MAX_VALUE`? Sorry if you have already answered this question elsewhere.
sounds good. Let's put it off for now.
I see what you mean. I do not have any heart feelings here. Would be interesting to see in experiments how the two approaches differ.
That's fair. My concern about the impact was whether it results in non-termination of the probing rebalance cycle, if we always prefer to re-assign the prior active and always propose to move the task to the same caught-up standby, but never consider just giving the active to the caught-up standby, since there is a prior active.
FYI: Mine produces the same as John's
prop: Could we pass into `getMovements()` the number of warm-up replicas and only compute as many movements as needed instead of computing all movements and then using just the first couple of movements.
We don't use star imports in Java classes
Wouldn't the metadata request always happen before the failed calls are executed again? I believe the logic flow goes: 1) Metadata Request 2) ListOffsets --> 2/3 failed partitions 3) Metadeta Request 3) ListOffsets --> success You can use some inspiration from the test here: https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java#L2997-L3057 This test mocks a 3 node cluster with 2 topic partitions which initially start out to have the same partition leader and then mocks a retriable error and both partitions change to have a different leader. What I was more thinking for your patch was to recreate the same exact testing logic you have for the single node clusters for the multi-node clusters. So in a multi-node clusters with varying topic partitions, testing that the patch you have now respects the retry config and the backoff config for each call made. An example test could be you have 3 different partitions that retry different amount of times (one partition retries once, another retries twice, and yet another retries thrice), and let's say the max retries is 2, then the call associated with the partition that is trying to retry a third time should fail. Along those lines, you could also test having a multi-node cluster that tests retry-backoff in the same way.
I just reviewed your new test `testListOffsetsMaxAllowedNumRetriesSuccessWithMultipleLeaders` and it looks good! Overall happy with the test coverage introduced here.
@dajac just to clarify, are you concerning that the `generation()` may change between the check and the error-log? If yes maybe we do not need to synchronize the whole function, instead we just get a reference of the returned `generation()` call and use that in the error-log, since the generation object is immutable.
@guozhangwang Yes, keeping the reference is fine. I was concerned by the check itself because we were calling `generation()` twice in the previous implementation thus we could get two different instances. ``` generation() != Generation.NO_GENERATION && !protocolName.equals(generation().protocolName) ``` I haven't thought about the error-log but it is also a good point.
Isn't it true that most of the time this method will get only supported `Callback` implementations? In those cases, we'll never need the `unsupportedCallbacks`. I originally thought it might be worth making this more efficient, but I don't think it's worth it since the default constructor for `ArrayList` is pretty efficient in recent JVMs (with lazy allocation of the underlying array when the first element is added to the list).
Would it help to actually list the method that was used, in case somebody thought they were using basic? ```suggestion log.trace("Request credentials used {} authentication, but only {} supported; ignoring", BASIC, method); ```
Typically, most users push logs to something like elastic/splunk and should be able to lookup the logs from the archive. I'm not too concerned about this since properties file based credentials store shouldn't typically be used in a production environment. So,I'm fine leaving the log statement in here. But let's see what the committers have to say fro this :).
Should this be `warn`. Its likely an incorrect set up that can potentially cause this and might be useful to know it by default.
do we really need that we matched it? Can't we do all all of this is a single log statement? We can include size of credentials map and authenticated boolean. This will help keep the old structure.
I'm suspicious of summing the various latencies, rather than just measuring the time from the start of the method until now, since it would hide any unexpected sources of overhead.
Just to be sure it's not sliding by unnoticed, there may be some overhead in the `taskManager.process` call. When we do process some records (`process > 0`), this overhead is counted in `totalProcessLatency`, but when we didn't process records (`process == 0`), the overhead gets counted in `totalPunctuateLatency`. The "solution" would be to move `final long processLatency = advanceNowAndComputeLatency();` and `totalProcessLatency += processLatency;` to immediately after the `taskManager.process` (i.e., unconditionally account for time spent), although the `processLatencySensor` recording needs to remain conditional. Also, note there are knock-on implications to this question, since there also may be overhead to `punctuate`, and if `punctuated <= 0`, then we also don't account the time for that, and so forth with commit.
Nit: perhaps mention that the order of the listeners config is important: ```suggestion // correct listener is chosen when https listener is configured before http listener and advertised listener is http ```
We should use try-with-resource to make sure the AdminClient gets closed.
Should we actually capture `AssertionError` instead of `InterruptedException`? I think when it times out the former is thrown not the latter.
It would be sufficient. I assumed that we were checking before the lock to avoid synchronization overhead, so I left that in place.
Sounds good. I prefer it the way you suggest anyway. I'll change it.
iiuc, this is actually the bug that we are fixing here. On the current trunk, if flush or cache.close throws, we do _not_ close the wrapped store. Now, with this change, we still call flush, cache.close, and wrapped().close, regardless if some of them throw.
Yeah, something like that sounds good. Still, I'd like to select the right location after we need to use it from two or more different packages.
Do we need to validate twice before grabbing the lock? Maybe only the validation at line 283 is sufficient.
Why not make it `final` and return an empty list? It seems better to avoid using `null` if possible
Just to avoid allocating an empty list in the common case where we don't throw any exceptions.
We need to check again after we grab the lock, otherwise the store might get closed after we check but before we grab the lock. Once we get the lock, we're guaranteed that this block is serialized wrt `close()`. But we can still check beforehand to avoid grabbing the lock if it is closed.
They do the same thing, but the majority of calls are on `wrapped()`.
This check means we don't have to construct the exception message string if we're not going to throw.
We need to check again after we grab the lock, otherwise the store might get closed after we check but before we grab the lock. Once we get the lock, we're guaranteed that this block is serialized wrt close(). But we can still check beforehand to avoid grabbing the lock if it is closed.
The check itself (ie, for `isEmpty()` makes sense) but I would still try to avoid the `null`
They do the same thing, but the majority of calls are on wrapped().
We can extract a helper, but maybe we can do _that_ in a follow-up at least, since the concurrency controls needs to be standardized across all the caching stores anyway.
This is what @guozhangwang was asking for.
I just plunked this class where it was needed right now; we can move it later if we want to use it elsewhere.
The first exception will be the "cause"
The rest of the exceptions are listed as "suppressed by" the exception we're throwing.
> because it creates ambiguity AFAIK, it's not ambiguous: a later thrown exception would "overwrite" the former. But it's better to collect all exceptions anyway.
```suggestion addValueMetricToSensor( sensor, TASK_LEVEL_GROUP, streamsMetrics.taskLevelTagMap(threadId, taskId), name, PROCESS_RATIO_DESCRIPTION ); ``` req: Please use `StreamsMetricsImpl#addValueMetricToSensor()`
req: Please rename this unit test to reflect the new verifications.
prop: I find `assertThat()` better readable than `assertEquals()`.
req: Please use a constant for the number of stream threads here and on line 260.
req: The names of this method and the previous method should be switched.
Thank you for this! I forgot to remove them.
`is [not] supported`
nit: move `logContext` to its own line
Yes I'm talking about using a `mock(Producer.class)`.
looks like the test is failing at below `self.kafka.stop_node(node)` call, without `snapshot.trust.empty` config. Can we check for topic/znode existence in ZK after upgrade. We can use ZK cli/kafka-topic describe? This way we can test ZK server availability and znode data existence after upgrade.
We don't need backwards compatibility with the original signature of createTopics. It's not a public API
It's fine to keep compatibility for now. However, I do wonder if we should just throw ApiException to the caller since the producer can block for max.block.ms. This needs a KIP discussion.
I was thinking that all retriable errors should observe max.block.ms if not already. For non-retirable errors, just throw the exception immediately.
Ok, we can keep this as its.
nit: alignment looks a little off
nit: fix indention (similar below)
Got it. thanks for explanation!
We can remove the extra call in the variable here. ```suggestion CallRetryContext failedCallRetryContext = failedCall.callRetryContext(); ```
Let's rename the method to be more explicit about what we're updating here. ```suggestion public void updateCallRetryContext(CallRetryContext failedCallRetryContext) { ```
Since we have this logic of checking what the type of `t` is in multiple places, we could consolidate this logic within `call.retry()`, since we are already passing the throwable instance. We could then also rename the method to be a bit more encompassing, like `attemptToRetry` or something.
it might be a bit cleaner to invert this if statement to write it like this: ```java if (this.retryContext.tries() <= maxRetries) { log.debug("{} failed: {}. Beginning retry #{}", this, prettyPrintException(throwable), retryContext.getTries()); runnable.call(this, now); } else { failWithTimeout(now, throwable); } ```
nit: Normally for getters we have the convention of dropping the `get` from the method name.
Checkstyle failure: ``` Name 'JITTER_MAX' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName] ```
Seems as if this refactor hasn't been updated on the PR
I think we can simplify the `fail` logic even further by combining `failWithTimeout` and `fail` and converting `failWithTimeout` to a boolean that determines whether or not the call should be failed with a timeout like so: ```java final void fail(long now, Throwable throwable) { if (failWithTimeout()) { log.debug("{} timed out at {} after {} attempt(s)", this, now, this.callRetryContext.tries() + 1, new Exception(prettyPrintException(throwable))); handleFailure(new TimeoutException(this + " timed out at " + now + " after " + this.callRetryContext.tries() + " attempt(s)", throwable)); } else { handleFailure(throwable); } } private boolean failWithTimeout() { return aborted || calcTimeoutMsRemainingAsInt(now, deadlineMs) < 0 || this.callRetryContext.tries() > maxRetries; } ``` And then we can replace all occurrences of `failWithTimeout` with just `fail`.
Instead of hardcoding these numbers, can we define them explicitly to make the code easier to read? Like this for example: ```java double JITTER_MIN_VALUE = 0.8; double JITTER_MAX_VALUE = 1.2; double jitter = Math.random() * (JTTER_MAX_VALUE - JITTER_MIN_VALUE) + JITTER_MIN_VALUE; ```
Nit: Can we change the wording of this to the following ```suggestion log.debug("{} attempting protocol downgrade and retrying.", this); ```
Similar logic exists in `ClusterConnectionStates.updateReconnectBackoff`. Maybe we could extract it to a utilities class.
This seems not aligned with original logic.
We could mention the pending shutdown in error log instead.
Let's be consistent to name it throwable
prop: ```suggestion assertThat( getMovements(stateConstrainedAssignment, balancedAssignment, tasksToCaughtUpClients, maxWarmupReplicas), equalTo(expectedMovements) );
req: ```suggestion rankForTask01OnClient1, rankForTask01OnClient2, rankForTask12OnClient1, rankForTask12OnClient2 ); ```
By default, the partitions and replications are set to default value from broker's config when they are ignored. This PR sets the default value to 0 and it causes topic metadata contains zero replica and partition. ``` List<Node> replicas = new ArrayList<>(replicationFactor); for (int i = 0; i < replicationFactor; ++i) { replicas.add(brokers.get(i)); } ``` I worry that the following operations, like #describeTopics, will return incorrect information about topic.
It seems like this would have initially appeared unnecessary because we already removed it on L230 when we first put it into the dirtyTasks map. But what appears to have happened is that we actually got an exception during commit, and added some more tasks in L252-L253, which were not removed from the task map. What is the overall algorithm here? Offhand, it seems like we should only remove the task after we know it is closed, which means we should delete the `iterator.remove();` on L230 and keep the line you've added here, along with adding a similar line between L264-265 (after closeClean). I'm also wondering if we should really do closeDirty on L271, or just add it to dirtyTasks. If we keep it there, then we also need to remove it from the task map at that location.
Yes, we do.
This is common enough that there's a util for that and is used extensively: ```suggestion Utils.closeQuietly(retryWithToleranceOperator, "retry operator"); ```
```suggestion Utils.closeQuietly(retryWithToleranceOperator, "retry operator"); ```
Regardless how high performance we want this code to be, as a matter of principle we shouldn't create an exception (and pay the cost of filling its stacktrace during instantiation) if we don't need to throw it. I sketched a modification just with a +1 line, but feel free to adjust to your style of preference. ```suggestion ConnectException e = null; for (ErrorReporter reporter : reporters) { try { reporter.close(); } catch (Throwable t) { e = e == null ? new ConnectException("Failed to close all reporters") : e; e.addSuppressed(t); } } if (e != null) { throw e; } ```
Here's a nice blog re: exception costs and when they occur: https://shipilev.net/blog/2014/exceptional-performance
Is ```Exception``` in method signature necessary? the method signature of ```ErrorReporter#close``` don't include checked exception.
Could we extract the group setup logics until the `coordinator.ensureActiveGroup();`, they seem to be redundant to repeat a couple of times.
nit: after.. what? I think you can drop "in time after." Here is the assertion that is used: ``` assertThat("Condition not met within timeout " + maxWaitMs + ". " + conditionDetails, testCondition.conditionMet()); ```
Was the boundary check wrong? You changed `>=` to `>`.
Ah, I see. I was concerned that we could skip over the STARTING state, but the mock consumer won't let us. This LGTM, then.
How about: ```suggestion "Variables cannot be used in the 'plugin.path' property, since the property is " + "used by plugin scanning before the config providers that replace the " + "variables are initialized. The raw value '{}' was used for plugin scanning, as " + "opposed to the transformed value '{}', and this may cause unexpected results.", ```
I think what you just described is precisely what I would consider something worthy of logging at `WARN` level. i.e. something went wrong and it is very likely that the application will behave in unexpected ways. `ERROR` level to me should be reserved for catastrophic failures that the application can not recover from. Something that would prevent the worker to start up, in this case. But, at the end of the day, everyone has their own interpretations and there is no right or wrong answer here. Now, `WARN` level being polluted by unused configs - that is something that should be considered a bug that we should fix separately, IMO.
```suggestion log.warn( ```
How about: ```suggestion + "/opt/connectors\n" + "Do not use config provider variables in this property, since the raw path is used " + "by the worker's scanner before config providers are initialized and used to " + "replace variables."; ```
Great to see this test case !
```suggestion INVALID_RECORD(87, "This record has failed the validation on broker and hence will be rejected.", InvalidRecordException::new), ```
As before, we can use `assertThrows`.
We can use `assertThrows` here.
as before, this could be if-then-else without the increment below for the `if` case.
make it if-then-else since we dont't need the increment in the line below? Also split this line since we don't include the statement in the same line as `if`.
Do we need `String.format` here? Seems like String concat would be fine.
I think we want to make the string computation lazy, but not lose info. We could do that by overriding `toString`.
Ditto here, we can use AssertionError
nit: we can throw AssertionError here to indicate this should not happen.
nit: `changelog, repartition and output topics names` (as there are other internal topics, we might want to rephrase this more general: ``` The returned set of topic names may include user (e.g., output) and internal (e.g., changelog, repartition) topic names. ```
super nit: no `.` at the end or start sentence with `[T]he` :)
I wont block on this, but to better facilitate updates to ducktape and not worry about re-pinning on every update, should this instead be: `"ducktape>=0.7.0,<0.8.0"` - As in stay on the 0.7.x line
Not sure why it's the case? I think the previous pending txn should have aborted in step 4.
Ah, you're right.
Why the second client will have two pending transactions? Upon migrated the task the initTxn should cause the pending transaction failed.
nit: we could define this transition list in a variable to be reused.
+1 It seems we do not need the actual groupId here, just a boolean flag.
Hmm, this sounds to me that the StreamProducer's own `partitionsFor` did not return the num.partitions so we ended up calling `send` with `partition == null`, since otherwise we will get the `partition` as ``` partition = partitioner.partition(topic, key, value, partitions.size()); ``` where `partitioner` is the `StreamsPartitioner` and the producer's own partitioner should not be used.
nit: extra space after second COMMIT
Gah! You're right. We should also _remove_ the client from `uniqueClients` when we `poll`.
Ah, sorry about that @ableegoldman ; I wasn't able (or was too lazy) to follow the `git praise` trail through the class movement. Well, kudos to you, then. :)
prop: I would not add the client if it is already contained in the set.
I'd just like to say what an awesome tool for optimization this class is. Kudos to you and @cadonna .
I think it's just a computer-sciencey matter of principle. `clientsByTaskLoad` is a linear collection, so every `offer` would become `O(n)` if we did a `contains` call on it every time. Right now, it's only `O(n)` when we need to remove the prior record for the same client, and `O(log(n))` otherwise. Does it really matter? I'm not sure.
I do not remember having contributed to this awesomeness. It is all @ableegoldman 's merit.
Got it, thanks!
I guess both are acceptable solutions (ie, creating two repartition topics or throwing an exception). Your proposal is more user friendly but results in a more expensive deployment. The question might be, what do we try to optimize for? \cc @vvcephei @guozhangwang
Thanks for the discussion, all. Coming back to this proposal, and considering the points you've raised, it seems like we should re-use the generated repartition node when the name is generated, and create two repartition nodes when they are named. The purpose of re-using the repartition node in this PR isn't exactly to optimize anything, just to avoid throwing the exception that happens when we currently try to create the exact same repartition node twice. We could instead _always_ create two nodes, but this is needlessly wasteful. Reusing the same-named node makes perfect sense. When the operations are named, on the other hand, there is no problem right now, since we are creating differently named nodes. Since there's no problem, we shouldn't "solve" it ;) It's true that this isn't the most optimal physical plan, but for anyone who cares enough to look into it, they can just add the repartition node first, as you suggested @mjsax; we don't need to throw an exception to force them to fine-tune their program. The other option is that they can enable topology optimization, which will also collapse the named repartition nodes in a well-defined way. Compatibility is a concern, and it seems like it's satisfied if we follow this path: 1. You currently cannot reuse the same stream in two anonymous joins, so we can share the node without breaking any program 2. You currently _can_ reuse the same stream in two _named_ joins, and we will create two (named) repartition topics. We have no choice but to maintain this, or we will break compatibility. 3. Inserting a repartition node is well defined to break compatibility, so people will know they have to reset. 4. Adding Optimization is well defined to break compatibility, so people will know they have to reset. Have I missed some consideration? Thanks, -John
Thanks @vvcephei -- that is convincing.
My bad. My suggestion inserted a typo. At least I saw it before I start the build. ```suggestion return new HashMap<>(connectorConfigCallback.get(herderRequestTimeoutMs, TimeUnit.MILLISECONDS)); ```
```suggestion return new HashMap<>(connectorConfigCallback.get(herderRequestTimeoutMs, TimeUnit.MILLISECONDS);); ```
If we throw here, we will not execute the current `finally` block to call `release()`
If we would have written `throws Exception` from the beginning on, this change would not be necessary... (Just to back up my preferred coding stile to only use `throws Exception` in tests.)
nit: due _to_ no known leader URL ... or similar
nit: unneeded parenthesis
IMHO, we should always get the exception and assert on the error message (otherwise, it exception might be throw because of. different reason the the test does not test what it is support to test -- we have seen this issue in the past).
Do we really want to do this? I understand that and empty topology does not make sense, and it would be appropriate to log a WARN -- but do we need/want to reject it? Also, should we instead throw an `InvalidTopologyException`? Furthermore, should we add a similar check to `StreamsBuilder.builder()` to raise this error even earlier (we would still nee this check though).
Thanks for clarification.
Well, the `FallbackPriorTaskAssignor` also gets the same input but can't look at the lags. I figured if we're going to make a distinction between assignors that can check the lags and those that can't, StickyTaskAssignor should fall into the latter category.
I'm all for useful logging 
Hah, I got the last word! Just kidding, fwiw I'm not trying to block this PR on the matter so it's fine by me if you merge as-is. Only wanted to make sure we're being fair to our assignor friends :P
I buy the argument that it's weird (aka bad) to supply an input that may be invalid just because we know that particular implementation doesn't use the input. Of course, we don't actually input the lags directly; we just happen to supplement the `ClientState` with the computed lags _in some cases_, and the HATA just happens to use that. It seems only slightly less weird to _not_ invoke something because we haven't added some supplemental information that isn't even required in the first place. What if we add a marker interface (warning: terrible placeholder name coming) called `UsingClientLagsAssignor` that only HATA implements to indicate that it needs this extra information on the `ClientState` to do its job. This will also be useful if we want to eventually make the task assignor fully user customizable; I expect someone who implemented a custom assignor that does not require task lags would be surprised (and annoyed) to find out that their custom assignor was skipped because we couldn't compute superfluous client data. Or, we could just move the lag computation into the HATA. But personally I'd rather leave it in the SPA for the above reason
Can we remove the `AndHighAvailabiltiyEnabled` suffix from the test name? And/or just generally shorten it if you have a better idea
prop: ```suggestion final boolean followupRebalanceNeeded = assign( TASK_0_0, TASK_0_1, TASK_0_2, new TaskId(1, 0), new TaskId(1, 1), new TaskId(1, 2), new TaskId(2, 0), new TaskId(2, 1), new TaskId(2, 2), new TaskId(3, 0), new TaskId(3, 1), new TaskId(3, 2) ); ```
You are an exemplary boy scout!
I see your point. What I do not like so much is that it is not very intuitive to require successful lag computation for sticky assignor. I understand that if lag computation is not successful other parts of Streams will fail, but it is not the responsibility of this class to avoid that. I think what I am trying to say is that the verifications should be done where they are required to make the code easily comprehensible. I am just imagining me coming back to this code and trying to understand why the lag computation must be successful for the sticky assignor.
prop: Could we package this logic into a factory method to make the code more readable? ``` final TaskAssignor taskAssignor = createTaskAssignor(boolean lagComputationSuccessful); ```
Probably a better alternative to the marker interface is just an abstract assignor class that adds the lag info to the `ClientState` which the HATA and potentially future custom assignors could implement
Fine with me (although it does slightly detract from the opt-out possibility). WDYT about adding a retry backoff though? I'm a bit concerned we might just end up stuck in a loop of useless rebalancing, and waiting the full `probing.rebalance.interval` doesn't feel right either
Since we are only going to verify number of partitions, I think we could just set value as integer
Let's try to be consistent to use `copartition` instead of `coPartition`
How can we be sure that `partNums` is not empty? If empty, `next()` would throw.
I don't think we consider `SslFactory` a public API, so I think we are probably good here.
I've noticed there's only a single rebalance, and we need at least 3. maybe set ` streamsConfiguration.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 1000); streamsConfiguration.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 500);` or similar
eventually all 4 threads should be down
given that the previous messages say "Reading to ..." maybe it would make sense to say: ```suggestion log.trace("Read to end offset {} for {}", endOffset, topicPartition); ```
Similar to the above, seeing a message that says `read` might be easier to read in context than `consumed`. How about: `Behind end offset {} for {}; last-read offset is {}`
unboxing will happen in the comparison in the `if` branch anyways, so probably better to do it early declaring the type `long` here.
```suggestion log.trace("Behind end offset {} for {}; last-consumed offset is {}", endOffset, topicPartition, lastConsumedOffset); ``` nit: multiline calls don't need to be on their own line in AK and tab is equal to 4 spaces (here we need 2 tabs)
```suggestion if (storeSupplier.retainDuplicates() && enableCaching) { ``` Should we only log if we're changing the configured caching? (Also applies below)
not a huge deal, but technically, these should have brackets.
```suggestion .getStore(300_000L, storeName, streams, QueryableStoreTypes.keyValueStore()); ```
Note that `ConfigEntry` is considered a public API. This change is not backwards compatible, but we could consider exposing a separate method to expose the value as an optional. There is a process for changing public APIs: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals.
Sounds good. I think this is convincing :)
nit: we can put kafkaStreams in a try block.
Yeah I was thinking about debugging purposes as well. If it is more noise the usefulness, I'm fine with dropping it.
nit: ditto here, we can put `driver` in the try block. And ditto elsewhere.
req: Actually deleting topics after test is critical for some tests: I've encountered some cases where the same topics are reused mistakenly across different test cases within the single class. But I feel that it is better to put the topic deletion in the `@after` function while leaving `cleanUp()` as part of the test function itself.
This is a really nice touch  Although without attempting some degree of stickiness in the standby task assignment it seems unlikely to actually find a standby on a caught-up client..
Fair enough. Let's leave it as-is.
nit: parameters are not aligned.
Like said earlier, I think we could just return `return new StreamsResetter().run(parameters, cleanUpConfig) == 0`
I see, this is indeed weird, please file a JIRA so that we could clean in a follow-up PR if others feel the same way.
Ah, perfect! This is much simpler than what I was thinking. Thanks.
I just took another look at the definition of streamTime, and it actually looks like it might be computed wrongly. The way it works is that the "stream time" for a task is computed most of the time in `org.apache.kafka.streams.processor.internals.PartitionGroup#nextRecord`, i.e., it's the max timestamp of any record _polled from the PartitionGroup_. However, when we commit, we commit the "partition time" for each TopicPartition, which is set when we move a record into the head position for that queue. During restoration, we read these committed timestamps for each TopicPartition, and we (incorrectly) set the "stream time" to be the maximum over the "partition time" of each partition in the PartitionGroup (aka Task). This is incorrect in two ways: 1. it should be the minimum, not the maximum (since we would choose the record with the minimum timestamp to process next) 2. the timestamp of the _head enqueued_ record (partition time) is not the timestamp of the _last dequeued_ record (stream time). I'll file a Jira ticket capturing all this. In the mean time, I'd suggest that we just update the docs to reflect the correct definition of "stream time": `which is defined as the largest timestamp of any record processed by the task`. Then, we can fix the code to make this true all the time. Currently, it's only true in steady state, not immediately after restoration.
The previous approach was intended to avoid having this logic in many places.
Nit: capitalize please.
Yeah, it's a bit error prone to have that logic in every constructor. We could move the `propsToMap` method to a utility class and use it on the consumer too.
I know that this check was here in some fashion before, but I'm drawing a blank on why we need to verify this log line. It seems like _just_ checking the version number logs and nothing else would be the key to a long and happy life.
We can actually just delete these lines now.
Thanks, all. This doesn't seem like the best way to verify what we're trying to verify, but it also seems about the same as before. I'm happy to leave this here for now. If/when the test breaks again, I'd prefer for us to put in a more reliable and direct mechanism.
I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular. So, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though 
Again naming here can be misleading. `ignored` is more like unused in the try block. But also that's not the point of this idiom. It's about suppressing exceptions from `finally` instead of the originator. How about `suppressible`? Also, `unused` might be even better, because `ignored` is untrue especially if `closePartitions` is the only method that throws. But I think `suppressible` highlights the intentions here specifically.
Although I dig supressible with 1 p, like "sup? exception what are you up to?" I'm afraid I'll have to abide by the usual rules and recommend: `suppressible` here  (probably the IDE will complain about a typo too).
I don't think we should expose `Jetty` here. Yes, we're following the Jetty grammar and format for these, but let's not unnecessarily expose the internals. ```suggestion public static final String RESPONSE_HTTP_HEADERS_DOC = "Rules for REST API HTTP response headers"; ```
The advantage of using `ConfigDef.validator` on the `response.http.headers.config` config key is that this constructor call would throw an exception if any invalid value is used, and much sooner, too.
Nit: ```suggestion public void testDefaultCustomizedHttpResponseHeaders() throws IOException { ```
Nit: ```suggestion public void testValidCustomizedHttpResponseHeaders() throws IOException { ```
If using `ConfigDef.Validator`, all of these lines would go away, and we actually don't need mocks of any kind.
Should we have tests for the `DistributedConfig` class? Again, much of the logic should be the same, but the tests would each be simpler if using a `ConfigDef.Validator`.
This should be package-level protected: ```suggestion // Visible for testing static void validateHeaderConfigAction(String action) { ```
Looks good. I like the additional checking that you're doing here.
This could be package-level protected, right? The only place it should be called is in `ResponseHttpHeadersValidator` and in tests. ```suggestion // Visible for testing static void validateHttpResponseHeaderConfig(String config) { ```
I think this method should be on `ResponseHttpHeadersValidator`, not the `WorkerConfig`.
How about putting the `server.stop()` and `server = null` in a finally block? Also, `CloseableHttpResponse` is `AutoCloseable`, so we could actually use a try-with-resources here: ```suggestion server = new RestServer(workerConfig); try { server.initializeServer(); server.initializeResources(herder); HttpRequest request = new HttpGet("/connectors"); try (CloseableHttpClient httpClient = HttpClients.createMinimal()) { HttpHost httpHost = new HttpHost(server.advertisedUrl().getHost(), server.advertisedUrl().getPort()); try (CloseableHttpResponse response = httpClient.execute(httpHost, request)) { Assert.assertEquals(200, response.getStatusLine().getStatusCode()); if (!headerConfig.isEmpty()) { expectedHeaders.forEach((k, v) -> Assert.assertEquals(response.getFirstHeader(k).getValue(), v)); } else { Assert.assertNull(response.getFirstHeader("X-Frame-Options")); } } } } finally { server.stop(); server = null; } ```
This class should have a `toString()` method that describes what's required, so some string like: > Comma-separated header rules, where each header rule is of the form '[action] [header name]:[header value]' and optionally surrounded by double quotes if any part of a header rule contains a comma
Might be nice to have quite a few of these tests that verify various values are invalid and valid, to act as regression tests.
How about defining a static immutable list as a constant: ``` private static final Collection<String> HEADER_ACTIONS = Collections.unmodifiableList( Arrays.asList("set", "add", "setDate", "addDate") ); ``` so that these lines can become: ```suggestion if (!HEADER_ACTIONS.stream().anyMatch(action::equalsIgnoreCase)) { throw new ConfigException(String.format("Invalid header config action: '%s'. " + "Expected one of %s", action, HEADER_ACTIONS)); ``` This eliminates the duplication of the literal values (which is prone to future errors) and makes the code more readable.
Should probably add this as the first bit in this method: ```suggestion String strValue = (String) value; if (value == null || strValue.trim().isEmpty()) { return; } ```
Nits: ```suggestion throw new ConfigException(String.format("Invalid format of header config '%s'. " + "Expected: '[action] [header name]:[header value]'", config)); ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header name '%s'. " + "The '[header name]' cannot contain whitespace", headerName)); ```
Shouldn't this look for other whitespace characters, per the exception message? Something like: ```suggestion if (headerName.isEmpty() || headerName.matches("\\s")) { ```
Nit: ```suggestion throw new ConfigException(String.format("Invalid header config '%s'", config), e); ```
Nit: ```suggestion String.format("Invalid format of header name and header value pair '%s'. " + "Expected: '[header name]:[header value]'", header)); ```
lol, I forgot to apply my own suggestion to use 'interpreted', good catch.
I assume this was unintentional.
I feel logging all of the records even at TRACE level will be too much. For example, our system tests often have TRACE enabled. Huge single-line log messages are difficult to consume both visually and in systems like elastic.
nit: do you mind using `List<String>` and `Arrays.asList(...)`? I don't think array declaration is better if the result is not going to be used as an array. Also, won't work if you try to reinitialize a declared variable.
FWIW, I'd rather just inline it and avoid maintaining APIs that are just for tests in our production code.
Thanks for the clarification, makes sense.
Can we clarify that `resolve_canonical_bootstrap_servers_only` applies to the boostrap urls only. For advertised servers, both `use_all_dns_ips` and `resolve_canonical_bootstrap_servers` behave the same.
Each of these configs have an impact on bootstrap and advertized servers. So, we should be clear on what they do for each case.
We don't need to call `toString()`
BTW there's another report that line 159 can also fail: ``` java.lang.AssertionError: Expected: is <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:159) ``` Which is a bit mystery to me, since I cannot really think of a way how that could happen. ANyways, for now removing that listener all together seems good to me.
I am not sure if this is the right fix. Note that "restoring" and "standby task update" are two different things, and a standby task should. never use the "restore" code path what this assertion verifies. What could explain a restore is the migration of the active task from one instance to the other. However, this should actually not happen either. Could you reproduce the issue locally? We recently worked on rebalancing so maybe the issue is with regard to that.
I am not 100% sure about the root cause of the issue... If you have a good suggestion for a better error message, that would be great. I have not idea how we could improve it atm.
Why do we need this? If we don't inject an error, we start phase 6 in RUNNING state and it should not rebalance.
I see. Guess it's good as-is.
```suggestion Map<String, Object> topicSettings = config instanceof DistributedConfig ? ((DistributedConfig) config).configStorageTopicSettings() : Collections.emptyMap(); ``` I know that `TopicAdmin#defineTopic` checks for `null`, but I think using `null` with collections is better to do when such optimization matters. Wdyt? (btw you don't have to use the ternary operator, I just added it to make the suggestion clear). Also, if you change here, please change in the other files too.
Should we start connect rest server if we want this code can running successfully. but I not seeing where to start the connect rest server in MirrorMaker. In my case, there got an exception which cause for unknown host url
Actually, I'm now thinking that when we moved the `ChangelogReader` out of the stream thread, should we just consider removing the bulk loading logic for everyone.
Yup, that makes sense to me. I'm thinking about the world where standbys (and also restoring tasks) are executed on different threads. The concern about IQ are valid indeed that with a large set of un-compacted L0 files. In the even larger scope, where we would have checkpoints I'd believe that bulk-loading would not be very necessary since we would not have a huge number of records to catch up any more :)
Here are the current bulk loading configs: ``` dbOptions.setMaxBackgroundFlushes(4); columnFamilyOptions.setDisableAutoCompactions(true); columnFamilyOptions.setLevel0FileNumCompactionTrigger(1 << 30); columnFamilyOptions.setLevel0SlowdownWritesTrigger(1 << 30); columnFamilyOptions.setLevel0StopWritesTrigger(1 << 30); ``` Setting aside the problems these are causing users [even for active tasks](https://issues.apache.org/jira/browse/KAFKA-9062), they basically mean "shove everything into the lowest file level and never attempt to limit these writes". This is useful if you're just trying to shove a lot of data into a store as fast as possible but not necessary need to use it immediately after, which is (debatably) the right thing for restoring tasks but definitely not appropriate for standbys*. We will attempt to restore a batch of records once per main thread loop, which means doing a lot of other stuff in between. There's no reason not to just use normal mode writing for standbys AFAICT -- also bulk loading will make IQ on standbys pretty annoying at best. *In the larger scope, perhaps when we move standbys to a separate thread, I'd say we actually should be turning on bulk loading. BUT we need to issue a manual compaction every so often, and ideally not flush them during every commit (related to [KAFKA-9450](https://issues.apache.org/jira/browse/KAFKA-9450)
Actually even for standby tasks, it should also be beneficial to use bulk-loading right (e.g. if the standby is far behind the active and has a large amount of records)? I'm thinking that in the long run, maybe we could optionally allow restore callbacks to be triggered for standby as well: we can use some simple heuristics such that if the changelog log-end offset - standby task's store offset > certain threshold, we trigger onRestoreStart(), and then we can goes back from the "sprinting" mode to normal mode after we've been close enough to the log-end offset. At the mean time, we can maybe hack a bit so that when `segment.toggleDbForBulkLoading` we set a flag and in the other we reset the flag, then during restoreAll we check the flag to decide whether enable bulk loading for newly created segment.
```suggestion LOG.warn("listOffsets request failed.", e); ``` Thanks! (minor suggestion to make the log message more typical)
nit: ```suggestion throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + " has a topic name which matches the regex in " + SinkTask.TOPICS_REGEX_CONFIG); ```
I think we can use a utility method provided by the `ConfigDef` class here: ```suggestion List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST); if (topics.contains(dlqTopic)) { ```
I think it does do trimming for the splits, based on the regex that's used for the splitting. I tried out a few local examples and they seemed to confirm this; parsing something like `" \t foo \t, \tbar\t , b a z "` returned the list `["foo", "bar", "b a z"]` for me.
 fair enough
How about returning a Set instead of List? ``` return topics .stream() .filter(topic -> !topic.isEmpty()) .collect(Collectors.toSet()); ```
Should we log the topic name for this exception? For example, ```has a topic name (xxx) which```
It might be a little more verbose, but perhaps we could phrase it like this? ``` The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size. ```
the condition is always false if you don't add brackets. ``` throw new IllegalArgumentException("Topic pattern to subscribe to cannot be " + (pattern == null ? "null" : "empty")); ```
Should we also handle the corrupted tasks here (before this line), so that they can be already cleaned up before the next round? Or, alternatively, should we move `taskManager.handleCorruption(e.corruptedTaskWithChangelogs());` to before the attempted commit (it looks like it could be outside the try block as well).
Sounds legit. Thanks.
NVM, I realized it should never happen.
nit: to be consistent, we can just add `consumer` to `membersWithOldGeneration` and then let them to be cleared at the end.
It looks like we dont need to make a copy here anymore
> getAllTopicsInCluster() ```getAllTopicsInCluster(false)```
weird that the import became unused, even though there were no other code changes...
It doesn't look like this was related to the bulk loading, but I'm guessing it was.
Actually, I'm not sure we necessarily even _need_ to call on the `FallbackPriorTaskAssignor`, we just need to schedule the followup and remove the affected tasks from the assignment
I was tempted to say we should just return an empty assignment, which would prompt everyone to rejoin again immediately, but I think the FallbackPriorTaskAssignor is a preferable alternative. IIUC, we should be able to rely on the precondition that any previously assigned tasks we correctly initialized _before_ they were assigned initially, right? So we know they are all safe to keep working (if possible) while we wait a suitable backoff period before trying to create these topics again. I could see the idea to instead just remove any tasks we couldn't initialize instead of calling the FallbackPriorTaskAssignor, but if I'm reading this code right, we might just have failed to verify that the topics exist, not only fail to create topics we know didn't exist. So, we might actually remove tasks that were previously assigned if we do this. It's not clear which strategy is better, since it would depend on the exact nature of the failure, but maybe at a very high level, it's better to continue processing existing work and delay starting new work than potentially to start new work but delay processing existing work. Or we could try for the "best of both worlds", where we assign the union of all previously assigned tasks and any new tasks we _were_ able to set up. Finally, even if we re-assign previously assigned tasks, I'm not sure if we actually need/want to use the FallbackPriorTaskAssignor in particular. There doesn't seem to be anything wrong with just computing a new assignment for a subset of the tasks while we also schedule a re-attempt to set up the rest of the tasks after a back-off period.
> assign the union of all previously assigned tasks and any new tasks we were able to set up I was worrying about the case where some internal topics got deleted, and we would cause trouble for the previous owner of the corresponding task. But I suppose if the topic was deleted randomly in the middle of processing, the thread would die anyway, so the odds of the original owner _not_ dying on internal topic deletion is pretty low. With this strategy, we would at least contain the blast radius to just that current owner since once it dies, that task has no previous owner and would not be assigned. So I'm pretty strongly in favor of this idea. Arguably we could just incorporate this into the existing `FallbackPriorTaskAssignor` since it will just reduce to the current one in the case all topics have been validated. I'm not sure if that would be more work or less, though.
For 441 we added a `nextScheduledRebalance` field to the assignment in order to signal when a followup rebalance is needed. Can we leverage that here as well so we don't have to go through the whole ordeal of `onPartitionsLost`? Check out the call to `fetchEndOffsets`in `StreamsPartitionAssignor#populateClientStatesMap` where we schedule a followup rebalance on the leader if the `listOffsets` request fails. I think we can reuse the same logic/code path and keep track of a general flag like `adminClientRequestSuccessful` so the assignor can still finish the assignment
Good point.. what if we just call on the `FallbackPriorTaskAssignor` like we do when `listOffsets` fails, and then remove any tasks that involve internal topics we failed to create? And schedule the followup rebalance for "immediately"
For one thing, it's nice for _us_, so we can easily tell when it's been deprecated "long enough" to remove. I can recall trudging through git history in the past to figure this out. For users, maybe you don't care, but I personally find it nice when my libraries do this for me. It's just good bookkeeping, and it gives me some confidence that the maintainers are doing proper, tidy maintenance. If it provides a "third party" supporting opinion, the Scala language designers thought this was important enough to build it in as a separate field of the "deprecated" annotation: https://docs.scala-lang.org/tour/annotations.html
It's a little nice for future reference when we also say when it became deprecated, such as "since 2.6".
I think we might want to skip the re-registration higher up the call stack. In `StateManagerUtil#registerStateStores` we call `store.init` on each store which ultimately results in this `registerStore` being called
nit: we could make the warn log entry more clear that we did not override the registered the store, e.g. "Skipped registering state store {} since it has already existed in the state manager, ..."
+1, we can rely on `storeManager#getStore` inside `StateManagerUtil` to check if the store is already registered.
Re: your concern, I don't think we can assume that a user's state store's `init` method is idempotent. AFAIK nothing should change that's relevant to the state store registration, but if something does (eg TaskCorrupted) we'd have to wipe out everything and start it all again anyways
Nah, I think we should actually keep this (although `IllegalStateException` seems to make more sense, can we change it?) -- we should just make sure we don't reach it
If we expect no warmups, we can assert it here with: ```suggestion assertValidAssignment(0, allTaskIds, emptySet(), clientStates, new StringBuilder()); ```
Huh, good catch!
Might it be simpler to just do: ``` topicCounterMap.put(topic, prevPartition -1); ```
@xiaodongdu, I was not suggesting getting rid of the field. It's fine to have a new field, but we should call the field `kafkaClusterId` rather than `clusterId` since the latter could be misinterpreted to mean the _Connect_ cluster ID.
The Kafka cluster ID is passed into the constructor, but is this supposed to represent the Connect cluster ID or the Kafka cluster ID? Since this is in Connect code, without a context we'd assume it was the Connect cluster ID.
To be clear, according to `StreamsConfig`, we do NOT allow `max.warmup.replicas = 0`. It must at least be 1. Or was your statement hypothetical, that it would be OK to allow it? Anyway, I am in favour of keeping the `> 0` check here.
prop: Could you add a method `taskIsNotCaughtUpOnClientAndCaughtUpClientsExist()`? Applying De Morgan's law every time I read this code gives me headache.
Not sure if it really matters, but this is not a uniform distribution (because MAX_VALUE and MIN_VALUE are not integer multiples of 1000 days. If you wanted a uniform distribution, it looks like you can use the bounded `nextInt` and cast to `long`. Also, FYI, `Math.abs(Long.MIN_VALUE) == Long.MIN_VALUE` (which is a negative number), due to overflow.
Should we wait until all brokers and Connect workers are available, via something like: ``` connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, "Brokers did not start in time."); connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, "Worker did not start in time."); ```
This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests. We could instead wait until the connector is actually running, using something like: ``` connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS, "Connector tasks did not start in time."); ```
Hmmm... Good point. Let leave it as-is. It's also covered in integration tests that the right store is returned.
Can we split this as follows: ``` final StoreQueryParameters parameters = (StoreQueryParameters.fromNameAndType(keyValueStore, QueryableStoreTypes.keyValueStore()).withPartition(numStateStorePartitions + 1); final InvalidStateStoreException exception = asserThrows( InvalidStateStoreException.class, () -> storeProvider.getStore(parameters) ); assertThat(exception.message(), equalTo("...")); ``` And remove the `(excpected = ...)` annotation. (1) We should always limit the code that might throw the exception (eg, if `withPartition` would throw an `InvalidStateStoreException` the test should fail, but would pass in it's current setup) (2) We should always verify the exception cause -- `getStore()` could throw an `InvalidStateStoreException` or multiple reasons and we should make sure it's throwing for the reason under test. Same below for the windowed case
Could use Collections.emptySet() if reduced to Set
Use 4 space format to align with other tests.
nit: if it will return `topicDescriptionSuccessFuture`, then we should not use `leaderNotAvailableTopic`
Could be simplified as `topicManager.makeReady(Collections.singletonMap(leaderNotAvailableTopic, internalTopicConfig));`
This test seems to be overlapping with `shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound`. I don't think we need both to return `LeaderNotAvailable` unless they are evaluating different scenarios.
nit: we could set a final int for numRetries as: ``` put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), numRetries); ``` and use (numRetries + 1) here to clearly indicate we are trying to go beyond the retry limit.
exception not used.
Testing against log message is error-prone and hard to maintain, I think just making sure the thrown exception type is expected should be sufficient.
I don't think this line was actually changed other than formatting. Please remove it to avoid changing lines we don't have to.
Let's not create the `InternalSinkRecord` until *after* the transformation chain has been applied. That way we're not affected by any SMT that creates a new `SinkRecord` via a constructor (where we'd lose our `InternalSinkRecord`) rather than `newRecord(...)` (where we'd keep the `InternalSinkRecord`). ```suggestion ```
```suggestion waitForCondition( this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each." ); ```
do we need this invalid config step here
extract to variable
would be nice if this was a method in the config
This line would not need to be affected. ```suggestion recordActiveTopic(sinkRecord.topic()); ```
Nit formatting: ```suggestion WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter( id, sinkConfig, connectorClass, keyConverter, valueConverter, headerConverter); ```
Nit formatting: ```suggestion return WorkerErrantRecordReporter.createAndSetup(adminProps, producerProps, connConfig, keyConverter, valueConverter, headerConverter); ```
This line would change to: ```suggestion // Apply the transformations SinkRecord transformedRecord = transformationChain.apply(sinkRecord); if (transformedRecord == null) { // The record is being dropped return null; } // Error reporting will need to correlate each sink record with the original consumer record return new InternalSinkRecord(msg, transformedRecord); ```
Nit: let's avoid adding new lines in code otherwise unaffected in the PR.
Does this test ever encounter this exception? I don't think we will be able to backport this test to < 2.6 because the method won't exist at all, much less generate the exception that is being caught here. If anything, this generates a less informative NPE later in `put`, and hides the actual root cause.
At a higher level, why are we not reusing the `RetryWithToleranceOperator` here? I thought that was kind of the intent of the KIP, that this new `report(...)` method is just more way to capture problematic records using the existing DLQ functionality. I understand that might require other refactoring of that class (like returning a Future from the produce-like methods), but it seems like it would simplify things substantially by avoiding having to create our own producer and reuse a lot more of the functionality, such as metrics, retry count, logging, using the same producers, etc.
We don't need to make this change, do we? Let's try to minimize the changes to the existing code.
Once again, please add trace log messages before an after this line.
Hmm, let's just have this delegate to the super method. It's internal, so we need not include the original record details. ```suggestion return super.toString(); ```
Let's make this protected. ```suggestion protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, ```
These can be final.
Nit: new line is unnecessary, and there's a misspelling: ```suggestion log.error("Encountered an error while awaiting an errant record future's completion."); ```
nit: indentation is a bit off here.
these overrides don't seem to add much.
nit: I think we always keep `=` with the left operand.
Also, topic can never be `null` if it's coming from a parsed config value that doesn't have `null` as its default value. (another way to think of that is that you can't pass a `null` value from properties)
let's add `protected` here to be symmetric to what other fields that are accessed by the context have as scope
The `SinkConnectorConfig` class isn't part of the public API; we can modify it without a KIP if we want.
nit: it was correct before
```suggestion protected final List<Future<Void>> futures; ```
Suggestion (can't add because of the deleted line): ``` List<Future<RecordMetadata>> futures = reporters.stream() .map(r -> r.report(this)) .filter(Future::isDone) .collect(Collectors.toCollection(LinkedList::new)); ```
nit: initialization is not required
```suggestion return futures.stream().allMatch(Future::isDone); ```
nit: extra blank line ```suggestion ```
IIUC, spotbugs complained if these were not here.
never mind then. I'll leave this to AI.
Is that for tests? Anyway, we can revisit in a cleanup in the future.
nit: it was correct before
I think we don't really need to make this change anymore, since it's only refactoring the existing code that we don't need to actually change anymore. (This PR is no longer using this logic in multiple places.)
We should use the length of the key and value in the record: ```suggestion int keyLength = key != null ? key.length : -1; int valLength = value != null ? value.length : -1; consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(), record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength, valLength, key, value, headers); ```
It'd be better to not change these lines, because we don't intend to change the logic -- yet doing so adds risk and increases the size of this PR.
Since we often have just one reporter, it is probably worth avoiding the unnecessary allocations: ```suggestion if (reporters.size() == 1) { return reporters.get(0).report(this); } List<Future<RecordMetadata>> futures = new LinkedList<>(); for (ErrorReporter reporter: reporters) { Future<RecordMetadata> future = reporter.report(this, callback); if (!future.isDone()) { futures.add(future); } } if (futures.isEmpty()) { return CompletableFuture.completedFuture(null); } return new ErrantRecordFuture(futures); ``` And since we don't know how many futures we'll add to the list (and it will likely be just zero if the DLQ is not configured or just one for the DLQ), let's use a `LinkedList` instead to avoid excessive allocation when adding the first element to the `ArrayList`.
Why use a function here? We can use a simple variable here. (I suggested a function offline to avoid having to pass in the converters. But passing in the converters into this class encapsulates this logic nicely.)
Let's avoid unnecessary blank lines. ```suggestion ```
Rather than have a list of futures, why not have a single `Future` delegate that is either a `CompletableFuture.allOf(...)` or a single feature? This makes the constructor a little more complex, but it would simplify all of the other methods tremendously since they merely have to delegate (except for `cancel()` and `isCancelled()`, which can stay the same: ```suggestion public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) { if (producerFutures == null || producerFutures.isEmpty()) { future = CompletableFuture.completedFuture(null); } else { futures = CompletableFutures.allOf(producerFutures); } } ``` This will make `get(long, TimeUnit)` behave more correctly by requiring that all futures complete within the stated time.
We need to override the `newRecord(...)` that has all the parameters: ```suggestion public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) { this(originalRecord, record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(), record.timestampType(), record.headers()); } public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic, int partition, Schema keySchema, Object key, Schema valueSchema, Object value, long kafkaOffset, Long timestamp, TimestampType timestampType, Iterable<Header> headers ) { super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers); this.originalRecord = originalRecord; } @Override public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value, Long timestamp, Iterable<Header> headers) { return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key, valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers()); } ```
How about clarifying this a bit: ```suggestion // Generate a new consumer record from the modified sink record. We prefer // to send the original consumer record (pre-transformed) to the DLQ, // but in this case we don't have one and send the potentially transformed // record instead String topic = record.topic(); ```
Let's add a trace log message before and after this call.
Nit: let's remove this blank line, since it's unrelated to other changes.
Nit: let's remove this blank line, since it's unrelated to other changes.
Should we trim this? ```suggestion this.dlqTopicName = connConfig.dlqTopicName().trim(); ```
This can be package protected and final: ```suggestion final LinkedList<Future<Void>> futures; ```
```suggestion byte[] value = valueConverter.fromConnectData(topic, record.valueSchema(), record.value()); ```
```suggestion // Most of the records will be an internal sink record, but the task could potentially // report modified or new records, so handle both cases if (record instanceof InternalSinkRecord) { ```
Let's rename this to `awaitAllFutures()` since this really is not a getter method.
Let's use the queue-style access, since it saves us from having to clear the list and would work if we need it to be concurrent. ```suggestion Future<?> future = null; while ((future = futures.poll()) != null) { try { future.get(); } catch (InterruptedException | ExecutionException e) { log.error("Encountered an error while calling "); throw new ConnectException(e); } } ```
Nit: let's remove this blank line, since there already are quite a few.
We should also send a note to the VOTE thread on the mailing list to mention that the KIP was updated.
Since this is a public API, we should include this change in the KIP.
It's useful if the simulation test is deterministic. That way failures are easy to reproduce. Perhaps we can use a shared `Random` instance (between this class and the coordinator) with a defined seed.
nit: maybe `disconnect` is a better name given actual behavior.
Wondering if it would be more useful if we can control the faults more explicitly. For example, we could add a hook to make the coordinator temporarily unavailable and to restore it later.
To make this really interesting, we would need to add some sequence number bookkeeping. Really its the sequence/epoch bookkeeping which makes the implementation so complex.
Similar to the offset commit path, it would be useful to validate here that each partition that was written to was first added to the transaction properly.
Another class of failure that we can simulate is when a request reaches the broker and gets handled, but the connection is lost before the response is sent.
nit: use `logContext
nit: make `Metrics` a field
nope, that's not possible
Would it be better using a separate topic in order to keep a partition without any records? By changing this topic it affects existing checks in all tests
We can use `Collections.singletonMap()` here
This test is overly complicated. I think it could: - Create a topic - Produce messages to all partitions but one - Consume all messages - Start a single MirrorMaker2 instance primary->backup - Use `RemoteClusterUtils.translateOffsets()` to retrieve offsets - Assert offset for the last partition is 0 For example, something along these lines (this cuts a few corners so you'd need to improve it) ```suggestion @Test public void testReplicationWithEmptyPartition() throws Exception { String consumerGroupName = "consumer-group-testReplicationWithEmptyPartition"; Map<String, Object> consumerProps = new HashMap<String, Object>() {{ put("group.id", consumerGroupName); put("auto.offset.reset", "earliest"); }}; String topic = "test-topic-empty"; primary.kafka().createTopic(topic, NUM_PARTITIONS); mm2Config = new MirrorMakerConfig(mm2Props); // produce to all test-topic-empty's partitions *but the last one*, on the primary cluster produceMessages(primary, topic, "message-1-", NUM_PARTITIONS - 1); // Consume, from the primary cluster, before starting the connectors so we don't need to wait for discovery Consumer<byte[], byte[]> consumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic); consumeAllMessages(consumer, NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1)); consumer.close(); waitUntilMirrorMakerIsRunning(backup, mm2Config, "primary", "backup"); Map<TopicPartition, OffsetAndMetadata> backupOffsets = RemoteClusterUtils.translateOffsets( mm2Config.clientConfig("backup").adminConfig(), "primary", consumerGroupName, Duration.ofMillis(CHECKPOINT_DURATION_MS)); OffsetAndMetadata oam = backupOffsets.get(new TopicPartition("primary." + topic, NUM_PARTITIONS - 1)); assertNotNull(oam); assertEquals(0, oam.offset()); } ```
this overwrite of mm2config should go in the setup method, IMHO
Do we still need these 2 blocks? In `setup()` we already consumed all messages
As this does not change, I wonder if we could direct initialize `consumerProps` when it's declared
Yes, does not hurt to leave it. Just for sure.
The admin configs is built in ```KafkaStreams``` construction. Could we reuse it? ```scala adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId))); ```
The previous code handles overflow.
I think the original code was unintentional. We changed it in #8702 to the following: ```scala int joinGroupTimeoutMs = Math.max(client.defaultRequestTimeoutMs(), rebalanceConfig.rebalanceTimeoutMs + JOIN_GROUP_TIMEOUT_LAPSE); ```
We should add a similar check for `TransformerSuppliers` -- This check must be done in the DSL code though, as `Transformers` are a higher level concept. I checked the code and all passed transformers through https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java#L1251 -- so we can add the check there.
```suggestion * @param transformerSupplier an instance of {@link TransformerSupplier} that generates a newly constructed {@link Transformer} ```
```suggestion * @param transformerSupplier an instance of {@link TransformerSupplier} that generates a newly constructed {@link Transformer} ```
Can we add this to all other methods, too? (ie, all that take a supplier, Processor or Transformer?) And maybe also to `ProcessorSupplier#get()` and `TransformerSupplier#get()` -- I think it would be best to provide redundancy :)
nit: blank line missing here
nit: I'm sure these fit in a line shorter than the one below
this method won't work with empty `prefix` but that's not obvious just by reading its name.
let's add `ConfigDef.NO_DEFAULT_VALUE` in one of them
nit: fits in one line
nit: fits in one line
nit: extra blank line
There are already static imports for some Assert methods, so no reason not to make assertNull and assertNotNull static imports too. And I don't think the `asList()/forEach()` are really providing much benefit here. It would be clearer if you factored out a method: ```java assertNotNull(def.configKeys().get(prefix + HasDuplicateConfigTransformation.MUST_EXIST_KEY)); assertNull(def.configKeys().get(HasDuplicateConfigTransformation.MUST_EXIST_KEY)); assertImplicitConfigDef(def, prefix, PredicatedTransformation.PREDICATE_CONFIG, ConfigDef.Type.STRING); assertImplicitConfigDef(def, prefix, PredicatedTransformation.NEGATE_CONFIG, ConfigDef.Type.BOOLEAN); } private void assertImplicitConfigDef(ConfigDef def, String prefix, String keyName, ConfigDef.Type expectedType) { assertNull(def.configKeys().get(keyName)); ConfigDef.ConfigKey configKey = def.configKeys().get(prefix + keyName); assertNotNull("Implicit '" + prefix + keyName + "' config must be present", configKey); assertEquals("Implicit '" + prefix + keyName + "' config should be a " + expectedType, expectedType, configKey.type); } ```
I think we should assert that the "predicate" and "negate" config were the implicit ones and not the ones defined by `HasDuplicateConfigTransformation`. I guess we could do this by having `HasDuplicateConfigTransformation`'s configs use different types ("negate is `BOOLEAN` in both) and then asserting the expected types.
DeSerializers -> Deserializers
It seems like we are just verifying the headers are deserialized. Should we just use mock deserializers? Moreover, we could build a customized `boolean` deserializer which outputs true when the header is provided, or false otherwise.
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
It'd be better to avoid reformatting code to change indentation settings. AFAIK, in Streams at least, we tend to use indents of 4 spaces.
bump (it looks like this didn't get changed)
It might be nice to have a sanity check here that `offset` is non-negative, since that would indicate we've unexpectedly received a sentinel value. I thought we did that already, but it's obviously not here.
Aha! Thanks. Yeah, I'd be in favor of coding defensively here as well.
Why we need to call `streams.close()` inside the function given they are always called in `tearDown`? Ditto below.
Should we call close in the `finally` block? Here and elsewhere
Well, won't we end up deleting the topics before closing it if we never reach the first `streams.close` ? Or does it not really matter in that case since something has already gone wrong (just curious, I'm fine with it as-is btw)
Sounds good, though it'd be better to have a single (long) log message to prevent them from being separated by other log messages from other threads.
Why set this? zero is the default anyway
nit: simplify -> `throws Exception` (also above)
In general, we should rely on default IMHO. And as you set `ack=all` it seems you care about reliable data delivery, so setting `retries=0` seems to contradict it somewhat.
Cool, I was kind of hoping you would put this in a separate integration test class
I actually had a similar thought, but I am torn though. Using two variables required to keep them "in sync" was is not great. However, using `null` is less explicit... Thus overall I am fine either way as both seems to provide the overall same good/bad ratio.
Do we need two parameters here? Could we still just use one `Map<TopicPartition, Long> checkpoint`, and then in `writeCheckpointIfNeed` if it is not null write it and then set it to null.
That should work.
A `if (clean)` should be sufficient now
Actually I'm not too concerned of relying on `null` to indicate no need to checkpoint, what I originally pointed out is that we are unnecessarily accumulating and then distributing checkpoints between task manager and task, which is already resolved in your PR. So I'd suggest we still just use a single nullable `Map`.
Should require non-null for `fetchPosition`
After a second thought, I don't feel strong about it.
Should we use the private static constructor in this class? Ditto below.
consumer is unused.
No kidding... I assumed it was possible to create topics without cleanup policies but it looks like you're right. My bad!
same question around log level as above
I think my concern was invalid to begin with but your refactoring is certainly an improvement. LGTM 
I wonder if distinguishing like that, with `null` and `empty` pays off. Why not return an empty collection in both cases and simplify the checks on the return values of this method? This method doesn't seem to be the one to use when somebody wants to determine whether a topic exists or not.
nit: don't we need a space between the varargs type and the variable name? I'm surprised mainly at checkstyle here.
same question around log level as above
instead, you can just check: ``` if (retryWithToleranceOperator.failed()) { throw retryWithToleranceOperator.error(); } ``` because we are already storing the error in the processing context. you can expose that through the operator.
calc -> calculate
Are you sure? I don't see this struct being used for read anywhere.
We could just use int.
nit: we could require non-negative value for `numOfRepartitions`
`...build a graph to calculate partition number of repartition topic, and numOfRepartitions of underlying TopicsInfo is used for memoization.`
package level access should be fine.
Like we discussed offline, the downstream is actually not necessary.
is a topic node
`with TopicNode and TopicsInfo` looks weird to phrase like this when we are defining `TopicNode`, maybe something like `with topic information associated with each node`.
Could replace with addAll: `allRepartitionSourceTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());`
`a graph containing`, and correct space between `1. Build`
do a link: `method setRepartitionTopicMetadataNumberOfPartitions` -> `{@link #setRepartitionTopicMetadataNumberOfPartitions}`
I haven't seen this struct being used.
`calcRepartitionNumForTopic` and `calcRepartitionNumForTopicInfo` are pretty hard to differentiate, better to name it more distinct.
Does `TopicsInfo` work for a map key? Looking at its override equals, it doesn't seem to check all fields for equality.
we could use `computeIfAbsent`
Right, by "check for RESTORING" I meant "throw an exception if state is restoring". It seems odd to check for RESTORING during `suspend` but not in any other StandbyTask method. Either it can never be in RESTORING and we are completely sure of that, and shouldn't check for RESTORING, or we should always check whether it's RESTORING and not just during `suspend` (eg also in `postCommit`)
Just to clarify, I would support doing the former, ie don't check whether it's RESTORING here at all. But we should at least be consistent
I just mean, why not inline that? I'm just imagining coming back to this code in a few months and wondering why we need to suspend a task before recycling, or why we call `prepareCommit` but don't then actually commit, etc
Nevermind, I see that's the pattern we follow everywhere else
Why not just make `suspend` a no-op if the task is RESTORING? That seems more in line with how we handle things elsewhere
Oh duh, I thought this was StreamTask. In that case, why would we check for RESTORING at all? We don't check for RESTORING state anywhere else in StandbyTask AFAICT (maybe Guozhang thought this was StreamTask like I did?  )
Seems like we could use a separate illegal state exception here for `RESTORING` as we should never hit it.
a standby task is never in
nit: we can throw illegal-state if the state() == RESTORING since it should never happen.
Ditto for line 195: we do not need to update the snapshot since we are closing the task already.
Similar here, maybe we could leverage `transitionTo` to help throw the exception.
Can we still specify that we failed during the JoinGroup? eg `Rebalance failed on JoinGroup with {}` or something
I assume you mean the JoinGroup response handler 
Different question: can we report the exception as the "cause", rather than just getting the toString of it? ```suggestion log.info("Rebalance failed.", exception); ```
There might be slight performance gain if we just say "Attempt to join group and receive member id required error." instead of passing in the error.
We should still handle fatal exception IMHO, such as FencedInstanceIdException
I can see it either way. It seems like this PR is about sending the heartbeats _optimistically_ during rebalance, so there doesn't seem to really be any harm in ignoring the response for now. If we ignore the errors, then everything should still work, as the JoinGroup or SyncGroup response will tell us that we've been fenced next time we poll. It seems like the advantage of handling the error here is that we can potentially rejoin just a tiny bit sooner by not having to wait for the JoinGroup or SyncGroup response. But it's not clear to me that it's actually ok not to handle those responses, so then we would also need to make sure the response handling logic can detect that the response has already been invalidated if we've sent a new JoinGroup request in the mean time. This definitely has the potential to decrease the MTTR, but I'm wondering if we should take on the complexity right now, or consider it as a follow-on optimization.
Should this be inside `isTraceEnabled()` to avoid computing `remainingMs()` in the case that trace logging isn't on? ```suggestion if (log.isTraceEnabled()) { log.trace("Sending heartbeat request with {}ms remaining on timer", heartbeatTimer.remainingMs()); } ``` (also below)
Should we also reset the generation here? With the new condition above, we may now enter this block if generation is _not_ `NO_GENERATION`. I'm not sure if we want to have the generation set to some value but state set to `UNJOINED` and `rejoinNeeded := true`.
Sure, but that led to the opposite problem, in which the enum was inconsistent with the state. In regard to position, I think we should handle this at transition time as mentioned below. If we ensure that position is not null in the fetching and validating states, then I don't see a problem changing `hasPosition` to check it directly.
I think the invariant that we try to maintain is that we should have a position if we are in the FETCHING state. I'd suggest we detect this in `transitionState` and raise the exception at that point. Otherwise, we could reach an illegal state and the consumer would just stop fetching the partition. Failing fast is probably preferable. What I have in mind is just something like this: ```java private void transitionState(FetchState newState, Runnable runIfTransitioned) { FetchState nextState = this.fetchState.transitionTo(newState); if (nextState.equals(newState)) { if (position == null && (nextState == FETCHING || nextState == AWAIT_VALIDATION)) throw new IllegalStateException(); this.fetchState = nextState; runIfTransitioned.run(); } } ```
Would it make sense to move `hasPosition` to `TopicPartitionState`? Then we could just turn this into a null check on `position`.
nit: ```newPosition``` can be created lazy.
Since the usage is a bit different, maybe we could change the name to `requiresPosition`. Then this check seems a little more intuitive: ```java if (this.position == null && nextState.requiresPosition()) { throw new IllegalStateException("Transitioned subscription state to " + nextState + ", but position is null"); } ```
Would it make sense to set `position` explicitly to null if the `FetchState` does not expect to have it. For example, it seems currently when we reset the offset, we leave `position` at whatever value it had previously. If we were initializing, then it would be null. If we had an offset out of range, it would be non-null. It might be easier to reason about the logic if it is always null in the AWAIT_RESET state.
Sounds fine to me.
nit: maybe we could check for null first so that we avoid the nesting below (and reduce the diff) ```java if (position == null) throw new IllegalStateException("Missing position for fetchable partition " + completedFetch.partition); if (completedFetch.nextFetchOffset == position.offset) { ... ```
nit: fix doc
Since we have the check for `hasValidPosition` at the start of this method, we _could_ raise an exception. However, in the success case, we currently just ignore the response if the position is null. I'm ok with either option.
Does the deletion logic affect the success of each test? If not, I would prefer to keep it to simplify the broker side log for each subsequent tests, such that we don't have a lot of unused topics.
nit: use `IntegrationTestUtils#safeUniqueTestName` for the name
This is going to be modified and accessed on potentially different threads, right? If so, we should add the `volatile` modifier here.
This is going to be modified and accessed on potentially different threads, right? If so, we should add the `volatile` modifier here.
Why is the order of these methods different than in `ConnectorStatusListener`? Also, the `TaskStatusListener` methods always forward the method to the delegate _last_, whereas the methods of the `ConnectorStatusListener` use a mixture. Let's make them consistent.
Nit: the methods of the `ConnectorStatusListener` and `TaskStatusListener` classes are in very different orders. It would help readability to have them in the same order. IMO, the order of the `TaskStatusListener` methods is nice because it follows the lifecycle.
Would be better to describe what `cases` are referring to.
nit: I think we could just use different error code here, instead of building the entire data in branches. Also `data` should be final.
Since `RETRY_BACKOFF_JITTER` and `RETRY_BACKOFF_EXP_BASE` will be the same for all the clients, should we move these two defaults in `CommonClientConfigs.java` instead? That way we can access this across the `Consumer`, `Producer`, and `KafkaAdminClient`.
nit: just simplify to `throws Exception`
@mjsax Got it. Thanks for your response!
Maybe just deprecate public method instead of removing it
nit: topicMetadata.fetchesRemainingUntilVisible = Math.max(0, topicMetadata.fetchesRemainingUntilVisible -1);
nit: I think that we can use `toConfigObject(topicMetadata.configs)` here.
If we hit an exception in `handleRevocation` why would we continue here? Are we still in a "clean enough" state to actually continue? Below we call `completeTaskCloseClean(task)` what seem incorrect for this case as it might close clean task even if we did not successfully commit before.
You're right :)
> Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right Yes. > I think we can skip the call if consumedOffsetsAndMetadataPerTask is empty. For non-eos, yes, because for non-eos `commitOffsetsOrTransaction()` would only commit offsets via the consumer (this can be skipped if empty). However, for eos (alpha and beta), we might have a pending transaction that we need to commit on the producer, too.
Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right? I think we can skip the call if `consumedOffsetsAndMetadataPerTask` is empty.
Sounds good, in that case the nested try-catch would be necessary.
`postCommit` only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in `close()` for the eos-case (if just blindly for all cases as we do atm).
Yeah I think if the actual `consumer.commit` call failed, then we should not trigger postCommit for any one. As for `postCommit`, I think it should never fail (we swallow the IO exception happened, because for non-EOS it is just fine, for EOS we would bootstrap from scratch).
Is there a different PR removing that? I still see ``` if (clean) { // since there's no written offsets we can checkpoint with empty map, // and the state current offset would be used to checkpoint stateMgr.checkpoint(Collections.emptyMap()); offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets()); } ``` in Standby.close() in trunk.
nit: feels like overkill to deprecate test cases. Since they reference RETRIES directly, I don't think we need to worry about them not getting removed
I think the deprecation warning was part of a scheme to mark which tests should be rewritten: https://github.com/apache/kafka/pull/8864/files#r439685696
nit: indentation is off here. You may break at the second `new` and have the argument in one line probably
let's stream the topics and use a collector in the end.
Remove the debug statements
I think `checkpointed` is just misleadingly named -- it's really more like checkpoint-_able_ offsets IIUC
I have always been, and continue to be, mystified about why we're 100% a-ok with having corrupted state stores in ALOS mode. I understand that there are some kinds of corruption that are unavoidable without some extra work in the stores, but this check is just explicitly ignoring detectable corruption, which seems a bit extreme. Can we remove this conditional? The rest of these checks seem to apply equally well under all execution semantic modes.
I'm fine as well, will make a reference to 10055 of this PR
Oh yea! @mjsax
It seems not possible in Java
could be replaced by lambda.
Ok, it's your call. I think this might make the tests flaky, but I guess we can figure that out later.
Why remove this? Do we need to instantiate this class now? (I only see static members still).
these are really strange side-effects to have in an assert statement. I see what you are trying to do, but this is probably not the way to do it.
fwiw this doesn't adhere to kafka style guide (looks like Kafka Streams to me)
Ah got it, I'm still think about it as the string template and was overlooking that. SG.
nit: should we have a newline for each partition? Otherwise that ling maybe too long.
This might not be safe. If we use the "zero-copy" flag as suggested below, we can just duplicate the ByteBuffer instead.
Should we fix this in older branches (2.5/2.4), too? (ie follow up PR)
Not 100% sure how `OnSubsequentCall` is meant either. But what you say seems to make sense and thus it should be different test. Thanks for the extra mile splitting them up!
I think this gets called by the consumer thread, which is different from the thread which calls `close()`. I think that it may be necessary to mark this variable as volatile.
Shouldn't this be volatile? Yes, it's true that `WorkerSinkTask.close()` is always and only called from within the `WorkerTask.doRun()` after the tasks determines it will stop. However, the `onPartitionsRevoked(...)` method is called from the consumer thread, and making the field volatile is the only way to ensure that the consumer thread reads a non-cached value.
I wasn't aware that the onPartitionsRevoked was called by close on the same thread, good to know for the future. I'm fine with this as-is.
Could it be replaced by ```isStopping()```? It seems to me both flags are similar and we don't need to add more duplicate.
Yeah, I think the log messages other than this one are fine as DEBUG.
nit: since this is indicating entry into a method, I think it can be trace level.
I'm not convinced that changing this to DEBUG is worth it. The number of source records output in this log message can be instrumental in some cases. For example, if the producer does not keep up with the source task (for whatever reason), this currently INFO-level message appears shortly before the following ERROR-level message: ``` ... ERROR WorkerSourceTask{id=...} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter) ``` The number of outstanding source records reported by this line is an important factor in determining how to tune the producer and `offset.flush.timeout.ms` value.
I think you missed this one.
nit: since this is mostly indicating exit from the method, I think it can be trace level too.
Could just use `false`
could use `assertFalse`
unnecessary whitespace change
something like `inferredInternalTopics.containsAll(specifiedInternalTopics)` might be easier to understand here
nit: we should also filter for internal topics specifically, like what's done in `StreamsResetter#matchesInternalTopicFormat`. Actually you can probably just invoke that method directly for the filter here (it can be made static if necessary)
Hah, maybe I should have asked why are all of those tests not also in `AbstractResetIntegrationTest`. Seems like almost everything that applies here would also be good to test in the SSL version of the test (which AFAICT is the only other one to extend the AbstractResetIntegrationTest). But I'm ok with leaving it as is, and maybe we can just look into this as followup work unless there is a good reason for them to be where they are (which I can't think of but my imagination is not endless)
line is too long
We could refactor out a helper function here.
I really like this class.
Interesting observation. Of course, hitting the leader with a request doesn't tell you that other workers have started, so that's applicable in tests like this one, which start only one worker here, etc. This doesn't seem to be a race condition we encounter often, so I'm fine with an ad hoc specific fix here given the reduced timeout. I'd be surprised if it took noticeable time to load the services after the herder is submitted to its executor.
Should we change the assertions then? I'd assume this will be useful to other tests as well.
This gets initialized during the rebalance and IQ isn't available until Streams has reached RUNNING.
I looked through the StreamsMetadataState and it does seem like it could technically be null if this instance was never assigned any active or standby tasks at all, ever. That really _shouldn't_ happen, but of course it can if you massively over-provisioned your app and we shouldn't throw an NPE over that. Seems like this is actually an existing bug that we should fix. Then we can improve the initialization check on the side
Hmm, why do we still keep it? Based on the reviews for previous version, I believe that there is some strict ordering for getting `localMetadata` initialized to be non-null on L352 first before hitting this logic, but still a null check sound more resilient to me, unless we want to have a NullPointerException to be thrown explicitly.
We need to actually initialize this now, although we can just initialize the value to null. ie ``` private AtomicReference<StreamsMetadata> localMetadata = new AtomicReference<>(null); ```
This could be final
Very good catch. Thanks, @abbccdda .
Yeah good catch, see above
I did a simple experiment with one broker and one consumer. After the consumer joined the group, I did a kill -9 to stop the broker. I had the consumer log the poll timeout that was used in `pollForFetches`. What I saw is this: ``` [2020-07-07 08:53:10,825] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,825] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,825] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,825] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,839] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,839] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,839] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) [2020-07-07 08:53:10,839] INFO [Consumer clientId=consumer-foo-1, groupId=foo] Polling for fetches with timeout 0 (org.apache.kafka.clients.consumer.KafkaConsumer) ```
Hmm, not sure if I buy that. `trySend` would only return a value less than 5s if there was a pending request which needed to be send. However, as soon as it got sent, it would go back to 5s. So I'm not sure if it alone can account for a tight poll loop.
Do we want this to be `poll(0)`? Otherwise we're still blocking here.
Worth explaining how the channel builder is closed now.
Do we still need a shutdown hook to delete with `file.deleteOnExit();`? I think the latter can still be triggered even with abnormal exits.
Could we combine the finally block with L45-46? Also I was thinking whether we should close the producer thread as well.
Hi all, thanks for the discussion. To start at the beginning, yes, I was advocating for throwing an IllegalStateException at the earliest possible moment when we can detect the illegal state. Right here in the code, we are making an invalid assumption. Namely, that if a task to be recycled is active, then it has already been suspended and committed, and if it is a standby, then it still needs to be suspended and committed. Why should this be true? Because some other code deep inside another set of nested conditionals thirty lines above looks like it does that right now? Experience says that this situation will not survive refactoring. We cannot test every branch, so we need to make assertions about the state being valid when it's in doubt. We could make an argument that if this assumption becomes incorrect, than we'll throw an exception later on before any state becomes corrupted, which would be good. We could make a stronger argument that the exception we throw later on will be perfectly crystal clear about the cause and therefore we won't spend weeks poking around a flaky test or a user bug report trying to figure out what happened. But both of those arguments would depend on even further assumptions about stuff that may or may not happen elsewhere in the code base. The best thing to do at all times is validate potentially dangerous assumptions. This looks very much like a potentially dangerous assumption. I'm asking that we validate it.
I think @vvcephei 's confusion comes from a change that we now require all tasks to be transited to `suspended` before transiting to `close`: previously, we allow e.g. a running task to be closed immediately and inside the task itself the logic actually did the "running -> suspend -> close" logic, i.e. it is totally agnostic to the TM. In a refactoring with eos-beta we changed it. So now the responsibility is kinda split between the two: TM needs to make sure the transition is valid and the task verifies it. By doing this we avoided the "pre-/post-" functions.
Your words sound good to me, but they are directly in contradiction to the code here, which skips suspending and committing active tasks because it assumes they have already happened. In other words, there is an assumption here that the active tasks are in some kind of "committed" state, while the standbys are in either "created" or "running". If we're going to have a branch that explicitly assumes the task is already committed, then I'd like to verify it, otherwise experience says it will become false after refactoring, and we'd wind up trying to track down an IllegalStateException later on. On the other hand, if these transitions are idempotent, we can just include them in both branches (or rather move them outside the conditional block).
There should never be multiple requests, right? If there were, a second request might arrive between 168 and 169, violating the desired property. In that case, we should grab a lock instead. As long as there's only one requesting thread, and it always waits for the commit right after requesting, then we should be good.
Ok, I was thinking more along the lines that we have just one requestCommit instance while we would have multiple punctuator instances all using it. It looked like a bug, but it seems like it'll be fine.
Yeah, that's what I was thinking
In this test we would have multiple punctuator indeed but they would be executed by a single thread sequentially so that's fine.
Hm. What if we hit a TaskMigratedException during `handleRevocation`? We would never finish committing them so `commitNeeded` would still return true and `prepareCommit` would return non-empty offsets right? It's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we _attempted_ to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways. This might be beyond the scope of this PR, but just to throw out one hacky idea we could add a `commitSuccessful` parameter to `postCommit` and then always invoke that after a commit so that `commitNeeded` is set to false. (If `commitSuccessful` is false we just skip everything else in `postCommit`)
Happy to see this go 
While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)
This seems to be a "hack" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from `tasksToCloseClean` when we add it to `tasksToCloseDirty`
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
should it be `putIfAbsent` (cf `cleanUpTaskProducer()`)
Seems like double logging? We have a `log.error` each time before `taskCloseExceptions.put()` is called in `handleCloseAndRecycle`
Nit: Can we keep `SUSPENDED` after `RUNNING` case? We use the same order in all methods and always follow the "natural" state transition order, that is CREATE, RESTORING, RUNNING, SUSPENDED, CLOSED.
Can we use an ordered set then? Just to make sure we can't end up with a task appearing more than once in the same list/set
Alternatively, now that we enforce checkpoint during suspension, we could just remove the `pre/postCommit` for active tasks in `handleAssignment`. It just seems nice to be able to assert that we never call `pre/postCommit` after a task is suspended
I know that's kind of another large change, so feel free to tell me to drop it  Or one of us can consider as followup work.
It's still making me a bit uncomfortable that we call `prepare/postCommit` during `handleRevocation`. and `handleAssignment`. Maybe I'm being paranoid but experience has shown it's been difficult for us to keep track of which methods need to be called when, and in what order. It seems like, now that we've decoupled flushing from committing, the only reason for calling pre/postCommit in `handleRevocation` is so that the record collector is flushed before committing offsets. So what if we extract the record collector flushing out into a separate StreamTask method that is only ever called in `TaskManager#commitOffsetsOrTransaction`? I haven't thought this all the way through but it just seems like we may as well go all the way in decoupling flushing from committing and split them out into separate methods. Maybe `preCommit` and `postCommit` have become relics of the past
I thought we would catch and save the first exception thrown by a rebalance listener callback, and then rethrow after all rebalance callbacks have been invoked? In this case that would mean `handleAssignment` would still get called, and then we would throw an IllegalStateException and bail on the rest of `handleAssignment` for no reason. The IllegalStateException itself is not the problem, since only the first exception (the TaskMigrated) would ultimately be thrown up to `poll`. But we should still go through the rest of `handleAssignment` in order to properly clean up the active tasks and manage the standbys (since we don't need to close standbys in case of TaskMigrated)
But `ProcessorStateManager` doesn't handle global tasks
It seems a little odd to have `handleCloseAndRecycle` not do this but just update the taskToCloseDirty list, since it handles everything else.
The default ZK session timeout and ZK connection timeout are both 18 secs. So we can just change both to 18secs here.
Is it worth adding a note about the use of REPLICA_NOT_AVAILABLE below? I think we are effectively deprecating its usage in this PR.
This would also be a good place to be clearer about the interpretation of this error. Maybe something like this? > For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition.
Should we do the timeTaken check immediately after L270 assertion to indicate that we are testing on the back-off triggered for group authorization failure? The current flow looks a bit weird as the second `ensureCoordinatorReady` doesn't do any back-off.
This seems not good...we shouldn't be casting to an abstract class. Let's just add whatever we need as a method on the `Task` interface
Don't we want to modify the TaskManager so the StreamThread doesn't have to wait for all tasks to finish restoring? It should be able to start processing any active tasks as soon as they finish restoring in the other thread.
nit: may as well fix alignment as well
Then we wouldn't have caught this bug  . The most dangerous aspect of the generated protocols is the down-conversion to older formats since it gets poor test coverage.
Do we need a `Set` here? Not sure we need to pay the cost of creating the `HashSet`, etc.
nit: connections => connection
typo: `for for`
nit: we could initialize on the parameter definition.
Thanks, this one has always made my head spin when reading the logs
super nit: processor --> process (or bet yet, 'client')
Could we add a follow-up ticket instead? Someone in the community could pick it up.
We should add an empty line here to avoid mixing with the other maps with production use cases.
This alignment looks weird.
Not at all. This is a perfect case for ArrayList. LinkedList iteration is very slow due to pointer chasing (in comparison).
FYI https://twitter.com/joshbloch/status/583813919019573248 :)
@dajac Won't this create a list regardless of whether there are timed out entries? Let's see whether this addresses @ijuma 's concern on the other PR.
Why are we using `LinkedList`? It's very rare where it should be used.
Yeah. Even if it's "compatible" and does not break anything, it's still fall into the "public api change" category...
We should use `aasertThrows` and verify the error message similar to the `TransformerSupplier` test
Yes. Sorry for using the wrong name...
Yes, those suppliers must return a new instance on `get()`, too, so the new check should be done for them as well.
This seems to be a public API change that we cannot do without a KIP. Seem you added it so you can pass the different suppliers into `checkSupplier` ? Also not sure if `checkSupplier` must be as "complicated" as proposed.
> But maybe just using the implementing class name is fine. That was my though, too.
Generate the type and write it out to the internal state.
nit: remove extra line
format looks weird, maybe just do 4 spaces
nit: could make access private and get an accessor.
This can be static
This can be static
This can be static
These blocks of assertions are quite hard to read. Can we try to make them more digestable? We could perhaps extract temporary variable to reduce the number of `.get()`. We could also define an `verifyDescription` helper that verify a `LogDirDescription` for instance. It may be worth having dedicated unit tests for the new and the old APIs as well.
This can be static
This can be static
The left side can be `Map`
This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say `assertDescriptions`, that verifies a descriptions map contains the information about a single log dir/topic partition? Something like `assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture)`.
nit: What about extracting the construction in a small helper method `prepareDescribeLogDirsResponse` that create a response for one LogDir and TopicPartition? It seems that the same block of code is used in many tests.
Probably better to save for a follow-up, but potentially we can get rid of this conversion by using `FetchablePartitionResponse` directly in the broker.
Yeah, `Optional` support would be awesome. I was actually thinking how to do it. I may give it a shot during the weekend ;)
That would mean loading data from disk to compute equals and hashCode for FileRecords. That's pretty unusual for such methods.
@mumrah : equality for the generated messages should mean bytewise equality. So if two FetchResponseData instances contain the same data, they should be equal, even if one is using MemoryRecords and the other is using FileRecords. Same for hashCode, of course. If it's too much trouble to change the Records class, you can just write a static utility method in MessageUtils and invoke it from the generated classes. I expect that we won't be doing this kind of comparison except in tests, so you don't need to optimize the method too much.
The hashCode of `MemoryRecords` takes into account the buffer position, so it's kind of useless. `FileRecords` doesn't even define it. We should consider defining the hashCode and equals of `Records` to be identity based.
Perhaps instead we could add this to a mixin type. Then if we find cases where getting accessing to the `ApiMessage` generally would be useful, we could just use `instanceof` checks. These would ultimately go away after the conversions are finished.
I have a PR that does need. I really need to get that over the line.
In the parsing logic, we still convert to struct first before calling `AbstractRequest.parseRequest`. I think we could bypass the `Struct` conversion by changing `AbstractRequest.parseRequest` to take the `ByteBuffer` instead of the `Struct`. ```java public static AbstractRequest parseRequest(ApiKeys apiKey, short apiVersion, ByteBuffer buffer) { ``` Then in the fetch case, we could just call this method.
Is there an advantage to pulling this up? Seems like we still need to update a bunch more classes. Until we have all the protocols converted, it might be safer to find another approach.
I'm ok saving this for #7409.
@hachikuji @mumrah @cmccabe I have put together a prototype to support java.util.Optional in the auto-generated classes. It a good draft at the moment but it is a good basis for discussions: https://github.com/apache/kafka/pull/9085
nit: not a big deal, but I feel like calling `flush` should really be the responsibility of `write`.
Let's open a jira for getting rid of the toPartitionDataMap if we don't address it in this PR. It's a pretty large part of the cost here and there are only a few places we would have to deal with it. I think we should fix it sooner rather than later too.
Pretty nice if this is all the manual code we need. If we wanted to go a little further, we could push `toSend` into the generated class as well. That will be necessary if we ever want to get of the current `AbstractRequest` and `AbstractResponse` types and replace them with the generated data classes (which was always the plan). However, I think this could be left for follow-up work.
I don't think `FileRecords` and `MemoryRecords` instances can be compared directly, if that's what the question is about.
Similarly, we can get rid of all this.
As an aside, it would be awesome to add `Optional` support to the generated classes. We have had so many bugs which were caused by sentinel values sneaking into unexpected places.
@mumrah Have we considered dropping the `PartitionData` class entirely in favour of using `FetchRequestData .FetchPartition` directly in the broker? The main difference is that `FetchPartition` does not have an `Optional` for the leader epoch but returns the default value (-1) instead.
super nit: taskCreationLock --> taskDirCreationLock
nit: I don't feel strong about the style here, but maybe we should consider align with other functions which has the first parameter on the same line with function name.
Same here, one parameter per line and align
How about using different ```transaction_timeout``` for different mode? For example, lower timeout for hard_bounce of client and higher timeout for broker. I try to avoid higher waiting time (```progress_timeout_sec```) when encountering other error.
Not exactly sure. Guess it was just a random decision to do it this way.
Hah, this was pretty janky. Good catch
We can remove this field now that it's unused
`replicaing` -> `replicating`
nit: The sentence sounds slightly better if you remove `the`
We can move that line after the `waitForCondition()` block to just commit once all records have been consumed.
`replicaing` -> `replicating`
```suggestion final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore("aggregated", ofMillis(1200L), ofMillis(100L), false); ```
nit: `testAggregateRandomInput` to match up with other test names
Can we actually wrap the whole `testProcessorRandomInput` test in the try-catch? Or at least, everything after the initial setup? Would be nice to have the seed in case something weird happens during the processing itself
Oh right, forgot that it doesn't have the window times either. Nevermind then
It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
For readability, could we mark the final results for each window? We want to make sure all the intermediate results are as expected, but what we really care about is what we got in the end. It would just help to have the critical output easier to find and get oriented in the tests
Can we insert one that's like right on the border of the retention period? So if the streamtime at the end is 2,000 then the window cut off is 800 (or start time of 700), and verify that anything starting before 699 is gone and everything after that is there.
nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here
```suggestion public void shouldDropWindowsOutsideOfRetention() { ```
Aha, so there was a good reason for it 
The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.
Awesome, thanks for cleaning up some of these older tests 
The input is the same for each test so the output is too, right? Maybe we can we pull all the output verification into a single method
nit: extra spaces after the `->`
Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere
nit: we could just use `Map` for `startOffsets` and `endOffsets`
Double-brace initialization is an anti-pattern. It would be preferable to use `mkProperties`.
"different" for sure, but this implies that one might have an operator the other does not. The observed issue is, that even if both contain the same operator, they might be added in different order (and thus be named differently) to the `Topology`, thus we should stretch that order matters.
nit: formatting: (we should also get the exception an verify the error message) ``` final TopologyException exception = assertThrows( TopologyException.class, () -> new StreamTask( ... ) ); assertThat(exception.getMessage(), equalTo("...")); ```
`Topic not found` sounds like as-if the topic was not found in the cluster -- however, what actually happened is that we received a record but the record's topic is unknown in the sub-topology. Similar to above, "deterministic" is not really easy to understand. I would also not phrase it as a question, but as a statement: ``` ... This may happen if different KafkaStreams instances of the same application execute different Topologies. Note that Topologies are only identical if all operators are added in the same order. ``` Or similar.
Nit: insert `<p>` tag to actually get the new paragraph rendered. Nit: `Topology -> `{@link Topology}` It's not really clear what "deterministic" means. We should elaborate more.
`incompatible runtimes and unexpected results` -> `incompatible runtime code and unexpected results or errors.`
`szTest` is a terrible test name: `shouldThrowTopologyExceptionIfTaskCreatedForUnknownTopic`
We should not use this annotation but rather use `assertThrows` (we still have some code that does not use `assertThrows` but we try to lazily migrate our tests, as it provides a better test pattern).
nit: fix alignment.
@junrao Regarding "With this change, if we record a large value, the observed effect of the value could last much longer than the number of samples. " -- This will not happen with this approach. If we record a very large value, we never move to the bucket with timestamp > current timestamp (of the recording time). This approach can only add the value to older buckets, which did not expire, but never to the buckets "in the future". For example, if we only have 2 samples, and perSampleQuota = 5, and say we already filled in both buckets up to quota: [5, 5]. If new requests arrive but the timestamp did not move past the last bucket, we are going to be adding this value to the last bucket, for example getting to [5, 20] if we recorded 15. If the next recording happens after the time moved passed the last bucket, say we record 3, then buckets will look like [20, 3].
@junrao If I understood your proposal correctly, we will keep Rate calculation the same, but additionally implement TokenBucket (traditional way) which will tell us when quota is violated. This would be much easier. However, I think, it would not fix our issue (that we are trying to fix) of too large throttle times during bursty workload. ClientQuotaManager calculates throttle times by comparing rate (based on how we record Rate) to quota, which I think would result in the same behavior as before unless we change the way we calculate throttle time as well.
@apovzner @dajac : Currently, we calculate the delayed time based on QuotaViolationException.value and QuotaViolationException.bound. I was thinking that we could pass some additional info in QuotaViolationException so that we could calculate the delayed time properly. Overall, it seems that decoupling the observation from token bucket based quota might be easier to understand. As for monitoring the remaining credit, we could add a separate metric. Also, it seems that quota only makes sense for rates. So, instead of making quota available on arbitrary measurable, we could just make it work for Rate.
should this be `< 0`? Otherwise, we skip index 0
This should be the same unit as the unit in `Rate`, right? If so, I think someone could create Rate as: ```new Rate(TimeUnit.MILLISECONDS, new TokenBucket())``` Or ```new Rate(new TokenBucket(TimeUnit.MILLISECONDS))```
```suggestion "Updating global state failed due to inconsistent local state. Will attempt to clean up the local state. You can restart KafkaStreams to recover from this error.", ``` Just a thought to indicate why just restarting would recover anything.
NVM, I realized it is at the per-node metrics level, we just have two types of nodes (source and termination node) for each task.
Could you test `maybeRecordE2ELatency()` through `process()` and `forward()`? Although you test `maybeRecordE2ELatency()`, you do not test if the recording is done during processing, but that is the crucial thing, IMO.
```suggestion * 1. Reassign as many previously owned partitions as possible, up to the maxQuota ```
Sorry for the delay, I suggest we revert this part to properly scope this PR.
I also think it's better to start with burst size credits. We would normally create the sensor on the first byte/request/etc. and before that, the <user,client> is idle which means they were "accumulating credits".
If we want the burst to be more similar to original behavior, it seems like this should be `#samples`. With the current implementation, we can do 1 unit of work in the oldest window and then accept a burst right at the end of the last (not full yet) sample. Which means that the max burst size is almost at #samples * quota (if sample = 1 sec, quota is in units/second). Does this sound right to you? Also, I think we should take into account `config.timeWindowMs`, because it could be something other than 1 second.
Should we start with 0 credit or the full burst credits? The benefit of the latter is that during initialization, the requests won't be throttled as much due to a cold start.
Rate actually allows the windowSize to be close to the full samples * perSampleWindow. The logic around `config.samples() - 1` is just to make sure the windowSize contains at least that many full windows. So, to match that behavior, it seems that burst should use `config.samples()`.
ah right, `lastUpdateMs` will make sure that bucket would be full on the first `record()`.
hm.. maybe I just did not notice `config.timeWindowMs`. All good now.
"this until it is" doesn't quite parse.
This warning seems to miss the most likely scenario, that the user just passed the arguments in the wrong order.
nit: Use braces & separate lines
Do you know why we have all these ReadOnlyWindowStore methods also declared here in WindowStore? We don't need reverse variations of these I guess? 
Idk, the current defaults make sense to me. If a user has a custom store and wants to use the new `backwardFetchAll` with both longs and Instants, all they'd have to do is override the long-based `backwardFetchAll` method (they have to implement the long version no matter what, since this is what gets used internally to Streams). If we just throw UnsupportedOperationException directly from the default implementation of the Instant-based `backwardFetchAll`, then they would have to override that as well in their custom store. So we should just let the Instant default to the long method so users only have to implement one method instead of two (plus they would have to do the Instant validation themselves, etc)
Ah yes, I was thinking about ReadOnlyWindowStore exposed in IQ only. All good.
It seems like your indentation is set to 8 spaces instead of 4.
This is all the same for all three methods except for the `KStreamAggregate`/`KStreamWindowAggregate`/etc right? I think if you wanted to further deduplicate things you could factor this out into a method that accepts a `Function< KGroupedStreamImpl, KStreamAggProcessorSupplier`>, and then each of the `build` methods can just pass in a function that returns `new KStreamWindowAggregate` or so on. I'm not sure it's really worth it or not, but it can be done in case you were wondering. Up to you whether you want to do it
Honestly I think it's fine to just name all three of these `build`, since they accept different parameters and it should be pretty clear from the context whether it's windowed or not. But being more descriptive is never a bad thing either. Your call 
Also you can remove the `Vin` and `W extends Window` generics on this method
Don't need `Vin` and `W extends Window` here
Don't need `Vin` here
Can remove the `W extends Window` generic
Can we call this something like `ensureCopartitioning` or `processRepartitions` or something? My take is that the copartitioning is the main point of this method so that's probably good to include in the name
We can remove the `<W extends Window>` now, right? Also it looks like the `initializer` input isn't needed anymore either
WDYT about naming this just `build`? It's not as clear, but I think it's in line with the naming conventions elsewhere. For example we have `KStreamWindowAggregate` and `KStreamAggregate` (then maybe we can come up with a more descriptive name for the method currently called `build`
@rajinisivaram Yes, merging to just trunk seems fine to me.
@rondagostino are we ok with merging this to trunk? Since this is not required for existing tests which either use ZK or PLAINTEXT brokers, not planning to backport to older versions.
Does this need to be here? I'm concerned that having it here will eventually lead to us using it for authorization, which it shouldn't be.
"or null if this was not redirected"
"or null if this was not redirected"
We could port this function when it is actually needed.
Same question for ProcessorSupplier for using a delegate, but is minor to me.
I'm thinking whether it makes more sense to let `MockProcessor` encapsulate a delegate `MockApiProcessor` so that we could also use existing tests to verify the correctness of the migration.
Writing one log message per record, seems excessive, even for `DEBUG` There's a trace level message above for the record. Maybe we want to append the topic name there.
```suggestion "If auto.create.topics.enable is enabled on the broker, the topic will be created with " + "default settings", topic); ```
```suggestion log.trace("Topic creation by the connector is disabled or the topic {} was previously created." + ```
You are referring to the inclusion of topic properties besides the topic name. Yeah that's fine.
nit: `Instead, they will fall within the [0, timeDifferenceMs]` -> `Instead, we will put them into the [0, timeDifferenceMs] window as a "workaround",`
> most recent max timestamp Huh? I think I know what you're trying to say here but it seems like two different phrases got a bit mixed up here
nit: can we make this `if startTime == 0` ? That seems slightly easier to understand, and then all the conditionals can be in terms of startTime which is a bit more intuitive since that's what we're iterating over. Context switching between startTime and endTime kind of makes me lose my train of thought
It took me a second to get this -- can we explicitly check `if startTime == timestamp + 1` instead of falling back to `else` and implicitly relying on the fetch bounds? You can just get rid of the `else` altogether or throw an IllegalStateException if none of the specific conditions are met and the else is reached, whatever makes sense to you
I think this fetch might break if you go into the negatives, should just fetch starting from 0
We need to make sure the `fetch` bounds don't go into the negative. We only call `processEarly` if the record's timestamp is within the timeDifferenceMs, but here we search starting at timestamp - 2*timeDifferenceMs
This is really just the timestamp of the previous record, right? Can we call it something that reflects that
Given, that we call `processEarly` only if `0 < timestamp < timeDifferenceMs`, we know that `timestamp - 2 * windows.timeDifferenceMs()` would always be negative? Thus, we can just pass in zero here? If this is correct, we might want to add a check at the beginning of this method: ``` if (timestamp < 0 || timestamp >= timeDifferenceMs) { throw new IllegalArgumentException("..."); } ```
`currentWindow` is probably more traditional but `existingWindow` sounds good too
It was my suggestion to explicitly check `if (startTime == timestamp + 1)` instead of just falling back to `else`, for code clarify and safety, so blame me. But +1 to adding the `else throw IllegalStateException`
nit: `aggregated` -> `aggregate`
Also, cool, I think I understand the concept here but some of the details are a bit fuzzy. Basically if we don't find a right window agg that means we didn't find any windows (besides the combined window), which in turn means that there can only be a single record in the combined window (otherwise you'd get a right window for the earlier record). So we need to use the combined window agg for the current record's right window. But we should only do that if the one record is actually after the current record, right? I think you actually do implicitly check that is the case below but it's pretty subtle: basically in `previousRightWindowPossible` you would return false if `rightWindowStart > currentRecordTimestamp`. But we can check that right here and make it explicit, so that `rightWinAgg` only ever means the aggregate that we will actually put in the current record's right window. Then we can also clean up `previousRightWindowPossible`
Ok, I see that `rightWindowStart <= currentRecordTimestamp` isn't necessarily true when you call this from `processEarly` but I think you're kind of abusing this poor method  . I would keep things simple here and make sure the parameters always mean exactly the same thing when you call this, ie `rightWindowStart` should _always_ mean that "the start time of the right window for the record which is previous to the current record" . If that means some duplicated boolean checks here and there, so be it.
Actually, do we even need the `endTime >= timestamp` part of the condition? We're really just iterating over the single dimension of the `startTime` from 0 to `timestamp + 1`
Are you sure it's actually returning something? Have you tested it with a rocksdb store or just with the in-memory store? I think the in-memory store would handle this fine since it never serializes the key/timestamps, but if you have a rocksdb store (or a caching layer) then the range query works by looking up any data between the serialized bounds. Unfortunately a negative long is lexicographically greater than a positive long when serialized to bytes. The "negative" is encoded as a leading 1 -- which means the lower bound ends up being "larger" than the upper bound. I would assume that would result in no data being returned, but I'm not actually 100% sure what would happen
nit: flip both lines: ``` final long startTime = next.key.window().start(); windowStartTimes.add(startTime); ```
+1 to this
Ah, good call, that makes sense to me
nit: `1` -> `1L`
nit: `fall between 0 < inputRecordTimestamp` -> `fall between 0 <= inputRecordTimestamp`
nit: `maxRecordTimestamp > timestamp` -> `maxRecordTimestamp >= timestamp` nit: missing space: `// We`
nit: log an error and include the relevant info (eg `windowStart` and `inputRecordTimestamp` at least). Same for the IllegalStateException in `processEarly`
nit: log an error and include the `inputRecordTimestamp`
Same question as above about `currentRecordTimestamp` (It seems best to me, to use the same variable name for the same think throughout all methods.)
Fair enough. I was thinking of `current` in the context of the while loop, but given that we refer to the "current record" elsewhere, `currentWindow` might be ambiguous
I thought it was called `rightWinAgg` because it's the aggregate that goes in the current record's right window. Of course we had to find this aggregate from some other existing window, eg from the "last window left of the current record".
See above: we shouldn't rely on `previousRightWindow` here. Actually I don't think we need it at all? (assuming we move the check in it to the condition above where we use the combined window agg)
The suggest is not bad, but required to add the `else-throw` to make sense. Otherwise, an programming error could slip undetected.
> I think the case you're referring to above is saying that for the out-of-order case, the previous record's right window should already exist -- this line is dealing with the right window of the current record. Ah. I missed this. @lct45: the explanation makes sense. Thx!
Well, `currentWindows` sound like the window of the current record, while this variable point to other windows, too.
I think we should always assign the `next.value.timestamp` value to a variable with an explicit name, eg `windowMaxRecordTimestamp`, because it's pretty non-obvious what it means and easy to forget
Do we need to use `getValueOrNull` here? It seems like `rightWinAgg` should never be null if we are using it to create a right window
Oh good point, we definitely need the key. But I think separating them turned out well
By the way, I think we should also check if the record is so old that even the latest window it could possibly create/affect would be dropped, and then not process the record at all. (ie basically check if the current record's right window would be dropped) We can record on the lateRecordDropSensor and log the message using the current record's left window.
True, I guess there's no reason the initializer can't return null. Nevermind then
Wait...what's going on here? Aren't we just creating a new `ValueAndTimestamp` that's identical to the `rightWinAgg`? We don't need to make a copy, I assume
```suggestion * windows with negative start times, which is not supported. Instead, they will fall within the [0, timeDifferenceMs] ```
I know it's effectively the same thing, but it feels a bit harder to reason about a "hypothetical previous record's right window that may actually not be a previous record at all" than just "we do/do not have a previous record"
This is also kind of unclear (what is a max right window?), but I get that we can't call it `previousRecordRightWindow` since we don't know that it is a previous record or not at this point. I think yet again, just keeping track of the previous record's timestamp as we iterate through the windows, will be the most clear; if `previousRecordTimestamp` is still null by this point, we know right away that we don't have to create a previous right window. And then we can actually drop the `rightWindowNecessaryAndPossible` check altogether, since we know the current record has to be in range of the right window of the previous record (since we're in `processEarly`). The one exception is if the previous record and current record are on the same timestamp, so we can actually skip the previous right window creation if `previousRecordTimestamp ` is `null` OR equal to `timestamp`
Couldn't there still be a record to the left? Like we could have a record at 5 and at 50 and nothing else, then all we would have so far is the combined window and one at [40, 50], but `rightWindowAgg` would be null. So that's why we need to check that the `combinedWindow.maxTimestamp > timestamp` (and if not then we should leave `rightWinAgg` as null). Looks like this is what you're doing, so not suggesting any changes, just trying to make sure I have this right.
Here too, is this kind of moot now that we can just track the `previousRecordTimestamp`? IIUC all we really want to do is make sure that the left window is not empty, which is actually a pretty simple calculation in terms of the `previousRecordTimestamp`
Alignment is still off, the first parameter should be on the same line as the method declaration. Also, if this method is only ever used in deciding whether to create the previous record's right window, then let's name the parameter `previousRecordRightWindowStart` or something. Of course, you could also just pass in the `previousRecordTimestamp` now that we have that, and then do the math in here. Whatever you think makes the most sense
I guess "Necessary" still seems kind of open-ended/vague. By that you mean, "is not already created", right? But maybe we should wait until we see the final form of this method in case there are any further changes, and then we can go back and try to fish out a more specific name
I guess you could just have a separate `setPreviousRecordTimestampIfNecessary(window, previousRecordTimestamp)` method that sets the `previousRecordTimestamp` to the window's max timestamp if it's larger. And then if it ends up that `previousRecordTimestamp == timestamp` then we can automatically skip all of the window creation below, which is nice
Yeah sorry I didn't mean that we shouldn't have any conditionals here whatsoever, I just meant that we don't need the combined window check (or really anything other than what we need to accurately set `previousRecordTimestamp`)
Yeah, you're saying that we just always keep track of the `previousRecordTimestamp`, but before we go ahead and create a left window for the current window we just actually verify that the previous record is within range? That makes sense to me, actually if anything I feel like it will make `rightWindowNecessaryAndPossible` even more clear to put it in terms of `previousRecordTimestamp`. What I'm realizing from this is that it's easier to understand these boolean checks in terms of the actual record locations, in general. Maybe it's just my mental model, I still picture a rectangle sliding over boxes on a timeline 
I think with this replacement then we might be able to get out of doing any kind of special handling for the combined window outside of `processEarly`
> That being said, I get that this is confusing. Do you think changing the check to `if (endTime == windows.TimeDifferenceMs() && !isLeftWindow(next))` would make it seem cleaner? Haha no, I don't think saying `if (!isLeftWindow(next)): then next = latestLeftTypeWindow` would be less confusing. If we call a variable `leftTypeWindow` then it should _always_ be a left type window. That said, I now see what you meant here and it's the same problem as above, with the same fix of replacing `latestLeftTypeWindow` with `previousRecordTimestamp`. In that case I think we can just remove this check entirely (ie, don't explicitly check if it's the combined window), and all we need to do is make sure `previousRecordTimestamp` is set correctly
Is this the same as in the non-early `process`? Maybe we can factor it out into its own `createRightWindowIfNeeded`(or whatever) method.
This doesn't look right..why would we need to pass in the `key` and `value` to `createRightWindow` ? The distinguishing feature of the current record's right window is that it doesn't include the current record at all. I see that `createRightWindow` ultimately calls `putAndForward` which takes a key and value, but that just seems misleading. I think we should either pass in `null` to `putAndForward` for things we don't need, or better yet (imo) don't use `putAndForward` for the right window creation and just have a clean separation between creation of the right window and everything else
```suggestion * Created to handle records where 0 < timestamp < timeDifferenceMs. These records would create ```
Seems like we should move this above into the top-level `process` instead of first calling `processInOrder` and then calling `processEarly`. For one thing, since we actually do need to iterate the full range for the early records, we can just call `processEarly` without having to decide between `processInOrder` and `processReverse`
Sounds good. I just want to avoid someone trying to simplify the tests in the future without understanding that this test is verifying both features work together.
nit: could avoid the last space after `is correct.`
Seems good enough as a bug fix, but I was wondering whether we could detect the dynamic topic is configured or not to make sure we are not actually allowing some other bugs to catch in TTD
Ok, let's just keep it in our back pocket for now.
We could detect if the processorTopology contains only `StaticTopicNameExtractors` and still throw in that case if the topic name isn't in the topology.
At this point, we know that `mappedKey != null`, otherwise, we would have dropped the record.
This condition seems unnecessary complex. Should it not just be: ``` if (mappedKey == null || value == null) { ```
Oh yeah, duh. Nevermind this 
We need to remove this too, right? We shouldn't forward anything regardless of whether it's a left join, if the mapped key is null then there's nothing to map it to
Why remove this check? The `valueGetter.get` does an actual table lookup, which would be wasteful if we're going to skip this record anyways because the mapped key is null. Also, I'm pretty sure the lookup would throw an NPE
Should we really catch NPE here? It seems like if the user wants to return a non-null mapped key from a null key, then they should handle the null case specifically in their `keyMapper` and not just throw an NPE. In general, an NPE is a sign that something has gone wrong. I would be pretty surprised if I threw an NPE explicitly in my user code and it just got swallowed and interpreted as if I had actually returned null.
`KafkaConfigBackingStore.this.sessionKey` is already a `volatile` member variable. So, given that no other common data structures are altered in this block, I'm suspecting that your change means that we might as well get rid off the `synchronized` block altogether in this `else if` branch here. I'd like to give it a second thought in the morning but lmk what you think.
I don't understand what this test is doing. Why do we need background clients instead of producing upfront and consuming the data mirrorred at the end of the test? It looks like we are testing the primary->backup scenario but we are restarting the backup cluster. The source connector should not interact with the backup cluster.
By catching Throwable, aren't we silencing the error and not immediately failing the test? Also we should not be catching Throwable, if the JVM throws an Error we want it to fail the test and never want to handle it. There're a bunch of them in these files
nit: we tend to have a new line at the end of files
We can use `Collections.singletonMap()`
We can use `StringDeserializer.class.getName()`
Well in `MirrorConnectorsIntegrationSSLTest`, we can override it, but we should put the default implementation in the base class
This should be done in the SSL class. The base class should not be aware of SSL and just use configurations from the concrete classes
Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.
This is unused too
Isn't MM2/Connect using at least once by default? ie, the producer in the runtime can cause duplicates.
I find having a method specific for SSL strange. Callers should not have to know, this should be retrieved automatically based on the cluster being targeted
It looks like this is not used anywhere
What do we do if there's an exception? If it's expected, let's make it clear
`ConsumerRecords` -> `ConsumerRecords<byte[], byte[]>`
We can use `List<Class<? extends Connector>` to avoid the warning
This can go with the other `org.junit` imports below
We can remove this
Could the cluster aliases be constant as these are used all over the place
Why do we have SSL specific methods here? Could we move all the SSL bits into the SSL class? We have fields for the configurations. So we could set them accordingly (without or without SSL) in each concrete class. Then in the base class, we just use the fields to create the clusters without having to know if it's SSL or not.
Yes it will
Should we use this check as a condition to wait? Sleeping 10 secs feels pretty brittle
This restarts all brokers. I find it strange this takes a `EmbeddedConnectCluster`. Also I wonder why this is static.
We can use `putIfAbstent()`
`brokerConfig` is a `Properties`, so we can call `getProperty()` to get back a `String` directly
Just let the Exception flow, that will automatically fail the test
A few lines in this file (like this one) contain tabs, we use spaces in Kafka
But why is this needed here? I don't know what the other test is doing but I don't understand why it's used here
I find it strange that this method closes the consumer it received.
This field is unused
We don't need this field, this could be a local in `startClusters()`
This still needs to be addressed
I'm assuming it's worth keeping this 2nd loop separate from the first one for performance reasons. At first glance it looks strange to iterate over the same collection twice in a row
We could use a "for each" loop here, something like: ``` for (Class<? extends Connector> connector : connectorClasses) { connectCluster.configureConnector(connector.getSimpleName(), mm2Config.connectorBaseConfig( new SourceAndTarget(primary, backup), connector)); } ```
Yes but the code that created the consumer should close it. If I call `waitForConsumingAllRecords()`, I'd not expect it to close my consumer instance.
We can return `listeners != null && listeners.contains("SSL")`
This intermediate `List` is not really useful. We could just change the loop below to iterate over the connector classes and call `getSimpleName()` on each of them
Instead of doing: ``` if (condition) { return true; } else { return false; } ``` You can do: ``` return condition; ```
slf4j logger methods logs exception if the last argument is throwable. so this is fine.
@cnZach Looks like the intention was to print out the exception with stack trace? You would want to use: ``` public void warn(String msg, Throwable t); ```
nit: I think we can remove `synchronized` here as well
building a topology is done on the main thread when calling `StreamBuilder.build()` so I think it's safe to remove `synchronized`.
I think it would be valuable to have all the tests run with both the forward and reverse iterators. You can actually parametrize the test class itself so that it runs multiple times with different input: the syntax is kind of hard to explain (and understand) but you can look at EosBetaUpgradeIntegrationTest as an example. It's parametrized by a `injectFailure` boolean -- you can do the same thing with a `forwardIteration` boolean. Then you could force it to run in the forward direction by providing a custom `WindowBytesStoreSupplier` that supplies a custom `WindowStore` implementation where the appropriate `fetch` method throws UnsupportedOperationException. You should be able to just extend one of the existing built-in stores (eg RocksDBWindowStore or InMemoryWindowStore) that just overrides `fetch`. Let me know if you have any questions about how all this would work
Instead of specifying the whole thing for both cases, you could just create a ``` final WindowBytesStoreSupplier supplier = inOrderIterator ? new InOrderMemoryWindowStoreSupplier(...) : Stores.InMemoryWindowStore(...) ``` and then pass that into the `Materialized` without having to list the whole topology out twice.
This is how I typically break up ternaries. ```suggestion final WindowBytesStoreSupplier storeSupplier = inOrderIterator ? new InOrderMemoryWindowStoreSupplier("InOrder", 50000L, 10L, false) : Stores.inMemoryWindowStore("Reverse", ofMillis(50000), ofMillis(10), false); ```
But I wouldn't be afraid to just use a full if/else block, either. ```suggestion final WindowBytesStoreSupplier storeSupplier; if (inOrderIterator) { storeSupplier = new InOrderMemoryWindowStoreSupplier("InOrder", 50000L, 10L, false); } else { storeSupplier = Stores.inMemoryWindowStore("Reverse", ofMillis(50000), ofMillis(10), false); } ```
I knew John would know what's up with the weird type nonsense 
The Materialized builder is notoriously vulnerable to "weird type nonsense" because it falls into a gap in Java's type inference system when you use chained methods. Let's see what happens when you implement @ableegoldman 's earlier suggestion.
Yes, this would be better. Not sure if it helps, but for reference, this is what we did in `org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinMaterializationIntegrationTest#getTopology`
It should ultimately be the same for both iterators, but there might be some weird type nonsense going on. These problems should go away if you go with the approach of just setting a `StoreSupplier` based on `inOrderIterator` and then only specifying the topology once
nit: I'd suggest we just inline this function inside `StreamTask` since 1) this is only triggered with EOS enabled, and its name `deleteCheckPointFile` maybe a bit misleading, and 2) it is a very simply function anyways.
SGTM. We can keep it as is then.
nit: `log.debug("Deleted check point file upon resuming with EOS enabled");`
nit: The method signature for StatAndConfig has MetricConfig parameter first and MetricName second. This one has the reverse order.
```suggestion * <p>Note: The current implementation of either forward or backward fetches on range-key-range-time does not ```
Instead of adding headers each time, maybe we can pre-create the headers list and pass to ProducerRecord() constructor.
I was looking up the format in more detail and understand now what going on. The code does not seem to be ideal IMHO, but no need to change it in this PR.
We should update this test and use `assertThrows` instead of `try-fail-catch`.
We should not print the stacktrace imho, but verify the exception message using `assertThat`.
There is an extra line here.
Few nitpicks here: Convert an ISO8601 based timestamp to an epoch value .... @return epoch value of a given timestamp @throws ParseException for timestamp that doesn't follow ISO8601 format
Honestly it kind of seems like there is enough divergent logic to merit splitting this up into separate methods for the manual vs cleanup-delay cases.
This log will be incomplete. We report the exception as the cause: ```suggestion log.warn(String.format("%s Swallowed the following exception during deletion of obsolete state directory %s for task %s", logPrefix(), dirName, id), exception); ``` This feedback applies to pretty much all the warn/err logs in this PR.
Just noticed something else wrong with this message. `e` will get ignored. We should instead use `log.error(String.format("...", logPrefix(), appId), e)`
the method ```clean``` catches ```Exception``` already. Could we get rid of those try-catch statements? the code ```log.error("{} Failed to release the state directory lock.", logPrefix());``` can be moved to ```clean```. For example: ```java public synchronized void clean() { // remove task dirs try { cleanRemovedTasksCalledByUser(); } catch (final Exception e) { log.error("{} Failed to release the state directory lock.", logPrefix()); throw new StreamsException(e); } ``` ```java private void cleanRemovedTasksCalledByUser() throws Exception { for (final File taskDir : listAllTaskDirectories()) { final String dirName = taskDir.getName(); final TaskId id = TaskId.parse(dirName); if (!locks.containsKey(id) && lock(id)) { try { log.info("{} Deleting state directory {} for task {} as user calling cleanup.", logPrefix(), dirName, id); Utils.delete(taskDir, Collections.singletonList(new File(taskDir, LOCK_FILE_NAME))); } finally { unlock(id); // for manual user call, stream threads are not running so it is safe to delete // the whole directory Utils.delete(taskDir); } ```
cleanRemovedTasks -> cleanRemovedTasksCalledByUser
Can we at least log a warning with the exception we're swallowing? Same for the `catch (final OverlappingFileLockException | IOException e) ` above
duplicate line of 340, can probably pull out of the if. Then there should be no need to make a separate method as the only divergence is a log msg.
We recently "fixed" `listAllTaskDirectories` to guarantee that it never returns null. We just missed to update all the null checks when we did that
Might be better to use an `Exception` variable `firstException` and rethrow at the end if not `null` -- IIRC, behavior is undefined if we throw a second exception (ie, `finally` would executed after the first (outer) `catch` block.
That's what Bruno originally did, but the original `cleanRemovedTasks` method branched on the `manualUserCall` flag in several places and was pretty difficult to follow (imo). So (also imo) it's cleaner to split it up into two methods that make it clear what the expected behavior is in each case. Just my 2 cents
Nit: I'd suggest we do not expose internal class names in log entries, e.g. here we can say "Processed {} records with {} iterations, invoking punctuation now", ditto below.
Is this necessary with the logs inside restore()? maybe can include snapshotState so we can see if it's STARTING or RUNNING? because we don't see the state unless it enters the initialization. Not sure if this would be useful
I guess if you are removing `this.` above, you could remove it here as well for consistency.
I guess if you are removing `this.` above, you could remove it here as well for consistency.
nit: `this` is not really needed, since it's not ambiguous here as in constructor,
Interesting. This looks like it's probably just a consequence of the way the test is set up. Some debugging should clear it up.
Oh, sure. Now I know why you picked this name :)
I do not think we need to emulate the behavior of the java uncaught exception handler here. I do not see why a user should try to reset the uncaught exception handler. If we receive this requirement, we can still add it. I have the impression the "reset" makes this code unnecessarily complex.
Sorry, my bad! It doesn't matter whether it is called or not, since the `IllegalStateException` is thrown.
Also here it should be: ```suggestion new AtomicInteger() ).updateThreadMetadata(getSharedAdminClientId(CLIENT_ID)); ```
The closing parenthesis should go to new line: ```suggestion new AtomicInteger() ); ``` Please check all your changes in this file.
Ah, gotcha. Thanks.
What's our plan for the global thread? I didn't think of this during the KIP discussion, and sorry if it was brought up there and I just forgot about it. But it seems like we should still give users a non-deprecated way to set a handler for the global thread.
I think there is a misunderstanding regarding `metrics.close()`. We should not close the metrics here. We should stop the `rocksDBMetricsRecordingService` with `rocksDBMetricsRecordingService.shutdownNow()`. That is what the KIP says.
nit: I would delete these two line
nit: I would delete this line
The closing parenthesis should be in a new line. ```suggestion assignmentErrorCode ); ```
Why did you add this line? I would remove it.
I would avoid this `null` check by setting the `streamsUncaughtExceptionHandler` by default to the default uncaught exception handler.
I think it is better to throw if the passed in exception handler is `null` and set the default uncaught exception handler in the `StreamThread` constructor.
This test misses the verification whether the `StreamThread#setUncaughtExceptionHandler()` is called. This has been missing also before this PR.
What about not considering this case here and logging and throwing the exception where the handler is called in `StreamThread`. So we would have all handling logic for this case in one place.
Let's tweak this API to Throwable: ```suggestion StreamsUncaughtExceptionHandler.StreamsUncaughtExceptionHandlerResponse handle(final Throwable exception); ``` Here's a good explanation of why: https://stackoverflow.com/questions/2274102/difference-between-using-throwable-and-exception-in-a-try-catch The benefit is that we could handle `Error`s as well as `Exception`s. However, this comes with the obligation that we should not continue to use the thread after an Error occurs. I think we can deal with this restriction reasonably.
```suggestion * Set the handler invoked when a {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG internal thread} * throws an unexpected exception. These might be exceptions indicating rare bugs in Kafka Streams, or they * might be exceptions thrown by your code, for example a NullPointerException thrown from your processor * logic. * <p> * Note, this handler must be threadsafe, since it will be shared among all threads, and invoked from any * thread that encounters such an exception. ``` I think it's wrong to say that this is invoked when the thread abruptly terminates, because it's not. That's how the JVM handler works, but we're actually executing this handler while the thread is still running, and in fact that thread itself is what calls the handler. It also seemed appropriate to elaborate a little more on the usage of this method.
```suggestion public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler uncaughtExceptionHandler) { ``` We prefer to resist the urge to abbreviate, especially in the public-facing APIs.
```suggestion * @Deprecated Since 2.7.0. Use {@link KafkaStreams#setUncaughtExceptionHandler(StreamsUncaughtExceptionHandler)} instead. ```
It seems safer to just call the nonblocking close method: ```suggestion close(Duration.ZERO); ``` That way, it'll properly set the state, stop the cleaner thread, etc.
Oh, I forgot; the reason you're doing it this way is to transition to ERROR, not actually shut down, right? In that case, it seems pretty odd to call this option "shut down", since it doesn't actually _shut down_, it only kills all the threads, leaving the final "shut down" as an exercise to the user. If I recall correctly, the preference of the group was in favor of this behavior, in which case, I'd advocate for a different name. Maybe just `STOP_STREAM_THREAD`, `STOP_ALL_STREAM_THREADS`, and `STOP_ALL_STREAM_THREADS_IN_CLUSTER`. I've been on the fence about whether I should leave this feedback or not, but decided to go ahead and pass it on to you because I just got confused by the names, despite having recently participating in that discussion. So it seems likely that users would also be confused and think that we're doing the wrong thing by not actually shutting down the client.
I can't tell; are we guaranteed to actually send the joinGroup request by now? Maybe it's safer to just keep running this loop until we get back the "you should shut down" response assignment. We _could/should_ add a condition into `runOnce` so that we don't actually process anything once we have `shutdownRequested` set.
I'm not sure this is right, actually. If we wanted to use the current thread to send the "poison pill" subscription, it needs to keep running and call poll again. Maybe instead we should have a default implementation of the uncaughtExceptionHandler that invokes the legacy one and then returns `SHUTDOWN_STREAM_THREAD`, and then the implementation of that case in KafkaStreams would be to re-throw the exception.
Thanks for clarifying this.
It certainly might. I'm just wary of how far into the uncanny valley we're going here. Streams is going to be put into a state that's very similar to the one that `close()` produces, but not identical. What will then happen when they _do_ call close? What will happen when we realize that something else needs to be done as part of closing the instance (will we even remember that we should consider doing it here as well)? OTOH, we could instead change direction on the "error-vs-shutdown" debase and just make all these methods call `close(ZERO)` instead. Then, the _real_ close method will be invoked, and Streams will go through a well-known transition through `PENDING_SHUTDOWN` to `NOT_RUNNING`. It would then be a problem for a later date (after KIP-663) if someone wanted to request that instead the app should stop all running threads so they can manually call "addThread" later.
We have a helper for this pattern. See `TestUtils.assertFutureThrows`
I had a slight (? well, maybe not so slight) different idea, which assumes that we only want to have the control at the whole app level, rather than a per-store level at IQ APIs only: suppose we just have, e.g. a config on Streams that enables bypass or not for IQ, then upon calling the get/range/etc APIs at the cached layer: 1) Whether the current thread is a stream thread, if yes, never bypass the cache. 2) If not, depend on the config to decide whether bypass the cache. Doing so we then would not need the unwrap/rewrap and the template types as well. The question then goes back to: whether we would need bypassing at the per-store or global level? Personally I think it's not necessary for a finer-grained control, mainly because that in the long run with the new IQ API and the caller likely to have a totally different way to handle consistency / staleness, i.e. we would deprecate `staleStore` per store or global configs in the long run anyways, and the API calls itself is associated with a token to indicate whether allow staleness or bypass cache.
Thanks for clarifying, @ijuma. Seems like we could verify the round-robin behavior without relying on specific number of IPs.
The test is verifying the round robin that _our_ code does, not the DNS server. We should ensure we still have that coverage in the meantime.
Hmm, so previously (and also with this change) we are assuming that we will get different resolved IPs for each connection to apache.kafka.org? This seems to rely on round-robbin DNS resolution that we can't really control. I think these assertions will always be prone to failure unless we can control the DNS server like @dajac suggested. I guess we can commit this to unblock the tests, but we should definitely prioritize a better fix.
Ditto here, if we think we should pay attention to any errors excluding things like coordinator loading in progress let's just make them all info.
Yes, we used an older version of the test libraries and thus existing code uses a different pattern. We just try to use the newer (and better) pattern in new code and migrate existing code lazily to the new pattern.
Oh, I see. We actually want to pass in an incorrect key-type to get the ClassCastException. So you are right, for this case we cannot specify the generics. Keeping the code as-is and to suppress the warning is fine than. Totally missed this. Thanks for clarifying.
instead of `fail` we should use ``` assertThrows( ClassCastException.class, () -> producer.send(record); ); ``` For this case, we can remove the `catch` block below.
It's unfortunate for admin we use `source.admin` as the prefix ... So we'd be left with a configuration like: ``` "source.cluster.bootstrap.servers": "localhost:9092", "source.cluster.security.protocol": "SASL_SSL", "source.cluster.producer.some-producer-setting": 123, "source.cluster.consumer.some-consumer-setting": 123, "source.admin.some-admin-setting": 123 ```
Maybe not including `cluster` for all the client specific settings would be slightly better, WDYT? ``` "source.cluster.bootstrap.servers": "localhost:9092", "source.cluster.security.protocol": "SASL_SSL", "source.producer.some-producer-setting": 123, "source.consumer.some-consumer-setting": 123, "source.admin.some-admin-setting": 123 ```
We should test settings with the `source.cluster.` prefix too. Same in the test below
Yes I'm running MM2 in a Connect cluster.
These changes are going to break existing users. For example, I have connectors with a few settings prefixed with `consumer.`. I wonder if we could keep the old behaviour (even if partially broken) while adding the proper prefixes
I don't think the format mentioned in https://github.com/apache/kafka/pull/9313#discussion_r498298987 would break compatibility.
We could get rid of `SOURCE_ADMIN_CLIENT_PREFIX` and `TARGET_ADMIN_CLIENT_PREFIX` and instead use `SOURCE_PREFIX + ADMIN_CLIENT_PREFIX` or `TARGET_PREFIX + ADMIN_CLIENT_PREFIX` in the method getting Admin Client configs so they are the similar to the Producer and Consumer methods
I meant add support for that format. We obviously want to keep supporting the existing formats
What do you think about combining these log messages? ```suggestion int ceilTasks = (int) Math.ceil((float) totalActiveTasksNum / totalWorkersNum); log.debug("New average number of tasks per worker: floor={}, ceiling={}", floorTasks, ceilTasks); ```
```suggestion ``` Do we need both of these debug messages? After all, `worker.assign(...)` is just adding a `ConnectorTaskId` to a collection. How about keeping the first one since this is at this point an on-going process and we've not actually assigned anything to the actual worker node.
```suggestion ``` Do we need both of these debug messages? After all, `worker.assign(...)` is just adding a string to a collection. How about keeping the first one since this is at this point an on-going process and we've not actually assigned anything to the actual worker node.
```suggestion log.debug("Tasks on worker {} is higher than ceiling, so revoking {} tasks", existing, numToRevoke); ```
nit: unneeded change
We can use the fact that these are non-negative integers. ```suggestion int ceilConnectors = floorConnectors + ((totalActiveConnectorsNum % totalWorkersNum == 0) ? 0 : 1); ```
We can use the fact that these are non-negative integers. ```suggestion int ceilTasks = floorTasks + ((totalActiveTasksNum % totalWorkersNum == 0) ? 0 : 1); ```
```suggestion log.debug("New average number of tasks per worker rounded down (floor) {} and rounded up (ceil) {}", floorTasks, ceilTasks); ```
```suggestion log.debug("Assigning lost tasks to {} candidate workers: {}", candidateWorkerLoad.size(), candidateWorkerLoad.stream().map(WorkerLoad::worker).collect(Collectors.joining(","))); ```
```suggestion log.debug("New average number of connectors per worker rounded down (floor) {} and rounded up (ceil) {}", floorConnectors, ceilConnectors); ```
This assignment is unused now.
unused `node` variable here and other methods.
Overflow is not uncommon when using things like Long.MaxValue for timeouts, so assuming negative is not a bug seems dangerous.
Another way to test this might be to use `MockClient.enableBlockingUntilWakeup`. That would let us block in `Consumer.poll`.
nit: maybe we can wrap the flushing and hwm updating logic in a `flushFollowerLog` as well.
I'd suggest we add a few more sentences about "always create a new Record upon forwarding" v.s. "reuse the object overriding its key/value/timestamp/header fields", e.g. which way is more plausible when.
(I'll offer an opinion here because I stressed the importance of it on the mailing list, though I do think this is already pretty good. Feel free to take it or leave it, it's just a rewording that may or may not add clarity) > Note that as long as the processor is receiving a record downstream of a Source Isn't every record received downstream of a source one way or another? IMO the up front emphasis should be on _how_ the record came from the upstream processor. My attempt: > The record metadata is defined if the record currently being processed was passed through parent processor(s) directly from a Source. It is undefined if the record was forwarded from within a Punctuator.
I like the extra-detailed error messages, thank you.
We may be able to save all this hassle by defining alias group as a map of `target` to a list of `deprecated` configs? We defined this as a 2-dim array but we always convert it to lists...
It seems like we ought to just define `log` at the AbstractTask level and avoid having two almost identical `maybeInitTaskTimeoutOrThrow` method definitions.
Sounds good. An alternative is to add an `abstract Logger log()` to AbstractTask's interface, which would make it clearer that the logger is still going to have the appropriate class name.
This is the wrong format for this log message. The exception won't be logged. You have to format the string first: ```suggestion log.debug( String.format("Timeout exception. Remaining time to deadline %d; retrying.", deadlineMs - currentWallClockMs), timeoutException ); ```
Not sure whether this change is necessary. The value should be validated already.
Would it make sense to extract these into separate unit tests? `testDescribeUserScramCredentials` could receive `users` as a argument such that we could reuse the code logic.
Sorry, I was not clear. It would be great if we could define separate unit tests for the different cases that we want to test. In case of failure, we would know directly which one of the cases has failed. This is why I suggested to pass an argument `users` as an argument to `testDescribeUserScramCredentials`.
We tend to use the steam api for such small transformations but I don't feel strong about this.
nit: I suggest to continue using the stream api here. It keeps the code smaller.
I guess the problem is we don't have a separate principal in tests that we can assign those to. Could we separate out `Create` and `Alter` into another method, so that tests can set them as necessary or would that impact every test? We expect brokers to have only `ClusterAction`, but our docs and generally everyone expects broker to have more permissions. This sort of makes our tests run with a combination of permissions that we never expect a principal to have - broker principal should have only ClusterAction, no one else should have ClusterAction. Separating out into two methods may be better even if we end up using the same principal in tests.
Do we _know_ that it will resolve the problem? Maybe better: ``` Changing the location of state.dir may resolve the problem ```
@mjsax you can't run multiple instances on the same machine with the same state.dir. For one thing, the locking mechanism is per-process*. If you run two different instances then you can get an active task on one instance and the corresponding standby on another. They would each think they owned the lock for that task directory, and concurrently access it (leading to the FileNotFoundException if one of them deletes the checkpoint, for example) *on some systems. On others it isn't, but then you hit the opposite problem where a task is deadlocked because the other process grabbed the lock for its directory first
Currently, running multiple threads within an instance is _not_ the same as running multiple instances on the same machine in the same state dir. I suppose technically if you could configure the two instances to have the same client UUID then the task assignor would make sure not to assign the same active/standby task to anyone on that machine, but I think there might be some unexpected side effects to running two separates instances with the same UUID. Anyways, if you're aware of the problem enough to configure the client Ids directly, you should be aware enough to just configure to use separate state.dirs. I don't think I buy the argument about it being wasteful just because there's really no reason to ever do this in a production app. But people seem to do it in testing all the time, and run into this issue, hence the warning here
Thanks for clarification @ableegoldman!
Actually maybe we should wrap it in an `if hasPersistentStores` so that users won't get this warning if they don't have any persistent state
Yeah, it should be `0` the first time you call it, then `1` the second time, and then back to `0` again on the third call
Oh right duh I was thinking it was just a single bit but it's a byte. In that case we should have a test that verifies it goes from `0` to `1` to `2`, etc -- might be good to verify the behavior on overflow as well, if you call `subscriptionUserData` the max_value number of times
Thanks. Should we instead make `referenceContainer` a field and access its members to get the references? The purpose of `AssignorConfiguration` is to parse the configs, not to be a general container for the configured values. Otherwise, we wouldn't need to assign any of the other fields here.
I've seen alternative solutions floating around that use a configurable source here. Basically, the configuration passed to configure() is consulted to find the "source cluster", rather than looking at the topic name. That approach lets you return an actual source here, which obviates the new canTrackSource() method etc.
If a public API change like this is required, you will need to propose a small KIP. I'm unclear why it's required tho, and ideally we would not alter the existing API if possible. If a new method is required, I think "track" is too ambiguous and should not be used here.
`InternalProcessorContext` is already public interface but it's in `internals` package, so I figured it is okay? Anyways, this is not much blocking this PR, so feel free to merge it anyways and we can keep discussing here while you merge.
Thanks. I will make another pass now.
Wonder if we should consider adding max inflight behavior directly to `MockClient`. Seems like a notable difference from `NetworkClient`.
I'd suggest keeping the format slightly closer to what we had before. Specifically: `Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted`
I also like @kkonstantine's suggested format.
```suggestion assertFalse(appDir.exists()); ``` :)
```suggestion if (stateDir.exists() && !stateDir.delete()) { ``` Do we need to check `hasPersistentStores` here? It seems sufficient just to check if the directory exists.
Let's check that `previouslyAllocated`'s capacity is `batchSize` else `buffer.limit(sizeBytes)` is going to throw with a less useful stacktrace.
I vote yes for this. I think if we use this for writing snapshot from the state machine, then minimum size is a more interesting metrics for flushing to disk vs lingerMs. If we implement this so that either one has to be true then the client can set the `lingerMs` or `minSize` to MAX_VALUE if it wants to ignore those values.
Is this going to be used for non-testing code in the future? If it is only going to be for metrics purposes maybe we can allow it to be non thread-safe just to not blocking on other critical paths.
Could make this assertion a little stronger: ``` assertEquals(new TestElement(i, i), coll.find(elements.get(i))); ```
```suggestion ConfigDef.Importance.HIGH, ```
```suggestion ConfigDef.Type.LIST, ```
```suggestion ConfigDef.Importance.HIGH, ```
```suggestion ConfigDef.Type.LIST, ```
```suggestion ConfigDef.Importance.HIGH, ```
```suggestion ConfigDef.Type.LIST, ```
It includes the partition-level error. It seems to me it follows the docs. ` The number of each type of error in the response, including {@link Errors#NONE} and top-level errors as well as more specifically scoped errors (such as topic or partition-level errors). `
A metadata request has topics as the "scoped error" (one cannot request a metadata request for a given partition). So, it looks to me that we introduced a regression here. We should check if there are similar cases for other requests.
Maybe we shouldn't say "error" when it's not an error. I'm just imagining the mailing list questions that will start pouring in... ```suggestion log.info("Received version probing code {}", AssignorError.VERSION_PROBING); ```
While the code is cleaner, the error counts map (`errorCounts`) is re-computed 3 times now. Once in `hasCoordinatorMoved` and twice in `shouldRefreshCoordinator`. It would be better if we could get it once and reuse it.
Could we address this issue in separate PR? It seem to me this issue does not belong to "trivial cleanups" :)
@dongjinleekr Could you please take a look at this ```throws```? It was added by your PR (https://github.com/apache/kafka/pull/7083).
request "got" re-sent to the control
this line can be added to tail of above line
```{@link org.apache.kafka.common.KafkaFuture#get}``` => ```{@link org.apache.kafka.common.KafkaFuture#get()}```
@chia7712 Okay. Let me see. :smiley:
@chia7712 `InterruptedException` is not thrown anymore; This method does not call `KafkaFutureImpl#get` method anymore. It's safe to remove it.
I seem the right fix would be to add the missing `{@code }` annotation in L184? ``` <pre>{@code ``` The closing `}` is already in L190.
The pattern I had in mind was a little different. I was thinking something like this: ```java int localId = 0; int otherNodeId = 1; int epoch = 2; Set<Integer> voters = Utils.mkSet(localId, otherNodeId); RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters) .withVotedCandidate(epoch, otherNodeId) .build() ``` Then we don't have the awkwardness of the partial reliance on the static `LOCAL_ID`. I like this better because the ids have to be explicitly declared in each test case, which makes it easier to follow.
nit: it is a tad vexing to see all the `context` prefixes. I guess another option might be to define `RaftClientTestContext` as an abstract class so that the test method can define the test behavior within the scope of a subclass. For example: ```java new RaftClientTestContext(builder) { void run() { assertTrue(client.isShuttingDown()); ... } } ``` Not required, just an alternative to consider.
nit: we have assertions like this in many test cases. With a more direct api to update quorum state, we can move these assertions into that api.
nit: this indentation looks kind of funky
I'm somewhat inclined to add the local id to the builder rather than making it constant. It makes the builder a bit more self-contained. On a similar note, it would be nice to push the other static config values into the builder as well.
Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage). I am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the `Fetch` and `BeginQuorum` protocol and not from the state machine (Kafka Controller) itself. It is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.
> At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair? Sounds fair to create a Jira and revisit this later.
> I had considered this previously and decided to leave the fetches in purgatory while the election was in progress to prevent unnecessary retries since that is all the client can do while waiting for the outcome. On the other hand, some of the fetches in purgatory might be from other voters. It might be better to respond more quickly so that there are not any unnecessary election delays. I'd suggest we open a separate issue to consider this. Sounds good to create a Jira for this.
Yeah. I was thinking of the same thing, "hold the requests in purgatory". But like you said, maybe this optimization is not worth the added complexity.
Yeah. You want to force an epoch change in the case that the old leader stays leader and partially replicated data was lost. This would force followers to truncate to the new leader's log state.
That looks correct to me with the clarification that "in response to a request" has two cases: 1. The leader handles a fetch request. This implementation calls "update high watermark 2. The follower handle a fetch response. This implementation calls "update high watermark" I think that `pollListeners` should only fire a `Listener::handleCommit` for new listeners in `pendingListeners`.
Interesting that Java let's the outer class (`KafkaRaftClient`) access this private method. This makes reasoning about the concurrency non-trivial.
Hm ok this might be a problem. Since this is thrown from another catch block and not from the try block, it won't be caught by the catch block below and will slip through the exception handler.
```suggestion if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) { log.warn("Exception in global stream thread cause the application to attempt to shutdown." + " This action will succeed only if there is at least one StreamThread running on ths client"); } ``` This looked a bit off...
Oh you're totally right, sorry for letting my paranoia start spreading conspiracy theories here  Given all this I'd still claim that the FSM is in need to being cleaned up a bit (or a lot), but if you'd prefer to hold off on that until the add thread work then I'm all good here. Thanks for humoring me and explaining the state of things. I just wanted/want to make sure we don't overlook anything, since there's a lot going on. For example in the current code, if the global thread dies with the old handler still in use then we'll transition to ERROR. However the user still has to be responsible for closing the client themselves, and it will ultimately transition from ERROR to NOT_RUNNING. Whereas if we transition to ERROR as the result of a SHUTDOWN_APPLICATION error code, the user should NOT try to invoke close themselves, and the ERROR state will be terminal. That's pretty confusing eg for users who use a state listener and wait for the transition to ERROR to call close(). We should make sure that ERROR has the same semantics across the board by the end of all this work. Anyways I'm just thinking out loud here, to reiterate I'm perfectly happy to merge this as-is. But for reasons like the above, I think it's important to tackle the FSM in the next PR and make sure it all gets sorted out by the next AK release
I suspect the tests didn't catch this because we would still transition out of ERROR to PENDING_SHUTDOWN and finally NOT_RUNNING in this case. But really, we shouldn't transition to ERROR in the first place
Since this is at the end of runOnce, I'm wondering if it also makes sense to log whether we committed or punctuated, and whether/how many records we polled at the beginning of the method. Basically, it seems like, if it's a good idea to log some information once per cycle, then it's probably a good idea to summarize everything you'd want to know.
This line looks about the same as L665. I'm wondering if we really need the "else" in this case.
Is it important to make the distinction between "Processed zero records, going to poll again" and "Finished processing 0 records, going to poll again"? Also if we are not saying its processed anything maybe we should log what it is pulling from
We should probably change the exception class here since `ConfigException` takes a config name and value while this one is passing in the exception message and cause like other exception constructors. Then existing log entry would be sufficient as well.
@d8tltanc in your PR with the test adjustments, please reduce the priority of this. Either `LOW` or `MEDIUM`, we can discuss more in the PR.
nit: A better general pattern is to use `assertEquals` comparing against empty list. Then if the assertion fails, the message will show what was in the collection.
nit: Please use 4 instead of 8 spaces indentation.
This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.
nit: ```suggestion final KafkaMetric metric = metric("prefix-scan-rate"); ```
I do not think you need to put an entry if you use mocks.
This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.
To get rid of the test failure, you need to change this: ```suggestion final KafkaMetric metric = metrics.metric(new MetricName("prefix-scan-rate", STORE_LEVEL_GROUP, "", tags)); ``` Sorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.
Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.
We could probably use `ByteBufferOutputStream` which already handles expansion.
Maybe we can just a better name for `path` since it makes this code look suspicious.
It might be useful to review the history behind `Utils.atomicMoveWithFallback`. It's not clear to me why this case is different from some of the other situations that it is used.
the ```length``` is neglected
which method can throw ```InvalidStateStoreException``` in this case? It seems to me the potential methods are caught by ```assertThrows```
Why not handling the ```InvalidStateStoreException``` in the helper method ```until```
Understood. I think that we should revert this. I think that it makes sense to wait until we complete the migration of the remaining requests. We should have them pretty soon now.
I wonder if we should name the field `%sSizeInBytes`. I just looked at the result and having `"records":83` in the request log is not super clear to me.
Would something like the following work? ``` buffer.printf("_node.set(\"%sSizeInBytes\", new IntNode(%s.sizeInBytes()));%n", target.field().camelCaseName(), target.sourceVariable()); ```
I think that we can safely assume that when a request/response is serialized with `verbose` equals to `false`, we are not going to deserialize it. Therefore, I suggest to drop the handling of `verbose` on the read path.
Instead of changing the usage of this method everywhere in the code base, how about generating an overloaded method which call this one with `verbose=true`? I only expect this one to be used by the request logger at the moment so it is also more convenient.
Ouch! Sorry about that!
Good to know the subtle difference. "acknowledged" sounds good to me.
I think we have two options here. This class can contain either: ```java private final Map<Integer, VoterState> voterReplicaStates = new HashMap<>(); ``` where we change this expression to `boolean hasEndorsedLeader = grantingVoters.contains(voterId);` and remove the field `private final Set<Integer> grantingVoters = new HashSet<>();` or: ```java private final Set<Integer> voters = new HashSet<>(); private final Set<Integer> grantingVoters = new HashSet<>(); ``` and change a few of the methods here use these two sets instead of `voterReplicaStates`. I like the first option but it is up to you.
Just a nitpick. It doesn't need to be a object method as it doesn't use any of the instance fields. When reading and maintaining a lot of code it is good to know that a method doesn't use instance fields.
Yes, that's correct. You will have to change the implementation of `grantingVoters()` to something like: ```java public Set<Integer> endorsingVoters() { return voterReplicaStates .values() .stream() .filter(voter -> voter.hasEndorsedLeader) .map(voter -> voter.stateId) .collect(Collectors.toSet()); } ``` Notice the change in the method name. We can also change `nonEndorsingFollowers` to `nonEndorsingVoters` for consistency.
nit: we can make this a `static` function and rename it to something like `convertToVoters`.
Yeah, if you don't mind, it seems like a gap. Thanks!
@vamossagar12 Up to you I guess. I'm ok doing it all here since the changes seem pretty small.
I think the original KIP stated that the LeaderChange message would encode the set of voters that had voted for the leader. We thought this might be useful for debugging. Later on, we had a change of heart and decided it would just be the set of voters. Now I'm thinking it might be useful to have both. The log will always remember who the voters were at the time of the election and which voters had granted the leader's candidacy, which could be helpful in case of misconfigurations. For the set of voters which voted for the current leader, I think what we want is `CandidateState.grantingVoters`. However, by the time `onBecomeLeader` is fired, we have already dropped the `CandidateState`. One option is to carry `grantingVoters` over to `LeaderState`. We might also be able to pass it through `onBecomeLeader`. This will be easier if we get rid of the call to `onBecomeLeader` in `initialize()`. Following KAFKA-10527, it is not possible to initialize as a leader, so we could raise an exception instead.
Similar to `LeaderState::nonEndorsingFollower`, I think you want to add a method to `LeaderState` with the following signature `public Set<Integer> endorsingFollower()`.
```suggestion List<Class<?>> expectedClasses = expectedClassesFor(schema); ```
Strictly speaking, this shouldn't be necessary as `SCHEMA_TYPE_CLASSES` should have a `Schema` instance for all `Schema.Type` literals. And with `SchemaBuilder` a connector or converter cannot create a schema instance with a null `Schema.Type`. However, it is possible to construct a `ConnectSchema` instance with a null `Type` reference (like what `FakeSchema` essentially does in the existing test), which of course without this change would result in this method returning a null list. So +1 for this line change since it simplifies the error handling in the calling code.
Given that this is only used in one place, and the string that it creates is itself added to a formatted string, I think it'd be a bit more readable if we removed this method and inlined all the logic where it's being invoked right now.
A bit of a nitpick, but this operation is O(n) for LinkedList. Better to just `add(streamThread)` if you want to use LinkedList.
I think, it is better to keep the default initial capacity of an `ArrayList`. Otherwise, the first time a stream thread is added, we immediately run into a memory allocation. Since we do not know how many stream thread we might expect, let's use the default. We could also consider using a `LinkedList` since we never access by index in production code.
Please simplify to ```suggestion Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count())); ```
Currently only the constructor adds elements to the list. All other accesses afterwards are read-only. So, we would not need a synchronized list as far as I see. When we implement the add and remove streams thread APIs, we probably need synchronization. My proposal is to leave it a synchronized list now just in case we forget to think about it afterwards and then to reconsider how we synchronize the accesses.
I would prefer to use `List` instead of `ArrayList` to be more generic.
Wonder if it might be simpler to initialize `partitionsToRetry` from the request key set.
No strong opinion. It's an unlikely case anyway, so I'm not sure it calls for special treatment.
I hope we can get rid of those conversion in the future :)
```java if (tagged) { buffer.printf("int _sizeBeforeArray = _size.totalSize();%n"); } ```
```java if (tagged) { buffer.printf("int _sizeBeforeBytes = _size.totalSize();%n"); } ```
Would it be easier to do ``` if (!progressMadeThisIteration) { throw new TaskAssignmentException("Failed to compute number of partitions for all repartition topics"); } } while (numPartitionsNeeded); ``` (and don't have the `if (numPartitionsNeeded)` after the while loop) I guess both is correct and it might be a matter of taste. Also don't have a strong opinion about it. It's just an idea.
It might still be nice to see the stacktrace here (even if it also gets logged elsewhere). If you want to do it, don't forget you have to change to using `String.format` for the variable substitution. I don't feel strongly in this case, so I'll defer to you whether you want to do this or not.
I'm wondering if we should make this `info` or `warn` level. It doesn't seem like it would be very verbose, and it might be nice to see by default because it will have secondary effects later on when we try to start a new transaction, but get blocked. But I also don't feel strongly about it, so I leave it to your discretion.
Seems this duplicates `L733`. Might be good to extract into a small helper method.
Why does this need to be a `Long` instead of a `long`? The numerical value of the variable is only immutable if we use a `long` here.
IMO, the code would be easier navigable if you inline this method. Without the removed check, there is not really a reason to have a method here.
I see that this check was there before, but I actually think it is not needed because the configs are validated and there `CACHE_MAX_BYTES_BUFFERING_CONFIG` is specified as at least 0.
If this line is duplicated, it should go in a method. When I proposed to move it inline, I was apparently not aware that the same line was used somewhere else.
> LGTM? If this is a question, should it be LGTY? 
Well, `getCacheSizePerThread` would eventually return zero (with growing number of threads), what means that every put() into the cache would result in an immediate eviction. So I don't think we need to do anything for this corner case.
I think this can be a `final long` if we remove the check as I proposed below.
The read/write of ```closed``` must be in lock so this improvement is not acceptable to ```BufferPool```.
```suggestion UNKNOWN_REPLICA_STATE(107, "Replica state change only supports OfflineState, see ReplicaState.state ", UnknownReplicaStateException::new); ```
Please use a collection with at least two topics to test the loop over the collections.
Recently, we prefer to use `assertThat()` instead of `assertEquals()`.
If we swallow the exception here, and the test always throws an IO exception, we will never notice. I guess it would be better to use `fail()` with a message.
Same minor nitpick about whether or not we need to check for an empty group ID.
nit: why do we need to pass the base offset as a separate parameter? We are already passing `batch`.
Hmm... It seems a little inconsistent to use the offset of the first record. When the batch is empty, we use the base offset of the batch. Shouldn't we do the same here? Otherwise we will end up with some batches which have base offset smaller than the segment base offset. Note that the base offset is always preserved by compaction.
I believe that a call to `DeleteRecords` can advance the start offset to an arbitrary offset which could be in the middle of a batch. I do not think today that we have any hard guarantees that we won't return batches or even records that are lower than the current log start offset. It might be worth experimenting with this to clarify what the behavior is today. With that said, given the fact that we always return the whole batch anyway, my preference would probably be to try and keep the log start offset aligned on a batch boundary.
It seems like it would be more direct to check `baseOffsetOfFirstBatch < 0`.
nit: this looks misaligned
As above. Should we `TimestampedWindowStore` for this class.
Can we also rename `StreamsGraphNode` to `GraphNode`? The `Streams` prefix is a bit confusing, IMO, because `StreamSourceNode` and `StreamsGraphNode` seem really similar although they are quite different.
nit: fix indention (same below in other constructor)
I like your changes. What I meant is that we could change the constructor of `SourceGraphNode` to: ``` public SourceGraphNode(final String nodeName, final Set<String> topicNames, final ConsumedInternal<K, V> consumedInternal) ``` and the one of `StreamSourceNode` to: ``` public StreamSourceNode(final String nodeName, final Set<String> topicNames, final ConsumedInternal<K, V> consumedInternal) ``` In this way, we have a set of topics as soon as possible in the code path from the public API. I think this makes it clearer that it is not possible to have duplicates of topics internally. To keep this PR small, I would propose to just do the changes for `SourceGraphNode`, and do the other changes in a separate PR.
Here you should just need a queue as for `clientLevelMetrics`. We need a map for the other levels because there can be multiple objects for each level, e.g., there might be multiple stream thread and each one manages its sensors under a key in the map. However, there is only one client on client level.
Unit tests for this method are missing.
I am fine with consistency and clean-up, but I would like to have the clean-up in a separate PR.
Although, we use this in other methods, I think the following is a bit simpler to read: ```suggestion final Sensor sensor = metrics.getSensor(fullSensorName); if (sensor == null) { clientLevelSensors.push(fullSensorName); return metrics.sensor(fullSensorName, recordingLevel, parents); } return sensor; ```
You can inline the value of variable `key` here and remove `key`. ```suggestion final String fullSensorName = CLIENT_LEVEL_GROUP + SENSOR_NAME_DELIMITER + sensorName; ```
This should be: ```suggestion final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1); ```
```suggestion resizeThreadCache(threads.size() + 1); ```
Can we also assert that the state gets to `RUNNING` after the new thread has joined
Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup. On a second thought, we are probably not 100% safe because if a transition from `NOT_RUNNING` to `RUNNING` is added (or any other transition that goes from the above mentioned states to `RUNNING` or `REBALANCING`), we would still not do the clean up.
> Unfortunately I don't think we can shutdown a thread until we have started it. Have a look at https://github.com/apache/kafka/blob/aeeb7b2f9a9abe8f49543a2278757722e5974cb3/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L976-L983
nit: If it happens that you need to push another commit, could you fix the indentation here? Sorry that I haven't noticed this before.
Assume the following thread list [t2, t3, t4], `threadIdx` would be 4, which is already there. You should keep the currently used `threadIdx`s and check those to decide on the next `threadIdx`.
nit. remove unnecessary `this.`
```suggestion synchronized (stateLock) { if (isRunningOrRebalancing()) { streamThread.start(); return Optional.of(streamThread.getName()); } else { return Optional.empty(); } } ```
We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.
Please adjust indentation: ```suggestion mkMap( mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()), mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2), mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class), mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class) ) ```
IMO, `createStreamThread()` would describe the behavior better.
I think it would be cleaner to pass `cacheSizePerThread` to `resizeThreadCache()` instead of the number of stream threads. We would then just call `getCacheSizePerThread()` once instead of once in `addStreamThread()` and once in `resizeThreadCache()`. We would also just need to compute `threads.size() + 1` once.
Add missing `<p>` tag
I see. So we exploit that possible state transitions are limited. Thanks for explaining. Makes sense.
That is not what I meant. But it might not matter much anyway. While we need to loop over all used names in L951 below to reuse, we don't need to compute `names` from scratch but would just modify `names` each time we add/remove a thread. But it's not perf-critical so re-doing the computation is fine, too.
Maybe we had some cyclic dependency at some point in the past? Not sure.
Ah. Was missing that as you assign `handler` that is not a `StreamsUncaughtExceptionHandler` but a `Consumer<Throwable>`...
What about checking for the state and do the clean-up only if the state is not `PENDING_SHUTDOWN` and not `ERROR` and not `NOT_RUNNING`? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.
It would be good to verify that all versions are tested in `testSerialization`.
I think it would be simpler to just log the current `leaveReason` right here at the warn level, and then pass in a more brief description to `maybeLeaveGroup` rather than add a flag to that method just for this one case
I think it may be more useful to describe the cases where it will _not_ send a LeaveGroup and describe what this actually means (also it should have been 'and' not 'or' in the original): ```suggestion * Sends LeaveGroupRequest and logs the {@code leaveReason}, unless this member is using * static membership or is already not part of the group (ie does not have a valid member id, * is in the UNJOINED state, or the coordinator is unknown). ```
The only invocation of `WorkerGroupMember#maybeLeaveGroup` in fact already does log a warning as to why instead of relying on `maybeLeaveGroup` to do so. Imo we should do something similar for the "consumer poll timeout has expired" case
'final' is unnecessary here :)
Yes, Both the old handler test and the close client should have 2 threads. We need to ensure that after a rebalance the old handler has attempted the process the record twice and the client shutdown only once. We can not be sure of that with only one thread.
I think this was actually correct as it was (and ditto for the above). One alternative suggestion: ```suggestion * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread} ```
The order is not really that important here, either way works
Hey @wcarlson5 , can you take a look at this? If we change the default number of threads to 1 will we be reducing test coverage or not testing the correct thing anymore? FWIW I think for tests where the number of threads doesn't matter, we should default to 1. But I'm not sure which tests do/do not rely on using multiple stream threads
Not for this patch, but all of this boilerplate we need to build the topic groupings gets annoying. It is such a common case that it might be worth writing a special type that lets the parser construct `Map<TopicPartition, Data>` directly since that is really what the code wants. Alternatively, maybe we could flatten the schemas and introduce compression.
In the case of the error code, I think it might be better to be explicit.
Ah, you are right: `valueOf()` `Throws: IllegalArgumentException - if the specified enum type has no constant with the specified name, or the specified class object does not represent an enum type.` Thanks for catching/testing for/fixing it.
Could we reuse ```mechanismName``` (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/ScramMechanism.java#L79) so we don't need this duplicate replacement.
It's not a bid deal, but I found the naming here a bit confusing, since the `nonExistingSourceNode` is clearly added to the topology down on line 223, so it definitely exists. But I'm not sure what a better name would be...msybe `removedSourceNode`? Idk
It seems to me ```putIfAbsent``` is more readable than ```containsKey + setProperty```.
Is ```JoinGroupResponseHandler``` a better place to log error? For example, the error ```UNKNOWN_MEMBER_ID``` is log twice. (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L605)
nit: I think it's better to just print the e.message in a single line.
The main reason for https://github.com/apache/kafka/pull/7312/files#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832R230-R234 is the following: * inside ensureCoordinatorReady called by the main thread, we may break out of the loop at line 248 below, without knowing what's the final state of the future. * and that future could be completed by the other thread (hb) later, and replaced by a new `future` object. In that case, when the main thread calls ensureCoordinatorReady again, it will "miss" the previous future's contained fatal error. So thinking about it again, I think we would still want to maintain the exception but only if it is a fatal one inside the handler (i.e. we do not probably need to register another listener just to bookkeep that exception, but just piggy-back this logic inside the handler listener directly), and then inside the while loop, we check if a previous future already gets a fatal exception and if yes, throw it to fail the whole client.
nit: extra space.
I think this is a better approach, but we need to be careful about the callee inside hb thread: ``` if (findCoordinatorFuture != null || lookupCoordinator().failed()) ``` i.e. a hb thread sending a discover-coordinator request would also cause a future to be assigned, but that future would only be cleared by the main thread caller. Thinking about that for a sec I think this is okay, but maybe worth having a second pair of eyes over it.
if you just have ``` log.debug("FindCoordinator request failed due to {}", e) ``` Then e.toString would be called which would usually be `e.name(): e.getMessage()`.
That makes sense, we can `clearFindCoordinatorFuture` inside the hb thread as well.
The nested condition is a bit awkward, how about this: ``` if (findCoordinatorFuture != null) { // if it has failed, clear it so that hb thread can try discover again in the next loop in case main thread is busy if (findCoordinatorFuture.failed()) { clearFindCoordinatorFuture(); } // backoff properly AbstractCoordinator.this.wait(rebalanceConfig.retryBackoffMs); } else { lookupCoordinator(); } ```
I think the issue is spot-on! The logic here becomes a bit hard to understand for other readers now and I'd suggest update the cmment as: "Clear the future so that after the backoff in the next iteration, if hb still sees coordinator unknown it will try re-discover the coordinator in case the main thread cannot" Otherwise, LGTM.
I'm guessing the root source of this all is a bad assumption that the assignment would be stable if a stable `CLIENT_ID` was used? I remember we discussed that back when you first wrote this test, I'm sorry for any misinformation I supplied based on my own assumption about how the CLIENT_ID would be used :/
Thanks for cleaning up the variable names 
Yes, I think so
I think "failed second client" is correct. It's the 2nd client, which has failed, not the 2nd client to have failed (English is confusing  )
@ableegoldman is it related to the UUID randomness? If yes please ignore my other question above.
Nice catch. Reminds me though, why the second rebalance may not be deterministic in migrating tasks back? I thought our algorithm should produce deterministic results? cc @ableegoldman
Was just thinking about how long a. transaction might possibly be open. 1 minute SGTM
Could surround this call with new lines as you did for the others? Makes the calls under test more visible.
You verify the wrong name here. ```suggestion assertThat(name2, CoreMatchers.not(Optional.empty())); ```
Here you should test if the stream thread has the name of the stream thread that was removed before.
This is not a guarantee that we give in the KIP. Assuming that always the first stream thread is removed is too strict for this test.
IMO, this string should give details when the condition is not met, like `stream thread has not been added`. Same applies to the other wait conditions.
nit: Since you imported statically also the other matchers, you could also statically import this one to be consistent.
Could you please add some line breaks? This and some of the other verifications are too long.
Please remove empty line.
You should also verify the return value of `removeStreamThread()` here.
Ah, sorry I didn't think of this/mention it before, but I think we actually need to wait for a _transition_ to RUNNING, and not just for it to be in the state itself. It probably takes a little while after removing a thread for the rebalance to occur, so it's probably already in RUNNING. Pretty sure there's some other integration test util that watches for the REBALANCING -> RUNNING transition, though
The number of elements is not always 1. Each created thread-level sensor is added to this queue, e.g., `processLatencySensor`, `pollRecordsSensor`, etc. Check out the callers of `threadLevelSensor()`. Each queue contains all thread-level sensors for one single stream thread.
I think you can also drag this line into `getSensor()`. Sorry for not noticing it before.
Ah, now I got it! Sorry! Makes sense! In that case we can reuse `REPLACE_THREAD` also for the global stream thread. Forgot about that!
I actually would be in favor of calling the enum value `REPLACE_STREAM_THREAD`. A stream thread is a stream thread and a global stream thread is a global stream thread. I am aware that the KIP calls the enum value differently, but we also have a config that is called 'NUM_STREAM_THREADS_CONFIG' and we have also 'addStreamThread()' and `removeStreamThread()`. So I guess, the name to the outside of this is stream thread and not thread. We have also other threads in Kafka Streams like the state directory cleaner thread and the RocksDB metrics recording thread.
The point of this test should be that a global stream thread is not replaced but the client is shutdown instead. Hence, the uncaught exception handler should return `REPLACE_THREAD`, not `SHUTDOWN_CLIENT`.
I would rename it to `REPLACE_STREAM_THREAD`.
Please rename to something like `shouldShutDownClientIfGlobalStreamThreadWantsToReplaceThread()`.
I think it would be better to have a test that shows that a new thread that replaced a failed one, actually is able to process records. So, I would let the new thread process some records and then shutdown the client with a normal close. Maybe similar applies to the shutdown tests. First let the client/application process some records and then throw an exception that shuts down the client/application. I guess, this last paragraph is something for a separate PR.
I think it would be cleaner to extract this code to a separate method.
Do we need to shutdown the dead stream thread? `completeShutDown()` will be called anyways.
Please rename to `replaceStreamThread()`.
nit: "can not" -> "cannot", same below
A test is missing for a global stream thread that calls the uncaught exception handler.
It seems ```exitProcedure``` and ```haltProcedure``` can be local variable
Could we remove those local variables? For example: ```java resetToDatetime(client, inputTopicPartitions, Instant.now().minus(duration).toEpochMilli()); ```
It seems to me ```IllegalStateException``` is more suitable for this case. Also, please update docs of ```flush()```
```If the thread is callback thread. Calling flush() from callback is not allowed since it makes deadlock.```
```Calling flush() from callback is not allowed since it makes deadlock.```
Could you add a flag after ```producer.flush()```? We should make sure ```producer.flush()``` fails.
I don't think this is valid since this callback is triggered by another thread. The assert failure can't be aware by test thread.
this local variable is redundant
Pardon me, why this change is required.
Maybe there is an easier way, but I found the following: Set the config to: ``` final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.singletonMap("test", "property")); ``` and then check it: ``` internalTopologyBuilder.buildSubtopology(0); assertThat(internalTopologyBuilder.stateStores().get("store-this-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.stateStores().get("store-other-join-store").loggingEnabled(), equalTo(true)); assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); for (final InternalTopicConfig config : internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.values()) { assertThat( config.getProperties(Collections.emptyMap(), 0).get("test"), equalTo("property") ); } ``` Without ``` assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.size(), equalTo(2)); ``` the test would pass without checking the config if `buildSubtopology()` is not called because no changelog topics would be registered in the topology. So it basically checks that `buildSubtopology()` is called.
I think you meant this here: ```suggestion new HashMap<>() ```
Suggestion: ```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingDisabled(); ```
```suggestion left.join( right, (value1, value2) -> value1 + value2, joinWindows, streamJoined ); ```
```suggestion left.join( right, (value1, value2) -> value1 + value2, joinWindows, streamJoined ); ```
```suggestion final StreamJoined<String, Integer, Integer> streamJoined = StreamJoined .with(Serdes.String(), Serdes.Integer(), Serdes.Integer()) .withStoreName("store") .withLoggingEnabled(Collections.emptyMap()); ```
How about making ```SizeDelimitedSend``` be a static method in ```ByteBufferSend```? For example: ```java public static Send withSizeDelimited(ByteBuffer buffer) { ByteBuffer sizeBuffer = ByteBuffer.allocate(4); sizeBuffer.putInt(0, buffer.remaining()); return new ByteBufferSend(sizeBuffer, buffer); } ```
nit: maybe we split 50 to 20/30 to avoid some reading difficulty? :)
I think, it would be good to verify that a second call to `peekNextKey()` right after the first call to `peekNextKey()` returns the same value, since this is the main difference between `next()` and `peekNextKey()`.
Please remove empty lines here and in the other test methods.
```suggestion final Bytes toBytes = Bytes.wrap(to.getBytes()); ``` Please also check other occurrences of this additional space.
```suggestion final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key3Bytes, true ); ``` Please also fix the other wrong indentations.
You usually do not want to mock the class under test, because you want to test it. Also partial mocks should only be used if absolutely necessary. A rule of thumb is if a partial mock is needed then most probably the design has a flaw. In this specific case, you should mock RocksDB's iterator. For the class under test, you should test `hasNext()`, `next()`, `peekNextKey()` and `close()`, because those are the one exposed (`makeNext()` should actually de declared as `protected`, IMO). ```suggestion final String key1 = "a"; final String key2 = "b"; final String key3 = "c"; final String key4 = "d"; final String value = "value"; final Bytes key1Bytes = Bytes.wrap(key1.getBytes()); final Bytes key2Bytes = Bytes.wrap(key2.getBytes()); final Bytes key3Bytes = Bytes.wrap(key3.getBytes()); final Bytes key4Bytes = Bytes.wrap(key4.getBytes()); final byte[] valueBytes = value.getBytes(); final RocksIterator rocksIterator = mock(RocksIterator.class); rocksIterator.seek(key1Bytes.get()); expect(rocksIterator.isValid()) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(true) .andReturn(false); expect(rocksIterator.key()) .andReturn(key1Bytes.get()) .andReturn(key2Bytes.get()) .andReturn(key3Bytes.get()) .andReturn(key4Bytes.get()); expect(rocksIterator.value()).andReturn(valueBytes).times(4); rocksIterator.next(); expectLastCall().times(4); replay(rocksIterator); final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( STORE_NAME, rocksIterator, Collections.emptySet(), key1Bytes, key4Bytes, true ); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key1Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key2Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key3Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key4Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(false)); verify(rocksIterator); ```
```suggestion expect(rocksIterator.isValid()).andReturn(false); ```
Please rename to `shouldCloseIterator()`.
If you change the `toInclusive` to `true` in this test, the test also passes. That means, that you are not testing `toInclusive` in this test. I suggest the following test: ``` public void shouldExcludeEndOfRange() { final RocksIterator rocksIterator = mock(RocksIterator.class); rocksIterator.seek(key1Bytes.get()); expect(rocksIterator.isValid()) .andReturn(true) .andReturn(true); expect(rocksIterator.key()) .andReturn(key1Bytes.get()) .andReturn(key2Bytes.get()); expect(rocksIterator.value()).andReturn(valueBytes).times(2); rocksIterator.next(); expectLastCall().times(2); replay(rocksIterator); final RocksDBRangeIterator rocksDBRangeIterator = new RocksDBRangeIterator( storeName, rocksIterator, Collections.emptySet(), key1Bytes, key2Bytes, true, false ); assertThat(rocksDBRangeIterator.hasNext(), is(true)); assertThat(rocksDBRangeIterator.next().key, is(key1Bytes)); assertThat(rocksDBRangeIterator.hasNext(), is(false)); verify(rocksIterator); } ```
I see. You are talking about the broker side log cleaner thread -- not the local state directory cleaner thread. (windowed stores also use the concept of segments, not just topic-partitions). However, it seems we want to extract this fix into a separate PR to be able to cherry-pick it to older branches? Seems to be unrelated to this PR.
nit: this alignment might be a bit off. We can follow the pattern of 2 tabs with first and last line on their own if that looks better I guess.
That's right 
I believe PassThrough is only used for the cogroup but now I think you can remove it completely.
I think these lines are too long. The ones in the other methods too
Ok. then LGTM
I noticed that `queryableName` is a different parameter than the one we actually build the parent processors with (`storeBuilder.name()`). It wouldn't surprise me if there's a subtle difference between them.
Yes, that sounds like the right thing to do. Thanks!
> Does this only use the separator if there are more than one element? yep
This variable should be declare `volatile` because the callback is executed on a different thread.
nit: missing space between `or` (line above) and `there`
Seems this `fail` did not work as expected? Otherwise the test would have failed all the time? Maybe we should rather set a boolean flag that we evaluate outside of the callback to let the test fail? Also, we have one run with zero exceptions and one run with 2 exception (one exception type each) -- not 4. Thus, we need to handle this differently for the error-injection and the "clean run" differently depending on the boolean test flag.
Or we make each test create task instead of creating task in ```setup```
@dajac Nice question. It is related to #7409 and the replacement is https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java#L80
```suggestion * @deprecated Since 3.0.0; use {@link MockProcessorContext#setRecordTimestamp(long)} instead. ```
This class is public API, so we cannot remove `setTimestamp` but can only deprecate it. We also need to update the KIP to mention the deprecation and the newly added methods.
That alternative looks good to me :)
It seems to me this initialization can be rewrite by following code. ``` private static final Map<Integer, ApiKeys> ID_TO_TYPE = Arrays.stream(ApiKeys.values()) .collect(Collectors.toMap(key -> (int) key.id, Function.identity())); ``` The benefit is that he static block can be removed.
> it doesn't seem particularly beneficial It seems to me the benefit is the error happens from runtime/test_runtime to build time (generate message code).
It seems to me this check should be moved to ```ApiMessageTypeGenerator``` as it is a generated code from json.
It seems to me previous serialization does not carry version field so we have to use this ugly code to handle different version.
> 1. Inside append, while we are already holding the lock, we can check if the accumulated bytes (including completed and currentBatch) have reached minFlushSize. If so, we can call completeCurrentBatch to ensure that completed holds all the data that needs to be drained. > 2. Inside timeUntilDrain, if the linger timer hasn't been reached, we can iterate completed and check if there are enough bytes to flush. Then we don't need to acquire the lock unless we need to drain. I like this suggestion.
In the snapshot tests, we can set the `maxUnflushedBytes` to the same value as `maxBatchSize`.
@vamossagar12 and @hachikuji How about we call this `append.linger.bytes`? Excuse the back and forth on this name put let me explain. In the current implementation this does determine when the bytes will be flush because the `KafkaRaftClient` flushes the `Log` every time it drains the `BatchAccumulator` and it appends to the log. https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java#L1871-L1879 In the past, we have talked about delaying the flush/fsync to the log. I believe that the invariant that we need to satisfied is that the leader cannot increase the high-watermark past the flushed offsets. Followers need to flush/fsync before sending a Fetch request since the leader assumes that the followers have safely replicated the offset in the Fetch request, If in the future we implement this logic or something similar, I think the name `append.max.unflushed.bytes` would not be accurate since it contains the word "unflushed". And the leader may decide to have more than `append.max.unflushed.bytes` bytes unflushed.
Outside the scope of this PR but how about changing this description to: > The duration in milliseconds that the leader will wait for writes to accumulate before appending to the topic partition.
```suggestion public static final String QUORUM_APPEND_MAX_UNFLUSHED_BYTES_DOC = "The maximum number of bytes that the leader " + "will allow to be accumulated before appending to the topic partition."; ```
If we decide to keep this, how about we call it `quorum.append.max.unflushed.bytes` or something like that? Basically it's the maximum number of bytes that the raft implementation is allowed to accumulate before forcing an fsync.
Batching has a lot of implications in the system. The flush behavior is just one. It also impacts the segment size and the ability to do down-conversion efficiently since we have to read the whole batch into memory. Basically any time we need to do anything in memory with the batches, we have to allocate at least enough memory to hold the largest batch. The controller is designed to write small messages, so I do not think 1MB (say) would be much of a constraint. For example, we have avoided storing assignment state in a single message as we did with Zookeeper. We can reconsider it if ever needed, but I'd like to keep batches relatively small if possible.
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
No `else` needed since we used `return` for both other cases. For the exception, I think we can just throw `ClassCastException` since `IllegalStateException` doesn't fit very well for this case. I would also make the message a bit more generic to avoid it going stale when we add more `Records` subtypes. For example: ```java "The record type is " + partition.records().getClass().getSimpleName() + ", which is not a subtype of " + Records.class.getSimpleName() + ". This method is only safe to call if the `FetchResponse` was deserialized from bytes."
Suggestion: ```text Returns `partition.records` as `Records` (instead of `BaseRecords`). If `records` is `null`, returns `MemoryRecords.EMPTY`. If this response was deserialized after a fetch, this method should never fail. An example where this would fail is a down-converted response (e.g. LazyDownConversionRecords) on the broker (before it's serialized and sent on the wire). ``` ```
There is one place in this PR that we check for null when computing the records size, maybe we can use this utility function there.
Does this ever fail? If so, it would be good to explain under which conditions it can fail. Also "This is used to eliminate duplicate code of type casting." seems a bit redundant.
Can we update this not to use `responseData`? Then we at least have the right behavior for the broker and we can fix the clients in the subsequent PR.
Yeah, you can use a volatile field and synchronize on the assignment if still null. And then file a separate Jira to remove it from the class in a separate PR.
It seems like we do it for a couple of cases. It doesn't seem worth it given that fetch is one of the hot paths. We could have static helper methods instead if helpful.
I think I would move the method to a test utility so that tests can use that instead.
I think it would be better to make this a static factory method and keep the constructor for the case where we receive `FetchResponseData`.
```throws Exception ``` is useless.
It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in `RebalanceSourceConnectorsIntegrationTest` or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.
We should deprecate this one too I believe.
nit: extra newline here
Also mention that this returns by topic name if the request used topic names. otherwise returns null.
We need to keep the Admin API backwards compatible. An application that was written using the 2.7.0 should not break if it is compiled with a 2.8.0 clients jar. You can always add an internal class with shared code to avoid duplication, but the public API itself needs to remain compatible.
I'm also curious what implications there are for changing the original DescribeTopicsResult when using old clients.
I think this parameterization is a pretty good idea, and I can add it to my delete topics PR. But if we are going to change the public API, we should update the KIP and potentially update the mailing list with the changes.
Since this is a protected method in a public API, we should probably keep the method and deprecate. It can just invoke the method below.
`LATEST_27` musts be imported above.
You are right. Checking ```Error reading field 'field2'``` should be enough for this test case.
nit: could you try to deduplicate code here and in the other unit tests? Here for example, you could have one method like this: ``` private void shouldThrowIfNoPeekNextKey(final Supplier<MemoryLRUCacheBytesIterator> methodUnderTest) { final ThreadCache.MemoryLRUCacheBytesIterator iterator = methodUnderTest.get(); assertThrows(NoSuchElementException.class, iterator::peekNextKey); } ``` and then two public tests ``` @Test public void shouldThrowIfNoPeekNextKeyRange() { final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics())); shouldThrowIfNoPeekNextKey(() -> cache.range(namespace, Bytes.wrap(new byte[]{0}), Bytes.wrap(new byte[]{1}))); } @Test public void shouldThrowIfNoPeekNextKeyReverseRange() { final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics())); shouldThrowIfNoPeekNextKey(() -> cache.reverseRange(namespace, Bytes.wrap(new byte[]{0}), Bytes.wrap(new byte[]{1}))); } ``` Admittedly, in this specific case, we would not win much but for other unit tests in this test class it may be worth. Try and then decide if it is worth or not.
I know. It's just that we already use a mocking framework and we could use something like: `EasyMock.expect(factory.apply(EasyMock.anyObject())).andReturn(mockTopicAdmin).anyTimes();` if we also defined `factory` to be a mock as well. That could allow us to evaluate expectations on the mock more accurately (e.g. with a capture if we had to). But sure, if we need something quick and easy we can go with that. It's just that I noticed a mixed use of mocks with this variable that simulates what the mocking framework offers already.
Is there a specific action on the mock we wish or can verify here instead of implicitly using a aux variable for that? Replay, expectation and verify should help us verify the action or its absence. I'd have to check closer what such action could be, if there's any. Maybe you can see that more easily.
What is the reason for having `assertDoesNotThrow` here and below? The test will fail if an exception is thrown, so seems like unnecessary noise.
```suggestion "<li><code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>: Assigns partitions to consumers in a round-robin fashion.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.StickyAssignor</code>: Guarantees an assignment that is " + "maximally balanced while preserving as many existing partition assignments as possible.</li>" + ```
```suggestion "<li><code>org.apache.kafka.clients.consumer.CooperativeStickyAssignor</code>: Follows the same StickyAssignor " + ```
I guess it's kind of a confusing error to see. The case on the broker is when the write to the log failed because of a timeout. I wonder if it would be useful to suggest the cause in the message. For example: > JoinGroup failed with a REBALANCE_IN_PROGRESS error, which could indicate a replication timeout on the broker. Will retry.
If you change the line above, then let's change this one too and fix the asymmetry
nit: since we are setting auto commit interval, perhaps we should set enable auto commit explicitly rather than rely on the default
while the verifying consumer itself is not part of the transaction
s/Consumer/The verifying consumer
The change makes sense to me. I don't think anything would stop the auto-commits from going through. Even if there was such a mechanism, it seems better to explicitly disable it.
`doWork` is just one iteration. `ShutdownableThread` has the loop. I'm ok with the change, but we probably will need to copy over some of the shutdown logic.
Maybe we could have a variable called `previousTopic` to make this code more readable.
It means "if and only if".
nit: `bump` -> `bumped`
Much better name :)
No worries, let's keep the scope small for now. Just wanted to raise the question
Would it be cleaner to just close/remove the task producer inside `tryCloseCleanAllActiveTasks`? Could we just close/remove the task producers before closing clean or do we specifically need to close them first? I don't remember...
I'm +1 on demoting these log entries! :)
Actually on a second thought.. if users configures `-1` it means they probably do not care about enforced processing, while on the other side the INFO entry may flood the logs here. So NVM.
Should we log INFO if we are indeed enforcing processing? I.e. there are some empty partitions.
Is it more convienent to pass in the `log` object from AbstractTask to the PartitionGroup constructor? It is created with the logContext including the task-type / task-id.
nit: line 365 above can be moved down now since it is only needed before line 379.
This test case is duplicate to https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/common/internals/TopicTest.java#L65. I feel it is fine to remove it.
This change may be overkill as this is not an inner class.
That's right, I don't think force quit uses the interrupt path. But anyways I don't think it's possible to force quit a Kafka Streams application until we get around to making it a desktop application. I was just trying to draw a parallel -- my concern here is just that there is absolutely no way to recover should a StreamThread hang during shutdown, meaning you will just need to kill the process and do an unclean shutdown. With ALOS it's maybe not so bad, but with EOS an unclean shutdown requires wiping out all state stores and completely restoring them from scratch. So there is absolutely something to lose besides just the annoyance of having to manually kill the Streams process. I just got a notification from a report of Streams being stuck during shutdown in an EOS application, so I promise I'm not making this up  . I can point you to it if you'd like -- unfortunately we never even managed to determine the root cause, so there may be more bugs out there which cause this besides just the ones we currently know
Might be helpful to have a short description here (or somewhere else if you can think of a better location) which explains that this type is reserved for the controller record types.
`localIdOrSentinel` would be more accurate, I think.
Fair enough, we could relax later when we experiment out the static quorum changes.
Yeah, that would be the closest for primitive types.
Maybe we do this else where but can we log at DEBUG if there is nothing done and there is tasks assigned to the thread? It might be good to be able to confirm that its not getting stuck here or its actually not polling anything.
The above probably doesn't need to be applied here as well, i don't think
I have a similar question here, other than that LGTM
Nit: you can remove `value =`
Not sure what EE is here.
Not a big fan of variable shadowing, which below you avoid by calling the local variable `tCompletableFuture`. Probably good idea to apply the naming here to and remove `this.`
How about defining two helper methods, one for each cases? * `private void maybeRewrapAndThrow(ExecutionException exception)`; and * `private void maybeRewrapAndThrow(CompletionException exception)`
the fact that two variables are used here must be a remainder of an earlier iteration. ```suggestion KafkaCompletableFuture<U> result; ``` can be declared here at the top level and we can remove the inner declaration.
I believe just returning from the catch branch is fine. No need for this variable. (you return from any path)
nit: Would it make sense to move `throw e` into `maybeRewrapAndThrow` to let `maybeRewrapAndThrow` throw in both cases? More generally, I wonder if we could handle all the case in `maybeRewrapAndThrow` and use it everywhere.
nit: I would rather use the full name instead of using acronyms.
nit: We could omit `res` and return directly in the two places below.
the naming used above seems better here ```suggestion Throwable exception = null; ```
I don't like the fact that we throw in two places... Could we at least make the name of `maybeRewrapAndThrow` a bit more explicit? It is only about `CancellationException` in the end so we could name it `maybeThrowCancellationException` or something like this. Moreover, the method does not really "rewrap" anything, right? It just checks the type and throw it.
> A new method also means their code is then not binary compatible with older client versions. yep, the BC could be broken. What I really care is the source compatibility. It seems to me a public API should be source compatible to next major release :) This patch is good to go and we need more time (rather than vespene gas ... StarCraft is my favor game :) ) to reach consensus.
"do nothing" is probably the right thing here.
> But this wasn't described in the KIP and wouldn't be a source compatible change (existing code with a catch (InterruptedException) As it can cause compatible change, we should deprecate it and then add a new method (for example: get(T value)) to replace it. This can be discussed in another issue :)
Do we still need `synchronized`? It seems to me `CompletableFuture` can deal with thread issue.
typo: CompleteableFuture -> CompletableFuture
typo: CompleteableFuture -> CompletableFuture
Good question. AK 3.0 is a good opportunity to do such breaking changes so I would be in favour of doing it. Let's see what other think.
nit: the typical thing to do is invoke the other constructor. e.g. `this(props, true)`
I think we can avoid this pattern if we refactor `parseVoterConnections` to use a new method called `parseVoterConnection` that knows how to convert a `String` to a `Node`.
What do you think of combining these two checks to one and call it `waitForTransitionFromRebalancingToRunning()`. They are always used together.
It seems to me the method ```waitForTransitionFromRebalancingToRunning``` can do the assert as well because we always call ```assertThat(waitForTransitionFromRebalancingToRunning(), is(true)``` in this test.
This is unnecessary as junit always create a new test class for each test case.
Couldn't we simply wait for the current state to become `RUNNING`? ```suggestion private void waitForRunning() throws Exception { waitForCondition( () -> kafkaStreams.state() == KafkaStreams.State.RUNNING, DEFAULT_DURATION.toMillis(), () -> String.format("Client did not transit to state %s in %d seconds", expected, DEFAULT_DURATION.toMillis() / 1000) ); } ```
nit: just to better visually separate condition from `if`-block ```suggestion if (historySize >= 2 && stateTransitionHistory.get(historySize - 2).equals(before) && stateTransitionHistory.get(historySize - 1).equals(after)) { return true; } ```
We normally use `assertThat()` in new and refactored code. Please also change the other occurrences. ```suggestion assertThat(hasStateTransition(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING), is(true)); ```
I think it would be better to wait until the Kafka Streams client id in state `RUNNING` and then verify if the history of the states transitions after adding the stream thread is first `REBALANCING` and then `RUNNING`. Currently, the order is not verified as far as I can see.
I think `stateHistory` or `stateTransitionHistory` would be a more meaningful name for this variable.
this one need to be migrated as well.
It would be nice to force on migration so as to avoid big patch :)
Are we sure this should not be an error? Should we not rethrow the exception? After all it may make data readable to the outside world. I do not request a change, I just wanted to put it up for discussions.
This seems fine at the moment since this isn't particularly performance critical. For the longer term if we have many of these downgrades, we could consider using ApiVersions in the underlying NetworkClient to make the decision before sending the request.
nit: `equals` => `equal`
However, the ```@RecordBuilderSource(haveInvalidCompress = true)``` is a inscrutable to me :(
What we have here is not exactly what I had intended. I have to take a closer look, but just wanted to share that.
BTW, ```testAppendedChecksumConsistency``` need to test with zstd. The "magic array" should be changed to magic_2 when input is zstd.
this test case should test zstd + magic_2. We can changes the magic code according to compression type.
My point was ```haveInvalidCompress``` is hard to understand which compression is invalid. In current test cases, only zstd is possible to be excluded, right? If so, the name like "NotZstd" is more readable to me.
Why do we have `Args` here? The `toString` is used in the JUnit test display and hence why it doesn't include anything besides the parameters before this change.
Ah, yes, the magic is hardcoded here.
Guess we can keep it, but this helper doesn't seem to be doing much for us anymore.
Is ```assumeTrue``` used in this test? I test ```RaftEventSimulationTest`` but there is no ignored test cases.
please revert those changes as first argument should be "expected" value.
This change is unnecessary.
the fist argument is still "expected value" so we don't need to switch the args. There are many similar occurrence.
fair enough :)
Also, it should make sure the input arguments (from ```ProducerRecord```) are equals to getter of ```TestRecord```. For example: ```java assertEquals(expectedRecord.getHeaders(), producerRecord.headers()) ```
Could we separate this assert by ```assertEquals```? It can produce more meaningful error message.
nit: use `andStubReturn` instead of `andReturn().anyTimes`. No need to change this now, don't want to block the fix, just fyi for future PRs
I think it caused a test to fail but not everytime. It also could have been fixed since then as changes have been made. If all the tests pass it's probably fine
My ide tried to optimize this as well. At the time not passing in cacheSize caused some expections. I would be careful about making this change without need
```suggestion Plugins.compareAndSwapLoaders(originalClassLoader); ```
The KIP talks about bootstrapping the topicId for the metadata topic. Is that part done already? I don't see it included in this PR.
I think we actually want it to be readable _only_ by the user, and explicitly restrict permissions for all other users. The patch which originally broke things for Windows users was trying to tighten up the security in exactly this way
If you only have `file.setWritable(true, true)` then the directory will still be writeable by non-users, I assume? I actually don't know the details of the `File#setXXX` methods -- but we don't want it to be writeable by just anyone. Should we instead do something like ```suggestion set &= file.setWritable(false) && file.setWritable(true, true); ```
I know we need both `File` and `Path` instance to complete the permission setting. What I meant is that: 1. We already had the `File` instances, i.e. baseDir, stateDir 2. `Paths.get(stateDir.getPath())` --> what happened behind the scene, is we created a Path instance from the path string in the file instance (i.e.`stateDir.getPath()`) ```java public final Path getPath(String var1, String... var2) { ... return new UnixPath(this, var3); } ``` 3. `path.toFile()` --> what happened behind the scene, is we created a File instance from the path string ```java public final File toFile() { return new File(this.toString()); } ``` So, currently, we will created an additional File instance via `path.toFile()`. If we pass the File instance into `configurePermissions()` directly, it'll only created a `Path` instance. ex: ```java configurePermissions(stateDir); // pass the file instance directly ``` ```java private void configurePermissions(final File file) { final Path path = Paths.get(file.getPath()); // created path instance if (path.getFileSystem().supportedFileAttributeViews().contains("posix")) { .... } else { boolean set = file.setReadable(true, true); // won't create additional File instance here .... } } ```
Ah, I see. Thanks for the explanation
We should be generally careful about replacing simple code paths with more complex ones (in terms of the underlying bytecode) in Kafka. Kafka is a high performance system, so code readability is one factor, but not the only one.
Indeed, we would want to verify that. It didn't look like it was when I originally checked, but I may have missed something.
The use of `ByteBuffer.get(...)` here does not account for the fact that it may not be positioned at the beginning. Kafka has two `Utils.readBytes(...)` methods that we should probably use. This code would then simplify to: ```suggestion byte[] rawBytes = Utils.readBytes(byteBuffer); return castByteArrayToString(rawBytes); ```
formally we should almost always check that a `ByteBuffer` is indeed backed by an array with `hasArray` or this part my throw exceptions now: https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html#array--
We don't need this extra blank line
Let's use `value instanceof byte[]` so it's like the other cases
We can inline this like you've done for the byte[] case below
Wildcard imports should be caught by checkstyle, and should fail the build. In any case, please replace with non-wildcard imports.
the field "ERROR_CODE" is never used.
It is worth being a bit more explicit about the issue, maybe something like "Expected 16 bytes, received Y bytes."? It may be a bit difficult to figure out why the parsing failed otherwise (since the string will have only valid characters).
Also, would it be better to check the string size before decoding? If there's a bug and a really large string is passed, it's a bit safer to error before parsing (and to only include the first N characters in the exception).
To be consistent with the semantics of `KafkaStreams#close`, the overload with no parameter should probably default to be fully blocking, ie with a timeout of `Long.MAX_VALUE`. Also, to avoid duplicate code, I would just have this method call `removeStreamThread(final Duration timeout)` instead of doing everything twice. Again, something like what we do for `#close`
Ok, this is going to be a little tricky...`removeMembersFromConsumerGroup` is async so we have two options. (1) just ignore the returned result and hope that it succeeded, or (2) check the returned `KafkaFuture` and wait/make sure that it succeeded. Probably we should go with (2) and just apply the remaining time of the timeout. If you haven't mucked around with the KafkaFuture class before, I believe `KafkaFuture#get(long timeout, TimeUnit unit)` is what you'd need here
I'm not sure how `removeMembersFromConsumerGroup` would behave if you passed in `""` as the `group.instance.id`, do you know? If not then let's just be safe and check what `streamThread.getGroupInstanceID()` returns, and skip this call if there is no group.instance.id (ie if not static)
nit: move the import to the other `o.a.k.*` imports
This line is a bit long. ```suggestion final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient.removeMembersFromConsumerGroup( config.getString(StreamsConfig.APPLICATION_ID_CONFIG), new RemoveMembersFromConsumerGroupOptions(membersToRemove) ); ```
It does seem like kind of a gray area. Still, the TimeoutException isn't necessarily saying that it failed, just that we didn't wait long enough for it to finish the shutdown. But we have at least definitely initiated the shutdown -- besides, if the thread really is stuck in its shutdown then it's probably a benefit to go ahead with the `removeMembersFromConsumerGroup` call to get it kicked out all the sooner. But, in the end, we really make no guarantees about the application should a user choose to ignore the TimeoutException (though they absolutely can). I can imagine that some users might choose to just swallow it and decide that they don't care if the shutdown is taking a long time. It's hard to say
Yeah I think that makes sense here
Ugh I forgot that `KafkaFuture` still throws a regular java TimeoutException. Such a mess -- btw we should log an error here (or a warn?) before rethrowing
```suggestion * @throws org.apache.kafka.common.errors.TimeoutException if the thread does not stop in time ```
```suggestion final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceID.get()); ```
(Tbh that drives me crazy, I once spent like 4 hours debugging something only to realize that I wasn't using the correct TimeoutException  )
There's actually a kafka-specific version of `TimeoutException` that you should use to keep in line with other kafka APIs. It's `org.apache.kafka.common.errors.TimeoutException`
nit: put this import above with the others (IDE often misplaces these since we follow a weird import ordering in places)
```suggestion + " for the following reason: ", ```
nit: ```suggestion log.warn("Thread " + streamThread.getName() + " did not shutdown in the allotted time"); ```
Since we use this check mutliple times, could you please extract `!streamThread.getName().equals(Thread.currentThread().getName()` to a variable named `callingThreadIsNotCurrentStreamThread` or similar.
nit: Just added some more information to the messages. ```suggestion log.error("Could not remove static member {} from consumer group {} due to: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new StreamsException("Could not remove static member {} from consumer group {} the following reason: ", e.getCause()); ```
nit: I would throw it as an error since I do not see any difference in severity compared with the execution exception. I think if you add the java timeout exception as the cause of the Kafka timeout exception, it will print also the stack trace of the Kafka timeout exception with `Caused by`. However, it will print the stack trace later in the logs. But that is OK, IMO, and we do that also in other places I guess. ```suggestion log.error("Could not remove static member {} from consumer group {} due to a timeout: {}", groupInstanceID.get(), config.getString(StreamsConfig.APPLICATION_ID_CONFIG), e); throw new TimeoutException(e.getMessage(), e); ```
I think you need this line before line 32 to have the imports lexicographically sorted.
Do we need this in the abstract class? I was thinking we would only be able to access `InetSocketAddress` if the type is `InetAddressSpec`. Otherwise the type protection from `AddressSpec` loses its bite.
Currently our equals method assumes non-null addresses. I can't think of a case where we would want it.
We also don't want to accept null addresses, right? I was thinking we could do this: ```java if (address == null || address.equals(NON_ROUTABLE_ADDRESS)) { throw new IllegalArgumentException("Invalid address " + address); } ```
A common pattern for classes like this without any state is to create a static instance. ```java public static final UnknownAddressSpec INSTANCE = new UnknownAddressSpec(); ```
The identity function could pass this test, but wouldn't have the behavior we need in the BasicAuthSecurityRestExtension. I wonder if there's a way to confirm that the mockConfiguration has been evaluated prior to calling `get()` on the returned supplier.
please remove this change (unnecessary Indent)
You have to call ```put(node, requests)```
seems like it should at least be info, if not warn
Can we also include the cause when we throw exceptions? It's not always helpful, but it has been invaluable for debugging many times since we started to include the cause.
Missing newline character.
Hm...I'm not necessarily that concerned about calling `mainConsumer.committed` twice in rare cases (although maybe that would not be so good, since those rare cases happen to be those in which this is probably more likely to time out, right?) But personally, just coming into this code from the outside, it's super confusing to have two different methods for initializing the offsets. It seems more convoluted that way, to me. Also maybe I am missing some context here but why do we call `initOffsetsIfNeeded` from `initializeIfNeeded` rather than from `completeRestoration` in the first place? We don't need to initialize main consumer offsets until it transitions to running
cool, thanks, this seems much cleaner to me
@nicolasguyomar We already log the memberId here as we log the entire generation object (which includes the memberId). This was changed recently: https://github.com/apache/kafka/commit/7e7bb184d2abe34280a7f0eb0f0d9fc0e32389f2#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L504. Previously, it was logging the generationId only.
If we're removing the redundant `AbstractCoordinator.this` here, we might as well do it 4 lines above too, imo.
`return` is not necessary
nit: There is an extra space before `+`.
Ah right. I can't think of a better way neither.
That makes sense. I did not think about the reconfiguration case.
It's good refactor to use `assertThrow`, but the failed message is missed. Please add it in `assertThrow`. Thanks.
Is this condition check necessary? It is rare in code base.
nit: Do these tests need a `PowerMock.verifyAll()` at the end? I see a `PowerMock.replayAll()` call but not any `PowerMock.expectLastCall()` or anything that makes it seem like powermock is being used functionally here.
Nit: In unit tests, "ensure" usually means "verify". We're instructing here, not verifying. But we're setting up one of the two important conditions in this test, so maybe phrase it like that? ```suggestion // When automatic topic creation is disabled on the broker ```
I've logged https://issues.apache.org/jira/browse/KAFKA-12380 for shutting down the worker's executor. Again, it's not an issue in runtime, but a *potential* issue in our tests.
Nice catch. And since we're here, why not make these `private`, too? They're currently not used outside of this class.
nit: i don't want to have to guess what 0 means ```suggestion new Thread(() -> closeProducer(Duration.ofSeconds(0))).start(); ``` also, don't we have an executor for this sort of stuff...
How about keeping this `private` and adding a protected `isCancelled()` method? Currently, the `cancelled` field is encapsulated entirely within the `WorkerTask` class, and modified only via the public `cancel()` method. We can just as easily keep the encapsulation. OTOH, if we were to make `cancelled` protected, we'd lose that encapsulation and make it a bit more complicated if a future developer did want to add logic upon cancellation.
The producer's `close(...)` method can throw an `InterruptException` if the method fails to join the IO thread. This can theoretically happen even if the timeout is 0 if the thread is interrupted (e.g., the executor is shutdown) _before_ the join can wait. Although the likelihood of this is small, what do you think about catching `InterruptException` and ignoring the error? ```suggestion } catch (InterruptException t) { // ignore, since this is likely due to the worker's executor being shut down } catch (Throwable t) { ``` Two things. First, the producer throws `InterruptExeption`, not `InterruptedException`. Second, even though the `WorkerSourceTask::close()` that calls this `closeProducer(Duration)` method doesn't _directly_ use the executor, the `Worker` does use that same executor to stop this `WorkerSourceTask`, which ultimately does call `WorkerSourceTask::close()`. IOW, this `closeProducer(Duration)` method is always called from the executor, and the executor could be shutdown at any moment, thus the potential `InterruptException`.
Again, a bit more information would be more useful: ```suggestion // Then if we delete the connector, it and each of its tasks should be stopped by the framework // even though the producer is blocked because there is no topic ```
Yeah it's a little awkward/misleading, but ideally no one should be hitting this exception in the 1st place once we have this fix. So this seems fine for 2.6.2
not sure whether this is a kind of behavior change. The docs of ```KafkaConsumer#poll``` indicates that ```timeout``` is used to wait available records. ``` * This method returns immediately if there are records available. Otherwise, it will await the passed timeout. * If the timeout expires, an empty record set will be returned. Note that this method may block beyond the * timeout in order to execute custom {@link ConsumerRebalanceListener} callbacks. ``` Maybe we can introduce a new API ```poll(Duration, Options)``` (similar to KafkaAdmin. The options enables us to adjust the poll behavior for specific use cases. Also, it opens a room to give various ```poll``` in the future.
```{@link BrokerState#RUNNING} ```
Which future? Did you mean `Event`. I assume that `deadlineNs` is the deadline for scheduling/executing the `run` method. The `run` method can take longer than `deadlineNs`.
The headline description says "Schedule an event to be run at a specific time" yet this param's description seem to indicate a different behavior.
The headline description says "Enqueue an event to be run in FIFO order." but looking the param description it doesn't looking like it is FIFO order since it support prepend, append and deferred. What do you mean by DEFEREED? How is the delay specified? I see the param `deadlineNsCalculator` but it looks like this describe the maximum amount of time this event is allowed to stay in the queue before it is cancelled by the queue.
In Java the unit of measure `timeUnit` and scalar `timeSpan` are declare in reverse order. For example: ```java void beginShutdown(String source, Event cleanupEvent, long timeSpan, TimeUnit timeUnit); ```
"the event will be prepended to the queue" : This is no longer true in the implementation.
"empty" should be clear enough. Will update it in #10092
Nit: To make sure we don't have any default/fall-back offset of zero encoded anywhere, it might be better to test with different offsets values for endOffset/beginningOffset and the target offset? Atm, if we would `seekToBeginning` as fallback instead of `seektToEnd` this test would still pass. Maybe best to just use 5, 10, 20 (or similar) for start, end, target.
I actually think that a single combination beginning=5 end=10 should be sufficient.
If @jeqo has not objections, I think we should remove it for both cases.
just in case, latest consumer-group reset-offset is comparing against `ListOffsetsResponse.UNKNOWN_OFFSET` instead of `null`. https://github.com/apache/kafka/blob/0bc394cc1d19f1e41dd6646e9ac0e09b91fb1398/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L680-L681
nit: could we move this initialization closer to its usage in L285? or just `.minus(Duration.ofDays(1))` could be simpler.
nit: ```suggestion " is empty, without a committed record. Falling back to latest known offset."); ```
Fair enough :)
Thank you for pointing this out! I will have a look at this in the next days.
Here, I would use `bb` as the prefix to cover the case where a key is a prefix of the prefix. ```suggestion final KeyValueIterator<Bytes, byte[]> keysWithPrefix = byteStore.prefixScan("bb", stringSerializer); ```
Yeah, the underlying store compares the serializer bytes lexicographically, it doesn't have any concept of "Integer" or any other type. And the really tricky thing is that it scans lexicographically, which means from left to right, whereas when we serialize things we usually do so from right to left. eg `2` in binary is `10` whereas 11 in binary is `1011` and 13 is `1101`. The problem here is that the serialized version of 2 is a different number of bytes than the serialized form of 11/13, so the lexicographical comparator is effectively comparing digits of a different magnitude.
```suggestion return completeOrForwardRequest(cb, path, method, headers, body, resultType, new IdentityTranslator<>(), forward); ```
same nit as above
nit: format got unaligned. Please check the suggestion fixes it ```suggestion new ConcurrentHashMap<>()); ```
In this project width can be longer. For old lines let's keep it like that. New is fine to format based on a 100 char width (I believe) but again not required currently. ```suggestion new ConfigValue(configEntry.getKey(), configEntry.getValue(), new ArrayList<>(), new ArrayList<>()); ```
```suggestion return completeOrForwardRequest(cb, path, method, headers, body, null, new IdentityTranslator<>(), forward); ```
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, null, TP0_VALUE.array())); ```
```suggestion capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), null)); ```
Instead of doing this, I think we can have the following 3 methods in `SslFactory`. Thoughts? ```java public SSLEngine createSslEngine(Socket socket) { return createSslEngine(peerHost(socket), socket.getPort()); } /** * Prefer `createSslEngine(Socket)` if a `Socket` instance is available. If using this overload, * avoid reverse DNS resolution in the computation of `peerHost`. */ public SSLEngine createSslEngine(String peerHost, int peerPort) { if (sslEngineFactory == null) { throw new IllegalStateException("SslFactory has not been configured."); } if (mode == Mode.SERVER) { return sslEngineFactory.createServerSslEngine(peerHost, peerPort); } else { return sslEngineFactory.createClientSslEngine(peerHost, peerPort, endpointIdentification); } } /** * Returns host/IP address of remote host without reverse DNS lookup to be used as the host * for creating SSL engine. This is used as a hint for session reuse strategy and also for * hostname verification of server hostnames. * <p> * Scenarios: * <ul> * <li>Server-side * <ul> * <li>Server accepts connection from a client. Server knows only client IP * address. We want to avoid reverse DNS lookup of the client IP address since the server * does not verify or use client hostname. The IP address can be used directly.</li> * </ul> * </li> * <li>Client-side * <ul> * <li>Client connects to server using hostname. No lookup is necessary * and the hostname should be used to create the SSL engine. This hostname is validated * against the hostname in SubjectAltName (dns) or CommonName in the certificate if * hostname verification is enabled. Authentication fails if hostname does not match.</li> * <li>Client connects to server using IP address, but certificate contains only * SubjectAltName (dns). Use of reverse DNS lookup to determine hostname introduces * a security vulnerability since authentication would be reliant on a secure DNS. * Hence hostname verification should fail in this case.</li> * <li>Client connects to server using IP address and certificate contains * SubjectAltName (ipaddress). This could be used when Kafka is on a private network. * If reverse DNS lookup is used, authentication would succeed using IP address if lookup * fails and IP address is used, but authentication would fail if lookup succeeds and * dns name is used. For consistency and to avoid dependency on a potentially insecure * DNS, reverse DNS lookup should be avoided and the IP address specified by the client for * connection should be used to create the SSL engine.</li> * </ul></li> * </ul> */ private String peerHost(Socket socket) { return new InetSocketAddress(socket.getInetAddress(), 0).getHostString(); }
given that these `EnumSets` are used so much throughout the code, we should just calculate them once and reuse them, rather than recalculating each time. There are only 3 listener types so it could just be in a static map or something.
As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too. Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process.
> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2. > > Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred? Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance.
We need to choose at least a live replica.
Hmm, it seems that we should only do `newLeader != partitionInfo.preferredReplica()` if this is a preferred leader election.
Yes, the alterIsr doesn't change leader, but generates a PartitionChangeRecord. On replaying this record, the code following code bumps on leaderEpoch? ` PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);`
Hmm, not all partitions with ISR containing the shutting down need to change the leader.
In the ZK based code, we also take live brokers into consideration when selecting a new leader.
Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader.
Currently, for controller initiated ISR change (controlled shutdown or hard failure), we always bump up the leader epoch. Also, the name alwaysBumpLeaderEpoch is a bit weird since the code in handleNodeDeactivated() doesn't directly bump up leader epoch.
Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case.
This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception.
In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr.
Currently, the follower never removes the leader out of ISR. So, perhaps we should just throw an exception if this is not the case.
We probably should name this sth like removeFromIsrAndMaybeChooseLeader.
It seems that we are logging at the debug level. I am wondering if we should log at WARN as before in ZK based appoach. ``` if (exception instanceof ApiException) { log.debug("{}: failed with {} in {} us", name, exception.getClass().getSimpleName(), deltaUs); return exception; } ```
It might be useful to log both the old and the new value.
We are generating an UnregisterBrokerRecord.
> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later? It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr.
I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2.
I believe we should surround this section of code with the following to be sure we never drop the last ISR member: ``` // never remove the last ISR member if (partition.isr.length > 1) { int[] newIsr = ... etc... } ```
(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout. (2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker. > It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ? That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR.
nit: typo `snapshoId`
Maybe a useful `Invariant` we can add here is that there always exists a snapshot corresponding to the log start offset.
It should be `LONG_POLL_RETURN_ON_RESPONSE` rather than `LONG_POLL_RETURN_ON_RECORDS`
How about updating `nullableSeenMetadata` only if we don't return records? ```java if (longPollShouldReturn(records)) { ... } else { if (nullableSeenMetadata == null) { nullableSeenMetadata = new HashMap<>(records.metadata()); } else { nullableSeenMetadata.putAll(records.metadata()); } } ``` The benefit from above code is that we don't need to handle duplicate metadata which exists on both `FetchedRecords` and `nullableSeenMetadata` when it succeed to get records and metadata in first loop.
Not sure why we need to have `FetchedRecords.FetchMetadata`? It is always converted to `ConsumerRecords.Metadata` directly. The `records` in `FetchedRecords` is `ConsumerRecord`. Maybe we can make `FetchedRecords` use `ConsumerRecords.Metadata` also.
Hmm, this doesn't seem great.
```suggestion * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot. * This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less * than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored * as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records * are not necessarily offset-aligned. ```
Thanks for doing this. I also noticed it was missing and fixed it in this PR: https://github.com/apache/kafka/pull/10085/files#diff-1da15c51e641ea46ea5c86201ab8f21cfee9e7c575102a39c7bae0d5ffd7de39R134-R137 Maybe reconcile the two changes and we can merge this one.
But currently we call `advanceNowAndComputeLatency` pretty much at the end of the `maybeCommit` method, so there shouldn't be a noticeable difference between advancing `now` at the end of `maybeCommit` vs advancing it immediately after `maybeCommit` returns, right? I'm also ok with just advancing `now` inside `maybeCommit`. The main thing that felt off was just that we compute the latency for no reason inside `maybeCommit`, and ignore the result
(and similarly move the `lastCommitMs = now` to after `maybeCommit` returns)
> Yes, that is a little odd -- as I said, happy to change advancedNowAndComputeLatency to now = time.milliseconds() within maybeCommit(). Sounds good to me, let's do it
Also kind of a nit, since technically this does work, but wouldn't it make more sense to just remove the `advanceNowAndComputeLatency` call in `maybeCommit`, and then just call `advancedNowAndComputeLatency` here as before? Otherwise we're just computing the latency inside `maybeCommit` for no reason, and throwing out the result.
thanks. that sounds good! can you name it something like `consumer_supports_bootstrap_server`? There are a bunch of other bootstrap server functions (`acl_command_supports_bootstrap_server`, `topic_command_supports_bootstrap_server`, etc.) so it would be good to be clear
It would be good to have a function in `version.py` to reflect this
In the other constructor we have some nice preconditions and defensive copies, should we do the same here? Or maybe this constructor is meant to be private to `cloneWithFencing` below
Should we spell out ISR? Though I suppose anyone working with the controller code should know what it stands for :)
How about changing the input type from `Throwable` to `TimeoutException`? The exception passed to this method should NOT be wrapped by `TimeoutException`. Using explicit type (`TimeoutException`) can prevent us from passing incorrect exception in the future.
nit: while we're at it, this could be `final`
It's mostly the flushing that concerns me, not really the offset commit. I don't think we need to make it synchronous, just that it seems silly to block that shared scheduler to complete it. My thought instead was to let the scheduler trigger the flush, but then let the task be responsible for waiting for its completion. While waiting, of course, it can continue writing to `outstandingMessagesBacklog`. So I don't think there should be any issue from a throughput perspective.
I'd suggest avoiding this pattern. Requiring to set variables to `null` can be error prone. Thankfully the `SharedTopicAdmin` class has an atomic variable that can sufficiently synchronize consecutive calls to `close` allowing only the first one to have the effect and ignoring any subsequent ones on the same object.
I don't think this is necessary. The SecurityFileChangeListener thread may not yet have started, but the watch services are already registered after `factory.configure(configs)`. The file change below should queue a change even if the thread hasn't started.
Yes, we should remove `sleep` in the tests and ensure they work without them.
Good point. As long as `SharedTopicAdmin` admin won't be recycled. That's negligible anyways, I was referring to the pattern mainly.
Using `admin = null` here allows to GC the unused admin instance earlier, right? Not a big gain, but also I don't see much benefit by using a variable such as `useAdminForListOffsets`
redundant log message. `readEndOffsets(Set<TopicPartition>)` has similar log.
How about `return admin.endOffsets(assignment);`
My main observation here is that keeping initialization in the constructor allows us to declare read only fields as `final`. It also offers better exception safety, in the sense that when an object is constructed (the call to its constructor succeeds) then we know that more or less we have a functional instance of that class. I'm not ignoring the cases where the initialization of variables has to happen at a later time during the call of `start`. But hopefully these cases are minimal, or else `start` ends up being the actual constructor of objects like this one here. Which can be redundant and in some cases more risky. As I mentioned, since `admin` is not immutable here (can be set to `null` later to support connection with older brokers) I'm fine keeping this call in `start`. But I'd still recommend not pushing initializations outside the constructor if they can happen at the time of the object construction (as is the case with `admin`'s initial initialization here).
We should verify the actual timestamp.
So to make sure we actually use "stream-time" we should change the test to actually punctuation twice? Would you mind doing a follow-up PR? Might also be worth not use a different timestamp compare to "wall-clock time" to make sure we don't by accident pass in current wall-clock.
For another, it seems to me "null" is more suitable than empty string.
How about using lambda? `(groupInstanceId.map(s -> ", groupInstanceId=" + s).orElse(""))`
Should we remove references to implementation from the interface? We can keep this doc in the implementation class.
We do expect RemoteStorageManager to have strong consistency on the data. We only relax the requirements on metadata consistency. So, it would be useful to make this clear.
it's => its
with in => within
We typically don't use java serialization. Is Serializable needed? Ditto in a few other classes.
Can we change the naming to use `ALL_CAPS`? For example, compare with the definition of `RemotePartitionDeleteState` in this PR.
Incase => In the case
upto => up tp
The KIP has the following method and is missing in the PR. `void updateRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)`
nit: extra line can be removed
Does it make sense for this to be higher than the fetch timeout? After the fetch timeout, we effectively give up on pending requests anyway.
During a rebalance, we should delete all entries for partitions we don't own any longer. Should we also pre-populate this map when we init a task (cf `StreamsTask#initializeMetadata()`)
At this static member is imported, we can replace all `Errors.INVALID_REQUEST` by `INVALID_REQUEST` in this class.
How about setting initial size of `records`? `new ArrayList<>(ids.size())`
I stepped through `testNoLeader` and it seems that -1 can indeed be a key in `isrMembers`. The `noLeaderIterator` makes the expectation explicit.
It's true and a partition could have isr and no leader. However, in that case, `isrMembers` in brokersToIsrs will still be updated with key from replicaId in isr and isr will never have -1 in its list. The noLeader info is only stored in the value of `isrMembers`.
The test case `BrokersToIsrsTest.testNoLeader` suggests that it is a possible case. It looks like the path through `ReplicationControlManager.handleNodeDeactivated` could result in a `PartitionChangeRecord` which has leaderId set to -1.
Hmm, why do we need to remove for -1 broker? It doesn't seem that brokersToIsrs tracks that.
Is this a server-side bug? the topic id exists in `topicsByName` but there is no `TopicControlInfo`.
nit: I guess we could initialize the size (a couple similar cases below as well)
This can be replaced by the constructor `public ResultOrError(Errors error, String message)`
Hmm, you mean PartitionChangeRecord? I don't see PartitionChangeRecord being generated from the topicDeletion request.
Catching `Throwable` will also catch errors which I don't think we want to do. But I don't even think we want to catch all exceptions and rethrow them as unchecked. The `deleteAllTopics` method is already marked as throwing `Exception`. If anything is thrown, we can let it bubble up and fail the test.
`ConsumerRecord<byte[], byte[]> consumerRecord`
Let's rename `headers1` and `headers2` here too
There is a typo here, "an a".
Also, I think it reads a bit weird. Not clear that "encoding an unsigned integer" in brackets means when reading the message.
Though now I look at the message for `UINT16` I see it would be consistent with that. Still I think because there are two types involved here, the Java type and the network type, including both is clearest.
Actually I mean getting a helper like: ``` void transitToSuspend() { log.info("Suspended {}", state()); transitionTo(State.SUSPENDED); timeCurrentIdlingStarted = Optional.of(System.currentTimeMillis()); } ``` since the operation is the same for all 3 transition calls
nit: log.info("Suspended {}", state());
nit: move this to where we call `enforceRebalance` as well
nit: add one whitespace at the end after "...state"
typo: The title of the last column should be `DELETE_SEGMENT_FINISHED`.
It looks like we don't support mixed mode testing. That seems worth a follow-up JIRA. It is definitely an interesting case from the perspective of the raft implementation since it involves two listeners.
Just want to point out that this assumes all controllers are voters. It would be worth a follow-up to support controllers as observers as well.
We really ought to be able to factor out a helper here. The code looks identical for both the controller and broker cases.
By the way, I sort of feel it would make our lives easier if we used `KafkaRaftServer` directly instead of building the controller, broker, and raft managers ourselves. For one thing, that would make it trivial to support mixed mode. We don't have to do that here, but I'm kind of curious if there is a reason that we don't.
It might be nice to factor out a helper to build the controller and broker nodes. It would make it a little easier to process this method visually.
nit: this seems unnecessary. We're already using the constant below anyway.
nit: Instead of calling it `dummy` which makes it sound hacky, maybe we could call it `uninitializedQuorumVotersString` or something like that. We have tried to make configuring with the `0.0.0.0:0` endpoint an explicitly supported feature.
nit: I think we could use a more convenient type, such as `Map<Integer, InetAddressSpec>`. Ultimately this just needs to make it down to `KafkaNetworkChannel.updateEndpoint` so the conversion to the config value is unnecessary.
nit: this loop is a little unconventional. Maybe we could use `pollFirstEntry` instead of the iterator? Similarly in `setNumKip500BrokerNodes`.
Well, maybe I should have pointed them _all_ out the first time.
Adding to `connectorProps` won't change the already instantiated `config`.
Same thing here with the `connectorProps` vs the `config`.
Same thing here with the `connectorProps` vs the `config`.
Hmm...but `resetStateAndRejoin(String.format("rebalance failed with retriable error %s", exception));` is only called in `joinGroupIfNeeded` which is only called in `ensureActiveGroup`, which is in turn only invoked in `ConsumerCoordinator#poll`. That said, inside `SyncGroupResponseHandler#handle` we would already have `rejoinNeeded = true` and only set it to false if the SyncGroup succeeds. So for that reason I guess we don't need the `requestRejoin` anywhere inside the SyncGroup handler
Ok cool, thanks. One last question then: after this refactoring, since we no longer call `requestRejoinOnResponseError` below, should we re-add the `requestRejoin()` call here? Or add a `requestRejoin` to the specific cases in the SyncGroup handler, eg ``` } else if (error == Errors.REBALANCE_IN_PROGRESS) { log.info("SyncGroup failed: The group began another rebalance. Need to re-join the group. " + "Sent generation was {}", sentGeneration); future.raise(error); } ```
@guozhangwang I think something may have been messed up during a merge/rebase: I no longer see `requestRejoinOnResponseError` being invoked anywhere
SGTM. If we find it flooding the logs and not helpful we can reconsider
It does not cover all versions. I had filed a PR for that (#10078) but I feel it can be fixed here.
How about using `e.toString()`? It shows the class name of exception. I feel it is useful also.
This is probably just my style bias, but I wonder if it would be easier (and more readable) to use static factories for atomic vs non-atomic ControllerResult. Something like: ```java ControllerResult.newAtomicResult(records, response) // and ControllerResult.newResult(records, response) ``` Boolean flags as such a pain and easy to mess up. I actually think it might be nice if there are no public constructors for ControllerResult and we use factories for everything. However, this would be a bigger change, so I'm fine if we defer it (if we even decide we need it).
why are we using RETRY_BACKOFF_MS_CONFIG here? this is just a sleep to avoid a tight loop in checking whether the futures are done right? In that case we wouldn't actually be issuing a new request, so we can probably just use a constant set to some small value (e.g. a few hundred ms).
this doesn't seem to be used
ditto about the log level (also for the below uses of `debug`)
why not just log this stuff at info level? we're not on a performance path (this gets executed once every rebalance right?), and it can always come in handy when you're debugging.
This is limiting the times when we perform the update. For example when sending response, the channel is active, but explicitly muted. Don't think we want to skip update in this case.
Indentation looks off. We indent 4 spaces.
The invariant that the leader most satisfy is that the `highWatermark <= flushOffset`. The current implementation satisfies this by flushing after every append and implicitly defining `flushOffset == logEndOffset`. At a high-level, I think the goals is to allow `highWatermark <= flushOffset <= logEndOffset`. On the follower, things are a little different. On the follower the `flushOffset == logEndOffset` before a `Fetch` request can be sent. This is because the leader assumes that the fetch offset in the `Fetch` request is the offset that the follower has successfully replicated. The advantage of appending without flushing as soon as possible replication latency. The leader cannot replicate record batches to the followers and observers until they have been appended to the log. I am not exactly sure how exactly we want to implement this since I haven't looked at the details but I think you are correct that on the leader side of things we want to increase the `flushOffset` in the `Fetch` request handling code as the leader attempts to increase the high-watermark.
I think it makes sense to move this flush to `KafkaRaftClient::onUpdateLeaderHighWatermark`. It is possible for the high-watermark to increase without the LEO increasing. To avoid unnecessary flush calls, the leader should remember in `LeaderState` the LEO when it flushed the log.
We are tracking the LEO in two places: 1. In `ReplicatedLog::endOffset`. This gets increase every time the log gets appended: https://github.com/apache/kafka/blob/28ee656081d5b7984c324c3ea3fc9c34614d17db/core/src/main/scala/kafka/log/Log.scala#L1302 2. The `LeaderState` also stores what is now the LEO. One suggestion is for `LeaderState` to instead store the "flush offsets". In `LeaderState` the follower's flush offset is the LEO but for the local replica the "flush offset" may not be the LEO. An example of the high-watermark increasing but the LEO not changing: 1. follower: LEO = 10 2. leader: LEO = 100, FlushOffset = 100, HW = 0 Follower successfully fetches for offset 10 => Leader: LEO = 100, FlushOffset = 100, HW = 10. Follower successfully fetches for offset 20 => Leader: LEO = 100, FlushOffset = 100, HW = 20. In this example if the leader already flushed to the LEO then there is no need to flush again when increasing the HW.
It seems like it would be quite intuitive to use `socket.connection.setup.timeout.ms`. Is it because of the complexity of `socket.connection.setup.timeout.max.ms` that we don't do this? Basically we don't know how long the network client itself is planning to wait for the connection to be established.
I wonder if we can raise this to info? This is a really important event to see in the logs, and we are always left guessing about it when the user does not have debug logging enabled. The frequency at the rate of the request timeout should keep it from being too spammy.
Ok. I think we might as well simplify the type since we already have logic which assumes one inflight request. It doesn't feel like we're buying ourselves much in terms of future-proofing.
Might be a good idea to use `Math.max(0, deadline - now)` since system time is non-monotonic.
We can implement that when handling a response, invalid cluster id are fatal unless a previous response contained a valid cluster id.
Yes. @dengziming in that example, the user has incorrectly configured the cluster. The user was configured it so that all of the controllers have each other's listener (connection) information but the cluster ids are different. The question is do we want to catch those misconfiguration early by shutting down the brokers/controllers? Or do we want to continue executing with the user potentially missing that the controllers/brokers are incorrectly configuration? There have been conversation of having the first controller leader generate the cluster id and replicate that information to all off the nodes. The currently implementation generate the cluster id in the `StorateTool` which the user has to run when configuring the controllers. I am okay leaving it as is and addressing this in a future PR.
What should we do if we see this error in a response? It looks like it would hit `handleUnexpectedError` currently which just logs an error. That might be ok for now. I think there is a window during startup when we could consider these errors to be fatal. This would be helpful detecting configuration problems. We probably do not want them to be fatal in all cases though because that might result in a misconfigured node killing a stable cluster.
I think a better way to do this is to modify `validateVoterOnlyRequest` and `validateLeaderOnlyRequest` so that we pass the clusterId. Then we can get rid of `getClusterId`.
We can use the dev version of the tool on the Kafka node via code like this: ``` node = self.kafka.nodes[0] cmd = ("%s org.apache.kafka.tools.ClientCompatibilityTest " "--bootstrap-server %s " "--num-cluster-nodes %d " "--topic %s " % (self.dev_script_path, self.kafka.bootstrap_servers(), len(self.kafka.nodes), list(self.topics.keys())[0])) ``` And then further down we can define the DEV script path like this: ``` # Always use the latest version of org.apache.kafka.tools.ClientCompatibilityTest # so store away the path to the DEV version before we set the Kafka version self.dev_script_path = self.kafka.path.script("kafka-run-class.sh", self.kafka.nodes[0]) self.kafka.set_version(KafkaVersion(broker_version)) ``` I tested this locally and it solves the problem.
How about setting "describe-acls-supported" to `false`? We can still test other features for raft.
Checkstyle fails because it expects indentations to be 4 spaces
We're missing the license header here
Just want to make sure I understand the background here: as of KIP-478 we no longer throw if there's no active RecordContext, but instead just return empty. Is that right? If so then yes, an updated test like `shouldReturnEmptyHeaderIfRecordContextIsNull` seems good to have
Got it. I missed that `votedIdOpt` is set to `empty` by the leader and the followers.
Could be merged with next condition: `electionState != null && electionState.epoch >= epoch && electionState.hasLeader()`
Yes. I meant to say `ElectionState` instead of `LeaderState`. `ElectionState` has a field called `votedIdOpt` for which `equals` checks for equality. This is not strictly required for having a "consistent" leader. I think for having a consistent leader for an epoch, only the `epoch` and `leaderIdOpt` need to match for all of the replicas.
Hmm. I was looking at the implementation for `hasConsistentLeader`. It checks that all of the `LeaderState` match. Which means that all of the replicas need to vote for the same leader. This is not strictly required for having a consistent leader. Maybe this works in this test because the number of voters is 3 and one of the nodes was killed.
Good idea but I think we do this a few lines down
should we make an effort to clean up created topics on failure? currently this method is not retriable
In this case I think we should include some error details here. In particular, the last seen error for each topic. I'm worried about cases where we try to create but the create times out but is eventually successful. We'd return an error back, but the user would have no way to know that setup failed because an internal topic already exists.
naming nit: topicConfigsWithRetention
Hmm, I'd just generate the randoms during set-up and add them to an array.
We don't want to be converting from int to string in the benchmark code.
As discussed before, for `fetchAll(final long timeFrom, final long timeTo)` we actually do not need to trigger this function at all since we know it should always return true. I think we can either 1) claim that `fetchAll(final long timeFrom, final long timeTo)` is also not optimal and people should avoid using it with the new schema, or 2) try to still keep that impl as optimal as possible, i.e. in `AbstractRocksDBSegmentedBytesStore#fetchAll` we have a condition like this: ``` return keySchema instanceOf TimeOrderedKeySchema ? return new SegmentIterator<>( searchSpace.iterator(), (....) -> true, TimeOrderedKeySchema.toStoreKeyBinary(0, from, 0), TimeOrderedKeySchema.toStoreKeyBinary(0, to + 1, Integer.MAX_VALUE), true) : // else return the normal implementation ```
Is this equal to previous check? The previous `assertEquals` checks the number of `topic partition`. By contrast, it check the number of `topic` now.
code style: `produceResponse : arrResponse`
Are all negative numbers acceptable? If so, the negative numbers which get positive by overflow (like `-1610612735`) encounter error.
Is this necessary? my IDE says it is redundant.
Can you do something like: ```java static final int tableSizeFor(int cap) { int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1); return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } ```
Or https://github.com/google/guava/blob/master/guava/src/com/google/common/math/IntMath.java#L56-L72 It is safe to look as it is Apache License 2.0.
Are there advantages of putting this simple if-check in a separate methods? Would it be simpler and more straightforward to just do the check here: ```suggestion if (executor != null) { executor.shutdownNow(); } ``` and then remove the `stopExecutor()` method? There is already precedence for an if-check a few lines above.
I am wondering if we should throw an `IllegalStateException` here, because it seems illegal to me to request a lock of a task directory in a state directory that does not exist.
seems dangerous to use the thread name to check ownership. might be safer to check reference equality on `Thread`
never mind - I see this isn't a change from what was happening before. Still seems good to do, but not necessarily in this patch
Ah, right! My bad!
I guess this logic is consistent with the current implementation. It might have been nice to make this an idempotent operation.
nit: i guess we could combine these two statements ```java if (replicationFactor.isPresent() && sortedBrokerIds.size() != replicationFactor.getAsInt()) ```
nit: it would improve readability to factor out some functions for some of the work here. Here we can have a separate function with a nice name for building the assignments
It's easier to follow these tests when they are broken down into smaller cases. That also gives us an opportunity to provide an opportunity to give a good name to the case we're verifying. For example, `testCreatePartitionsWithInvalidReplicaAssignment` or `testCreatePartitionsWithSmallerPartitionCount`.
nit: `Short.MAX_VALUE` is 32767
I think a more common error scenario is when a new member joining a group with ONLY old assignor in the configs, while all the existing members in the current generation have both old and new assignors. In this case the old would have to be selected. So in the error message, we should also clarify this case, e.g. saying something that "once a new protocol is selected, even though everyone still supports the old protocol it cannot be downgraded anymore --- even if a new member with only the old protocol joins the group"
Could we replace this with something like the following? ``` assertEquals(topics, requestWithNames.data().topics().map(DeleteTopicState::name).collect(toList)); ``` It is a bit easier to read and `assertEquals` gives the differences between all the expected and the existing topics when it fails.
nit: Could we move `new DeleteTopicsRequestData.DeleteTopicState().setName(topic1)` to a new line in order to align it with the following `new DeleteTopicsRequestData`? There are few other cases like this.
It would be better to use `assertThrows` instead of doing this because it is not clear where we expect this exception to be thrown. So, I would so something like the following: ``` if (version >= 6) { // Do the regular part } else { // assertThrows with the expected code that must throw. }
Should we use `result` here instead of `numWriteTries`? If not, maybe we can change generateBatch() to return a boolean instead
This is a weird line break. It would be better to shorten the line by assigning the result of `mapper.apply` to a variable.
nit: ".. in epoch {}"? Similarly for other logs.
You could return Optional[String] probably where defined string would be the rejection reason. This would mean renaming the methods slightly though.
I'm sure there must have been a reason for this in the original implementation, but it still surprised me to see it. I think it makes sense for the resigned leader to help another candidate get elected. I'll file a separate JIRA about this.
Filed this one: https://issues.apache.org/jira/browse/KAFKA-12607.
nit: unneeded newline
nit: can use method reference here i think
What if that never happens? It's better to always have a timeout in tests.
Seems like we should do a single `close` with some timeout. cc @cmccabe
This is done above in `before()` so it's probably not behind the flakiness, unfortunately. Not sure why we purge the local state twice but you're right, we should at least be consistent when we do and purge the state for all three Streams. Good catch
I think this mock is probably only used internally by Streams tests, so we don't need to log anything for users here
nit: `OnOn` ->
It would be best to merge https://github.com/apache/kafka/pull/10387 first then we don't have to undo this ignore.
Should we change the name to maxTimestampMs too? Ditto below.
Would it be possible to extract test code in this class and `RocksDBRangeIteratorTest` that actually tests code in class `RocksDbIterator` to a test class `RocksDbIteratorTest`. I think that would make the code more maintainable.
I could not find any unit tests for this method.
```suggestion * A byte array comparator used on lexicographic ordering, but only comparing prefixes. ```
```suggestion * A byte array comparator based on lexicographic ordering. ```
Unrelated to your change, but the bitwise OR is a little surprising. Maybe we can rewrite this to be more explicit? Glancing through our code, I don't see any other usages of this operator besides actual bitwise operations.
Hmm.. would we ever have customized error message that are not from `Errors` map? E.g. if pluggable modules record some non-ak errors.
Gotcha, I misunderstood the docs and thought it was ~1. Let's keep it as-is
I had to look this constant up :) Can we just make it check if the jitter is equal to zero (or maybe `<=` zero)? A caller of this method setting jitter to something like 0.5 might be surprised that there is no jitter added.
nit (formatting): might be easier to read if we move to new line: ``` log.info( "Could not create topic {}. Topic is probably marked for deletion (number of partitions is unknown).\n" ```
nit: one parameter per line -> move `retryBackOffMs` to its own line
nit formatting. `cause` is indented correctly, it's weird that it align to the arguments above that are not parameters of `StreamsException` (like `cause`). Better: ``` throw new StreamsException( String.format( "Could not create topic %s, because brokers don't support configuration " + "replication.factor=-1. You can change the replication.factor config or " + "upgrade your brokers to version 2.4 or newer to avoid this error.", topicName ), cause ); ```
The trickiness as demonstrated in the current PR though, is that if we first do the expiration we may get records that are matched to the current processing record, which need to be skipped from deleting/emitting before the join. I think it is still possible to simply the current logic without naive buffering. Because: 1) The current processing record's timestamp T is no larger than the updated max stream time T'; 2) The current processing record's matching record's smallest timestamp would be (T - window-size); 3) The expired records' largest timestamp would be (T' - window-size - grace-period), where grace-period >= 0. In other words, if the current processing's record timestamp T is smaller than T' (i.e. it's a late record and hence did not advance the stream time), then for all records that are within [T - window-size, T' - window-size - grace-period] assuming T - window-size < T' - window-size - grace-period, would have already been expired end emitted, and hence won't be found and matched; if the current processing's record timestamp T == T' (i.e. it is not a late record), then T - window-size is always >= T' - window-size - grace-period, which means that all joined record's timestamps should be later than the expired timestamps. That means, if we do the expiration first based on (T' - window-size - grace-period), the newly expired records' timestamps should all be smaller than any joined record's timestamps for that processing record generated later. And hence it is safe to just blindly expire them all without the `except` logic.
nit: we can move `inputRecordTimestamp` up and use it here.
Here if we refactor to `left / right` then this logic can be simplified as well since we would only care whether the deserialized key/value are left or right.
This reminds me about the test coverage: maybe we should also test that store.put / delete can be triggered while the iterator is open, and if the put / deleted elements would not be reflected from the iterator.
We can refactor it as: ``` if (!outerJoinWindowStore.isPresent() || timeTo < maxStreamTime) { context().forward(key, joiner.apply(key, value, null)); } else { } ``` Then `internalOuterJoinFixDisabled` can just be a local variable instead of a class field.
cc @mjsax as well, LMK WDYT.
Do we need to maintain it manually? Could we use `context.streamTime()` instead? Note that `context.streamTime()` might be slightly different because we advance it for every input record. Thus, if there is a filter before the join, the join might not get all records and thus it's locally observed stream-time could differ from the task stream-time. It's a smaller semantic impact/difference and it's unclear to me, if we should prefer processor-local stream-time or task stream-time? \cc @guozhangwang @vvcephei
Just to be more concrete here, I think we can move the expiration out of the loop https://github.com/apache/kafka/pull/10462/files#diff-6ca18143cc0226e6d1e4d5180ff81596a72d53639ca5184bf1238350265382a6R154 for fetching any records to join, based on the above analysis.
I guess that's possible, but _if_ the join result is large, we could run into memory issue buffering all join results? Also, sorting could be expensive and we can actually avoid it, and still guarantee that results are emitted in timestamp order: - we know that left/outer join result would have the smallest timestamps and thus we can emit those first (given that we use timestamped-sorted store anyway, we just scan the store from old to new and emit - for the inner join result, we get the output sorted by timestamp, too, because for the join key, data is sorted in timestamp order in the store, too
I think we can refactor the logic here as the following: 0) suppose the received record timestamp is T1, the current stream time is T2 >= T1; and we found one or more matching record from the other side, with timestamp T1' <= T2' <= T3' etc. The joined record would have the timestamp of T1` = max(T1, T1'), T2` = max(T1, T2'), where T1` <= T2` <= ... 1) After we get all the joined records, we do not call `context.forward()` yet, but just cache them locally. 2) We then range query the expired records store, and generate the joined records (and also delete the records), again we do not call `context.forward()` yet, but just cache them locally. 3) We merge sort on these two sorted-by-timestamp list, and then call `context.forward()` on the sorted join result records to emit. In this we do not need the following complex logic.
Before further optimization, we can use `store.putIfAbsent` for now.
I think we can move this logic into ValueOrOtherValue as another static constructor.
I personally was on the side of always using task stream time everywhere but more people feel that we should use processor stream time :P Anyways, all I'm trying to say is that we need to make an educated decision here, and if we concluded that either 1) we rely on task time here, but still use processor time on other expiration logic, or 2) we rely on processor time on all logic, or 3) we rely on task time on all logic, we have a good rationale for whichever we choose.
Ditto here, we can rename it to leftJoin / rightJoin to indicate if this joiner is for left or right.
nit: line to long should be ``` private void emitExpiredNonJoinedOuterRecords(final WindowStore<KeyAndJoinSide<K>, LeftOrRightValue> store, final Predicate<Windowed<KeyAndJoinSide<K>>> emitCondition) { ```
`e` is not a good variable name
Ah nvm then --- let's just keep it out of the scope of this ticket for now.
Side improvement: I think we should skip late record directly and also record it in `TaskMetrics.droppedRecordsSensorOrExpiredWindowRecordDropSensor`
`late` -> `out-of-order` -- if it's _late_ it would be _after_ the grace period and would be dropped.
nit: not introduced by this PR, but let's rename it to `otherWindowStore` for naming consistency.
@spena seems there are a few different conditions we can consider here: 1) record time < stream time - window length - grace length: the record is too late, we should drop it up front and also record the `droppedRecordsSensorOrExpiredWindowRecordDropSensor`. 2) record time >= stream time - window length - grace length, but < stream time: the record is still late, but joinable, since the stream time would not be advanced we would not have to check and emit non-joined records, but just try to join this record with the other window. Note that like @mjsax said, for the returned matching record, we also need to check if the other record time >= stream time - window length - grace length or not. 3) record time > stream time, we would first try to emit non-joined records, and then try to join this record.
@spena just ping to make sure you get this on the follow-up PR.
Well, while we should have a check like this, it seems it should go to the top of this method, next to the key/value `null` check? We should also add a corresponding `lateRecordDropSensor` (cf `KStreamWindowAggregate.java`). We can also `return` from `process()` early, as if we have a late record, we know that stream-time does not advance and thus we don't need to emit anything downstream.
Yes, it a bug in the current implementation...
No, this method is used to access the private member `currentNodeResource` from outside of the class.
I don't think it can be. It needs to be a TimelineHashMap to work and needs to receive the snapshot registry in the constructor.
Hmm, how do we prevent the deacitved broker from being selected as the new broker? It seems that at this point, clusterControl hasn't reflected the fenceBrokerRecord yet. Also, in the caller handleBrokerFenced(), we add the partitionChangeRecord before the fenceBrokerRecord. Ordering wise, it seems that it's more natural to add the fenceBrokerRecord first. Then, it's clear that the partitionChangeRecord is the result of the fenceBrokerRecord.
We should prefer the clean leader election even if shouldBeUnclean is true.
Hmm, the priority of a config is topic, node, node default. So, if the config is set at the topic level, we should just use the topic level setting and ignore the node level setting. If the config is not set at the topic level explicitly, then we move to the node level. If the node level is not set, then we move to the node default.
Hmm, if we performs an unclean leader election, the only replica in ISR should just be the new leader since the data in existing ISR is not guaranteed to match with the new leader.
If unclean leader election is enabled through a config change, we need to trigger a leader election on all relevant partitions without a current leader.
I think in this case, unclean leader election should be false for MYTOPIC since the topic level config takes precedence.
If there is an unclean leader election, we need to reset the ISR to just the leader. Since this needs to be done in multiple places, perhaps we could change bestLeader() to return both the new leader and the isr.
the key type of `topics` is `Uuid` and hence this check is weird. Maybe it should be replaced by `topicNameToId`
Okay, thanks! I have limited time at the moment. I'll try to look at it this week.
Since these string literals are now relevant elsewhere, we should make them reusable constants. Perhaps they should be enums? I realize now that perhaps the class names should have also been enums but .
The existing structure of this class bakes in the assumption that each plugin only appears once on the plugin path, and that the common use-case plugin path (returned by this method) is then valid. This change would make this method return an invalid plugin path, with flakey behavior when loading the duplicated plugin (which plugin gets loaded would be determined by iteration order over the PLUGIN_JARS hashmap). There are a couple of ways out: 1. avoid tackling this now, and separate the two plugins with different class names 2. have this method pick one of the duplicates to drop deterministically, so that the Plugins class doesn't have undefined loading behavior. 3. allow/deny some of these plugins from being included in this default plugin path, and keep some plugins back for the more specific tests 4. remove this method, and have PluginsTest explicitly include the needed plugins in each test, and/or a default list to include if none are specifically requested.
Yes, but plugins are now no longer unique by name. The `test.plugins.ReadVersionFromResource` plugin class is now provided by two different plugin jars, and when the DelegatingClassLoader attempts to load it, it is arbitrary which one is chosen. For example, if you set up Plugins with this path, and load the `test.plugins.ReadVersionFromResource`, will you get v1 or v2? I believe it depends on iteration order in the hashmap, and so happens to be deterministic here but not in general. Currently, Connect does not have first class support for plugin paths with duplicate plugins due to this ambiguity, so I don't think this method should return a plugin path which is unsupported.
nit: make each of these a separate constant
Let's check that `needsDrain` returns true after this point.
Similarly, can we skip the linger here? We want to ensure that the current batch that is being built will be immediately finished when we call `appendLeaderChangeMessage`.
Hmm I think a better test would be to skip the linger wait time. This would show that calling `forceDrain` causes us to forego the linger: ```java assertFalse(acc.needsDrain(time.milliseconds())); acc.forceDrain(); assertTrue(acc.needsDrain(time.milliseconds())); assertEquals(0, acc.timeUntilDrain(time.milliseconds())); ```
I was also confused by this logic for a while and I got an optimization idea from the [Raft dissertation](http://wcl.cs.rpi.edu/pilots/library/papers/consensus/RAFTOngaroPhD.pdf) about this, The sentence below is taken from section "5.4.2 Committing entries from previous terms": ``` There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity. ``` so we can also update commitIndex(highWatermark) if logEndOffset of all followers have passed the highWatermark. I don't think this is a good idea since it makes the logic opaque but will not necessarily really optimize any performance, so I just mention it here and feel free to ignore it .
That makes sense -- a `long -> int` cast is safe, but not the other way around.
We should use `long` as `commitInterval` is declared a LONG in `StreamsConfig`: ``` .define(COMMIT_INTERVAL_MS_CONFIG, Type.LONG, DEFAULT_COMMIT_INTERVAL_MS, atLeast(0), Importance.LOW, COMMIT_INTERVAL_MS_DOC) ```
It might be simpler to just use `int transactionTimeout` -- Java will auto-cast to long in ``` if (transactionTimeout < commitInterval) { ```
How about using java stream? ```java Arrays.stream(TransactionState.values()) .collect(Collectors.toMap(TransactionState::name, Function.identity())) ```
For even distribution, it would be useful to verify 2 things. (1) The leaders are distributed evenly when all brokers are unfenced. (2) If any broker is fenced, the new leaders are still distributed evenly.
Yes, we should add this as a possible reason as well.
`..the sender buffer due to memory unavailable..`
There is another possible reason. "metadata could not be refreshed within {@code max.block.ms}"
Hmm... I don't believe "cancelled" is a term we've used in public-facing surfaces in the past. For example, when a task takes too long to shut down now and we have to cancel it, we log the message that "Graceful stop... failed": https://github.com/apache/kafka/blob/5964401bf9aab611bd4a072941bd1c927e044258/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L866 Personally I think the additional code complexity is worth it; the original ticket mentions a case where these messages confuse users because they're generated for cancelled tasks, so I'd rather err on the side of making things as obvious as possible to them. It might be possible to keep things simple and eliminate branches by tweaking the message to make it clear that newer task instances won't be impacted by this failure, though. A possible downside to this is that it might be confusing if there are no newer instances that will be brought up on the worker (because the connector has been deleted, the number of tasks has been reduced, or the task has been reassigned to another worker). But with some careful wording we might be able to avoid misleading people into thinking that this message implies there's already another instance running.
Should we just write this as: ``` if (cancelled) { } else if(stopping) { } else { } ``` ? Should be equivalent but a bit easier to follow.
Looks good to me!
well, i thought a method reference would work here for the hash map, but I tried it and it doesn't seem to work. 
As this method is not supported, maybe we can move this implementation to interface? The benefit is that we don't need to add similar code to both `MockController` and `QuorumController`
Maybe add one more sentence: > .. underlying fetch behavior. The consumer will cache the records from each Fetch request and return them incrementally from each `poll`.
A couple other potentially interesting test cases: 1. After starting in a resigned state, verify that the node will become a candidate after the election timer expires. 2. Verify that we can vote for new candidates when in the resigned state after beginning shutdown.
Ah, I was suggesting to just replicate the `shouldInstantiateAssignor` and `shouldInstantiateListOfAssignors` tests exactly, but with the `classTypes` being eg `StickyAssignor.class` instead of `StickyAssignor.class.getName()`. For example ``` classNames = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances(classNames, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); ```
Ah, yeah, you'd need to do something more like what actually happens in the actual KafkaConsumer/`getAssignorInstances` code. eg ``` @Test @SuppressWarnings("unchecked") public void shouldInstantiateAssignorClass() { Object classTypes = Collections.singletonList(StickyAssignor.class); List<ConsumerPartitionAssignor> assignors = getAssignorInstances((List<String>) classTypes, Collections.emptyMap()); assertTrue(assignors.get(0) instanceof StickyAssignor); } ```
It's super awkward, obviously, but since this is what happens when we process the configs in the real code we should try to replicate that in the test
nit: this should be package private I think
we should do this `assertTrue` thing for the CooperativeStickyAssignor as well
I think it would make sense to style this test (and `shouldInstantiateFromListOfClassTypes` below) more like `shouldInstantiateAssignors` now, ie where we actually validate the assignors that are returned (eg `assertTrue(assignors.get(0) instanceof StickyAssignor)`). Previously this test was just making sure that we adaptor would work and we wouldn't throw an exception when constructing the consumer, that's why it's like this
Nice, thanks for the update. Looks good
```suggestion public void shouldInstantiateAssignor() { ```
This particular test doesn't make sense any more, since there is no "old" assignor type now that PartitionAssignor is removed
One tricky aspect is that we now generate the string even if logging is disabled. Let me think about that a bit more.
Could you make the error message to be more specific? E.g. "Partitioner generated invalid partition: %d.." to differentiate that a partitioner is used.
two calls to createWorkerTask
I think a default mock would be enough to get a better error message. Strict mocks also check the order of the calls which is not needed here. I would also pass `new MockTime(1)` instead of increasing the timeout, because the test retrieves the time twice. `new MockTime(1)` would progress time 1 ms each time. Hence, we would be within the 100 ms of the global `max.poll.interval.ms` and since we control the time progression the test would be more robust than by increasing `max.poll.interval.ms` to a higher value. With `new MockTime(1)`, we also avoid running the test indefinitely in case of a mistake in the production code, since after 50 time retrievals, the test would run into a timeout and the admin client mock would throw the unexpected method call error. Probably, it would be even better to use something like `new MockTime((Integer) config.get(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG)) / 3)`.
After this PR is merged, I will open a PR that changes also the other usages of `Time.SYSTEM` and `createNiceMock()` since I think that would make all tests more robust. I do not do it now because I am still working on that code for KIP-698 and I do not want to run into too many merge conflicts.
nit: import in the wrong place
nit: Add a note that since `key` is variable length while time/seq is fixed length, when formatting in this order varying time range query would be very inefficient since we'd need to be very conservative in picking the from / to boundaries; however for now we do not expect any varying time range access at all, only fixed time range only.
I think just calling `remove` is totally fine :)
We should be providing an instance of the TimeWindowedSerde to the Streams DSL in the application topology, not using this config. That's the idea of this KIP -- a Kafka Streams application should pass in a `new TimeWindowedSerde(innerClass, windowSize)` rather than using this like a default.
This should be `LOW` since the corresponding config for window size of a windowed serde is also `LOW`. The configs are grouped by Importance so you'll need to move it to the section below (and also it should be in alphabetical order).
We should log an error that prints out what the two configs actually are
I think it's fine to lose some state after a restart, presumably we'll just start up with all queries assumed to be good and then re-de-prioritize any that are still failing. We can iterate on this and let ksql persist this info in the command topic and give Streams hints when starting up that a query may still be failing, but just tracking it in-memory seems good enough for a first pass
We can use `computeIfAbsent(...)` to eliminate the prior newly-added line: ```suggestion allAddedPlugins.computeIfAbsent(pluginClassName, n -> new ArrayList<>()).add(plugin); ```
This could be rewritten a bit more compactly and a bit more functionally: ```suggestion return allAddedPlugins.entrySet().stream().filter(e -> e.getValue().size() > 1).map(e -> { String pluginClassName = e.getKey(); PluginDesc<?> pluginDescInUse = pluginDescInUse(pluginClassName); List<PluginDesc<?>> ignoredPlugins = new ArrayList<>(e.getValue()); ignoredPlugins.remove(pluginDescInUse); log.error("Detected multiple plugins contain '{}'; using {} and ignoring {}", pluginClassName, pluginDescInUse, ignoredPlugins); return pluginClassName; }).collect(Collectors.toSet()); ``` My suggestion also includes a reworded error message to put the more meaningful information near the front, and computes the unused plugins and includes them at the end of the message, just in case there are multiple.
```suggestion public ClassLoader connectorLoader(Connector connector) { ```
I realize that `pluginDesc` in this name is just the equivalent of `PluginDesc`, but generally we try to avoid abbreviations, which is probably more true in this case because the name seems even more mangled: ```suggestion PluginDesc<?> usedPluginDesc(String name) { ```
Since this is an error, it probably would be good to clarify the error message and provide an action. WDYT about: ```suggestion log.error("Detected multiple plugins contain '{}'; using plugin {} and ignoring {} plugins ({}). " + "Check the installation and remove duplicate plugins from all workers.", pluginClassName, usedPluginDesc, ignoredPlugins.size(), ignoredPlugins); ```
If we use `computeIfAbsent(...)` below, we don't need this line: ```suggestion ```
In cases like this, it's probably a bit more idiomatic in Connect to use a tertiary operator so that there is a single `return`: ```suggestion return inner == null ? null : usedPluginDesc(inner); ```
I know I suggested this text, but reading it now makes me think that users may not really know what "remove duplicate plugins" means. "Remove all of the ones listed here?" No, we mean all but the one you really want to use. :-D How about the following? ```suggestion + "Check the installation on all workers and if possible remove all but one of these duplicated plugins.", ```
Offline you suggested that we change this to warn, based on @C0urante's earlier observation that sometimes the same converter might be included by multiple plugins. This latter isn't an issue if it's the same converter version in all of them. So +1 to change this to warn: ```suggestion log.warn("Detected multiple plugins contain '{}'; using plugin {} and ignoring {} plugins ({}). " ```
Indentation is incorrect. It should be: ```suggestion log.warn("Detected multiple plugins contain '{}'; using plugin {} and ignoring {} plugins ({}). " + "Check the installation on all workers and if possible remove all but one of these duplicated plugins.", pluginClassName, usedPluginDesc, ignoredPlugins.size(), ignoredPlugins); ```
Mmmm, I'm not sure we should be making decisions here based on dynamic plugin loading for two reasons: 1. This change can be backported to older versions of Connect, which will never have that feature. 2. It's unclear exactly what the mechanism for dynamic plugin loading will be, and it's possible that a re-scan of all known plugins after loading has taken place (either the initial start load or a subsequent dynamic load at runtime) could still be beneficial Also, it's actually not that uncommon for 3+ copies of the same plugin to appear on the plugin path for a worker. For example, some connectors come packaged directly with converters; all it takes is at least two such connectors and a separately-installed copy of that converter to lead to that number of copies, without any error or misconfiguration on the part of the user.
Oh, gotcha--in that case, should we do a check somewhere else, since this will be triggered potentially multiple times for a single plugin? For example, if there are three copies of a connector, the warning will be logged twice right now, with different values for `inner.keySet()` each time. Also, it may also help to log exactly which one we're going to use either instead of or in addition to the complete set of discovered versions of the duplicated plugin.
nit wording: `partitions that aren't assigned to any current consumer`
Is this `null` assignment needed? Don't see the variable used after this.
So this is assuming the following `balance()` call could run beyond the next GC? In that case imho `assignedPartitions.clear()` would look better (having almost the same impact).
Right, sorry I misread that line.
It might be a good idea to send a slightly larger batch of data, for example I think in other integration tests we did like 10,000 records. We don't necessarily need that many here but Streams should be fast enough that we may as well do something like 1,000 - 5,000
I see. So the distinction is because in the revoked case we get a chance to await on the errant record reporter futures before we commit offsets for the revoked partitions, but in the lost case we've already lost the partition and don't get a chance to commit offsets. Since we already do not own the partition, we should not be reporting errors for it and should let the current owner take that responsibility. It should be noted that the cancelation is best effort, so there is a chance we duplicate reporting for the errant record.
what's a requested topic partition? Also, above we mention just `partition`
If we want this to be a concurrent map probably good idea to depict this in the declaration. (same as you do in the tests below) ```suggestion protected final ConcurrentMap<TopicPartition, Future<Void>> futures; ```
I wonder if we want to print the actual list of partitions here, which might be long. And do it twice. I see the same pattern is applied elsewhere. I understand the value of explicit listing.
I don't know if this change is required. The way I read the current implementation, we make sure that the paused partitions contain only assigned partitions in the block below, setting the paused partitions on context. We then rely on the code block in `iteration()` to resume partitions that should not be paused. ``` } else if (!pausedForRedelivery) { resumeAll(); onResume(); } ``` Setting this to anything other than false causes us not to resume partitions which we own that were not explicitly requested to be paused.
Ideally, we want to log this when the record is reflected through replay in the controller. That's also when we could log the new leaderEpoch.
One side effect of doing this is that it can force the leader to change unnecessarily. For example, the current leader could still be in newIsr, but may not be the first in replica list. bestLeader() will cause the leader to change in this case.
Yes, it probably doesn't make a big difference and it happens rarely. So, we could just keep the logic in this PR.
The reason that we want to remove the shutting down replicas from ISR is to optimize for latency. If we don't do that, when the broker actually shuts down, it can block the producer for replica.max.ms before the replica can be taken out of ISR. So, I think this optimization is still useful.
In the case of controlled shutdown, currently, we ignore the unclean leader election flag and always to elect a new leader cleanly.
It seems that we haven't added the topicId yet.
We use to log all leader and isr changes in info even for clean leader election.
This is definitely a nitpick, but can you put this one fewer lines? 2 lines should be enough for this. Same for the ones below.
Can do this in a follow-on
Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
I had the same question. It appears better to just duplicate the properties.
What about client security related properties? It's weird that we pick up "bootstrap.servers" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX.
`consumerProps` and `producerProps` are of type `Map`, therefore the `.toString()` is probably not readable. So you'd need to convert these into a comma-separated list sth like `K1=V1,K2=V2,...Kn=Vn`.
It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests.
Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage.
MILLIS => MS to be consistent with other places. Ditto in a few other places.
createMetadataTopic() is no longer used.
Slight clarification: `or non-null topic IDs`
`<byte[]>` this explicit type is unnecessary
How about ``` for (byte b : payload) { assertNotEquals(0, b); } ```
typo: byteArrray -> byteArray
nit: we could split this lone line by different key, value by new line to make it clear. ex: ``` String[] args = new String[] { "--topic", "Hello-Kafka", "--num-records", "5", .... }; ``` Same as below.
nit: remove the redundant line. Same as below.
`< Callback >` this explicit type is not necessary.
`asList` -> `Collections.singletonList`
redundant type arguments `<ProducerRecord<byte[], byte[]`
typo: FOLLOW_REPLICATION_THROTTLED_REPLICAS -> FOLLOWER_REPLICATION_THROTTLED_REPLICAS
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L149 can reference to this field
https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogConfig.scala#L150 can reference to this new field.
I would append a couple of batches after advancing the high-watermark. At this point the HWM equals the LEO.
Let's use `epoch` instead of `currentEpoch`. Since we are using `currentEpoch`, the `endOffsetForEpoch.offset` will equal the LEO. If you instead use `epoch` then the `endOffsetForEpoch.offset`. will be `4` which is less than the LEO (`5`).
This test is checking the `snapshotId` when the `KafkaRaftClient` is the leader. Let's have a similar test but when the `KafkaRaftClient` is a follower. Take a look at `KafkaRaftClient::testEmptyRecordSetInFetchResponse` for a simple example of how you can advance the high-watermark on the follower. Note that the followers don't need to wait for the "high-watermark" to reach the current epoch to set (and advance) the high-watermark.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
Seems like we should use `3L` instead. The leader would not have been able to advance the high watermark past the fetch offset of `3L`.
Thanks for cleaning up the code duplication.
That makes sense. I got confused by the fact that `AbortTransactionResult` takes a `Map` in its constructor. In this case, `all()` seems fine. Thanks for the clarification.
Is the `thenApply` really necessary here? It seems that `KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]))` already returns what we need here.
nit: You've put a `.` at the end of this exception message but not to the others below. I would add it everywhere or remove it here.
nit: This if statement is not required. The countdown latch won't wait at all if the timeout is <= 0.
If stop throws we won't count down the latch. No harm will result except there will be an erroneous log messages about exceeding the stop timeout.
It's not required to protect the await, but is to get the logging.
`timeoutMs` would be unambigous
We should name the thread so that thread dumps are a bit more informative. I _think_ these should be daemon threads because if we're prepared to basically ignore the non-return of `task.stop()` during runtime I don't see why we'd block jvm exit for them.
Could we just remove empty named topology dirs along the way instead of doing that in a second pass at the end? EDIT: nvm, after some thoughts I feel it is more complicated than easier.
I am assuming that this will be done in a later PR
I was asking more for a semantic one -- as long as this is not expected then I'm happy for this piece as is :)
Similarly, better to declare `throws IOException, ReflectiveOperationException`.
Probably simpler to just catch `ReflectiveOperationException`
I think we should use the same name here; the metrics-scope would differentiate rocksdb from in-memory, which is sufficient.
Maybe keeping it simple to always go with left is fine for now, I was just wondering if you have a specific reason for that :)
Hmm.. this makes me thinking: does it worth "working around" it to move the naming mechanism of the shared store to `sharedOuterJoinWindowStoreBuilder` above such that it always goes along with the other two store's naming patterns? As you can see here, if the store names are not provided but just the store suppliers, the existing stores would use customized name but the shared store would still use system-provided names.
Maybe we can also make it a bit simpler to just rely on the left side: if the supplier is given on the left side (which would always provide the name), then we name the shared as `thisStoreSupplier.name() + "outerJoinSuffix"`; if it is not, then we look into `streamJoinedInternal.storeName()`, and the last resort as `outerJoinStoreGeneratedName`.
That makes sense, however you might be able not include the new field from the hash to prevent a chaotic assignment if you wanted
nit: I would add a `:` after `set` to stay consistent with the other error messages in this PR.
This is the only remaining point of discussion. I don't have a strong preference for any of them so I leave it up to you.
nit: This empty line is not necessary.
nit: We could use `forEach` here. It makes the code more concise.
I have a small preference for `Future<Map<Integer, Future>>` because it seems more aligned to how we do it for other APIs but I don't feel strong about it.
Let's do it. :)
Out of curiosity, why did you not use `map` here? Is it to avoid the allocation of the `stream`? It doesn't seem that this is a hotspot though.
nit: ```suggestion "Mismatched article size between augmented articles (size " + consumedAugmentedArticles.size() + ") and consumed articles (size " + consumedArticles.size() + ")", ```
Is this just to prevent it from processing anything until you're ready to proceed? It seems like you can/are doing that just by controlling when to produce input messages and doing so one at a time (if that's accurate, then WDYT about renaming `process` to `processed` and flipping the boolean so it more clearly serves the purpose of indicating whether a record has yet been processed)
```suggestion processed = new AtomicBoolean(true); ```
nit: initialize `appId` with the prefix you want (eg `appID_`, or something more descriptive like `TaskMetadataTest_`) and then just append testId, ie ``` appId = appId + testId; ``` Just makes it easier to locate what the prefix is (same with `inputTopic` below)
No worries. I was only recommending to change the name if you stopped using it to gate the processing and just relied on producing messages to control when they could be processed, if you did want to do that. Either way is fine with me so you can just leave it as is
super nit: the message should explain what happened if the condition fails, ie it should be the opposite, something like ```suggestion TestUtils.waitForCondition(() -> !process.get(), "The record was not processed"); ```
Yeah. Might be worth to just remove this one even if we need to update all implementations.
This works (note that `Properties implements Map<Object, Object>)`: ``` Properties p = new Properties(); Map<String, Object> foo = new HashMap(p); ``` So you should be able to do `getBoolean(new HashMap(props), ...)` (Need to omit the generics though...)
Instead of using `toMap`, can't we just to `new HashMap<String,String>(props)`
I would still try to keep this assertion, something among the lines of: ```java if (clashOnPrefix) { assertThat(valuesWithPrefix.get(0), either(is("a")).or(is("b"))); } else { assertThat(valuesWithPrefix.get(0), is("a")); } ```
`targetAdminClient.listOffsets()` is easier and more efficient to get the LEO.
If I put ``` assertThat(store1.isOpen(), is(false)); assertThat(store2.isOpen(), is(false)); assertThat(store3.isOpen(), is(false)); assertThat(store4.isOpen(), is(false)); ``` on line 202 in `shouldThrowStreamsExceptionForOldTopicPartitions()` the test fails. Hence, we leak a state store.
> Hmm, for production, do we ever restart a thread even for illegal-state or illegal-argument? If the user decides to restart a stream thread in its exception handler it is possible.
Yeah, our attitude towards IllegalStateException has been pretty cavalier thus far, and it's one of the main things I'm concerned about with the REPLACE thread functionality. We should definitely be on the lookout for possible IllegalStateException occurrences in the codebase and try to triage them so things aren't just completely screwed up if Streams is allowed to continue after hitting one
There are a a `IllegalStateException` and a couple of `IllegalArgumentException`s on the path from opening the state store within `stateStore.init()` to line 182 in `this.registerStore()`. We do not close the state stores before we throw. I do not think this is relevant for production code, but we could leak state stores in unit tests if we do not explicitly close the state stores in the unit tests.
Not really related to this line. Could you verify that the state store is closed in the unit test that tests line 148? The name of the test is `shouldThrowStreamsExceptionForOldTopicPartitions()`.
+1 on stopping the app after a deterministic illegal * exception. I am not sure if all illegal * exception we throw are deterministic, though. I guess most of them are. For now, we could just shutdown the app for all illegal * exception and then consider to use a different exception if we discover that a illegal * exception is transient.
I definitely think we need to triage and maybe clean up the existing Illegal-type exceptions today. Some may not be deterministic, but we still just drop everything and shut down without any further attempts at cleaning up. In those cases it's probably down to the specific situation whether it's appropriate to continue doing so and disallow recovery from this, or just fix the handling so it does clean all resources
+1 on disallowing the app to continue after an illegal exception. We need to reserve _some_ kind of exception for actual critical, fatal system errors that a user can't just ignore to spin up a new thread. And that has essentially been the meaning of these illegal exceptions in Streams thus far. As I mentioned in another thread, I've been very concerned about this in the new handler since we haven't been strict in properly cleaning up after an illegal exception
On a second thought, it might also be relevant for production code since we now can restart the stream thread after a fatal error. This is not yet possible for a global stream thread, but it might be possible in future.
Now, I see what you mean. However, I am not sure it is a good idea to rely on the code in `GlobalStreamThread` that catches the fatal exception to clean up state stores (and all the rest). If we know, we throw a fatal exception, then we should clean up immediately before we throw. That makes the `GlobalStateManagerImpl` less error-prone, because it does not need to rely on a different class for its clean up , IMO.
nit ```suggestion public EagerBufferConfigImpl(final long maxRecords, final long maxBytes, final Map<String, String> logConfig) { ```
How hard could it be to have `cleanUp()` check set a flag for the shutdown thread to call clean up after it's done? We want to make sure the shutdown thread checks after it changes the state to `NOT_RUNNING` or `ERROR`. I think this a much more flexible pattern that would be good to support if it's running or rebalancing we would still want to throw and `IllegalStateException` or maybe `IllegalOperationException`. But either way it would be good to test when stream is in a` PENDING` state. But I think you would have to make that a unit test
I missed the fact that we moved the `waitForCondition` check _inside_ of the try-catch block... For this case, we need to call `close` explicitly of course, as we are still in the block and `close()` is not auto-called yet... Sorry for the confusion.
+1 -- also below in other tests.
We can remove this line now. `close()` will be cause automatically using try-with-resource clause.
As `KafkaStreams` implements the `AutoCloseable` interface now, `close()` should be called automatically when the `try {}` block is left -- that is the whole purpose of `AutoClosable` and try-with-resource construct -- it frees you up to call `close()` explicitly (so you cannot forget any longer).
Fine with me, too. Feel free to merge.
Sounds like we don't override the default after all? Or we have at least one test where that slipped through 
Fine with me either way. Thanks for the fix
nit: maybe clarify a bit better here, e.g. "EXACTLY_ONCE_V2 on the Streams config assumes that broker is at least 2.5 and hence understand ... ; if it is smaller than V2, then the brokers may be on older versions and hence ..."
I think it would be preferrable to call this "globalPartitionCount" and call the gauge "globalPartitionCountGauge" or something similar.
I think it would be preferrable to call this "globalTopicCount" and call the gauge "globalTopicCountGauge" or something similar.
For compatibility, the mbean needs to be `kafka.controller:type=KafkaController,name=GlobalTopicCount`. That means you need to pass "KafkaController" here instead of "ReplicationControlManager".
For compatibility, the mbean needs to be `kafka.controller:type=KafkaController,name=GlobalPartitionCount`. That means you need to pass "KafkaController" here instead of "ReplicationControlManager".
nit: we can do without the curly braces here and above. However, these will soon be replaced by the actual impl
Do we need this? Can't we just use the Time we pass into the constructor in tests? Not a big deal really, just wondering
nit: max-in-flight cannot be larger than 5 if idempotent producer is enabled.
typo: with message ordering preserved for any allowable **vlaue** --> **value**
The `enable.idempotence` will be true by default in 3.0, so we should perhaps have the "up to 5" thing first.
If we're adding all of one list to another list, we should use `extend`, e.g. ```python sasl_mechanisms.extend(self.additional_sasl_mechanisms) ```
nit: two spaces between code and `#`
Do we make use of this new argument anywhere? I can't seem to find any
Safer to check `is not None`
may happened -> may happen How about rephrase to `When talking to the startup phase of a broker, it is possible to receive an empty metadata set, which we should retry later.`
nit: remove `throws` declaration (not needed).
Seems we setup the window with `500ms`: ``` SlidingWindows.withTimeDifferenceAndGrace(ofMillis(500L),...) ``` Thus we should pass in the same value instead of `MAX_VAVLUE`. We should also change `TestRecord<String,String>` below to `TestRecorded<Windowed<String>,String>`
I think I'd remove it or at least tag it as an integration test.
I checked. `toString` is defined for `FileRecords`. It looks like it has enough information to debug the issue.
Thanks, all. I share the concern that this change could suddenly break something. Since the transformer has access to the ProcessorContext, people well may be accessing these members within the `transform` method. For example, a transformer may suddenly start to get an NPE on the topic name, and so forth. This behavior change might be ok on the grounds that the context we previously provided was actually incorrect, though. If we do go ahead with this approach, we should be sure to make note of this potential in the upgrade notes. It wouldn't hurt to have a ticket as well, for visibility's sake.
For the longer term, I feel that we either need to 1) store the topic / offset information into the upstream materialized store as well, or 2) just disable this optimization for KTable.transformValues(), or at least allow users to either opt-in or opt-out given their knowledge on the context. As for now, I think leaving the offset as -1 and topic as null seems okay -- admittedly this would break someone who's using the context for offset / topic, as they would get unexpected values or even NPE, but that's still a fix forward then getting incorrect values silently.
Are there ever situations where users would want the old behavior (to have access to the `ProcessorContext` for the record that triggered the lookup, rather than the context for the record that's being looked up)? For example, if the topic name is relevant for the transformer and all records (including the current one that triggered the lookup and the one being processed) are from the same topic, then the old behavior gives access to the topic name but this new behavior doesn't.
I think we don't need this one.
You can also remove the `@SuppressWarnings("deprecation")` at the top of the method.
You can also remove the `@SuppressWarnings("deprecation")` at the top of the method.
We should do this check first
Good point, @mjsax .
SGTM, thanks @mjsax to bring to our attention.
...if sending old values
Some of these lines have extra 4 space. Other lines have extra 8 spaces.
nit: missing a space after the comma. Also, it would be nice to be consistent with the use of comma vs period for the similar cases in this file
I can see the reasoning behind doing it this way, but for all the other RPCs we've just been letting the future complete as an error, and making the caller handle it. One issue with changing the pattern, I suppose, is that not all the controller functions return an RPC data structure that allows setting an error. So let's keep the current pattern here for now, where caller has to handle the future completing exceptionally.
nit: Empty line could be removed.
nit: Empty line could be removed.
nit: We could use `TestUtils.assertFutureThrows` here.
nit: I would move this block up, before calling `listOffsets`, in order to have the response ready. Same for other tests.
nit: Could we indent the block such that `}});` is aligned with `ListOffsetsResult`? Same for other tests.
As discussed yesterday, the matcher is not called. Therefore, I think that we should remove the logic here as it is misleading. The condition does not bring much anyway. Please, check the other usages of `prepareUnsupportedVersionResponse`.
This does not really align with the name of the test. Is it intentional to have it? The test basically verifies that the first request is failed with an `UnsupportedVersionResponse` error and that the second request does not get any response.
nit: We could use `singletonMap` here.
I would include the non-null constraint in the sentence, eg "Returns a non-null resource pattern on which this action is being performed".
We need to update this based on what the previous registration was, if any. If the previous registration was also fenced, we do not want to increase the fenced broker count. It is also possible for the new registration to start as unfenced. Please look at the record definitions. It would be useful to have a helper function that took as an argument the previous registration (or null) and the new registration (or null), and updated the metrics accordingly.
Can you search the file for references to this? If there are none, please remove it.
How about moving this ahead of `pollPhase()`? We are likely to be kicked out of the group while blocked waiting here, so it's better to be aware of that and re-join the group immediately, rather than doing the restore/etc still which may be all wasted work.
nit: I do think a short message would be helpful here, even if it's just "Failed to deserialize record with type {type}"
Maybe we can still improve the little helper. For example: ```java short readUnsignedIntAsShort(Readable input, String entity) { int val; try { val = input.readUnsignedVarint(); } catch (Exception e) { throw new MetadataParseException("Error while reading " + entity, e); } if (val > Short.MAX_VALUE) { throw new MetadataParseException("Value for " + entity + " was too large."); } return (short) val; } ```
Yes, I think you should try to experiment with putAll()/range/reverseRange/prefixSeek operations as you proposed with a simple Kafka Streams app. That would be great to better understand the potential of direct buffers for Kafka Streams. Maybe experiment also with different key and value sizes. I am curious if we will also get such improvements.
Also, we should try to avoid serializing data into byte[] arrays and then copy the data into directBuffers. Instead we should serialize directly into "direct" ByteBuffers. For this we might need to have RocksDBStore implement a ByteBuffer interface, e.g., KeyValueStore<Bytes, ByteBuffer>, or anything similar...
The only case I can think of that might have high concurrency on RocksDB state store is with interactive queries. Without interactive queries there is no concurrency on the state stores since only the stream thread that has assigned the stateful task owning the state store accesses the state store.
I'm not very familiar with the direct buffer usage pattern, but currently it seems we would still try to allocate a new buffer for each put call, whereas I "thought" the main benefits come from reusing the buffer across multiple put calls? @vamossagar12 @ableegoldman @cadonna please correct me if I'm wrong.
Re (1) if all use cases are single threaded then yes we can allocate some buffer(s) as part of the store. Otherwise, if you need to support multiple concurrent ops then you could pre-populate a queue with a N buffers, and N becomes the maximum number of concurrent requests you can server. If you're queue is empty then a request would have to wait until at least one of the outstanding requests completes and add its buffer to the queue. Again that might all not be needed given the API is single-threaded. Re (2), is there a max-size, maybe given by the maximum Kafka message size that is configured (if such a limit exists and is not too big)? If we don't want to change the API (I guess it would be the RocksDBStore interface that would be changed which is not exposed I think, but still) then splitting this work into part I where we copy heap to direct buffers, and then a part II where we directly serialize into direct buffers is a way to go.
Yes, I had the same thought as @guozhangwang, but I am also not familiar with direct buffers.
You might want to pull this up or into a separate method to avoid checking the direction twice
Hmm, can you add a log message here stating that we're going to be generating a snapshot, and how many record bytes we had since the last one? It's OK to start with INFO for now, I think...
Can you add a log here indicating that we're processing a snapshot load (and what offset we're loading...)
as above. use `StreamsConfig#...`
We can remove `requireNonNull` here, because `getter.keySerde()` would already throw a `ConfigException` if the default serde is null.
nit: `pc` -> `oldProcessorContext` `apiPc` -> `newProessorContext` `ssc` -> `stateStoreContext` below `c` -> `context`
as above. `requireNotNull` not necessary any longer
as above. `requireNotNull` not necessary any longer
`requireNonNull` seems not to be necessary
nit: toString not necessary
nit: we could just build the expected result as a whole set and compare
This test is highly overlapping with `testOldBrokerAbortTransaction`, could be good to refactor out a common flow.
I had a look at this as well and I could not find a clean way to do this. Overall, I believe that using commands is a great move for the future so I am also fine with leaving it as is. I think that we should aim for reworking the other tools to use a similar pattern in the future. We could perhaps file a JIRA to not forget about it.
Using commands is a really good idea. It makes the implementation so much clearer. One point to clarify is wether we want to prefix commands with `--` in order to remain consistent with the other command line tools. What is your take on this? It seems that we defined aliases for commands so we might be able to have both.
nit: `--topic` and `--partition` could be extracted as helper static functions
Could we define subclasses in their corresponding files instead of squeezing all of them into one file? Even better, we could get a sub-dir called `transaction` to contain all of them
I think we can avoid this alignment style. It leaves us with significantly less space to write lambdas, etc. (another indicator is that this style is not applied elsewhere in the file). Two tab stops in the line below should be fine, even if the declaration above is where it is now.
Seeing above, seems that this might as well be called `onRestart`. It's only naming but still would be nice to maintain consistency
nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. ```suggestion AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus) ? AbstractStatus.State.RESTARTING : connectorStatus.state(); ```
I don't think we have examples in Connect where we refer to an argument in the name of a method. Maybe we don't want to change this just yet with the opportunity of the changes introduced by this feature. Another observation is that we don't use `get`, `set` and possibly `build`. But since it wouldn't be obvious if it's an action or an object maybe leaving `buildRestartPlan` might be fine here. (`restartPlan` would be the alternative)
A bit confusing that a second assignment follows if the `if` statement is true. I'd also call the variable `taskState` (as opposed to `connectorState` above) Ternary can be used here as well: ```suggestion AbstractStatus.State state = request.shouldRestartTask(taskStatus) ? AbstractStatus.State.RESTARTING : taskStatus.state(); ``` (as with any suggestion from github, please check it compiles and conforms to the style)
Nit: or empty **if** this worker ....
Did you compare it with other methods? My understanding is that `onDelete` is the same in that respect and exists on the `Listener` as well. But I might have skimmed too quickly through the code
nit ```suggestion expectedSensor, StreamsMetricsImpl.CACHE_LEVEL_GROUP, tagMap, hitRatio, HIT_RATIO_AVG_DESCRIPTION, HIT_RATIO_MIN_DESCRIPTION, HIT_RATIO_MAX_DESCRIPTION ); ```
Worked fine when I tried it locally: ```java assertEquals(Collections.singleton(tp0), records.partitions()); ```
You could submit a minor follow-up if you like.
We can use `assertThrows` for this kind of pattern: ```java RecordDeserializationException rde = assertThrows(RecordDeserializationException.class, () -> consumer.poll(Duration.ZERO)); assertEquals(invalidRecordOffset, rde.offset()); assertEquals(tp0, rde.partition()); ```
Do we need `invalidData`? Seems like we can just do this: ``` if (i == recordIndex) { throw new SerializationException(); } else { i++; return super.deserialize(topic, data); } ```
Might be simpler to use the mock deserializer only for values.
nit: instead of `new HashSet<>(Collections.singletonList(tp0))`, you can use `Collections.singleton(tp0)`
It may also be useful to assert that the current consumer position is equal to `rde.offset`.
nit: remove extra newlines
```suggestion /** * Source topic partitions of the task. * * @return source topic partitions */ ```
```suggestion /** * Metadata of a task. */ ```
```suggestion * Metadata of a Kafka Streams client. ```
```suggestion /** * The value of {@link StreamsConfig#APPLICATION_SERVER_CONFIG} configured for the Streams * client. * * @return {@link HostInfo} corresponding to the Streams client */ ```
```suggestion /** * Client ID of the admin client used by the stream thread. * * @return client ID of the admin client */ ```
```suggestion /** * Client IDs of the Kafka producers used by the stream thread. * * @return client IDs of the Kafka producers */ ```
`TaskMetadata` -> `ThreadMetadata`
```suggestion /** * Changelog topic partitions for the state stores the standby tasks of the Streams client replicates. * * @return set of changelog topic partitions of the standby tasks */ ```
```suggestion /** * Client ID of the Kafka consumer used by the stream thread. * * @return client ID of the Kafka consumer */ ```
```suggestion /** * Metadata of the standby tasks assigned to the stream thread. * * @return metadata of the standby tasks */ ```
```suggestion /** * Metadata of the active tasks assigned to the stream thread. * * @return metadata of the active tasks */ ```
```suggestion /** * Name of the stream thread * * @return the name */ ```
```suggestion /** * Names of the state stores assigned to standby tasks of the Streams client. * * @return names of the state stores assigned to standby tasks */ ```
```suggestion /** * State of the stream thread * * @return the state */ ```
```suggestion /** * Metadata of a stream thread. */ ```
```suggestion /** * Time task idling started. If the task is not currently idling it will return empty. * * @return time when task idling started, empty {@code Optional} if the task is currently not idling */ ```
```suggestion /** * End offsets of the source topic partitions of the task. * * @return map source topic partition to end offsets */ ```
```suggestion /** * Offsets of the source topic partitions committed so far by the task. * * @return map from source topic partitions to committed offsets */ ```
```suggestion /** * Host where the Streams client runs. * * This method is equivalent to {@code StreamsMetadata.hostInfo().host();} * * @return the host where the Streams client runs */ ```
```suggestion /** * Port on which the Streams client listens. * * This method is equivalent to {@code StreamsMetadata.hostInfo().port();} * * @return the port on which Streams client listens */ ```
```suggestion /** * Task ID of the task. * * @return task ID consisting of subtopology and partition ID */ ```
```suggestion /** * Client ID of the restore Kafka consumer used by the stream thread * * @return client ID of the restore Kafka consumer */ ```
```suggestion /** * Source topic partitions of the active tasks of the Streams client. * * @return source topic partitions of the active tasks */ ```
```suggestion /** * Names of the state stores assigned to active tasks of the Streams client. * * @return names of the state stores assigned to active tasks */ ```
Using generic types instead of raw types for collections is preferable (we can fix elsewhere in the file too) ```suggestion List<?> items = (List<?>) value; ```
nit: I think formatting corrects this (at least on intellij). Can be fixed below too ```suggestion for (Object item : items) { ```
`instanceof` checks for `null` too. I wonder if it's better to combine these two cases to say that we expect a list with at least one value (meaning a non-empty list).
In 1st standby assignment: > Used clients will get incremented since we can allocate the client on different zone and cluster. usedClients=2 I think this used client should be 5 or 6, right? But I got your idea. Thanks for the explanation. Makes sense to me.
nit: additional space between `the` and `2nd`
I do not understand why you re-add `clientsOnAlreadyUsedTagDimensions`. Those clients were not modified and not polled for sure due to line 140.
Something does not work as expected in this algorithm. According to this doc, the assignor should fall back to distributing tasks on least-loaded clients. However, the following test case fails: ``` @Test public void shouldDistributeTasksOnLeastLoadedClientsWhenThereAreNoEnoughUniqueTagDimensions() { final Map<UUID, ClientState> clientStates = mkMap( mkEntry(UUID_1, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_1), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_0)), mkEntry(UUID_2, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_2)), TASK_0_1)), mkEntry(UUID_3, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_2)), mkEntry(UUID_4, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_1_0)) ); final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates); final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, CLUSTER_TAG, ZONE_TAG); new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs); assertEquals(1, clientStates.get(UUID_1).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_2).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_3).standbyTaskCount()); assertEquals(1, clientStates.get(UUID_4).standbyTaskCount()); } ``` The standby task for active task 0_0 can be put on client UUID_2 and the standby task for active task 0_1 can be put on client UUID_1 without breaking rack awareness constraints. Standby tasks for active tasks 0_2 and 1_0 cannot be put on any client without breaking rack awareness, so they should be distributed on least-loaded clients. However, that does apparently not happen, because client UUID_3 and UUID_4 are not assigned any standby.
For each standby of a single active task the set `clientsOnAlreadyUsedTagDimensions` is computed from scratch. I think this is not necessary since the clients on already used tag dimensions that we found for the first standby are still valid for the second standby and the clients on already used tag dimensions found for the second standby are still valid for the third standby and so on. This is true because we only add clients to the set `usedClients` but we never remove any. I think we can compute `clientsOnAlreadyUsedTagDimensions` incrementally for each standby of a single active task instead of computing it from scratch each time.
```suggestion // for the 2nd standby task assignment would effectively mean excluding all the clients. ```
I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`.
A ha, you're right! We only need the distinct count values. No need to refactor it then. Thanks for the explanation.
I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`.
nit: lastUsedclient -> lastUsedClient
Wouldn't this be equivalent and maybe a bit more concise? ```suggestion UUID lastUsedclient = activeTaskClient; do { fillClientsOnAlreadyUsedTagEntries( lastUsedclient, countOfUsedClients, rackAwareAssignmentTags, clientStates, tagEntryToClients, tagKeyToValues, tagEntryToUsedClients ); final UUID clientOnUnusedTagDimensions = standbyTaskClientsByTaskLoad.poll( activeTaskId, uuid -> !isClientUsedOnAnyOfTheTagEntries(uuid, tagEntryToUsedClients) ); if (clientOnUnusedTagDimensions == null) { break; } clientStates.get(clientOnUnusedTagDimensions).assignStandby(activeTaskId); countOfUsedClients++; numRemainingStandbys--; lastUsedclient = clientOnUnusedTagDimensions; } while (numRemainingStandbys > 0); ```
Map `statefulTasksWithClients` is only used to iterate over its entries. I think it would be better to use the following nested loops and remove `statefulTasksWithClients`: ```suggestion for (final TaskId statefulTaskId : statefulTaskIds) { for (final Map.Entry<UUID, ClientState> entry : clients.entrySet()) { final UUID clientId = entry.getKey(); final ClientState clientState = entry.getValue(); if (clientState.activeTasks().contains(statefulTaskId)) { assignStandbyTasksForActiveTask( numStandbyReplicas, statefulTaskId, clientId, rackAwareAssignmentTags, clients, tasksToRemainingStandbys, tagKeyToTagValues, tagValueToClients ); } } } ```
Although we never use the returned value from a standby task assignor, I would return `false` since a standby task assignment will never require a follow-up probing rebalance.
nit: we can directly break the while when `numRemainingStandbys == 0`, so that we don't need to run the redundant `findClientsOnUsedClientTagDimensions` in the last run. Ex: ```java countOfUsedClients++; numRemainingStandbys--; if (numRemainingStandbys == 0) { break; } clientsOnAlreadyUsedTagDimensions.addAll( findClientsOnUsedClientTagDimensions( clientOnUnusedTagDimensions, countOfUsedClients, rackAwareAssignmentTags, clientStates, tagEntryToClients, tagKeyToValues ) ); ```
Thanks for the explanation for partial rack awareness assignment. I think that algorithm makes sense. However, I don't think the implementation matches what you described. You said in the `shouldDoThePartialRackAwareness` test, in 2nd standby assignment for task_0_0, we will only consider `zone`, but in current implementation, we will also consider `cluster`. That is, when entering the `while (numRemainingStandbys > 0) {` loop, the `clientsOnAlreadyUsedTagDimensions` already excluded the `cluster_1` and `zone_1`. And in the 1st standby assignment, `UUID_5` will be chosen, we'll exclude `zone_2` only, and not exclude `cluster_2`. So , the only client left is `UUID_6`. That's the current design, isn't it? I don't see where we only consider `zone` in 2nd assignment. Could you help elaborate more? Thank you.
I think we should re-work the `assignStandbyTasksToClientsWithDifferentTags` method to match what you described. That makes more sense. Thanks.
Yes, a test is absolutely necessary!
I think info log would also be OK here. I imagine users that are wondering why their standbys are not distributed as they would expect. With this information they can at least try to fix it on the config level. This log message should only happen at rebalance time, which should usually be rather seldom. If we decide to put the log message on info level, you should also change a bit the wording and not use variable names in it. Maybe some hints what the users can do to fix this would also be nice. Is it possible to separate the concerns of this log message and the one on line 135? Something along the lines of here the rack-aware standby assignment did not work due the tag config and on line 135 the assignment did not work due to too low number of instances. We can then put both on warn or info (do not forget to also check the related log message in the default standby assignor).
Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
Currently the code iterates over the active tasks and assigns all standby tasks for each active task. If the standby tasks cannot all be assigned, we might end up with all standby tasks assigned for some active task but none for others. What do you think about to assign one standby task for all active task and then assign the second standby task for all active task, and so on. In this way, it is more likely that at all active tasks have at least one standby task assigned. I am aware that the default standby assignor has the same drawback.
I see what you want to do. However, the capacity is the number of consumers on the Streams client, i.e., the number of stream threads running on the Streams client. With this check, you only allow to assign standby tasks to clients that have less tasks assigned as stream threads running. That is actually rather an unlikely case. Normally, you have more tasks assigned to a Streams client than the number of stream threads running on the client. I would keep it simple and ignore the balance for now.
I see your point, but I do also not see the need for an internal state for which we need to avoid invalidation. Variables `numStandbyReplicas` and `numStandbyReplicas` are configs that can be stored as member fields of `ClientTagAwareStandbyTaskAssignor` or passed along to the methods that need them. Variables `tagKeyToTagValuesMapping`, `clientsPerTagValue`, `standbyTaskClientsByTaskLoad`, and `clientStates` can also be passed to the methods that need them. Avoiding state makes reasoning about code simpler and here it seems possible to avoid state. See `HighAvailabilityTaskAssignor`, it does not have any state.
When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting.
The variable name `polledClient` is unreadable. I think the variable is the client not having the same tag key/value, right? Could we give it a more meaningful name, ex: `clientUUIDNotOnUsedTagDimension`, or other better one if you have.
```suggestion // This statement checks if we have used more clients than the number of unique values for the given tag, ```
nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
IMO, the code is easier readable if you name the variables consistently like `tagValueToClients` and `tagKeyToTagValues` or `clientsForTagValue` and `tagValuesForTagKey`. I prefer the former because it better visualises the mapping, but that is a matter of taste, I guess.
Isn't this the same as: ``` clientsPerTagValue.computeIfAbsent(tagValue, (ignored) -> new HashSet<>()).add(clientId); ```
Isn't this the same as: ``` tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue); ```
```suggestion statefulTasksWithClients.forEach((taskId, clientId) -> assignStandbyTasksForActiveTask( numStandbyReplicas, taskId, clientId, rackAwareAssignmentTags, clients, tasksToRemainingStandbys )); ```
```suggestion final Map<TaskId, Integer> tasksToRemainingStandbys = computeTasksToRemainingStandbys( numStandbyReplicas, statefulTasksWithClients ); ```
Yes, I think that makes sense. In this way you can also directly test the method. BTW, you can simply pass `statefulTaskIds` to this method instead of `statefulTasksWithClients`. The keys in `statefulTasksWithClients` should be the task IDs in `statefulTaskIds` and the values in `statefulTasksWithClients` are never used.
```suggestion final Set<UUID> clientsOnAlreadyUsedTagDimensions = findClientsOnUsedTagDimensions( usedClients, rackAwareAssignmentTags, clientStates, clientsPerTagValue, tagKeyToTagValuesMapping ); ```
Sorry, I didn't understand the reason why we can't filter out clients located on that tag when `allTagValues.size() <= countOfUsedClients`. Could you help explain to me? Thanks.
No, it's still best to do it at the most specific place in my opinion.
Please use static imports to make this more readable.
That's a good point.
This catch is a bit weird to me. Could you create a true `CompletableFuture` carrying the exception `new TimeoutException()` instead of mocking object? For example: ```java CompletableFuture<RecordMetadata> future = new CompletableFuture<>(); if (success) future.complete(new RecordMetadata(new TopicPartition("tp", 0), 0, 0, 0, 0, 0)); else future.completeExceptionally(new TimeoutException()); ```
Okay I think I know what's the messy part here: we are setting the stores during KStreamJoin which is only at the parsing phase, but not the logical plan generation phase. The key difference is that the latter has access to the user's configuration whereas the former is not. And because of that we have an unclean settings, we should decide e.g. which stores to create only at the logical plan aka StreamStreamJoinNode. This is a general issue with Streams parsing/logical plan generation, and I will file a ticket for it.
I'm thinking exactly the opposite :) if we have a bug which would cause us to create a state store, checking it twice may actually mask the bug: we would end up creating the state store, and then on the second check not getting it, so the behavior is still correct, and it'll be hard for us to discover we are creating state stores unnecessarily. If we have a bug and do not create state stores when needed, then we would behave in the old way without the fix; the key point here is that, we only have one decision point to make, and either that decision is correct or buggy, we can get it surfaced quickly.
Please split this up into a separate check for `if ((stateDir.exists() && !stateDir.isDirectory())` and then throw an accurate exception, eg `state directory could not be created as there is an existing file with the same name`
nit: we could use `else if` here.
I think we're testing `testDir` Occupied here, not `AppDir`.
Same here, this exception message does not apply to the case this is trying to catch
ditto here, please add a separate check and exception
Since we are adding `fenced` to the RegisterBrokerRecord, do we also need to add a `fenced` field to the BrokerRegistrationRequest RPC? Or is it the case that only the controller will set the fenced state of this record
If any template type `K` could bind to `Void` then that seems fine -- I was not clear about this myself :P
@vvcephei +1 I like the last proposal: using `Void` and keep the option to migrate to `K/V` later.
This method is what I'm referring to: ``` /** * Starts the Kafka cluster alone using the ports that were assigned during initialization of * the harness. * * @throws ConnectException if a directory to store the data cannot be created */ public void startOnlyKafkaOnSamePorts() { start(currentBrokerPorts, currentBrokerLogDirs); } ```
@ijuma , I found why the test `testBrokerCoordinator` failed. It's because we use `putIfAbsent` here, but in the `testBrokerCoordinator`, we want to bind previous allocated port [line 114](https://github.com/apache/kafka/pull/10872/files#diff-234389ba52863064119a9fbb8d1649d6a039a28790b7c186357e60570b0af049L114). If we use `putIfAbsent` here, it'll use default port `0` to have a random assigned port as [line 120](https://github.com/apache/kafka/pull/10872/files#diff-234389ba52863064119a9fbb8d1649d6a039a28790b7c186357e60570b0af049R120). So, we should directly `put` property here to fix it. FYI
I'm not sure this is really the right place to test the `prefixScan` functionality for all of these different store types, this test class is really more for making sure the topology itself is all wired up correctly. If you're just trying to test a method on a specific store type, that generally makes sense to do in the test class for that store itself. In other words you don't need to have a separate test here for each underlying store type (eg `PersistentTimestampedStore` or `LruMap`, etc), there are dedicated test classes for that (like `RocksDBTimestampedStoreTest` or `InMemoryLRUCacheStoreTest`) That said, it sounds like the original bug report uncovered the missing implementations "when accessing the state stores through the processor context" -- which does sound like it could/would be reproduced through a test here. Maybe you can just pick a store type and write a single test that reproduces the issue when run without this patch, and I would consider that sufficient for this.
I know this is just how the other tests are doing it, but it's not really an airtight way to validate the expected results...if nothing is returned then we never enter the `while` loop and the test passes, even if we did in fact expect there to be actual output. The important thing here was just to make sure it didn't throw an exception so it still does that, but it would be good to fix this up maybe in a followup PR
as above (similar elsewhere)
1) Could you just treat them as Bytes all the same, and just convert to/from an Integer before putting/getting them from the store? That way you're still just handling Bytes like you are in this test, it just goes through an extra layer of de/serialization. Should be able to more or less copy over the existing tests with just a bit of extra code. Can you try this, in a followup PR? 2) Yes, I was just suggesting to merge them as a possible way to make things easier and do less work, if it's going to be more then please do file a separate ticket for it.
Yup, I think that's also fine.
nit: you can return directly from these branches and get rid of `result`
Hmm, I like `InvalidTopicException` over `IllegalArgumentException` if we are raising it through the future. Typically `IllegalArgumentException` is raised directly.
nit: misaligned (`handleDeleteTopicsUsingIds` as well)
This message is a little strange. We can certainly represent the topic id, but it is invalid. I wonder if it would make sense to raise `IllegalArgumentException` directly instead of through the result since this is likely a logical error of some kind.
Why do we copy the result of `handleDeleteTopicsUsingIds`? Seems like that method is already returning a fresh map.
typo: we want to test the **case** that poll() returns no records.
In two of this control messages we pass the `currentTimeMs` in this one we don't. It is nice to be consistent. I think that `appendLeaderChangeMessage` passed the `currentTimeMs` because it want to use the same time for the entire `poll` call.
How about moving the buffer allocation to `appendControlMessage`? Not sure we need different messages in the exception since we will have different stacktraces.
This is an existing bug but it looks like we never release (`MemoryPool::release`) if there is an exception before adding to `completed`.
There is code duplication between these 3 methods. Let's figure out a way to remove this duplicate code.
At this layer I think the best we can do is `forceDrain()`. It should be a noop if there are no records in the current batch.
I think that having tests that check this behavior for `SnaphsotWriter` is the best we can do.
```suggestion if (!batch.records().isEmpty()) { return Optional.of(batch); } ```
I think the risk of introducing `options()` is that some developers might accidentally use `values()`. The pattern used in `ConnectorType` is far better, as it overrides the `toString()` method. That doesn't handle the case-independence for parsing, though `ConverterType` is a better pattern to follow if that's required. Let's be consistent with the new enums, and have each follow one of those two patterns depending upon whether parsing case-independently is required.
Nit, to improve readability and precision, especially around how many Kafka transactions would be used: > Whether to enable exactly-once support for source connectors in the cluster by using transactions to write source records and their source offsets, and by proactively fencing out old task generations before bringing up new ones.
It's true that the `DistributedHerder.run()` is ultimately catching and handling this exception. But I feel like many users might not understand the significance of such an error nor how to correct their configuration. Rather than just re-throw that exception, we should probably wrap that exception with one that has a more instructive message, such as something like: > Enabling exactly once for source connectors requires a Kafka broker version that allows admin clients to read consumer offsets. Disable the worker's exactly once support for source connectors, or use a newer Kafka broker version. Plus, should this if block be before the `log.debug(...)` on the previous line? Seems like that log message might just confuse the situation since the worker will not read "to the end of log offsets with consumer".
IIUC, this changes the behavior of the `WorkerConnector` created below. Prior to this PR, the `WorkerConnector` was always created with the `Worker.offsetBackingStore`, even for sink connectors. However, with this PR, the `WorkerConnector` will be instantiated with a null `offsetReader` parameter, which will cause a NPE in `WorkerConnector#doShutdown()` and `WorkerConnector#cancel()` since `WorkerConnector` does not check for a null parameter there.
Nit: move these two static factory methods above the non-static member variables, so all static and non-static members are together.
Suggestion: ```suggestion // Use the desired topic for offsets ```
Suggestion: ```suggestion // Set up the offset backing store for this connector instance ```
It's probably worth while to mention that this method starts the task for a source connector with older behavior (without exactly once support).
Do we need to change these signatures from `ConnectorTaskId` to `String`? The `ConnectorTaskId` gives us the ability to define tasks-specific client configuration properties if necessary/desired. I'm afraid that switching to `String` will make it harder and more invasive to add that back in. Plus, if there's not a good reason to remove these, let's leave that for smaller PRs.
Thanks! This should have been done quite some time ago.
Note to future me: I didn't get this far in the PR.
IIUC, the `WorkerTransactionContext` is the only implementation of this. That means that if a connector is configured with `transaction.boundary=poll` or `transaction.boundary=interval`, a poorly-implemented connector could still call these methods and they'd unnecessarily accumulate records. WDYT about in such cases the `SourceTaskContext#transactionContext()` method returning a no-op implementation of this interface, so no harm is done if a connector implementation still calls these methods when `transaction.boundary` is _not_ set to `connector`? Maybe we could consider a warning log message if these methods are called by a connector inappropriately. But we have to be careful. While such log messages might be useful for the **developer** of a connector plugin, I would argue that _prolific_ warnings are actually harmful for a **user** trying to _use_ a connector plugin they didn't develop with a connector configuration that includes `transaction.boundary=poll` or `transaction.boundary=interval`. So maybe it's worthwhile for the "no-op" implementation to only log each warning once per method per instance.
Nit: using a new paragraph makes this stand out more. ```suggestion * <p>For backwards compatibility, the default implementation will return {@code null}, but connector developers are ```
WDYT about something like this: ``` /** * Signals whether the connector implementation is capable of defining the transaction boundaries for a * connector with the given configuration. This method is called before {@link #start(Map)}, only when the * runtime supports exactly-once and the connector configuration includes {@code transaction.boundary=connector}. * * <p>This method need not be implemented if the connector implementation does not support definiting * transaction boundaries. * * @param connectorConfig the configuration that will be used for the connector * @return {@link ConnectorTransactionBoundaries.SUPPORTED} if the connector will define its own transaction boundaries, * or {@link ConnectorTransactionBoundaries.UNSUPPORTED} otherwise. * @see TransactionContext */ ```
There are projects outside of Apache Kafka that do use this class. While this class is not a public API and we technically don't have to avoid breaking compatibility, it's fairly straightforward to maintain API compatibility and so we should do that.
Did you consider introducing a new `OffsetBackingStore` implementation that writes to two other `OffsetBackingStore` implementations? That might simplify the logic in this class and better encapsulate the double write behavior.
We don't need this `<p>` tag.
As mentioned above, IIUC this PR will sometimes pass a null `offsetStorageReader` (IIRC for sink connectors), but this class currently expects that to be null. Might be worth adding a `Objects.requireNonNull(...)` call here to help catch that situation.
Can this be reverted? The existing formatting appears to be correct. ```suggestion public class ErrorHandlingMetrics { ```
Rather than do this bookkeeping here, could we pass the `ErrorHandlingMetrics` instance to the `WorkerTask` class in its constructor, and then close it in `WorkerTask::removeMetrics`? It'd align nicely with the existing contract for that method, which is that it will "Remove all metrics published by this task."
Nit: ```suggestion @Override public void close() { ```
Nit: whitespace ```suggestion public void closeTaskErrorMetricGroup() { ``` Also, just curious, any reason we don't want to implement `Autocloseable` and rename this method to `close`? It'd align nicely with the precedent set in https://github.com/apache/kafka/pull/8442, for example, and would make this class easer to use with [Utils::closeQuietly](https://github.com/apache/kafka/blob/e8dcbb99bb3289193a9036599d87acd56e11499f/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L998-L1009) if we wanted to go that route in the future.
I see what you mean, and yea that is a fair point 
`0` is also an invalid epoch, right? Or does the raft client send `handleLeaderChange(LeaderAndEpoch(OptionalInt.empty(), 0))` to the listener? I still think it is fair to say that the raft client will never send `handleLeaderChange(LeaderAndEpoch(OptionalInt.of(...), 0))`
Does it make sense to do this check before the epoch validation? If we're not the leader and received an old epoch (which, if i understand, seems likely if we're _not_ the leader anymore), we will silently ignore in the above case.
unrelated, but maybe worth creating helper method that returns `Optional<GracefulShutdown>` to avoid these null checks throughout
Okay. We do similar synchronization for `append`. The `LeaderState` has an `epoch` and it is final. The part that may be tricky to implement is the `epoch < currentEpoch` case.
Got it. For future readers the function is `maybeCompleteShutdown`.
Why adding both joinAfterMs and joinBeforeMs? The records expire when `window().start() + joinAfterMs + joinGraceMs` are lower than maxStreamTime. For instance, say we have a record in the shared state store with time = 1. Now a new record arrives with time = 17. ``` inputRecordTime = 17 maxObservedTime = 17 minTime = 1 window = 10 (beforeMs = 10, afterMs = 10) grace = 5 ``` Isn't the record 1 suppose to expire and be emitted because 1 + 10 (afterMs) + 5 (grace) = 16? which is lower than maxStreamTime? With the condition you have, the minTime registered is 1, so `(1 >= 17 - 10 - 10 - 5)` is true, and thus it returns and do not emit the record 1 until another 10 ms has passed.
There's one extra thing to do. We should set `minTime = MAX` before opening the `store.all()` so it resets the minimum in case there are no records available in the iterator. This is an example I run: I have a few records in the shared state store (1,5,7). Then a new record arrives that expire all the 3 records. Record 50 for instance. For each record, the minTime will be set to 1, then 5, then 7. Now for every new record after 50 that is still part of the window, the condition at the beginning of this method `minTime > maxStreamTime - ...` will be false, thus opening the iterator again and again. If we reset the minTime to MAX, then the next time, the iterator will be opened, but no records will be available, so minTime will stay in MAX. And the future records that do not expire will not open the iterator because `minTime (MAX) >= maxObservedTime - ...`
Nice explanation! + 1 to reset to `Long.MAX_VALUE`
Shouldn't be good to move this code inside `StreamsConfig.InternalConfig`? I did that for the `getBoolean` so I could re-use it in other places. This is a good candidate for internal configs.
Hmm...I'm a little less sure about this, but I think we should make sure that WrappingStoreProvider's view of the stream thread storeProviders also stays up to date when threads are added/removed. Basically if a user calls KafkaStreams.store() then adds/removes a bunch of threads without refreshing the store provider, any subsequent get() on that provider would only see the threads that existed at the time KAfkaStreams.store() was called if we make a copy like this. We should be able to just modify the WrappingStoreProvider constructor/local field to be a Set or even a Collection instead, since all it ever does is loop over this. Then we can just pass in storeProviders.values() and it's all good
Same as above mentioned, the validation didn't get handled in new API.
This naming is confusing to uses. This will make user think that 24 hours grace period cannot be used anymore. No I don't think that's what we want. 24 hours is still good to use, if user believe that's what they want. That means, we don't **deprecate** the 24 hours grace period, just don't set as default value, so we should not name it as "deprecated", "old", things. Correct me if I'm wrong, @izzyacademy @ableegoldman . Thank you
Could we make this Constructor to call another overloaded constructor, to avoid duplicated codes? i.e. ```java protected JoinWindows(final JoinWindows joinWindows) { this(joinWindows.beforeMs, joinWindows.afterMs, joinWindows.graceMs, joinWindows.enableSpuriousResultFix); } ```
`out-of-order` (with `-`)
`of` -> `or`
`before or after`
`out-of-order` `window closed`
nit. I think there is `.` missing `since 3.0[.] Use`
This is not completely compatible with the behavior of older Streams apps. See #10953 for a fix and more details.
`out-of-order` (this may be a typo on other places, too) Can you fix everywhere? `window end` -> `window closed` (same -- please also fix elsewhere)
Missing `.` after `since 3.0`
`earlier or later` -> `before or after` (to avoid confusion with the term "late data")
Personally I think it makes sense to just disallow calling `ofTimeDifferenceAndGrace(...).grace(...)` entirely, this seems like abusing the API
Yeah it's an internal config so I hope they wouldn't assume anything from the name and extrapolate to what they can and can't use. That said, it does appear in these classes which are public themselves, so users are still going to see it. But the important thing is that it makes sense to us, the devs, who will actually be using it -- I think `DEPRECATED_DEFAULT_24_HR_GRACE_PERIOD` is a bit more clear, just need to sneak the word "default" in there somewhere imo
We should call out explicitly that this is setting the grace period to 0, which means that out of order records arriving after the window end will be dropped. Otherwise it's too easy to just use this method without thinking any further about the grace period and what it means/whether you want it
```suggestion * Return a window definition with the window size and no grace period. Note that this means out of order records arriving after the window end will be dropped. ```
I picked up all the non-testing followup work in this PR so we could try to get it into 3.0: https://github.com/apache/kafka/pull/11114
The Kafka project tends not to prefix accessors with `get`: ```suggestion public int handleSnapshotCalls() { ```
We should only store `handleSnapshotCalls` since `handleSnapshotCalled` is always equal to `handleSnapshotCalls > 0`.
nit: Could we add some java doc here to explain why we need this? Thanks.
Let's use `assertEquals`: ```suggestion assertEquals(0, raftNode.counter.getHandleSnapshotCalls()); ```
I think this is the issue you reported in the Jira. The `RaftClient.Listener` should not use `RaftClient.leaderAndEpoch` to determine if it is the leader. It should instead use `RaftClient.Listener.handleLeaderChange`. For this state machine `ReplicatedCounter` we should look at the `claimedEpoch` variable. I am going to create an issue to remove this method. cc @hachikuji
yeah, I was thinking the same.
Having names that sound like simple accessors probably makes this kind of performance issue more likely. Maybe we could rename `cluster()` to `buildCluster()` so that it is clear there is some work.
Is this also logged if there is nothing to be committed? Just double checking to ensure we don't introduce a new issue -- we only commit, if we have anything to commit IIRC.
Does this test even use standbys? IIRC, the idea was to ensure that the instance really started to process data, so we don't move to the next step, too quickly in the test.
It's a little unclear as to how we are removing things from the cache here. If I understand correctly, we stick an empty Optional in the cache to represent removal. And here by _not_ handling the empty optional case, we exclude the broker from the new image. But if that's the case, why do we need the Optional in the first place? Can we directly remove the broker from "changedBrokers" when handling UnregisterBrokerRecord? This question applies to some other Delta classes that are using Optional as well
nit: this doesn't throw IOException. You can remove all of these all the way up to `MetadataImage.write`.
I think we are guaranteed to have the listener present, but perhaps it's worth checking explicitly and throwing if it is not the case.
Yeah, I am not sure. I was thinking we might run into situations where we are trying to detect when a cached image has changed. It is a conventional thing to do, but I don't feel too strongly about it.
I think this could be done with `computeIfAbsent` like in "finishSnapshot" above
nit: might be helpful adding a little helper since we do this a few times in here
nit: I guess you could use `computeIfAbsent` here as well
nit: maybe worth adding `this == o` to these `equals` implementations. This applies to all of the similar classes in this PR.
Hm, kind of annoying that we have to return Properties here, but (as far as I know) there is no way to make an immutable Properties
You might consider using `OptionalDouble`.
There may be a couple validations we have in `ZkAdminManager.describeClientQuotas` that do not appear here. For example, we detect invalid mixes of ip and user searches: ``` if ((userComponent.isDefined || clientIdComponent.isDefined) && ipComponent.isDefined) throw new InvalidRequestException(s"Invalid entity filter component combination, IP filter component should not be used with " + s"user or clientId filter component.") ```
(very minor) nit: inconsistent naming of these in this class
nit: add a size? There are a few cases in here where we could do this.
Instead of "Using the newly updated metadata," maybe we can say this: > Resetting the last seen epoch to {}.
Yes, I was just pointing out that there is still a gap.
If you pass the new one, then you can probably get rid of `changedTopicId`
Yes, the logging is what I had in mind. The log message is misleading otherwise.
nit: seems like it would be more useful for the log message to indicate the topic ids that changed instead of the unrelated epochs.
We should also update the java doc to mention the TopicId changed case
nit: 2 spaces between `synchronized` and `String`
nit: maybe we can pull out a variable for `metadata.topic()` since there are 10 or so uses
nit: I think we're guaranteed that `topicId` is not null (in spite of the inconsistent `equals`), but it's still a little nicer to write this check as `!Uuid.ZERO_UUID.equals(topicId)`
```suggestion assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD)).gracePeriodMs()); assertEquals(0L, JoinWindows.of(ofMillis(DEPRECATED_OLD_24_HR_GRACE_PERIOD + 1L)).gracePeriodMs()); ```
Alternatively, you could use `24 * 60 * 60 * 1000L` instead of `DEPRECATED_OLD_24_HR_GRACE_PERIOD` in the tests.
I think you also need the following test (here and in the other `*Windows`): ``` assertThat(DEPRECATED_OLD_24_HR_GRACE_PERIOD, is(Duration.ofDays(1).toMillis())); ``` Otherwise we will not notice if `DEPRECATED_OLD_24_HR_GRACE_PERIOD` is changed by mistake.
We need to keep the old method as before and deprecate.
This is a breaking change in a public API since it removes the default constructor. In any case, don't really want this in the constructor, we should add methods for whatever we need. Actually looking at the rest of the changes in this class, we are repurposing an existing public API by changing all of its methods, we need to completely rethink this change.
We are using options in an inconsistent way here compared to other APIs. A good example to follow would be: ``` public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) ``` Options here are additional options that apply to the request. Data for the request comes from the first argument. We could do something similar for listConsumerGroupOffsets.
Don't we need to keep the existing methods for backward compatibility? We could perhaps deprecate them.
We need to keep this public method and deprecate. Perhaps throw an exception if multiple group ids were specified and retain existing behaviour for single group id.
Could you put these config settings before the overrides on line 100? Would be not so good to silently add the default serdes if the the overrides possibly unset them. I would also just use `put()` instead of `putIfAbsent()`.
Sorry, I realized it now. Why do you not just pass `config` where you now pass `defaultProperties()`? In this way we would not need `defaultProperties()` anymore.
nit: unnecessary newlines
nit: unnecessary newline
How about adding a `coordinators` method to `FindCoordinatorResponse` which would either return the list of coordinators (`data.coordinators()`) if not empty or would return a list containing a `Coordinator` created from the top level information. That would remove all the `batch` checks below.
Both of consumer coordinator and transaction coordinator parse response according to version. By contrast, it check `coordinators`. Could we unify the behavior? Could consumer coordinator and transaction coordinator check `coordinators` instead? It seems checking `coordinators` can simplify the code as we don't need to pass request version to each handler.
The above suggestion would also us to avoid having to pass the `requestVersion` down here.
ditto (code style)
this line can be merged. for example: ```java FindCoordinatorRequestData data = new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.rebalanceConfig.groupId); ```
nit: As we don't reuse `batch`, we could directly pass `list.subList(batchStartIndex, batchEndIndex)` to `accept`.
nit: Add `.` at the end of the sentence.
nit: We could instantiate a new `ListTransactionsOptions` here instead of using `null`. This would remove the `null` check below.
nit: This block is repeated in many tests. I wonder if we could define an helper for it.
nit: "topic name to limit search to. REQUIRED if --partition is specified."
nit: We could also define a helper method for this one to avoid the code repetition.
nit: This empty line could be removed.
nit: I would move this one next to `consumeInBatches` as they are used together.
please make sure the value of sensor is equal to mocked value.
Please using `Mockito.verify` to make sure `recorder.removeValueProviders(SEGMENT_STORE_NAME_2);` happens once when calling `recorder.removeValueProviders(SEGMENT_STORE_NAME_2);`
Please using Mockito.verify to make sure recorder.removeValueProviders(SEGMENT_STORE_NAME_2); is not executed when executing recorder.removeValueProviders(SEGMENT_STORE_NAME_2);
Please verify that no `sensor#record` is executed after `recorder.record(0L);`
why removing this line? It is intentional to initialize the recorder with different metrics.
why removing this line? this test is used to make sure we can't initialize the `recorder` multiple times with different task id.
for example: ```java recorder.addValueProviders(SEGMENT_STORE_NAME_1, dbToAdd1, cacheToAdd1, statisticsToAdd1); recorder.addValueProviders(SEGMENT_STORE_NAME_2, dbToAdd2, cacheToAdd2, statisticsToAdd2); recorder.removeValueProviders(SEGMENT_STORE_NAME_1); recorder.removeValueProviders(SEGMENT_STORE_NAME_2); Mockito.verify(recordingTrigger, Mockito.times(1)).removeMetricsRecorder(recorder); ```
Yes, it should be safe to convert to bytes and compare using MessageDigest.isEqual since SASL/PLAIN uses UTF8. Means more object creation, but that shouldn't be an issue. Utils.isEqual() may be worth adding if we think we may use it in other places as well in future for constant time array comparisons (in which case, we can make it generic).
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
Hmm, if we convert arrays to bytes, we need to be careful. If the arrays have different sizes, then the operation is not constant time.
How about using `MessageDigest.isEqual` method here also: ``` MessageDigest.isEqual(new String(password).getBytes(StandardCharsets.UTF_8), expectedPassword.getBytes(StandardCharsets.UTF_8)` ``` cc @rajinisivaram
We can return `false` on the first mismatch no need to check the rest of the arrays.
Good point. Btw, this particular class (the default SASL/PLAIN server-side callback handler) is not recommended for production use, so perhaps not that critical.
I believe the goal is to use constant-time comparison to prevent timing attacks, hence the walk through the arrays.
> this logic Seems only needed because we have the check in 309 (?) No, I don't think so. It should be for line 279: ```java // to handle the case that when there are still unassignedPartition left, but no more members to be assigned. if (unfilledMembersWithUnderMinQuotaPartitions.isEmpty() && unfilledMembersWithExactlyMinQuotaPartitions.isEmpty()) { throw new IllegalStateException("No more unfilled consumers to be assigned."); ``` In line 309, it is just an early error detect and log for it. Not related to `potentiallyUnfilledMembersAtMinQuota` (or now `unfilledMembersWithExactlyMinQuotaPartitions` members)
I tried to summarize both methods: What @guozhangwang 's meaning is, we can "lazily" detect the issue after assigning all unassignedPartitions. we don't need to clear the `potentiallyUnfilledMembersAtMinQuota` here, because as the "original" variable naming said: they are "potentially unfilled members", just keep them there. We "should not" assign partitions to them in this case because we've reached `expectedNumMembersAssignedOverMinQuota`. But if somehow, after assigning unassignedPartitions to all unfilledMembers, there are still some unassignedPartitions left. We can just assign them to `potentiallyUnfilledMembersAtMinQuota`. And after running out the `unassignedPartitions`, we can check: ``` numMembersWithMaxQuota == expectedNumMembersWithMaxQuota && numMembersWithMinQuota == expectedNumMembersWithMinQuota ``` to do error handling. VS. In @ableegoldman 's version , we find issue immediately and handle it. we computed the `potentiallyUnfilledMembersAtMinQuota` correctly (that's why we need to clear it). So, if the issue happened: > if somehow, after assigning unassignedPartitions to all unfilledMembers, there are still some unassignedPartitions left We can try to get member from `potentiallyUnfilledMembersAtMinQuota` and then assign unassignedPartition to the member. If we can't get member from it (i.e. `potentiallyUnfilledMembersAtMinQuota` is empty), we throw exception directly. Both ways can find errors when happened. Personally, I like Sophie's version more since it's much clear.
@ableegoldman , I reviewed it again, and found we forgot to sort the `unfilledMembersWithExactlyMinQuotaPartitions` list here, to have deterministic result.
Honestly it took me quite a while to understand the fix :P After understanding that I think maybe it's better to rename these two collections more explicitly: 1) `unfilledMembers` -> `MembersWithLessThanMinQuotaPartitions`. 2) `potentiallyUnfilledMembersAtMinQuota` -> `MembersWithExactMinQuotaPartitions`. And also (since the maxQuota is always either == minQuota or minQuota + 1): 3) `expectedNumMembersAssignedOverMinQuota` -> `expectedNumMembersWithMaxQuota` 4) `numMembersAssignedOverMinQuota` -> `numMembersWithMaxQuota`
nit: do you think we should log at ERROR since this is not expected really? Right now we would sort of "hide" such bugs and still be able to proceed silently; I feel we should shouting out such scenarios a bit louder in logs.
Increase the timeout makes sense to me.
Because we cleared them earlier now, the `membersWithOldGeneration` is not necessary any more. We can just iterate `membersOfCurrentHighestGeneration` here.
nit: add a space after "multiple". i.e. `despite being claimed by multiple[ ]`
`expectedAssignedCount` is not used any more.
I'm thinking we can have a test for package scope `partitionsTransferringOwnership` in `AbstractStickyAssignorTest`. I found we didn't test it before. We can verify the doubly assigned partitions and other revoked partitions are put into `partitionsTransferringOwnership` correctly.
`KAFKA-13046: Improve the test coverage for stickyAssignor` is created. Let me handle it! :)
Same thing about `linger.ms` and "if" is missing in the sentence. ```suggestion "try sticking to a partition(no matter if the 'key' is provided or not) until the batch is full, or <code>linger.ms</code> is up." + ```
Space was missing before the parenthesis, and "if" should be added to the sentence. ```suggestion "each record in a series of consecutive records will be sent to a different partition (no matter the if 'key' is provided or not)," + ```
Redundant space here ```suggestion "<p>Implementing the <code>org.apache.kafka.clients.producer.Partitioner</code> interface allows you to plug in a custom partitioner."; ```
Would it be possible to describe the strategy in more detail here? Something along the lines of how each record is assigned a new partition? I also wonder if it suffices to describe the RoundRobinPartitioner as the `strategy can be used when user wants to distribute the writes to all partitions equally`. In many cases, the DefaultPartitioner also tries to do this, though I am aware of the cases where it doesn't. (https://issues.apache.org/jira/browse/KAFKA-10888).
there is an issue (#8690) which RoundRobinPartitioner can cause uneven distribution when new batch is created. Maybe we should remind the known issue.
What do you think about putting `linger.ms` within a `<code>` block? ```suggestion "This strategy will try sticking to a partition until the batch is full, or <code>linger.ms</code> is up. It works with the strategy:" + ```
nit: this name seems misleading since order could be important for arbitrary collections. Since we only have a couple uses, maybe we can get rid of it and use `assertEquals(new HashSet(A), new HashSet(B))` for example.
This is very fussy, but for some reason, the phrasing here is bugging me. The addition of "must have" almost makes the event seem more uncertain and open to interpretation. Like we need to reassure the user that our deduction is correct. Maybe we can leave that part out? ```java log.info("Resetting the last seen epoch of partition {} to {} since the associated topicId changed from {} to {}"... ```
Any harm keeping this one? Seems like it simplified some of the uses, especially in tests.
I thought we changed the order of this in the 3.0 patch. We should be checking for a changed topic id before comparing epochs.
nit: extra space after `synchronized`
nit: simpler or not? ```java Map<String, Uuid> newTopicIds = topicIds.entrySet().stream() .filter(entry -> shouldRetainTopic.test(entry.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); ```
It could be updated in a separate thread. I cannot see how that would be a problem though. We do have synchronization in `Metadata`.
nit: unneeded newline
Yes, we can open a JIRA to do it later.
We can use `ApiResult.completed()`
We can use `ApiResult.unmapped()`
I think we put args on the same line unless the list is too big
nit: We should use `groupId.idValue` here and in the others.
I would also try to uniformize the logs and would use debug all the time except for the unexpected errors.
What's the purpose of this check? I am not sure where this `InvalidGroupIdException` thrown here will get to.
It seems incorrect here as well.
nit: There is an extra space before `new`.
We don't provide the error message in any other case. Should we remove this one for the time being? I think that it is a good idea but only if we do it across the board.
nit: Let's make all variables as final or none in this block.
As we expect a specific `groupId`, I would check the provided `groupIds`.
@showuon I just looked at the implementation of 'topLevelError' and I realized that it checks the partition errors as well. Therefore, it seems that we don't need to check them again afterwards. Sorry for this. I was not aware of this.
Hmm, this callback will be called from Producer's Sender thread and consumerManager.waitTillConsumptionCatchesUp() blocks until the timeout. This will block the Sender thread, which is not ideal.
It seems that it's more natural for producerManager to create a CompletableFuture and return it through publishMessage() rather than passing in a CompletableFuture from the caller.
nit: As said in the other PR, this is a good idea but I would only do it if we do it for all exceptions.
nit: We could revert this change as it does not bring much.
As we expect only `groupId`, I would verify the `groupIds` here and in `buildRequest`.
nit: We could revert this change as it does not bring much and re-align like it was before.
nit: past control records or aborted transactions when isolation.level=READ_COMMITTED
How about this. First, we can augment the message above to something like this: ```scala log.trace("Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`", position, nextPosition, completedFetch.partition, partRecords.size()); ``` This gives us enough information in the logs to see which partitions caused the `poll()` to return and it tell us exactly where in the log the aborted transaction/control records exist. Second, maybe we can add back your previous log line, but make it a little more terse and put it at trace level: ```scala log.trace( "Returning empty records from `poll()` since the consumer's position has advanced " + "for at least one topic partition") ```
nit: Worth double-checking, but I think we require "read_committed" to be lower case.
I'm inclined to either remove the log line entirely or move it back to its former location in `KafkaConsumer`. Will leave it up to you.
Can we check this and throw an exception if it's not true? I don't think this method is called often so performance should not be an issue.
assignedTopicPartitions could be updated concurrently and we are accessing it without lock protection here.
Below, maybeWaitForPartitionsAssignment() may add new assignedMetaPartitions. We need to seek those partitions from the beginning.
extra new line.
This means that if there is no change to a remoteLogMetadataCache, but there is new record for other partitions in the same metadataPartition, we still need to flush remoteLogMetadataCache.
Should we eventually shutdown the broker due to the IOException? IIUC, currently the exception gets logged and only the consumer thread dies. cc @junrao
Hmm, is this method the only place where we would sync the data to disk? I thought we would want to sync to disk periodically as well.
Should we wait until the consumer task completes before writing the checkpoint file? Otherwise, we could be reading the in-memory state while it's being updated.
Could we define it as `@BeforeClass`? I think we do not need to create input/output topics for each test case.
Btw: all those test only set differn window size and grace and thus it seems we can share more code (ie, setting up the topology). Only the window definition is different.
`TimeWindows .of` is deprecated, right? Do we need to test old API? If yes, I think we need a `@SuppressWarning` to make the build pass
Seems to be the exact same test as above (both don't use grace). Seem the difference is if window size enlarged retention time setting or not, but it's not reflected in the names of the tests.
This method is usually called `setUp()`.
Sorry, I was wrong. Have a 2nd look, and found `CollectionConverters` doesn't exist in scala 2.12. So, this change will not be compatible to scala 2.12. This change should be reverted, or maybe use `JavaConverters`? Thanks.
This is deprecated by scala (scala 2.12, not 0.12 :)). Here's the note from scala doc in 2.12.0, to explain why it is deprecated: > The transparent conversions provided here are considered fragile because they can result in unexpected behavior and performance. > Therefore, this API has been deprecated and JavaConverters should be used instead. I think that time, `CollectionConverters` haven't existed, so suggest to use `JavaConverters` instead. REF: https://www.scala-lang.org/api/2.12.0/scala/collection/JavaConversions$.html
This is part of KIP-516
I believe the jira was under the kip's main jira, but I can make the link clearer.
Correct me if I'm wrong, I just want to understand better the case you are raising. You mean to keep the `TaskMetadata` used for this test and then compare them with the `activeTasks`, right? But I don't really understand how this will prevent from discrepancies if the task has been revoked in between calls. Wouldn't it be then also not present in `activeTasks`? Sorry to hijack this for this question and thanks in advance!
This variable needs to be `final` says checkstyle.
```suggestion }, "The number of active tasks returned in the allotted time was not one."); ```
Got it, thanks!
I'm sure the odds of this are super low, but to really make this airtight you'd need to have the `List<TaskMetadata>` that you use for the test be the same one that you test in this condition, otherwise it's _possible_ for the one task to have been revoked in the split second between `waitForCondition()` and the new call to `metadataForLocalThreads()`.
This can fit in two or three lines... let's try to avoid "exploded" function calls that look like function bodies.
extra newline not needed here
LGTM. I think it was never correct to poll with zero in those cases, and just long polling even with MAX_VALUE is better.
I wonder if it would be preferable to use a more generic message which clearly mention the error encountered: `OffsetCommit request for group id {} returned error {}. Will retry.`. Without mentioning the received error, we don't really know what happened so the log is not that useful.
I would rather keep it inline to stay consistent with the other cases.
nit: `.` at the end of the sentence.
Might be nice to rewrite this using `assertThrows` ? (Similar below.)
@ccding It should be non empty based on the validator that is set for this config. If an empty string is passed then all the Kafka config properties will be passed, which is wrong. There should be null validation instead of passing an empty string as mentioned below. Same for the `REMOTE_LOG_METADATA_MANAGER_CONFIG_PREFIX_PROP` check. ``` config.getString(REMOTE_STORAGE_MANAGER_CONFIG_PREFIX_PROP) != null ? config.originalsWithPrefix(config.getString(REMOTE_STORAGE_MANAGER_CONFIG_PREFIX_PROP)) : Collections.emptyMap() ```
Hmm, why is this change needed? It doesn't seem like the PR is altering behavior such as these but maybe I'm missing something.
```suggestion // Emulate losing leadership in the middle of a non-atomic append by not writing ```
`KeyValueIterator` instance needs to close explicitly to avoid resource leak. Please fix it and the other similar places. Thanks.
instead of creating a new set, thoughts on just returning an empty collection? (`Collections.emptyNavigableSet()`)
It looks like these are backwards. If I understand this method signature, even though iteration is reversed, the "from" key is still the lower bound, and the "to" key is the upper.
@vvcephei is correct. The `from` is still the lower bound and and `to` is the upper bound in the `reverseRange` method definition. We can confirm it by checking the parameter validation in the `CachingKeyValueStore#reverseRange` ``` public KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from, final Bytes to) { if (from.compareTo(to) > 0) { LOG.warn("Returning empty iterator for fetch with invalid key range: from > to. " + ... } ```
You might consider: ```suggestion try (final KeyValueIterator<String, String> scanIterator = forward ? stateStore.range(null, null) : stateStore.reverseRange(null, null)) { TestUtils.checkEquals(scanIterator, dataIterator); } ```
Do you think it would be clearer if we don't rely on the defaults, but just explicitly include both branches? Oh, also, this isn't an immutable builder, so you can just do: ```suggestion if (cachingEnabled) { stateStoreConfig.withCachingEnabled(); } else { stateStoreConfig.withCachingDisabled(); } if (loggingEnabled) { stateStoreConfig.withLoggingEnabled(new HashMap()); } else { stateStoreConfig.withLoggingDisabled(); } ```
This would wait forever. Maybe we should do: ```suggestion TestUtils.waitForCondition(() -> stateStore.get(high) != null, "The store never finished populating"); ```
Hmm, I just noticed this... It applies to the other test, too. I can see some value in asserting that `range(null, null)` returns the same thing as `all()`, but I wonder if it would be better to validate that they both return the _right_ thing. It's just the fact that the underlying implementation delegates `all()` to `range(null, null)` that's making me think of it.
This idiom is used across a bunch of the tests, which is fine, but it's also a pretty generic bit of logic. I wonder if it would be better to move it to TestUtils. In fact, there are already a couple of utilities there for comparing iterators, and I bet there are also Hamcrest matchers that compare iterators. I'm fine with this as-is, if you prefer it; I just wondered if you knew about those other options.
Great catch, thanks @showuon !
Ideally, we'd always use brackets on control flow operators.
Nice catch! We actually allow `REBALANCING` and `RUNNING` state in this method.
Seems ok to me to wait and fix alongside other issues like KIP-300 in a "new and improved" DSL (or whatever we do there). If users start to complain and request a fix sooner then we can re-evaluate, but it's not like this was a user-reported bug to begin with.
Why do you remove this check? A `TimeWindow` should not allow this case.
Hmmm... Seems to be in issue... The actual final return type is `KTable<Window<K>, V>` and thus is window-type agnostic. So we already have such a "container". -- However, `windowedBy(SlidingWindow)` returns a `TimeWindowedKStream`... Return types are not easy to change... And I don't think we can just switch from `TimeWindow` to `SlidingWindow` as concrete type either for the sliding window case... Maybe we are stuck and cannot fix the bug without a breaking change? For this case, we would indeed need to carry on with the KIP (but we could only do it in 4.0...), but I am wondering if it's worth fixing given the impact? Also: we have a few issues with the current DSL that we cannot fix easily (eg KIP-300). Thus, a long term solution could be, to leave the current API as-is, and built a new DSL in parallel (we did this in the past when we introduced `StreamsBuilder`). This way, we can change the API in any way, but it would be a long-term solution only. It might also help with regard to the new PAPI that uses `Record` instead of `<K,V>` type, and that is not easily adopted for `transform()` (and siblings). We could change the whole DSL to `Record` (ie, `KStream<Record<K,V>` -- or course we don't need `Record` in the generic type -- it's just for illustrative purpose). It would also cover the "add headers" KIP, fix KIP-300, we could introduce a `PartitionedKStream` (cf current KIP-759 discussion) and a few other minor issue (like rename `KGroupedStream` to `GroupedKStream`) all at once... And we could cleanup the topology optimization step and operator naming rules (which are a big mess to understand which `Named` object overwrites others...) -- We can also get rid of the wrappers for `KeyValueStore` to `TimestampedKeyValueStore` and change the interface from `Materialized<XxxStore>` to `Materialized<TimestampXxxStore`) -- In the past it was never worth to start a new DSL, but it seem we collected enough individual cases to maybe justify this investment now? The only thing that we should consider is our investment into "versioned state stores / version KTables". If we build a new DSL it should be compatible to it -- if we cannot guarantee it, we might want to wait until we understand what API we need to versioned KTables in the DSL and make the cut afterwards? \cc @ableegoldman @guozhangwang @vvcephei @bbejeck @cadonna (also @inponomarev @jeqo @vcrfxia)
I would prefer to _first_ rename existing windows and not merge this PR using `SessionWindows` within `SlidingWindowAggregate`...
Are you saying the `CachingWindowStore` internally uses a `TimeWindow`? Or is the `TimeWindow` somewhere along the store supplier code path...? Either way, doesn't this mean there's still a hole in the API since you can't use a custom WindowStore for a sliding windowed aggregation with the windowSize set to 0? If the WindowStore is going to represent different kinds of constant-size windows, it should probably be agnostic to the specific type of constant-sized window.
It can sometimes be hard to get the logs for failing tests, so it might be nice to get the failure reason in the actual test output. Check out `Matchers`: there should be a way to compose the checks you're doing here manually. It'll be something like: ``` assertThat( message, is( oneOf( containsString("Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING"), containsString("The state store, source-table, may have migrated to another instance") ) ) ```
It might be more robust to use "contains" instead of "startsWith", but I won't insist on it.
nit: Could we add a error reason in this assertion, so that when the exception message is not one of these 2 messages, we can know what happened. Ex: "Unexpected exception thrown while getting the value from store."
This seems overly complicated. An easier structure to follow would be something like this: ```java String expectedType = "KafkaController"; Set<String> expectedMetricNames = Utils.mkSet( "ActiveControllerCount", "GlobalTopicCount", "GlobalPartitionCount", "OfflinePartitionsCount", "PreferredReplicaImbalanceCount" ); MetricsRegistry registry = new MetricsRegistry(); try (QuorumControllerMetrics quorumControllerMetrics = new QuorumControllerMetrics(registry)) { assertMetricsCreated(registry, expectedMetricNames); } assertMetricsRemoved(registry, expectedMetricNames); ```
nit: could use Utils.mkSet
Could we fail the test right here? It doesn't seem like there is much benefit to returning the missing metrics from the method. That would let us simplify this a little. Instead of this: ```java Set<String> missingMetrics = getMissingMetricNames(expectedMetricNames, expectedGroup, expectedType); assertEquals(Collections.emptySet(), missingMetrics, "Expected metrics did not exist"); ``` we could have: ```java assertRegisteredMetrics(expectedMetricNames, expectedGroup, expectedType); ``` We could probably also drop `expectedGroup` since we only have `kafka.controller`.
nit: When this assert failed, we'll see the error messages: `Expected metrics did not exist` ==> expected: `emptySet`, but was: `missingMetrics` I think we should change the error messages, ex: `Expect no missing metrics` ==> expected: `emptySet`, but was: `missingMetrics`
Do you think this level of detail is more suitable for trace? We have debug logging for high watermark advances in `KafkaRaftClient`.
Do we actually have to mock `generation()` and `rawConfig()` for this test? Looking at `connector()`, it looks like it only relies on the snapshot.
I don't think we need to mock `generation()` in this test.
We can drop the parenthesis here. Same below
What about inlining `transformations` and having something like: ``` when(plugins.transformations()).thenReturn(Collections.singleton(transformationPluginDesc())); ```
I don't think this mock is needed. Same below.
Nit: we usually write `private final`.
```suggestion this.tags = this.metrics.config().tags(); this.flushTimeSensor = newLatencySensor(FLUSH); initTimeSensor = newLatencySensor(TXN_INIT); beginTxnTimeSensor = newLatencySensor(TXN_BEGIN); sendOffsetsSensor = newLatencySensor(TXN_SEND_OFFSETS); commitTxnSensor = newLatencySensor(TXN_COMMIT); abortTxnSensor = newLatencySensor(TXN_ABORT); ```
@rodesai I see your point here. However, the downside of not throwing is that we will also not notice the bad behavior in our tests like the soak tests. I personally prefer to improve tests instead of downgrading the reaction to bad behavior. Assume in future somebody makes a change that breaks the assumption of the non-shared metrics registry, we would find this bug immediately during development instead of during production. Another option that comes to my mind is to classify exceptions that originate from the metrics framework differently in the uncaught exception handler, but that would probably need some more work.
That is not my point. My point is that the objects that call the constructor, i.e. tasks and threads, have a time object that they use for the their metrics (and probably for other purposes). Now that we also have metrics in the `StreamsProducer` that needs a time object, it is inconsistent to create a new time object in the constructor instead of passing along the time object from tasks and threads into the `StreamProducer`.
```suggestion import org.apache.kafka.common.MetricName; import org.apache.kafka.common.metrics.Metrics; import org.apache.kafka.common.metrics.Sensor; import org.apache.kafka.common.metrics.stats.CumulativeSum; import java.util.Map; ```
Ditto here and elsewhere.
Could you please already open the follow-up PR with scaffolding and link it here? I think otherwise we risk to forget about it.
`removeSensor()` would remove its associated metrics as well, I think we do not need the second call below.
I now saw that in the consumer tests you use `Duration.ofSeconds(1).toMillis()` and `Duration.ofMillis(999).toNanos()`. This makes it already clearer. I think a variable with a meaningful name for the lower bound would make it even clearer.
Could we add the description in the metricName as well indicating this is measured in nanos not millis? Ditto elsewhere.
I think, we should not keep this constructor. It seems to me that we risk to have different time objects for thread/tasks and their producers which has the potential to lead to inconsistent time between these components. If the removal of the constructor makes this PR too large (and I suspect it will), I recommend to make a separate refactoring for this constructor change and get that merged before this PR.
`Map::put` returns the previous value.
Yes, it looks like it affects the log message printed, right? It doesn't explain the behavior you are seeing.
nit: There is an extra space before `=` here and below. I am not a huge fan of using `TestUtils.fieldValue`. Did you consider making both attributes package private or something like this instead? `ApiVersions` is passed to the constructor of `NetworkClient` so we could access it this way as well.
I wonder if this is going to fail. The below code seems to be incorrect: ``` if failure_mode == "controller" and metadata_quorum != quorum.zk: raise Exception("There is no controller broker when using KRaft metadata quorum") ``` That maybe should be checking for `broker_type` instead of `failure_mode`.
can we use less vertical space here
Maybe a little subjective, but I think the test case would be more readable if we list these brokers directly. For example: ```java List<Integer> allBrokers = Arrays.asList(1, 2, 3, 4, 5); List<Integer> brokersToKeepUnfenced = Arrays.asList(1); ```
Is there a point to setting `min.insync.replicas` in this test? I am wondering why we don't just reuse the original create request.
This part is a little mysterious to me. We return an error, but the topic is created anyway? That seems surprising.
If you use `assertEquals` it will compare the content of the arrays and print their value if they don't match.
If we create the topic with an explicit assignment, then I think we can remove this logic to build the expected ISR since we would be able to assert the expected ISR directly.
Perhaps we can use `Uuid.randomUuid`? It's a little weird for all brokers to have the same incarnationId.
nit: it's a small thing, but the assertion failure message is more useful if we use the `Errors` type. ```java assertEquals(Errors.NONE, Errors.forCode(createTopicsResponseData.topics().find("foo").errorCode())); ```
You can call `fail` directly: `fail("Fencing of brokers did not process within expected time");`
Maybe you can replace this `while` loop with a util function `TestUtils.waitForCondition`. Ex: `TestUtils.waitForCondition(() -> fenceBrokers(), 3000, "Fencing of brokers did not process within expected time");` Just that it only accepted the `timeout` value, not the `waitIterations` like you did. FYI
nit: we can combine these two `try` blocks: ```java try (LocalLogManagerTestEnv logEnv = new LocalLogManagerTestEnv(1, Optional.empty()); QuorumControllerTestEnv controlEnv = new QuorumControllerTestEnv(logEnv, b -> b.setConfigDefs(CONFIGS), Optional.of(sessionTimeoutSec))) { ... ```
Since it's put inside test class now, we cat declare it as `private`.
It seems like these calls could be updated to use the `Record` itself instead of the key, value, and `InternalProcesorContext#recordContext`.
I think this might actually be a bug. IIRC, the Record forward call overrides the context, so you might have to actually set all the fields in Record from the context when you construct it in `toEmit.record()`. Specifically, I think we might be dropping the headers here. I might also be wrong, so you might want to double-check me first. It looks like we were never testing whether we propagate headers through Suppress. If it turns out that we never were, then there's also no problem. The case to look out for is if we _were_, but now we no longer are.
Ditto here, we should retain some version of this test and any others that are specifically intending to test the behavior of the old API (until the deprecation period has elapsed and we can remove it)
We should avoid outright removing tests that cover the old deprecated APIs until we actually get to remove those methods. Instead we can just scope the deprecation warning suppression to the individual tests that cover the behavior of the old APIs like here, and a few places below
Same here: we should leave this test here until we remove the deprecated API. (and just suppress the warnings for only this test method)
Let's leave this one in here, as long as we still support a deprecated API we should continue to test that it works. We just shouldn't use deprecated APIs to test other functionality unrelated to the API itself (eg that an exception is thrown for window size of 0). Then we can restrict the scope of the deprecation warning to just this one test, rather than the entire class
Alrighty, just give me a ping when the build is done!
nit: this name is a little funky, can we come up with something that describes what this list actually means? The only things I can think of are a bit clunky, but maybe `ASSIGN_FROM_SUBSCRIBED_ASSIGNORS` or `SUBSCRIBED_TOPICS_ASSIGNORS` or whatever sounds good to you 
No, I think it's correct to leave the Streams assignor out of this. Though I think it technically may not matter at the moment, since the Streams assignor will only assign topics from its own subscription and ignores the subscribed topics of the other members, we may want the flexibility to do something like this in the future.
Can we also rationalize the different names. Ig uess this method should be `checkpointsTopic`.
Can you also send an update to the VOTE thread? so all participants are notified. Thanks
`checkpointTopicSufficx` -> `checkpointsTopicSuffix`
Think about that a bit more, maybe we can make it simpler as: ``` if (keyFrom == null && keyTo == null) { // fetch all return true; } else if (keyFrom == null) { // start from the beginning return key.compareTo(getKey(keyTo)) <= 0; } else if (keyTo == null) { // end to the last return key.compareTo(getKey(keyFrom)) >= 0; } else { return key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0; } ```
nit: let's move `keyTo == null` up first so that if it does not satisfy, we do not need to trigger `getKey` anymore.
Your reasoning makes sense to me. From a first read, the PR looks pretty good. I will make a second pass on Monday to ensure that I cover all the cases.
Similarly here, would it make sense to integrate that check into keyBytes? I think there are similar cases in other stores.
Do you think it would make the code more clear if we rename the `keys()` to `rawKeys()`? Looking at the `RawKeyAccess`, the `keys()` makes sense. But looking at the `RocksDBTimeOrderedWindowStore.keys()`, the name does not tell much about what keys.
Just copying over the suggestion to here, so it's easy to find ```suggestion final Throwable throwable = assertThrows(NullPointerException.class, () -> supplier.get().process(record)); assertEquals(throwable.getMessage(), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
As the whole point of this PR is to provide better messages, I would also check in the test that the exception has the new enhanced message. Something like ```suggestion final Record<String, Integer> record = new Record<>("K", 0, 0L); assertThrows(NullPointerException.class, () -> supplier.get().process(record), String.format("KeyValueMapper can't return null from mapping the record: %s", record)); ```
Just a suggestion: ```suggestion Objects.requireNonNull(newKeyValues, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
Just a suggestion: ```suggestion Objects.requireNonNull(newPair, "The provided KeyValueMapper returned null which is not allowed."); ``` BTW, we should not output records since they might contain sensitive data.
We prefer to use `assertThat()`: ```suggestion assertThat(throwable.getMessage(), is(...); ```
nit: ```suggestion final Set<String> logMessages = appender.getEvents().stream() .filter(e -> e.getLevel().equals("WARN")) .map(LogCaptureAppender.Event::getMessage) .collect(Collectors.toSet()); ```
nit: ```suggestion = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(new DBOptions(), new ColumnFamilyOptions()); ```
nit: I don't think the copier uses group instance ID (maybe we could add support for that separately?), so I don't think `FencedInstanceIdException` is possible at the moment.
I filed this: https://issues.apache.org/jira/browse/KAFKA-13235.
This particular exception has a different vibe than the above two: those are just unavoidable possibilities with IQ since Streams can start rebalancing at any time and there's no way to ensure that won't happen before/during a query. But failing due to a thread that's `STARTING` is in a different ballpark -- it's not like this can happen for no reason, and it should be possible to wait until all threads have finished startup before issuing a query. For example, we can just wait for Streams to reach RUNNING after adding a new thread, like we do when the client itself is first starting up. (I'm not sure if the Streams state will transition to REBALANCING when a new thread is added, but it should.)
nit: we do this same thing in the other `#resize` for thread count changes, can you factor it out into a helper method? Then I think we can narrow the scope and make only that helper synchronized (should double check that though)
Should probably specify what kind of buffer in the name (esp. with KIP-770 adding another relevant buffer type)
Does this need to be a concurrent map? Seems to only be accessed by the StreamThread itself
Nit: we typically put `null` on the right hand side in the Kafka codebase. Same below
nit: could probably simplify these two assertions a little: ```java assertEquals(Collections.singleton(error), errorCounts.keySet()); ```
this method is unused but it maybe useful in the future.
There is probably a generic pattern that you could factor out here. Basically you are providing a translation from `Map<K, V1>` to `Map<K, V2>` in both of these cases. So you could construct a generic `MapView` or something like that with a `Function<V1, V2>` and save some of the duplication.
Maybe we could use a different value here.
As you point out, the old log message was: ``` log.info("{} flushing {} outstanding messages for offset commit", this, outstandingMessages.size()); ``` This log message had two things it'd be nice to keep: 1. `this` as the context; and 2. the number of records whose offsets were being committed (e.g., the number of acked records). I think both would be good to include, especially if we're saying the number of records whose offsets are _not_ being committed (yet). The `Pending` class seems pretty useful, but computing the number of acked records is not possible here. WDYT about merging the `SumittedRecords.committableOffsets()` and `pending()` methods, by having the former return an object that contains the offset map _and_ the metadata that can be used for logging? This class would be like `Pending`, though maybe `CommittableOffsets` is a more apt name. Plus, `WorkerSourceTask` would only have one volatile field that is updated atomically.
What do you mean by "we still waited for the data in the buffer to flush"? The `beginFlush()` method doesn't actually do any flushing; it merely performs the snapshot of the offset writer's data.
I wonder if it would be worth improving this log message slightly, to something like: > Timed out waiting to flush offsets to storage; will try again on next flush interval with new offsets Strictly speaking, it's unrelated to the changes made in this PR. But for users seeing this in the log it would be helpful to know that despite it being an error that should be looked into, the next flush interval will attempt to commit all (potentially-updated) offsets.
Also? ```suggestion assertEmptyRecords(); assertNoEmptyDeques(); ```
I believe that including the name of the property in the error message is redundant as that information will be available already in the REST response. I also think we may want to be clearer about the error message here. Users can't supply null values, but developers (by specifying `null` as the default value for a property in a `ConfigDef`, for example) definitely can, and we may want to make it clear which variety we're prohibiting. What do you think about this? ```suggestion .map(configEntry -> new ConfigValueInfo(configEntry.getKey(), "The JSON literal `null` may not be used in connector configurations")) ```
It seems like this isn't used. The rebalancing will end up in a running state which is waited for below. I suggest only have the condition below waiting for a running state before query the state store.
Fixed via https://issues.apache.org/jira/browse/KAFKA-13699
Why not? (We also support `currentSystemTimeMs()` -- we only don't support `currentStreamTimeMs()` because the global thread processed independently of the main threads, and there is no concept of stream-time for the global task.
// and if auto-commit disable or the coordinatorUnknown is true, the future will be // the asynchronous commit operation will not do. --> // null future means no offset commit request sent, so it is still considered completed
Sorry, I found we already log the error in `autoCommitOffsetsAsync`. We should remove the logging here. And make the if condition simpler as: ``` if (future == null || future.succeeded() || (future.failed() && !future.isRetriable())) { onJoinPrepareAsyncCommitCompleted = true; }
nit: remove `else` here, return null at the end.
Instead of letting the callee `maybeAutoCommitOffsetsSync` silently dropping the given offsets that no longer exists, I think we should let the callee to simply report unknown topics out to the callers after timer elapsed and let callers to handle them. The main reason is that unknownTopicOrPartition may just temporary and hence silently dropping them at the callee can lead to confusing behaviors. Note there are several callers: * When re-join group via `maybeAutoCommitOffsetsSync`: if `commitOffsetsSync` throws the unknown topic or partition exception after exhausting retries within the timer, the caller `maybeAutoCommitOffsetsSync` would log it upon capturing the exception and then wrap it as a `InterruptException` still. Note that the in the next poll call since `needsJoinPrepare` is false already we would not try to call `onJoinPrepare` again and if the topic is indeed deleted, the rebalance would remove the partitions from the subscription later. * When customer called via `Consumer#commitSync` directly: we can directly throw the timeout exception to the customer caller still. Note that since we would continuously log the error for `UnknownTopicOrPartition` error, user debugging it would still learn about the root cause why this commit call timed out.
1) I think the max.poll.interval.ms is no longer set to `Integer.MAX_VALUE` in 2.3.0 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-442%3A+Return+to+default+max+poll+interval+in+Streams). 2) For `Consumer#commitSync`, the `default.api.timeout.ms` is used if user do not specify the timeout, not the `max.poll.interval.ms`.
I don't think we need extra `toGiveUpTopicPartitions` to store the partitions to be deleted. We can log the warning message in L1103 here directly
Could we do this after we have `UnknownTopicOrPartitionException` happened? I think this issue is rarely happened, we can "lazily" clean it up. So, we can move this line into below `catch` block. (and need to add an `UnknownTopicOrPartitionException` case)
We only need the entry key, so it could be changed to `willCommitOffsets.keySet().iterator();`
For case 3, we should log an ERROR entry indicating that we are going to proceed with commit failed still and it may result in duplicated data being consumed, and for such cases we should notify the user.
I think these two overloaded `maybeAutoCommitOffsetsAsync` functions can be merged: compared with the other that returns `void`, this function did the following more: 1) `client.pollNoWakeup();` -> this has already been called inside the `commitOffsetsAsync` callee at https://github.com/apache/kafka/pull/11340/files#diff-0029e982555d1fae10943b862924da962ca8e247a3070cded92c5f5a5960244fR982, so this is not needed. 2) `invokeCompletedOffsetCommitCallbacks();` -> I think this is not required in the async call. As a result, we can just leave one `maybeAutoCommitOffsetsAsync` function that returns the future.
nit: additional new line
nit: additional new line
nit: additional new line
nit: additional new line
nit: additional new line
Do you think this is better? //We might get`UnknownTopicOrPartitionException` after submitting their offsets due to topics been deleted. We should update the offsets list here. The worst effect is that we may keep retrying to commit the offsets for the topics not existed any more, before timeout reached.
I believe the messages is in correct - it is the whole partitions for which any offsets are not committed. `log.warn("Synchronous auto-commit of offsets for partitions {} will be abandoned", toGiveUpTopicPartitions);`
nit: additional new line
OK, make sense.
It's a bit awkward to modify `onJoinPrepareAsyncCommitFuture` inside the `maybeAutoCommitOffsetsAsync` function since the function name itself indicate a general purpose, but specifically for join-prepare --- though I understand today it is indeed only used for that caller. How about letting the `maybeAutoCommitOffsetsAsync` to return the future instead of the boolean, and then let the caller a.k.a. the `onJoinPrepare` today to check if the future is completed or not.
I think upon close(), we can also use `maybeAutoCommitOffsetsAsync` and then we can remove the whole function fo `maybeAutoCommitOffsetsSync`.
I cannot understand this logic clearly.. my original thought is that, we do not reference the `onJoinPrepareAsyncCommitFuture` here at all, just create a future and return to the caller.
I think we should also log the error in `failed && isRetriable()` case
Any reason you change to import all classes under `java.util`? I think we should import what we used in this class only.
I think we can move this check to the caller (onJoinPrepare).
That is, in this way, it makes the check lightweight, and if we want to find out which partition cause the issue, we can "lazily" iterate them after the size check failed.
I think this check each time is quite expensive if the partition size is large. So, I think we can do "lazy check" for this case. Because `allOwnedPartitions` is a `Set`, we can check if any duplicated partitions existed via the size sum. That is: ```java // get the partition size we're going to add int consumerOwnedSize = subscription.ownedPartitions().size(); int prevSize = allOwnedPartitions.size(); allOwnedPartitions.addAll(subscription.ownedPartitions()); if (allOwnedPartitions.size() < prevSize + consumerOwnedSize) { // duplicated partitions in 2 consumers found // log warning here // if we want to find out which partition cause the problem, we can also iterate them here. } ``` WDYT? Thanks.
Not sure if we need to call out EOS in particular? It's incorrect in any case.
What if the class that's not found isn't the config provider but some dependency of it? Because the CNFE message isn't logged nor passed to the `ConfigException` this info would be lost.
```suggestion assertTrue(StreamSupport.stream(serviceLoader.spliterator(), false).anyMatch(sl -> sl instanceof DirectorConfigProvider)); ```
```suggestion "Skipping record due to null key or value. Topic, partition, and offset not known." ```
This was an oversight in the kip that added "current system time" to the public API. They only added it to the deprecated ProcessorContext. I planned to send a PR to fix it, but for now this is a fine solution.
You can use the default constructor `ReplicationControlTestContext ctx = new ReplicationControlTestContext();`
as above mentioned, the `listStore.all()` is not closed here.
`listStore.all()` will return a `KeyValueIterator`, which should be explicitly closed. I think we can re-use the iterator below this line, which is closed at the end.
Should we get a `seed` value and log it (to allow reproducing if the test fails)? ``` final long seed = new Random(System.currentTimeMs).nextLong(); log(seed); final Random rand = new Random(seed); ```
nit: `planned in` -> `planned for` ? (similar below)
```suggestion log.warn("{}.config() has returned a null ConfigDef; no further preflight config validation for this converter will be performed", headerConverterClass); ```
We should not duplicate code, but instead extract an internal `private` helper method `putInternal` that can be called by `put` and `putAll`
debug line should be removed.
Should we add some description for `topologyExceptionHandler`? ex: `This method will also set the {@code topologyExceptionHandler} for the {@newTopology}`
Do we want to be able to clean these up after we removing a topology? We might not but it might also be a good idea. Example we remove a topology with a handler and replace one with the same name without a handler or using the non named topology handler
Since we wrap the real exception with the NamedTopologyException, the logic inside `getActionForThrowable` need to possible peal off that layer (i.e. we may need to unwrap twice).. I'm thinking if we can do something simpler than wrapping / unwrapping here: 1) we are almost wrapping all non-streams exceptions as a StreamsException anyways, so let's just take a look at where we do not and enforce that. In a way we know that the thrown exception to the thread is either a StreamsException by itself and has no cause, or a wrapped StreamsException (but only wrapped once) with a cause. 2) We added a `topologyName` to StreamsException, where `null` indicate it's global. When throwing that directly or wrapping it on a non-streams cause, we set this field. 3) The uncaughtExceptionHandler would then expect only only see `StreamsException` as in 1) above or unchecked exception; for the former we look at that field directly, for the later we just treat it as a global fatal one. Then we do not need a wrapper exception anymore, and does not need nested try-catch either.
Let's keep the existing `trace` and `error` log lines in the `else` block. My suggestion is to add a line at the debug or trace level in the `if` block so users can know if an error is ignored.
We should not be logging at `ERROR` level for every single record if we aren't failing the task unless the user has explicitly enabled this by setting `errors.log.enable` to `true` in their connector config.
Actually, that may complicate things by causing records to be given to `SourceTask::commitRecord` out of order (a record that caused a producer failure may be committed after a record that was dispatched to the producer after it). So probably best to keep the error-handling logic here, but I do still wonder if we can respect the logging-related configuration properties.
We can use `==` to compare enums.
Do we really want to mention `BigDecimal` here (it's really an implementation detail)? All the user needs to know is that the scale present in the value differs from the scale specified in the schema.
We have `testJoinGroupRequestVersion0RebalanceTimeout` but `testOffsetFetchRequestBuilderToStringV0ToV7`, so rename `testJoinGroupRequestVersion0RebalanceTimeout`  `testJoinGroupRequestV0RebalanceTimeout` for consistency
Actually, I suppose it's not as bad as I first thought. We're removing more casts than we're adding.
What's the benefit of using `short` over `int`? It means we have to do a lot more type casts, but doesn't make a difference in terms of test coverage.
There's also https://github.com/apache/kafka/pull/11128 to consider.
I meant `public_html` directory.
To keep the test as simple as needed, what about simply using: ``` mm2Props.put(DefaultConfigPropertyFilter.CONFIG_PROPERTIES_EXCLUDE_CONFIG, "delete.retention.ms"); ```
`should be the same` -> `should be 1000` Also let's swap the arguments as the expected value should come first: ``` assertEquals("1000", backupConfig, ... ```
You're right. "the topic name or null if it is unknown" is what I meant.
I think we should add more explanation for null return value: ex: @return the topic name or null if there's only topic ID set.
```suggestion private static final String ACCEPTABLE_RECOVERY_LAG_DOC = "The maximum acceptable lag (number of offsets to catch up) for a client to be considered caught-up enough" + ```
Another disabled assertion.
Looks like this needs to be re-enabled.
typo: "number of partitions" -> "Replication factor"
Did you plan to implement this? If you not, you can use the trick you are doing in other places. E.g. ```java static ConfigurationValidator NOOP_VALIDATOR = (_, __) -> {}; ```
nit: maybe consider moving this so that we can use `ConfigurationValidator.NOOP` or something like that. Then it could also be used in the quorum controller builder then.
I believe it would be easier to interprete if you avoid using the if-else block and created the log statement in one assignment using conditionals. String logStatement = "{} partition" + (count > 1 ? "s have leader brokers" : " has a leader broker") + " without a matching listener, " + (count > 1 ? "which is " : "including ") + "{}"; log.warn(logStatement, count, missingListenerPartitions.subList(0, Math.min(10, count)));
`</node>` -> `</code>`
Ah, I see the confusion. The `#isTopologyOverride` method checks whether the config has been overridden for the specific topology, ie has been set in the Properties passed in to `StreamsBuilder#build` -- it's not looking at what we call the `globalAppConfigs` which are the actual application configs: ie those passed in to the `KafkaStreams` constructor. So basically there are two sets of configs. The value should be taken as the first of these to be set by the user, in the following order: 1) `statestore.cache.max.bytes` in `topologyOverrides` 2) `cache.max.bytes.buffering` in `topologyOverrides` 3)`statestore.cache.max.bytes` in `globalAppConfigs` 4) `cache.max.bytes.buffering` in `globalAppConfigs` Essentially, using `#getTotalCacheSize` on the `topologyOverrides` if either of them is set (which this PR is doing) and on the `globalAppConfigs` if they are not (which is the regression here). On that note -- we also need to move `##getTotalCacheSize` out of StreamsConfig, because it's a public class and wasn't listed as a public API in the KIP (nor should it be, imo). I recommend creating a new static utility class for things like this, eg `StreamsConfigUtils` in the `org.apache.kafka.streams.internals` package. There are some other methods that would belong there, for example the `StreamThread` methods `#processingMode` and `#eosEnabled` should be moved as well Hope that all makes sense -- and lmk if you don't think you'll have the time to put out a full patch, and I or another Streams dev can help out 
Sounds good! There's no rush, but I'll make sure we have your new PRs reviewed and merged quickly whenever they are ready, since you've worked so hard on this already. I'm sorry I wasn't able to make another pass on your original PR, but hopefully this won't be too much of a bother.
nit: directory -> directories, because -> because otherwise
There is a file lock on this file that causes the issue, which might be hiding another issue even on other platforms.
This line is not required.
Why do we need 2 identical chars in java doc? ex: 0 to `' ' {@code ' '}` are considered to be whitespace. `'/' {@code '/'}` is...
Users can make it consistent using connect API's configuration. For now, to keep the backward compatibility for the most use-cases, let's fix `isInternalTopic`
Am not sure I got why we need to check that separator can't be a dash and throw an exception. This check seems to me like an assumption about the naming convention of a topic which is why we moved internal topics to `ReplicationPolicy`.
> Oh no, this lines replace the original props.putIfAbsent(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "mm2-offsets." + sourceAndTarget.source() + ".internal");, etc. not Connect's internal topics. `DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG` is one of the connect's internal topics. ``` private static final String OFFSET_STORAGE_TOPIC_CONFIG_DOC = "The name of the Kafka topic where connector offsets are stored"; ``` My point is users already can control these types of topics using the `DistributedConfig` so there's no point in controlling them again using the separator. The main issue I think we need to fix first is preventing is the replication of these topics.
One issue with c. is that it works for new environments. If users already have MM2 running, it's using topics with the current names.
> only when MM2 is running in standalone mode. They are created in any mode if there is no value for DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG > If the user is running MM2 in connect mode, the user is responsible for configuring DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, DistributedConfig.CONFIG_TOPIC_CONFIG, etc. It is what you are meaning. Right? Small clarification, users can use `DistributedConfig` with any mode (even standalone) to override the name of these topics. And they always had the power to do so, even before KIP-690, and if this new topic name didn't match the `isInternalTopic` policy, it would replicate. The PR's approach is trying to control the Connect topics that MM2 needs to set up using the separator; this is where I am not sure it's a minor fix or something that requires a KIP that follows KIP-690. My suggestion, is to introduce the minor fix first and propose another KIP if you believe that Connect internal topics created by MM2 Workers should to be controlled by the separator as well.
Why are we generating the connect internal topic names here? if there's a rule for the topic naming convention it should be defined in one place which is `ReplicationPolicy`.
Yeah but I expect the value for this config to come from a class implementing the interface so in this case `DefaultReplicationPolicy`.
> mm2-offsets.{source}.internal includes - so allowing dash as a separator may be problematic I am still not sure why this is problematic? In your Jira, you mentioned the following > MirrorMaker2 creates internal topics to track the offsets, configs, and status of the MM2 tasks. But, these topics are not affected by a custom 'replication.policy.separator' settings - that is, these topics may be replicated against the user`s intention. This issue can easily be fixed by handling `.internal` in `isInternalTopic` default implementation. I would suggest just doing this small fix instead. I don't think we need to use the separator or replication policy to define any of connect's topics as users have already a way to override them using `<CLUSTER_ALIAS>.config.storage.topic`, `<CLUSTER_ALIAS>.offset.storage.topic` and `<CLUSTER_ALIAS>.status.storage.topic` configs. No point of controlling the same config in two different ways.
We did not have this check before, why is it needed? Also checks here are only applied when running in "driver" mode.
nit: extra line
nit: `log.error("Exception caught while committing active tasks: {}", activeTasksNeedCommit, e);`
nit: `log.error("Exception caught while post-committing task: {}", task.id(), e);`
nit: add back newline
Is the intention to allow termination to happen from other threads even if still running a test? i.e. do we want this test to pass as well? ```scala @Test def exitWithoutSettingUpHooks(): Unit = { val t = new Thread { override def run { try { Exit.exit(1, None) } catch { case e: RuntimeException => assertEquals("Attempted to terminate the VM in a junit test.", e.getMessage) } } } t.start() t.join() } ``` It's also maybe slightly nicer to not have to check the stack frames ```suggestion private static final boolean IN_JUNIT_TEST = Stream.of( "org.junit.platform.commons.util.ReflectionUtils", "org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor" ).anyMatch(clazz -> { try { Class.forName(clazz); return true; } catch (ClassNotFoundException ignored) { return false; } }); static void throwIfInJunitTest() { if (IN_JUNIT_TEST) { throw new RuntimeException("Attempted to terminate the VM in a junit test."); } } ```
I think the assertion on 219 would pass even if the 1st mocked interaction never happened. Do we need something to tighten up the expected behaviour? Maybe something like: ```java verify(kafkaBasedLog, times(2)).send(any(), any(), any()); ```
Is `|| memberId.equals(Generation.NO_GENERATION.memberId)` really necessary? My understanding is that a reset `memberId` implies that `generationId` was also reset. I guess that it does not hurt to have it.
nit: due to **partition** lost
`In Kafka 3.0,` -> `From Kafka 3.0,` `is default to true` -> `defaults to true`
With the idempotent producer, even if `max.in.flight.requests.per.connection` is > 1, the order is still guaranteed.
`We have specified <code>retries</code> default as Integer.MAX_VALUE, and` -> `The <code>retries</code> setting defaults to <code>Integer.MAX_VALUE</code>, and`
I think `record sends` reads better
`The default "all" setting` -> `The default setting "all"`
We only applied for `ElementType.METHOD`, so `ElementType.ANNOTATION_TYPE` can be removed.
This test is not great but I don't think disabling it is the way forward. I suggest keeping it but maybe reducing the number of retries. We can't really assert `poll()` will _indefinitely_ return `null` and there's little value in spending 100 seconds on this test.
Most tests end up calling this method twice, once explicitly and once via `teardown()`. Let's pick one way and stick with it.
Can we use this opportunity to improve the name of this test? `testMissingFile` sounds like we're testing an error case while this is testing using stdin instead of file.
```suggestion log.warn("The eager rebalancing protocol is deprecated and will stop being supported in a future release. " + "Please be prepared to remove the 'upgrade.from' config soon."); ```
nit: we can use stream here: `int totalTasks = threadLoad.values().stream().reduce(tasksToAssign.size(), Integer::sum);`
Thanks for your explanation. I've got it now, because we've change the total tasks count from original `active tasks` into total ones including active tasks. Thanks.
Make sense to me. We can keep it as is. Thanks.
I think the original algorithm assumes that when reaching this step, all consumers are at the min capacity. So that it could just do this to set `comsuersToFill`: ```java if (!unassignedStatefulTasks.isEmpty()) consumersToFill.addAll(consumers); ``` But I didn't see the where we change this algorithm. Please let me know where I missed. Thanks.
nit: we can use `threadAssignment.size()` to replace the `threadTaskCount` variable. Same as below.
I think we can narrow down the scope of this exception, if it is only going to be used during restoration time. But nevertheless we can discuss about this later.
We can use `new ProducerRecord<>(topic, "value");` to simplify it a tiny bit
We could get away with a single `*`
nit: creating restoredPosition is not required if !constinceyEnabled
nit: I'd make this final with no assignment, then assign it in both branches below.
I see the value in setting this precedent, but unfortunately, we can't do it, due to the need to continue supporting older brokers (we support older versions that don't allow record headers). Instead, if `!consistencyEnabled`, we should just not add headers at all (i.e., we should continue to pass `null` as the headers).
Good point! @vpapavas , can you handle stuff like this in a follow-on PR? I'm doing a final pass to try and get this one merged.
Even though I think this is a bit off, I'm going to go ahead and merge it, so we can fix forward. The overall feature isn't fully implemented yet anyway, so this will have no negative effects if we release the branch right now.
Does this need to be synchronized? See below
In a few other places where we have a constraint like this, I think we tend to say when this method can be called. So for example, something like: ```suggestion * Wait for all currently in-flight messages to be acknowledged, up to the requested timeout. * This method is expected to be called from the same thread that calls {@link #committableOffsets()}. ```
Is `removeAll` a supported operation by the underlying set of the hashmap here? Is it applicable? (same question above for `currentOffsets`)
@cmccabe I think you missed this change.
Did you mean: ```suggestion setBrokerId(2). setBrokerEpoch(100). ```
Did you mean: ```suggestion setBrokerId(3). setBrokerEpoch(100). ```
This interface should be name `public interface SnapshotWriter<T> ...`.
Let's keep this as a private method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
Add the `@Overrride` annotation to this method.
When you make `initializeSnapshotWithHeader` private, you may need to slightly change this implementation. E.g.: ```java return supplier.get().map(snapshot -> { RecordsSnapshotWriter<T> writer = new RecordsSnapshotWriter<>( snapshot, maxBatchSize, memoryPool, snapshotTime, lastContainedLogTimestamp, CompressionType.NONE, serde); writer.initializeSnapshotWithHeader(); return writer; }); ```
You're right! I just checked the parameter, and make sure there's no such parameter called `time`. But, yes, it's put the wrong name. Thanks for the review!
Nit: `log.warn("Could not decode offset metadata.")` I think it's better not to log `encryptedString` as we don't know what's in it, and we could potentially leak sensitive information.
I think we'll still want to advance the position, even if we otherwise ignore the record.
It looks like @patrickstuedi needed to reference the context in the concrete test classes that subclass this.
Thanks. I had this on my mind when I was working on the tests for the framework PR. It occurred to me that another approach would be to simply run a dummy query and check the returned position. But I guess that doesn't really count as a "unit" test, since the intervening store layers could in theory be changing the position as well, so that approach might not be testing what it thinks it's testing. This is more direct, and to be honest, I don't think it's that bad in the context of a unit test.
It might be worthwhile having a separate case which goes through the sequence described in the jira. Basically this: 1. Receive metadata response with topicID A. 2. Receive metadata response with UNKNOWN_TOPIC error. 3. Receive metadata response with topicID B.
nit: `Older` -> `older`; `topic Id` -> `topic ID`; `TopicId` -> `topic ID`; `Epoch` -> `epoch`.
nit: `foo` reads a bit weird here. I would just remove it. Similarly, `topic A` is weird because the topic is name `topic`.
Also, it might be better to use `synchronized (AbstractCoordinator.this) { }` to mutate both `rejoinReason` and `rejoinNeeded` in order to ensure that they are consistent with each others.
+1 for `getSimpleName` for the class. In addition to the David's suggestion, I think we should also remove the 2nd `due to`, because there is already 1 `due to` in the sentence. ex: `rebalance failed: '$message' ($class)`
The reason is a bit weird. I wonder if we could just leave it empty in the beginning.
I think you missed marking this one as "windowed"
```suggestion @Evolving public class WindowKeyQuery<K, V> implements Query<WindowStoreIterator<V>> { ```
```suggestion public static <K, V> WindowKeyQuery<K, V> withKeyAndWindowStartRange(final K key, final Instant timeFrom, final Instant timeTo) { ```
More explanatory error, as discussed.
Since I was fixing stuff anyway, I went ahead and fixed a bunch of formatting issues that I didn't bother mentioning before.
This just made it easier to inline the "unknown query" message.
```suggestion @Evolving public class WindowRangeQuery<K, V> implements Query<KeyValueIterator<Windowed<K>, V>> { ```
This is the reason I exploded the `serdes` reference in favor of functions for deserializing the key and value. When we're handling queries for non-timestamped stores, we need to be able to adapt the value deserializer.
Are these calling the right method? ```suggestion shouldHandleWindowRangeQuery( ```
We shouldn't return `null`, but instead return a "unknown query" result.
Dropped this unnecessary duplicate code, as discussed.
Looks like we were not handling the right query variant before, but it didn't come up yet because other tests were failing before we got to this point.
Note, the new version in StoreQueryUtils returns a Function, so that the iterators can just invoke the function on the value without having to know the right topic to pass in to the deserializer.
I moved this to StoreQueryUtils because we need it in WindowStore as well.
I think this was my bad from before. This store is not a timestamped store.
This is why the test was failing for you. The query is for a range of window start times, not record times. Since the window size is five minutes, the range `[now - 1 minute, now]` wasn't going to contain the actual window start time of `now - 5 minutes`. In other words, just a simple oversight :/ Sorry for the trouble.
Since we're specifying the key, we expect only to get back windows with the value for that key. The aggregation we specified is to sum all values for the key, and it comes out to `2` because we only write one value for each key; namely, the value is the same number as the key.
This isn't our fault. When we added the timestamped stores, we chose not to make SessionStores timestamped because the session bounds already have the end timestamp available, which is identical to the timestamp we would have stored in the value.
I moved the bounds into the plural check method, so we can check correct behavior for multiple bounds.
Setting the records' timestamps.
It seemed like a good idea to check a few other query configurations, but none of them showed any problems.
Added the `toString` so that the queries printed in exceptions will contain useful information.
Moved from the MeteredKeyValueStore. I still hope we can refactor the store hierarchy later to get rid of this entirely.
This message didn't need to be specialized.
This was the checkstyle error that was failing your build.
Huh, looks like @vpapavas and I missed the need to close the iterator before.
Dropped this unnecessary duplicate code, as we discussed.
These iterators need to be closed or they'll leak resources (it's the same for IQv1 as well).
Dropped duplicate code, as discussed.
nit: Indentation of those lines seems to be off here.
Good catch, thanks for cleaning this up!
It'd be more powerful to do an assertion on the complete set of returned plugins, since that will only require one test run to discover all differences between the expected plugins and the actual ones: ```suggestion Set<Class<?>> excludes = Stream.of(ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES) .flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of(SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS) .flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(true)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ``` (This assumes we split out `CONNECTOR_EXCLUDES`, but the same general strategy should apply even if we don't).
Same thought w/r/t performing assertions on the complete set of returned plugins: ```suggestion Set<Class<?>> excludes = Stream.of( ConnectorPluginsResource.SINK_CONNECTOR_EXCLUDES, ConnectorPluginsResource.SOURCE_CONNECTOR_EXCLUDES, ConnectorPluginsResource.TRANSFORM_EXCLUDES ).flatMap(Collection::stream) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> expectedConnectorPlugins = Stream.of( SINK_CONNECTOR_PLUGINS, SOURCE_CONNECTOR_PLUGINS, CONVERTER_PLUGINS, HEADER_CONVERTER_PLUGINS, TRANSFORMATION_PLUGINS, PREDICATE_PLUGINS ).flatMap(Collection::stream) .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginsResourceTest::newInfo) .collect(Collectors.toSet()); Set<ConnectorPluginInfo> actualConnectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins(false)); assertEquals(expectedConnectorPlugins, actualConnectorPlugins); verify(herder, atLeastOnce()).plugins(); ```
Nit: I know this is following the [existing style](https://github.com/apache/kafka/blob/c2ee1411c8bb73fcf96c12abeedbfe6fde2c6354/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java#L101-L118) in the code base, but do you think the separate `getConnectorPlugins` method is actually bringing anything to the table readability-wise? Think we could just as easily eliminate the `getConnectorPlugins` method and inline it directly here. Same thought with `getConnectorConfigDef`.
Doesn't this introduce the possibility of conflict between two plugins (or I guess specifically connectors, since those are the only ones we strip suffixes from) which have different fully-qualified class names, but the same simple class name? Or where they would have the same simple class name, except that one ends with `Connector` and the other doesn't? In practice this is unlikely to come up but if we support it at the moment, probably best to take care here to avoid introducing a potential regression, especially if someone for some reason wants to run, e.g., two different `MySqlSink` connectors on their worker.
Also, it seems like using a default of `PluginType.UNKNOWN` here might be suboptimal. If someone wants to the view the config for a REST extension, for example, they'll end up seeing an error message later on (in `AbstractHerder::connectorPluginConfig`) that says something like "Invalid plugin type unknown. Valid types are..." I think it'd be clearer to users if we could differentiate between these two cases: 1. User requests config for a plugin that does exist on the worker, but which we don't expose config information via the REST API for (such as a REST extension or a config provider) 2. User requests config for a plugin that doesn't exist on the worker Status-wise, In the case of 1, a 400 response probably makes sense, but for 2, a 404 response might be more applicable.
It seems like we're duplicating some of the logic contained in `Plugins` into this class by tracking class alias names and pre-computing plugin type based on them. Did you consider a `Herder` method that only accepted the name of the plugin, and took on the responsibility of deducing the plugin type itself? ```java List<ConfigKeyInfo> connectorPluginConfig(String pluginName); ``` In `AbstractHerder`, we could do something like this: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { Object plugin = Plugins.newPlugin(pluginName); PluginType pluginType = PluginType.from(plugin.class); List<ConfigKeyInfo> results = new ArrayList<>(); ConfigDef configDefs; switch (pluginType) { case SINK: case SOURCE: configDefs = ((Connector) plugin).config(); break; case CONVERTER: configDefs = ((Converter) plugin).config(); break; // ... Rest of switch statement follows same pattern, and rest of the method remains unchanged } ``` And in `Plugins` we could do this: ```java public Object newPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends Object> klass = pluginClass(delegatingLoader, classOrAlias, Object.class); return newPlugin(klass); } ``` Or alternatively, we could introduce a common interface for plugins that expose a `ConfigDef`: ```java interface DefinedConfigPlugin { ConfigDef config(); } ``` Which could really simplify some of the `AbstractHerder` logic: ```java @Override public List<ConfigKeyInfo> connectorPluginConfig(String pluginName) { try { DefinedConfigPlugin plugin = Plugins.newDefinedConfigPlugin(pluginName); ConfigDef configDefs = plugin.config(); // No switch statement on plugin type necessary // ... Rest of the method remains unchanged } ``` And the change to `Plugins` would be lightweight as well: ```java public DefinedConfigPlugin newDefinedConfigPlugin(String classOrAlias) throws ClassNotFoundException { Class<? extends DefinedConfigPlugin> klass = pluginClass(delegatingLoader, classOrAlias, DefinedConfigPlugin.class); return newPlugin(klass); } ``` Worth noting that if we want to differentiate to users between "this plugin is not on the worker" and "we don't expose config information for this type of plugin", we'd have to make a few further tweaks.
Now that we have separate `Plugins::sinkConnectors` and `Plugins::sourceConnectors` methods, we can abstract this a little, which should improve readability a bit and make it easier to extend for other plugin types in the future: ```suggestion static final List<Class<? extends SinkConnector>> SINK_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSinkConnector.class, MockSinkConnector.class ); static final List<Class<? extends SourceConnector>> SOURCE_CONNECTOR_EXCLUDES = Arrays.asList( VerifiableSourceConnector.class, MockSourceConnector.class, SchemaSourceConnector.class ); @SuppressWarnings({"unchecked", "rawtypes"}) static final List<Class<? extends Transformation<?>>> TRANSFORM_EXCLUDES = Collections.singletonList( (Class) PredicatedTransformation.class ); public ConnectorPluginsResource(Herder herder) { this.herder = herder; this.connectorPlugins = new ArrayList<>(); // TODO: improve once plugins are allowed to be added/removed during runtime. addConnectorPlugins(herder.plugins().sinkConnectors(), SINK_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().sourceConnectors(), SOURCE_CONNECTOR_EXCLUDES); addConnectorPlugins(herder.plugins().transformations(), TRANSFORM_EXCLUDES); addConnectorPlugins(herder.plugins().predicates(), Collections.emptySet()); addConnectorPlugins(herder.plugins().converters(), Collections.emptySet()); addConnectorPlugins(herder.plugins().headerConverters(), Collections.emptySet()); } private <T> void addConnectorPlugins(Collection<PluginDesc<T>> plugins, Collection<Class<? extends T>> excludes) { plugins.stream() .filter(p -> !excludes.contains(p.pluginClass())) .map(ConnectorPluginInfo::new) .forEach(connectorPlugins::add); ```
This field is not used, we can remove it.
`VALID_TYPES` can now be removed too
```suggestion "The desired Unix precision for the timestamp. Used to generate the output when type=unix " + ```
```suggestion "Note: This SMT will cause precision loss during conversions from, and to, values with sub-millisecond components."); ```
I know we've discussed this, but just for the record :-), I believe the type PositionBound could be saved, unbounded is basically an empty Position.
Sounds good, thanks for thinking about it.
I still think this entire check should be method on Position, as in, Position::dominates(PositionBound)
Should we check the `key` is not null here? Since in later callers e.g. `final KeyQuery<Bytes, byte[]> rawKeyQuery = KeyQuery.withKey(keyBytes(typedKeyQuery.getKey()));` we do not check if `getKey()` is null or not, and `keyBytes` function could throw if it is.
@vvcephei WDYT about having an extended `QueryResultInBytes` (just a placeholder for name) on `QueryResult<byte[]>` and move this function to that extended class? This way we can avoid mistakenly using the swap functions.
> I suppose we could throw an IllegalStateException, since the caller probably shouldn't be even trying to "swap" the result on a failed result to begin with, but this seems fine, too. If there is no strict reason not to throw an `IllegalStateException`, I would strongly advocate to throw. It not only guards against potential bugs, but also expresses the semantics for developers (ie, us) much cleaner and makes the code easier to read/reason about.
We don't really need this. You can just do `QueryResult.forResult` in the `MeteredKeyValue` store for example to get the typed result
I see. So we should add it elsewhere, too (of course not as part of the IQ work).
It seems that you are right, David. Closing when the selector is closed makes more sense.
Would it make sense to create a `ConnectionMetrics` class to hold all the connection metrics? That would give us an opportunity to improve all the `record*` methods as well. They could get the sensors based on the `connectionId`.
I see. Thank you.
Why do we need this? The idea of the ticket was to change existing code that catches `TaskCorruptionException`: after we wipe out the local state store, we trigger a rebalance if standbys are enabled.
I'm sorry to sound picky, but do you mind backing out these formatting changes? I'm only concerned because there's a lot of them. Otherwise, we'll just have duelling autoformat results between commits. For reference, I have indents and continuation indents both set to `4` for this project.
Sorry, not trying to golf here, but since everything between these branches is the same except the extractor function, I'm wondering if we should factor the checks out into a separate method: ```suggestion if (storeToTest.timestamped()) { shouldHandleRangeQueries( (Function<ValueAndTimestamp<Integer>, Integer>) ValueAndTimestamp::value ); } else { shouldHandleRangeQueries(Function.identity()); } ... private <T> void shouldHandleRangeQueries(final Function<T, Integer> extractor) { shouldHandleRangeQuery( Optional.of(1), Optional.of(3), extractor, mkSet(1, 2, 3) ); shouldHandleRangeQuery( Optional.of(1), Optional.empty(), extractor, mkSet(1, 2, 3) ); shouldHandleRangeQuery( Optional.empty(), Optional.of(1), extractor, mkSet(0, 1) ); shouldHandleRangeQuery( Optional.empty(), Optional.empty(), extractor, mkSet(0, 1, 2, 3) ); } ```
The content of this test does not fit with the name of the test. There is no other topology that continues processing.
Shouldn't you verify if topology 1 still produces output records at this point? When I read the test name I would expect that verification here.
This is unrelated to this PR but the check brings up a good point. Maybe instead of throwing an error on the pattern subscription finding an overlapping topic and then restarting the thread and trying again. Maybe we can pause a topology until the topics are not longer overlapping? EDIT: I can see this in the other PR
nit: `brokerId` -> `producerId record`
```suggestion log.debug("The offsets have been reset by another client or the group has been deleted, no need to retry further."); ```
Got it, thanks for the notes.
nit: add a space so it is "StreamsMetadata {...} topologyName=xyz"
Nice coverage with different num.partitions, thanks!
Do we want the raw topic names (without the prefix) or the decorated ones here? BTW The function/variable names are a bit confusing but they stored different things. Maybe we should just rename them to be more clear.
Was this left in accidentally? Looks like it overrides the line above, which seems more suitable for this test case.
Makes sense. Just wanted to suggest making it more readable and coincidentally I saw this would make it also same as the one in the equivalent streams class: https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java#L94
Can just return `name.startsWith(acl.resourceName())`
Ditto. Not using index.
For a follow-up, but we will probably need to add something like `AclAuthorizerBenchmark`.
nit: more simply ```java if ( acl.resourceType() != action.resourcePattern().resourceType()) return false; ```
nit: since we're not using the index, maybe use `for` with implicit iterator
nit: a bit more natural? ```java if (results != null && results.size() != futures.size()) { ```
nit: `ALLOWED == result`
nit: need to update this since resource name is before pattern type now
Not sure if it makes a big difference, but we could use EnumSet for these.
Just to be explicit, are you saying that `ClusterMetadataAuthorizer.addAcl` will be applied before the future returns, or just that the record will be committed. I guess the latter is really all we care about since the change must get propagated to the whole cluster in any case.
nit: I don't think we need the iterator
nit: `final` is redundant for private static method
@cmccabe (My suggestion might not be good) If it has to check `futures`'s `size` every time, what about assign one time and use many times? Because JVM has to calculate size everytime when it is called. ```suggestion final int futuresSize = futures.size(); if (throwable == null && results.size() != futuresSize) { ```
Seems you got the same :) and ditto elsewhere.
@mjsax please double check my understanding here since it's been a while since I thought about/worked with this stuff, but IIRC it's important to note that under the current Producer semantics at least, we can only do this kind of partial commit when using ALOS or EOS-v1. In KIP-447/EOS-v2, if one task/partition is bad then the transaction will need to be aborted, and thus all partitions/tasks will unfortunately need to be aborted as well. The API is a bit misleading since it implies you can choose which offsets to send to the transaction, but I recall @mjsax mentioning that the transaction is applied across all partitions at a time
nit: put this in the `Tasks` class with the other metadata
Correct. You need to distinguish between the read and write path here: the producer has no idea from what input partitions we consume, and thus it can only offer a generic `addOffsetsToTransaction` API and it's the user's (ie, our) responsibility to provide the correct offsets. What we write into the output topic has nothing to do with what we consumed from the input though: Even if you omit offsets for some input partitions, you still would have the corresponding pending writes on the output topic, so just omitting the offsets from the map does not help you to abort the pending writes for those partitions. What writes are committed has nothing to do with what offsets you pass to the producer.
nit: if we just clear it at the beginning of `#process` then we only need to clear it in one place, vs clearing it here and also in `#commitSuccessfullyProcessedTasks`
Sorry; one more formatting correction. This applies in several places in this PR: > If a method call is longer than 120 characters, switch to a single parameter per line formatting (instead of just breaking it into two lines only). https://kafka.apache.org/coding-guide.html
Thanks! @patrickstuedi consolidated the logic of actually reading the position checkpoints into StoreQueryUtils, as you suggested. Also, after some cleanup, we only have two instances of this now, in `AbstractRocksDBSegmentedBytesStore` and in `RocksDBStore`.
This isn't harmful, but it will also never be used, since this store is not exposed to IQ. I'm not sure if we still want this implementation or if we'd rather just throw an `UnsupportedOperationException`. I think that's something you can do in a follow-up PR if you like.
Could you also verify that the stream thread was not replaced? You could use `KafkaStreams#metadataForLocalThreads()` for that.
nit: extra empty line here
I think the reason why we need to set `assignmentSnapShot` here is because after assignment (ex: custom assignor), it might be possible that there are extra topics to be added, and we need to request new metadata and add that in to `metadataSnapshot` in `maybeUpdateGroupSubscription()` method, right? So, I think you could do like this: ```java if (skipAssignment) { // set the metadataSnapshot here for skipAssignment case assignmentSnapshot = metadataSnapshot; return Collections.emptyMap(); } ... maybeUpdateGroupSubscription(assignorName, assignments, allSubscribedTopics); // still keep this line here assignmentSnapshot = metadataSnapshot; ```
can you please also check that the partition id gets set to -1
As before, it's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
It's not necessary to do this. If you want to display a special error message when the assert fails, there is a three-argument form which lets you specify the error message.
nit: I think the check for `userConfiguredTransactions` is redundant now.
I think this can be simplified a little bit more by getting rid of this variable. How about this? ```java boolean userConfiguredTransactions = this.originalsContainsKey(TRANSACTIONAL_ID_CONFIG); boolean idempotenceEnabled = this.getBoolean(ENABLE_IDEMPOTENCE_CONFIG); if (!idempotenceEnabled && userConfiguredTransactions) { throw new ConfigException("Cannot set a " + ProducerConfig.TRANSACTIONAL_ID_CONFIG + " without also enabling idempotence."); } return idempotenceEnabled;
Thanks for adding coverage for sliding windows.
Can we make this a `case ROCK_DB` and add a ``` default: throw new IllegalStateException("Unknown store type: " + materialized.storeType()); ``` Similar elsewhere.
+1 -- also note that this sort of only makes sense when using the named topology feature, as otherwise you don't have multiple topologies and not really any reason to set any of these configs differently in the TopologyConfigs vs StreamsConfig passed in to the KafkaStreams constructor. That said, it will definitely happen, so I guess we should check for overrides whether the topology is named or not. Maybe you can use the `isTopologyOverride` method, and remove the check for `namedTopology ~= null`? ie, you can/should use `isTopologyOverride`
nit: curious why `storeType` is not enum but raw string. Typo in `in_memory` could make it `ROCKS_DB` type. e.g. `in_memry`. Maybe it's register in `in(ROCKS_DB, IN_MEMORY),` when defining and checked there
Could we use `switch-case` instead of `if-else` in case we add more store types in the future? Ditto elsewhere.
```suggestion private static String nullify(String string) { return Utils.isBlank(string) ? null : string; } private static Password nullify(Password password) { return (password == null || nullify(password.value()) == null) ? null : password; } ```
I told Bruno to do this -- we can discuss next week if you still have questions, but the motivation is to avoid letting the tasks of a topology/query get way out of sync with each other. This can result in missed output for example when a processor upstream of one side of a join is paused for a long time while the other continues proccessing records and trying to join them
I am not sure we want to move all the tasks in a topology? Maybe we can do that by task or sub topology? maybe topology is best but I will need to think about it a bit
It would be much simpler but unfortunately its not as simple as we first thought. The producer has only one transaction, so the records of the good tasks are mixed in with the records of the failed task and there is no way to separate them. So we need to take the tasks that we know will fail and process all the other tasks without them. That way we continue making progress. We can take the failing tasks and backoff and retry again later.
It might be nice to keep the tasks/topologies that have failed in another list entirely. Then when reprocessing after an exception we can run all the good tasks first and commit them before running the failures. This will be important for EOS as we con't commit only part of a transaction. The larger part of that doesn't need to be done it this PR but keeping the groups separate would be nice in my mind
I think the intent was to remove generation in the original PR.
I was going to ask why we're using the `test` prefix for a benchmark, but then I realized that many of the kafka benchmarks do that and I somehow didn't notice. :) Given that, it seems fine to leave it like this for now.
Moving `close` outside of locked scope LGTM
I think the expectation is that these 2 would be atomic (i.e. would be bad if one thread executed 615, then another thread executed 615 again and got the same sequence number, before the first thread got a chance to execute 616). Also I think the expectation is that batches that are ordered one after another in the queue would get the sequence numbers in the same order (i.e. that batch that is later in the queue would get higher sequence number). Previously these expectations were protected by the queue lock so "poll", "get sequence", "update sequence" would execute as atomic block, with this change the operations could interleave.
No, not a blocker.
Make sense, and that's also what I've seen. Thanks for confirmation!
I think the useful context that was removed is: > When producing to a large number of partitions, this path is hot and deques are often empty.
Can you please test with Java 11 or newer? Looks like you tested with Java 8 which uses the slower crc32c method.
Out of curiosity, do these make much of a difference? We can probably keep them, but it seems like the big problem was the `close` call being inside the lock (in the other method).
Looks like it won't happen since we only lock on deque object, but just want to confirm, to make sure it won't break anything.
Changes in the `ready` function LGTM
This approach creates a temporary object, so it's not clear that it's better. It would need benchmarking to confirm.
This microbenchmark doesn't seem to be written correctly. you are creating a singleton list of an int[] array in `testCreateHashSet1`. `inits` needs to be an `Integer` array rather than a primitive array to get the expected array/hashset behavior.
Do you know why we had this deprecated? Do we consider it a public API? Below we have code which checks for null `SharedTopicAdmin`. Seems like we can probably get rid of that. The only usage I could see was in a test case.
What do you think about merging the `restoredActiveTasks` and `exceptions` into a single queue, for both reporting completed tasks and failed tasks? We can potentially extend the `TaskAndAction` to have `PROCESS` for restoration completed tasks and `ERROR` for exception cases. Hence 1) between the main thread and the updater we just use two queues, rather than three data structures; 2) the `getRestoredActiveTasks` can be simplified without a timer, but just try to drain the next from the queue as consolidated together with `getExceptions`.
Yeah it makes sense. Sorry for not getting back to you sooner
It seems relevant to the implementer whether the configs have been validated or not. If they're not guaranteed by the contract to have been validated then the implementer might need to duplicate some of the validation logic in this method.
"Connector authors" might be less ambiguous than "Developers".
```suggestion + "not all connectors are capable of defining their own transaction boundaries, and in that case, attempts to instantiate a connector with " ```
Should this be included here, or should it refer to a dedicated section in the connect docs? I guess there's two cases: bootstrapping a whole new connect cluster, or upgrading an existing one. For the bootstrapping case it's not completely clear whether the "preparing" round is required.
Should we add a null check at the beginning? i.e. `Objects.requireNonNull()`
nit: **It** has no effect if a different TRANSACTION_BOUNDARY_CONFIG is specified.
I see. Thanks.
+1 for consistency
There's also an expected error: `PRODUCER_FENCED` which should be handled here.
wrong class name
Nit: I think we can simply say `Idempotence will be disabled...` (instead of `enable.idempotence` will be disabled...`)
Nit: remove "Note". "Additionally, enabling..." reads better.
I meant to have "Note that enabling idempotence requires this config..." before "Allowing retries...". And break the two parts with a paragraph.
Maybe we can set this to `false` in the `shouldDisableIdempotence` block? Seems a bit more natural.
Hmm, I think this logic and elsewhere is a bit confusing. If `retries == 0` _and_ idempotence has been enabled by the user, we need to throw. It doesn't matter if retries is set by the user or not. Of course, we only expect `retries == 0` if set by the user. But we are hiding a potential bug in the way we're checking this. Same applies for other configs.
I think it would read better if we remove `config` from the sentence.
Thinking about this some more, not sure there's a lot of value in forcing users to set `idempotence=false` in cases where they're setting `acks=1|0` or `retries=0`. So, I'd change the warning to info for these cases. `max.in.flight.requests.per.connection` is different since it's an implementation constraint that `idempotence` doesn't work when it's > 5 vs inherent to the configuration. For this one, I'd have a warning and mention that it will become an error in Kafka 4.0.
It reads weird to say > disabling <code>enable.idempotence</code> I would write the sentence as: > Allowing retries while setting <code>enable.idempotence</code> to <code>false</code> and <code>" + MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION + "</code> to 1 will potentially change the" I would also move this paragraph below: > Enabling idempotence requires this config value to be greater than 0...
We can simplify it to `stream().peek(throw).to()` without materializing a store.
Also I think by just checking that 1->A and 1->B are there we do not guarantee there's no duplicates due to re-processing right? I think we should check that the offset on the input topic can be committed and also there's no duplicates in the output.
We cannot disable tests like this.
Nit: ```suggestion log.warn("Executing {} only once, since retryBackoffMs={} is larger than total timeoutMs={}", descriptionStr, retryBackoffMs, timeoutMs); ```
Ideally we'd not wrap the exception if there are no retries, so I guess it just depends on how hard it is to make that work.
I think there's an edge case where `timeoutMs` is positive but small enough that the condition on line 77 is not met but the while loop on line 85 is not satisfied because the end time has already passed. In this edge case, we might not call the callable function (even once). One option is to change the while loop to be a do-while loop so that we always go through one loop. Another option is to compute the remaining time before line 77 and not update it before the while loop. Either would work, but one of the options may require fewer duplicated lines.
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
```suggestion * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again ```
```suggestion * @param timeoutDuration timeout duration; must not be null ```
How about: ```suggestion * <p>The task will be executed at least once. No retries will be performed * if {@code timeoutDuration} is 0 or negative, or if {@code timeoutDuration} is less than {@code retryBackoffMs}. ```
```suggestion * <p>A {@code retryBackoffMs} that is negative or zero will result in no delays between retries. ```
Nit: ```suggestion * executed exactly once. If {@code maxRetries} is set to {@code n}, the callable will be executed at ```
@philipnee can you please correct this spacing to reflect the project standards? Thanks!
I don't think it's necessary to warn in this case. Maybe debug, but not warn. If callers are concerned, they can check the parameters before calling the method.
Nit on the spacing so the description of parameters is column-aligned. ```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a * {@link org.apache.kafka.connect.errors.RetriableException} before retrying again; * must be 0 or more ```
```suggestion * @param callable the function to execute. * @param maxRetries maximum number of retries; must be 0 or more * @param retryBackoffMs the number of milliseconds to delay upon receiving a *. {@link org.apache.kafka.connect.errors.RetriableException} * before retrying again; must be 0 or more ```
Nit: since we require a non-null `Duration`, we should state that here: ```suggestion * @param timeoutDuration timeout duration; may not be null ```
All of the log messages in this method could use a description of the callable. WDYT about adding a `Supplier<String> description` parameter, and using that in the log messages. For example, the `KafkaBasedLog` could supply ``` () -> "list offsets for topic '" + topicName + "'" ``` then this log message might be: ```suggestion log.warn("Call to {} will only execute once, since retryBackoffMs={} is larger than total timeoutMs={}"), description.get(), retryBackoffMs, timeoutMs); ```
Likewise, this log message could be changed to: ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + "Reason: {}", attempt, description.get(), e.getMessage(), e); ```
And this exception message could also use the description: ```suggestion throw new ConnectException("Fail to " + description.get() + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
WDYT about this: ```suggestion long millisRemaining = Math.max(0, end - System.currentTimeMillis()); if (millisRemaining > 0) { Utils.sleep(millisRemaining) } ```
Nit: including "execute" here is completely unnecessary. ```suggestion log.warn("Attempt {} to {} resulted in RetriableException; retrying automatically. " + ```
Nit: including "execute" here is completely unnecessary. ```suggestion throw new ConnectException("Fail to " + descriptionStr + " after " + attempt + " attempts. Reason: " + lastError.getMessage(), lastError); ```
Are we intentionally not logging the exception as the extra parameter? If the exception wraps a more useful exception, we won't see any information about the wrapped exception unless we can see the stack trace in the warning log message.
```suggestion if (attempt < maxAttempts) { Utils.sleep(retryBackoffMs); } ```
Suggestion to improve the message: ```suggestion throw new ConnectException("Fail to retry the task after " + maxAttempts + " attempts. Reason: " + lastError.getMessage(), lastError); ``` And, if `maxRetries == 0` should we just call the function without any special handling, since we're not retrying? For example, should we add something like this very early in the method? Doing that would make it easier to phrase this message, since the `ConnectException` will only be used when at least 1 retry may be attempted. ``` if (maxRetries <= 0) { // no special error handling return callable.call(); }
WDYT? ```suggestion log.warn("RetriableException caught on attempt {}, retrying automatically up to {} more times. " + "Reason: {}", attempt, maxAttempts - attempt, e.getMessage()); ```
Nit: ```suggestion final long maxAttempts = maxRetries + 1; ```
Yeah, I think it's worth the bit of logic to fail more quickly.
What happens if `millisRemaining` is, say, 2 and `retryBackoffMs` is 1000? If `millisRemaining` is positive, then shouldn't we sleep for the smaller of `millisRemaining` or `retryBackoffMs`? IOW: ```suggestion Utils.sleep(Math.min(retryBackoffMs, millisRemaining)); ```
Oh... Thanks for the reminder. @RivenSun2 , could you summit a small KIP for this change? Thanks.
TBH, no, I've never considered that! Thanks for the reminder. @RivenSun2 , sorry, I forgot the `AbstractConfig` is a public API. But since the v3.2.0 KIP freeze has passed, and I think this KIP might need more discussion, could you submit another PR to revert this change first? After KIP discussion/voting passed, we can start to work on it again. If you don't have time to submit a revert PR within this week, please let me know. Thank you.
same here and below, we should assert false to `(config.unknown().contains("prefix.sasl.kerberos.kinit.cmd")` after using that config.
Before this PR, the `unUsed` set contains both `unUsed` and `unKnown` configs, because we didn't separate them. After this PR, we hope we can separate the `unUsed` and `unKnown`, so we created a new method `unKnown`. Under this situation, I don't think it makes sense to make `unUsed` set contains both `unUsed` and `unKnown`. That will just confuse other developers.
here, we should change to assert `unknown`, right? Same as below.
Another nitpick: to use 0 as the base store prefix, and 1 as indices and so on; the main thinking is that in the future we may extend it to have multiple indices with a single base.
nit: we usually align the parameters, like this: ``` public static Bytes toTimeOrderedStoreKeyBinary(final Bytes key, final long timestamp, final int seqnum) { ```
This reminds me that we should change the internal interfaces of `Segments` etc to use the new StateStoreContext, not the ProcessorContext anymore, will create a new JIRA.
Nit: no need newline. Also I think we do not need the `TimeFirstWindowKeySchema.` since it is within this class right? Ditto below.
Nit: param alignment.
I concern that by stripping off just the first byte for base key and index key, we would be paying too much byte array copies which would be much slower than other stores (for which we just blindly copy-paste the bytes). Maybe we can have some optimization in-place for the first draft? E.g.: 1) for index key, we know we just need to add the first byte and strip the last four byte of seq (? is that right, need to double check). 2) for base key, we can just do a swap-copy from the index key to switch the position of timestamp and key.
Basically I was wondering if it is really necessary to pad four 0 bytes on the key schema. We use `seqnum` for window stores that are used in stream-stream joins, since we need to maintain values of the same `[key, timestamp]`, but for aggregation windowed key we would not need to maintain uniqueness, and hence not necessary to use `seqnum`. I'm basically asking if just `[from, key]` is good enough as the lower range.
I'm assuming this is just extracting the inlined function and hence skipped and did not compare line by line :)
Thanks! I overlooked on its side effects..
`encryptedString` cannot be `null` IIRC -- seems we should not check for this condition to no mask a potential consumer bug, but rather fail-fast.
nit: in AK it's common to omit `get` prefix in getters. The name should be `processorMetadataForKey`.
nit: `addMetadata` -> `put`
nit `getMetadata` -> get`
`needCommit` -> `needsCommit`
`setNeedsCommit` -> `{@link #setNeedsCommit}`
Might be better to add a proper POJO maybe `StreamsMetadata` or something that wraps the `streamTime` Long plus `ProcessorMetadata` instead of using `KeyValue` ? We might add new fields later on what is easier to do for a new POJO.
I never knew about that double underscore trick. Thanks!
@cmccabe I think we can remove this static field from `ConfigurationControlManager`. I don't think we use it or need it anymore.
I think we misunderstood each other. I was asking for an integration test that verifies that the rebalancing works for the maximum length of the tags. To see if the subscription has still a size that works. Maybe also in combination with many tasks.
I was more thinking about 1000 stateful tasks, because the task offset sums might also inflate the subscription.
+1 for 100 tasks. Thanks.
I think, here it makes sense to wait until all Streams clients are `RUNNING` so that we know that the rebalance is done.
We already have `startApplicationAndWaitUntilRunning` or `waitForApplicationState` in `IntegrationTestUtils`. I think we can use `startApplicationAndWaitUntilRunning` in our tests.
Looks like there are some checkstyle violations in the commit. It's probably choking on not having a space between the `catch` and open parenthesis. ```suggestion } catch (ConnectRestException e) { ```
+1 to this
These should be non-empty checks. Thanks for catching.
We can make this clearer if it helps. Maybe something like FullFetch? Not sure if there is a better option
Much better than returning null
Just a small nit: this is how my brain works -- otherwise the condition is "reverse" and I need to flip it in my head
So we basically do 10 re-tries? Is this intended? Or should be just sleep for a hard-coded "backup time"
```suggestion if (System.currentTimeMillis() > expectedEnd) { ```
Maybe this looks better? ```suggestion // we're at the end of the input. if (queryResult.getResult().value() == batch1NumMessages - 1) return; ```
Would it make sense to do this check even before checking if the coordinator is known? Moreover, it seems that we could skip calling `doCommitOffsetsAsync` entirely by returning a completed future directly. What do you think? ``` if (offsets.isEmpty()) return RequestFuture.voidSuccess(); ```
I checked again, and I think it's OK. Thanks.
Oh.. I see. In this case, would it make sense to check `offsets.isEmpty()` first, then `!coordinatorUnknown()`, and the final else. That seems a bit more natural when reading it.
I considered this but I think that it is clearer when kept separated.
Hello and thanks for reviewing! The callout about record ordering was originally requested within Confluent; the same is mentioned at this page: https://developer.confluent.io/tutorials/message-ordering/kafka.html. We think this would also benefit AK docs.
So what about something like: ``` If the producer is configured with acks = 0, the {@link RecordMetadata} will have offset = -1 because the producer does not wait for the acknowledgement from the broker. ```
typo schedued and schdule
Preferred leader election is an optimization. If we can't move the leader to the preferred one, it seems there is no need to do anything extra.
The number has changed and 5 is no longer relevant.
Should we indicate the method of leader election that was performed here? Or at least indicate if it was an unclean election
The second half of this sentence reads a little odd, how about: ```suggestion * Perform leader election to keep the partition online. Elect the preferred replica if it is in the ISR. ```
Hmm, why do we want to skip the first replica? When election != Election.PREFERRED, we still want to consider the first replica as long as it's in ISR and is alive.
This is also an existing issue. We set the ISR here, but it can be overridden to targetIsr in tobuild() later.
Is this step necessary given step 1 and 2? The election != Election.PREFERRED case is covered in step 1 and the other case seems covered by step 2.
This is not a suggestion: we are sending `null` with the guarantee that we should have never forward for this key before. I think a good test case coverage would be to have a windowed aggregation emit final, followed by a join. The join results would need both the old/new values to be able to correct, and if emit final we should only emit once, with old value setting to null.
(63/64)^100 is ~20%.
You're right. I misread the line and thought that we were constructing the string before calling trace.
indent is not aligned with above lines.
I'm wondering if we should also clear the fetched messages in the buffer when revoking the partitions as well? As suggested in the ticket itself: ``` corresponding message in the memory (Fetcher.completedFetches) of pausedPartitions was not cleared, resulting in Fetcher#fetchedRecords() still fetching the message ```
Should this ever happen? If it does happen should we consider it a bug? Ditto for the other `hasNextCondition`.
nit: with multiple params that cannot fit in one line, we usually just have one param per line, ditto the other place.
I am not entirely sure about this. I think that `SASL_JAAS_CONFIG` should be prefixed with the SASL mechanism otherwise we ignore it. In this case, we log a warning in `loadServerContext`. I suppose this is the reason why we don't use a generic message for both cases here. It does not make sense to say that `SASL_JAAS_CONFIG` is not set if we don't use it without the prefix.
Is there any extra benefit in the large input tests? If not, maybe drop the large input tests and rename the first two tests.
Ah I see, all good then.
nit: whitespace is off a bit
There is a built-in for this `Function.identity()`
I wonder if "configsAuthorized" or similar would be better here. "includeConfigs" sounds like request option rather than authz enforcement
Can we unify the `dataLength` computation into a single place? It's a little bit scattered at hard to follow. Also with different versions, I like a more explicit pattern using switch: ``` switch (version) { case 0: encodeV1(); break; case 1: encodeV2(); break; default: throw... } ```
Seems we should get rid of this one after we have updated the full DSL to use the new `Record` type.
Semantically really bad to forward `0x3`, but well, it is what it is.
the test passes without the second poll. the first poll finishes the sync ``` INFO Successfully synced group in generation ``` before the second poll is triggered. The second poll notifies the assignor and gets committed offsets which i don't think is necessary in this test
